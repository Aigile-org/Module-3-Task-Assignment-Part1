id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13308032	It will throw Invalid lambda deserialization Exception when writing to elastic search with new format	"My job follows:
{code:java}
// 
create table csv( pageId VARCHAR, eventId VARCHAR, recvTime VARCHAR) with ( 'connector' = 'filesystem',
 'path' = '/Users/ohmeatball/Work/flink-sql-etl/data-generator/src/main/resources/user3.csv',
 'format' = 'csv'
 )
-----------------------------------------
CREATE TABLE es_table (
  aggId varchar ,
  pageId varchar ,
  ts varchar ,
  expoCnt int ,
  clkCnt int
) WITH (
'connector' = 'elasticsearch',
'hosts' = 'http://localhost:9200',
'index' = 'cli_test',
'document-id.key-delimiter' = '$',
'sink.bulk-flush.interval' = '1000',
'format' = 'json'
)
-----------------------------------------
INSERT INTO es_table
SELECT  pageId,eventId,cast(recvTime as varchar) as ts, 1, 1 from csv;
{code}
The full exception follows:
{code:java}
Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.Sink(table=[default_catalog.default_database.es_table], fields=[aggId, pageId, ts, expoCnt, clkCnt]) (1/1) (b51209fac96948c20e85b8df137287d3) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@bb5ab41.org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function. at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:471) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:459) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:393) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:155) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:518) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:720) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.runtime.taskmanager.Task.run(Task.java:545) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]Caused by: java.io.IOException: unexpected exception type at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1682) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1254) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
Caused by: java.lang.reflect.InvocationTargetExceptionCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 moreCaused by: java.lang.IllegalArgumentException: Invalid lambda deserialization at org.apache.flink.streaming.connectors.elasticsearch7.ElasticsearchSink$Builder.$deserializeLambda$(ElasticsearchSink.java:80) ~[flink-sql-connector-elasticsearch7_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1248) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2073) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151] at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2282) ~[?:1.8.0_151] at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2206) ~[?:1.8.0_151] at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2064) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568) ~[?:1.8.0_151]
 at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428) ~[?:1.8.0_151] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:562) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:550) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:511) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT] ... 10 more
{code}
Notice: everything works fine with former connector grammer."	FLINK	Closed	1	1	3568	pull-request-available
13187210	Add skip to next strategy	Add skip to next strategy, that should discard all partial matches that started with the same element as found match.	FLINK	Resolved	3	4	3568	pull-request-available
13391141	Add a global flag for enabling/disabling final checkpoints	We should have a feature toggle for the final checkpoint story.	FLINK	Closed	3	7	3568	pull-request-available
13340904	Avro Confluent Registry SQL format does not support adding nullable columns	"The {{AvroSchemaConverter#convertToSchema}} generates a union with ""null"" for nullable logical types, but it does not set the default value to null. In turn it makes it impossible to generate a backwards compatible schema from a DDL statement.

Example:
1. Create a table: {{CREATE TABLE t (id INT NOT NULL) WITH (/* avro confluent format*/)}}
2. Create a new table over the same topic or alter the old table with {{CREATE TABLE newT(id INT NOT NULL, optionalDescription STRING) WITH (/* avro confluent format */)}}
3. When reading from {{newT}} records inserted into {{t}} it will fail, because the {{optionalDescription}} has no default value."	FLINK	Closed	2	1	3568	pull-request-available
13248587	Include table examples in flink-dist	"We want to treat the table api as first-class API. We already included in the lib directory flink.
We should also include some examples of the table api in the distribution.

Before that we should strip all the dependency and just include the classes from  example module."	FLINK	Closed	2	4	3568	pull-request-available
13550488	SupportsReadingMetadata is not applied when loading a CompiledPlan	"If a few conditions are met, we can not apply ReadingMetadata interface:
# source overwrites:
 {code}
    @Override
    public boolean supportsMetadataProjection() {
        return false;
    }
 {code}
# source does not implement {{SupportsProjectionPushDown}}
# table has metadata columns e.g.
{code}
CREATE TABLE src (
  physical_name STRING,
  physical_sum INT,
  timestamp TIMESTAMP_LTZ(3) NOT NULL METADATA VIRTUAL
)
{code}
# we query the table {{SELECT * FROM src}}

It fails with:
{code}
Caused by: java.lang.IllegalArgumentException: Row arity: 1, but serializer arity: 2
	at org.apache.flink.table.runtime.typeutils.RowDataSerializer.copy(RowDataSerializer.java:124)
{code}

The reason is {{SupportsReadingMetadataSpec}} is created only in the {{PushProjectIntoTableSourceScanRule}}, but the rule is not applied when 1 & 2"	FLINK	Closed	3	1	3568	pull-request-available
13437148	JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate failed on azure	"
{code:java}
2022-03-31T06:11:52.2333685Z Mar 31 06:11:52 [ERROR] Tests run: 5, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 35.288 s <<< FAILURE! - in org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase
2022-03-31T06:11:52.2336004Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate  Time elapsed: 15.008 s  <<< FAILURE!
2022-03-31T06:11:52.2336907Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2337353Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2337876Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2338631Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2339436Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2340599Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInTerminate(JobMasterStopWithSavepointITCase.java:136)
2022-03-31T06:11:52.2342251Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2342896Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2343608Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2344234Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2344873Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2345590Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2346498Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2347221Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2347922Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2348580Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2349222Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2349860Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2350502Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2351172Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2352095Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2352949Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2353643Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2354298Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2354909Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2355535Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2356505Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2357142Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2357771Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2358400Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2359014Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2359614Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2360221Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2371694Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2372907Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2373992Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2375195Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2376592Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2377778Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2379338Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2380786Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2382151Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2383487Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2384979Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2386341Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2387454Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2389081Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2390447Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2391930Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2393389Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2394759Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2395544Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2396673Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2397347Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2397932Z Mar 31 06:11:52 
2022-03-31T06:11:52.2398639Z Mar 31 06:11:52 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend  Time elapsed: 15.004 s  <<< FAILURE!
2022-03-31T06:11:52.2399342Z Mar 31 06:11:52 java.lang.AssertionError
2022-03-31T06:11:52.2399793Z Mar 31 06:11:52 	at org.junit.Assert.fail(Assert.java:87)
2022-03-31T06:11:52.2400311Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:42)
2022-03-31T06:11:52.2400837Z Mar 31 06:11:52 	at org.junit.Assert.assertTrue(Assert.java:53)
2022-03-31T06:11:52.2401633Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsHelper(JobMasterStopWithSavepointITCase.java:159)
2022-03-31T06:11:52.2402751Z Mar 31 06:11:52 	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithRestartsShouldSimplyRestartInSuspend(JobMasterStopWithSavepointITCase.java:130)
2022-03-31T06:11:52.2403623Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-31T06:11:52.2404247Z Mar 31 06:11:52 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-31T06:11:52.2404961Z Mar 31 06:11:52 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-31T06:11:52.2405936Z Mar 31 06:11:52 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-31T06:11:52.2406676Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-03-31T06:11:52.2407520Z Mar 31 06:11:52 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-03-31T06:11:52.2408242Z Mar 31 06:11:52 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-03-31T06:11:52.2409245Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-03-31T06:11:52.2409940Z Mar 31 06:11:52 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-03-31T06:11:52.2410604Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2411358Z Mar 31 06:11:52 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-03-31T06:11:52.2412174Z Mar 31 06:11:52 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-03-31T06:11:52.2412786Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2413640Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-03-31T06:11:52.2414856Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-03-31T06:11:52.2416140Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-03-31T06:11:52.2417502Z Mar 31 06:11:52 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-03-31T06:11:52.2418495Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-03-31T06:11:52.2419110Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-03-31T06:11:52.2419737Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-03-31T06:11:52.2420361Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-03-31T06:11:52.2420986Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-03-31T06:11:52.2421601Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422359Z Mar 31 06:11:52 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
2022-03-31T06:11:52.2422969Z Mar 31 06:11:52 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-03-31T06:11:52.2423569Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-03-31T06:11:52.2424331Z Mar 31 06:11:52 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-03-31T06:11:52.2424922Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-03-31T06:11:52.2425464Z Mar 31 06:11:52 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-03-31T06:11:52.2426334Z Mar 31 06:11:52 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-03-31T06:11:52.2427379Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-03-31T06:11:52.2428432Z Mar 31 06:11:52 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-03-31T06:11:52.2429538Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-03-31T06:11:52.2430713Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-03-31T06:11:52.2431900Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-03-31T06:11:52.2433166Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-03-31T06:11:52.2434372Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-03-31T06:11:52.2435500Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-03-31T06:11:52.2436771Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-03-31T06:11:52.2437877Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-03-31T06:11:52.2439206Z Mar 31 06:11:52 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-03-31T06:11:52.2440452Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-03-31T06:11:52.2441694Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-03-31T06:11:52.2442881Z Mar 31 06:11:52 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-03-31T06:11:52.2443999Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-03-31T06:11:52.2445104Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-03-31T06:11:52.2446367Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-03-31T06:11:52.2447434Z Mar 31 06:11:52 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-03-31T06:11:52.2448170Z Mar 31 06:11:52 
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=5183"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13374114	Unnecessary entries in sql hbase-1.4 connector NOTICE file	"The NOTICE file for flink-sql-connector-hbase-1.4 lists dependencies that it does not bundle:

* commons-configuration:commons-configuration:1.7
* org.apache.hbase:hbase-prefix-tree:1.4.3
* org.apache.hbase:hbase-procedure:1.4.3"	FLINK	Closed	2	7	3568	pull-request-available
13296468	Support creating tables using other tables definition	We should be able to create a Table based on properties of other tables. This includes merging the properties and creating a new Table based on that.	FLINK	Closed	3	7	3568	pull-request-available
13364698	NullPointerException on restore in PojoSerializer	"As originally reported in [thread|http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Schema-Evolution-Cannot-restore-from-savepoint-after-deleting-field-from-POJO-td42162.html], after removing a field from a class restore from savepoint fails with the following exception:

{code}
2021-03-10T20:51:30.406Z INFO  org.apache.flink.runtime.taskmanager.Task:960 … (6/8) (d630d5ff0d7ae4fbc428b151abebab52) switched from RUNNING to FAILED. java.lang.Exception: Exception while creating StreamOperatorStateContext.
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:195)
        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:253)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeState(StreamTask.java:901)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:415)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedCoProcessOperator_c535ac415eeb524d67c88f4a481077d2_(6/8) from any of the 1 provided restore options.
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:307)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:135)
        ... 6 common frames omitted
Caused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when trying to restore heap backend
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)
        at org.apache.flink.runtime.state.memory.MemoryStateBackend.createKeyedStateBackend(MemoryStateBackend.java:347)
        at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:291)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)
        at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)
        ... 8 common frames omitted
Caused by: java.lang.NullPointerException: null
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.<init>(PojoSerializer.java:123)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:186)
        at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.duplicate(PojoSerializer.java:56)
        at org.apache.flink.api.common.typeutils.CompositeSerializer$PrecomputedParameters.precompute(CompositeSerializer.java:228)
        at org.apache.flink.api.common.typeutils.CompositeSerializer.<init>(CompositeSerializer.java:51)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializer.<init>(TtlStateFactory.java:250)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:359)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory$TtlSerializerSnapshot.createOuterSerializerWithNestedSerializers(TtlStateFactory.java:330)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546)
        at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)
        at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:505)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.snapshotsToRestoreSerializers(NestedSerializersSnapshotDelegate.java:225)
        at org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate.getRestoredNestedSerializers(NestedSerializersSnapshotDelegate.java:83)
        at org.apache.flink.api.common.typeutils.CompositeTypeSerializerSnapshot.restoreSerializer(CompositeTypeSerializerSnapshot.java:194)
        at org.apache.flink.runtime.state.StateSerializerProvider.previousSchemaSerializer(StateSerializerProvider.java:189)
        at org.apache.flink.runtime.state.StateSerializerProvider.currentSchemaSerializer(StateSerializerProvider.java:164)
        at org.apache.flink.runtime.state.RegisteredKeyValueStateBackendMetaInfo.getStateSerializer(RegisteredKeyValueStateBackendMetaInfo.java:136)
        at org.apache.flink.runtime.state.heap.StateTable.getStateSerializer(StateTable.java:315)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:54)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.createStateMap(CopyOnWriteStateTable.java:36)
        at org.apache.flink.runtime.state.heap.StateTable.<init>(StateTable.java:98)
        at org.apache.flink.runtime.state.heap.CopyOnWriteStateTable.<init>(CopyOnWriteStateTable.java:49)
        at org.apache.flink.runtime.state.heap.AsyncSnapshotStrategySynchronicityBehavior.newStateTable(AsyncSnapshotStrategySynchronicityBehavior.java:41)
        at org.apache.flink.runtime.state.heap.HeapSnapshotStrategy.newStateTable(HeapSnapshotStrategy.java:243)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.createOrCheckStateForMetaInfo(HeapRestoreOperation.java:185)
        at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:152)
        at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)
        ... 12 common frames omitted
{code}
"	FLINK	Closed	1	1	3568	pull-request-available
13313439	Can not select fields with JdbcCatalog	"A query which selects fields from a table will fail if we set the PostgresCatalog as default.

Steps to reproduce:
 # Create postgres catalog and set it as default
 # Create any table (in any catalog)
 # Query that table with {{SELECT field FROM t}} (Important it must be a field name not '{{*}}'
 #  The query will fail

Stack trace:
{code}
org.apache.flink.table.client.gateway.SqlExecutionException: Invalidate SQL statement.
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:100) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parse(SqlCommandParser.java:91) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.parseCommand(CliClient.java:257) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:211) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:142) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:114) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:201) [flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. null
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
Caused by: java.lang.UnsupportedOperationException
	at org.apache.flink.connector.jdbc.catalog.AbstractJdbcCatalog.getFunction(AbstractJdbcCatalog.java:261) ~[?:?]
	at org.apache.flink.table.catalog.FunctionCatalog.resolvePreciseFunctionReference(FunctionCatalog.java:570) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lambda$resolveAmbiguousFunctionReference$2(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at java.util.Optional.orElseGet(Optional.java:267) ~[?:1.8.0_252]
	at org.apache.flink.table.catalog.FunctionCatalog.resolveAmbiguousFunctionReference(FunctionCatalog.java:617) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.catalog.FunctionCatalog.lookupFunction(FunctionCatalog.java:370) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.catalog.FunctionCatalogOperatorTable.lookupOperatorOverloads(FunctionCatalogOperatorTable.java:99) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.util.ChainedSqlOperatorTable.lookupOperatorOverloads(ChainedSqlOperatorTable.java:73) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.makeNullaryCall(SqlValidatorImpl.java:1754) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5987) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6154) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl$SelectExpander.visit(SqlValidatorImpl.java:6140) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectExpr(SqlValidatorImpl.java:5574) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.expandSelectItem(SqlValidatorImpl.java:452) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelectList(SqlValidatorImpl.java:4255) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3523) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78) ~[flink-table-blink_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.lambda$parse$0(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:255) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.gateway.local.LocalExecutor$1.parse(LocalExecutor.java:430) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	at org.apache.flink.table.client.cli.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:98) ~[flink-sql-client_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
	... 6 more
{code}

The problem is that Calcite will try to check first if there is a built-in function with that name that allows calls without parenthesis. Therefore it will query the {{FunctionCatalog}} for that function. The logic in {{org.apache.flink.table.catalog.FunctionCatalog#lookupFunction}} is such that it will call {{JdbcCatalog#getFunction}} in the end, which in case of {{AbstractJdbcCatalog}} throws {{UnsupportedOperationException}}.
"	FLINK	Closed	1	1	3568	pull-request-available
13285020	Introduce a Java Expression DSL	Introduce the basic expressions. The new Java expression DSL should be feature equivalent to string based expression parser. It does not support calls with new inference stack yet.	FLINK	Closed	3	7	3568	pull-request-available
13317135	Docker e2e tests are failing on CI	"{code}
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=host&nocache=1&rm=1&session=z0y5c0io3wt7m3uqfb7zo7uds&shmsize=0&t=test_docker_embedded_job&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied
{code}
Will have to wait for [~rmetzger] to get back."	FLINK	Closed	3	4	3568	pull-request-available
13240421	Convert CatalogView to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	"Similar to [FLINK-12257] we should convert Flink's views to Calcite's views.

The tricky part is that we have to pass around the SqlParser somehow."	FLINK	Closed	3	7	3568	pull-request-available
13341804	Building flink-dist docker image does not work without python2	"The script {{common_docker.sh}} in function {{start_file_server}} tests existence of {{python3}}, but executes command using {{python}}:

{code}
    command -v python3 >/dev/null 2>&1
    if [[ $? -eq 0 ]]; then
      python ${TEST_INFRA_DIR}/python3_fileserver.py &
      return
    fi
{code}

The script {{python3_fileserver.py}} uses python2 {{SocketServer}} which does not exist in python3. It should use {{socketserver}}."	FLINK	Closed	2	1	3568	pull-request-available
13236795	Introduce Table API Planner interface	The planner interface is the bridge between base API and different planner modules. 	FLINK	Closed	3	4	3568	pull-request-available
13363745	DegreesWithExceptionITCase crash	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14422&view=logs&j=ce8f3cc3-c1ea-5281-f5eb-df9ebd24947f&t=f266c805-9429-58ed-2f9e-482e7b82f58b

"	FLINK	Closed	1	7	3568	pull-request-available, test-stability
13141178	Scala shell broken for Flip6	"I am trying to run the simple code below after building everything from Flink's github master branch for various reasons. I get an exception below and I wonder what runs on port 9065? and How to fix this exception?

I followed the instructions from the Flink master branch so I did the following.
{code:java}
git clone https://github.com/apache/flink.git 
cd flink mvn clean package -DskipTests 
cd build-target
 ./bin/start-scala-shell.sh local{code}
{{And Here is the code I ran}}
{code:java}
val dataStream = senv.fromElements(1, 2, 3, 4)
dataStream.countWindowAll(2).sum(0).print()
senv.execute(""My streaming program""){code}
{{And I finally get this exception}}
{code:java}
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph. at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$18(RestClusterClient.java:306) at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870) at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852) at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$222(RestClient.java:196) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:603) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:563) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:268) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:284) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745) Caused by: java.util.concurrent.CompletionException: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) ... 16 more Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:9065 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224) at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:281){code}
 "	FLINK	Resolved	1	1	3568	pull-request-available
13583654	JSON_QUERY should return a well formatted nested objects/arrays for ARRAY<STRING>	"{code}
SELECT JSON_QUERY('{""items"": [{""itemId"":1234, ""count"":10}]}', '$.items' RETURNING ARRAY<STRING>)
{code}

returns

{code}
['{itemId=1234, count=10}']
{code}

but it should:

{code}
['{""itemId"":1234, ""count"":10}']
{code}

We should call jsonize for Collection types here: https://github.com/apache/flink/blob/f6f88135b3a5fa5616fe905346e5ab6dce084555/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/functions/SqlJsonUtils.java#L268"	FLINK	Closed	3	1	3568	pull-request-available
13324973	Remove deprecated methods in ExecutionConfig	"We can remove:

- ExecutionConfig#isLatencyTrackingEnabled (deprecated in 1.7) 

Additionally, we should remove no-ops methods in ExecutionConfig.

    - ExecutionConfig#disable/enableSysoutLogging (deprecated in 1.10)
    - ExecutionConfig#set/isFailTaskOnCheckpointError (deprecated in 1.9) 

They are {{Public}}, however they became no-op operations, which can be argued already broke the stability guarantees."	FLINK	Closed	3	7	3568	pull-request-available
13556216	Implement type inference for reinterpret_cast function	https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/Reinterpret.scala	FLINK	Closed	3	7	3568	pull-request-available
13231976	Port utility methods for extracting fields information from TypeInformation	We need those methods in the api-module in order to create {{Table}} out of {{DataSet/Stream}}.	FLINK	Closed	3	4	3568	pull-request-available
13301012	flink legacy planner should not use CollectionEnvironment any more	"As discussed in https://github.com/apache/flink/pull/11794，{{CollectionEnvironment}} is not a good practice, as it is not going through all the steps that a regular user program would go. We should change the tests to use {{LocalEnvironment}}. 

commit ""Introduce CollectionPipelineExecutor for CollectionEnvironment ([c983ac9|https://github.com/apache/flink/commit/c983ac9c49b7b58394574efdde4f39e8d33a8582])""  should also be reverted at that moment."	FLINK	Closed	3	4	3568	pull-request-available
13380486	Performance regression on 25.05	"Tests such as:
* multiInputMapSink
* multiInputOneIdleMapSink
* readFileSplit

show regressions.

Regression in run for range: 80ad5b3b51-bb597ea-1621977169

It is most probably caused by: https://github.com/apache/flink/commit/ee9f9b227a7703c2688924070c4746a70bff3fd8"	FLINK	Closed	3	4	3568	pull-request-available
13572433	Parsing temporal table join throws cryptic exceptions	"1. Wrong expression type in {{AS OF}}:
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF 'o.rowtime' AS r "" +
      ""ON o.currency = r.currency
{code}

throws: 

{code}
java.lang.AssertionError: cannot convert CHAR literal to class org.apache.calcite.util.TimestampString
{code}

2. Not a simple table reference in {{AS OF}}
{code}
SELECT * "" +
      ""FROM Orders AS o JOIN "" +
      ""RatesHistoryWithPK FOR SYSTEM_TIME AS OF o.rowtime + INTERVAL '1' SECOND AS r "" +
      ""ON o.currency = r.currency
{code}

throws:
{code}
java.lang.AssertionError: no unique expression found for {id: o.rowtime, prefix: 1}; count is 0
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13434368	TimestampsAndWatermarksOperator should not propagate WatermarkStatus	The lifecycle/scope of WatermarkStatus is tightly coupled with watermarks. Upstream watermarks are cut off in the TimestampsAndWatermarksOperator and therefore watermark statuses should be cut off as well.	FLINK	Closed	3	1	3568	pull-request-available
13269264	Use higher granularity units in generated docs for Duration & MemorySize if possible	"It was mentioned on two occasions (https://github.com/apache/flink/pull/10216#discussion_r346866339, https://github.com/apache/flink/pull/10217/files#r347491314) that it would be better to use a higher granularity units if it is possible.

Right now for a default value of {{Duration.ofMinutes(1)}} the generated documentation will be printed as:

{{60000ms}}

but it would be better readable to print it as:

{{1min}}"	FLINK	Closed	3	4	3568	pull-request-available
13391350	InputStatus should not contain END_OF_RECOVERY	"We added the END_OF_RECOVERY enum value in order to support recovery of unaligned checkpoints with rescaling.

However the InputStatus is expose in a public interface via {{SourceReader}}. At the same time it is not a valid value which the {{SourceReader}} can return.

We should internally replace the InputStatus with an internal equivalent."	FLINK	Closed	3	1	3568	pull-request-available
13300004	Add open method to DeserializationSchema	"Additionally add support for it in connectors:
* Kafka
* PubSub
* RabbitMQ
* Kinesis"	FLINK	Closed	3	7	3568	pull-request-available
13263712	Update documentation regarding Temporary Objects	"* update references to deprecated methods
* describe the concept of temporary tables"	FLINK	Closed	3	7	3568	pull-request-available
13596954	Support LEAD/LAG functions in Table API	We should natively support LAG/LEAD functions in Table API.	FLINK	Open	3	4	3568	pull-request-available
13304842	TableEnvironment fromValues not work with map type and SQL	"{code:java}
Map<Integer, Integer> mapData = new HashMap<>();
      mapData.put(1, 1);
      mapData.put(2, 2);
      Row row = Row.of(mapData);
      tEnv().createTemporaryView(""values_t"", tEnv().fromValues(Collections.singletonList(row)));
      Iterator<Row> iter = tEnv().executeSql(""select * from values_t"").collect();

      List<Row> results = new ArrayList<>();
      while (iter.hasNext()) {
         results.add(iter.next());
      }
      System.out.println(results);
{code}
Not work, will occur exception:
{code:java}
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP f0) NOT NULL
converted type:
RecordType((INTEGER NOT NULL, INTEGER NOT NULL) MAP NOT NULL f0) NOT NULL
{code}
If change to {{Iterator<Row> iter = tEnv().from(""values_t"").execute().collect();}} will work."	FLINK	Closed	3	1	3568	pull-request-available
13201055	Time interval for window aggregations in SQL is wrongly translated if specified with YEAR_MONTH resolution	"If a time interval was specified with {{YEAR TO MONTH}} resolution like e.g.:
{code}
SELECT * 
FROM Mytable
GROUP BY 
    TUMBLE(rowtime, INTERVAL '1-2' YEAR TO MONTH)
{code}
it will be wrongly translated to 14 milliseconds window. We should allow for only DAY TO SECOND resolution."	FLINK	Closed	3	1	3568	pull-request-available
13320624	Create an uber jar when packaging flink-avro for sql client	"Currently users have to provide dependencies such as avro, jackson-core-asl, jackson-mapper-asl and joda-time in the job jar for DataStream jobs, or manually copy them into flink/lib in SQL jobs when using avro formatting.

It's better to generate an uber jar including these dependencies when packaging flink-avro module. "	FLINK	Closed	3	4	3568	pull-request-available
13238695	Port TableEnvironment to flink-api modules	"{{TableEnvironments}} should be a purely API class(es). Current implementation should be split into {{CatalogManager}} and {{Planner}}. The {{Planner}} should be discovered based on configuration. This will allow using either the legacy or the Blink planner.

This applies to the {{StreamTableEnvironment}}. The {{BatchTableEnvironment}} will be left as is. One will be able to use the new {{StreamTableEnvironment}} for stream processing with the legacy planner or stream and batch for the Blink {{Planner}}."	FLINK	Closed	3	4	3568	pull-request-available
13565276	Set ALWAYS ChainingStrategy in TemporalSort	Similarly to FLINK-27992 we should ALWAYS chaining strategy in TemporalSort operator	FLINK	Closed	3	1	3568	pull-request-available
13303573	BatchTableEnvironment#fromValues(Object... values) throws StackOverflowError 	"The Error can be reproduced by following code:
{code:java}
ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);
tEnv.fromValues(1L, 2L, 3L);
{code}
The Error is as following:
{code:java}
Exception in thread ""main"" java.lang.StackOverflowErrorException in thread ""main"" java.lang.StackOverflowError at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) at java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:110) at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.expressions.ApiExpressionUtils.convertArray(ApiExpressionUtils.java:142) at org.apache.flink.table.expressions.ApiExpressionUtils.objectToExpression(ApiExpressionUtils.java:100) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at org.apache.flink.table.api.internal.TableEnvImpl$$anonfun$2.apply(TableEnvImpl.scala:1030) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1030) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163) at org.apache.flink.table.api.internal.TableEnvImpl.fromValues(TableEnvImpl.scala:1032) at org.apache.flink.table.api.TableEnvironment.fromValues(TableEnvironment.java:163)
...{code}"	FLINK	Closed	3	1	3568	pull-request-available
13313068	Can not create a catalog from user jar	"The {{CREATE CATALOG}} statement does not work if the catalog implementation comes from the user classloader. The problem is that {{org.apache.flink.table.planner.operations.SqlToOperationConverter#convertCreateCatalog}} uses the {{SqlToOperationConverter}} classloader.

We should use {{Thread.currentThread().getContextClassloader()}} for now.

One of the ways to reproduce it is try to create e.g. a postgres catalog with the {{flink-connector-jdbc}} passed as an additional jar to {{sql--client}}"	FLINK	Closed	2	1	3568	pull-request-available
13383342	Deprecate/Remove StreamOperator#dispose method	"As discussed in the ML thread (e.g. https://lists.apache.org/thread.html/r34a05c77bddb2a7cb550c0b820d2a4aa8e1be882fc81bee501fb74e8%40%3Cdev.flink.apache.org%3E) we want to clean up the contract of {{close}} and {{dispose}} methods. 

We suggest introducing a new method finish(), deprecate or remove the dispose method and extract finish() part out of the close()."	FLINK	Closed	3	7	3568	pull-request-available
13297293	Improve literals conversion in ExpressionConverter	"There are couple of issues with the {{ExpressionResolver}} and literals conversion:
1. There is a lot of code duplication
2. Precision of certain types might get lost e.g. BINARY, CHAR"	FLINK	Closed	3	1	3568	pull-request-available
13370144	Test display last n exceptions/causes for job restarts in Web UI	This is the testing task for FLINK-6042. We should test whether the root causes for multiple restarts are properly displayed in the web UI.	FLINK	Closed	1	4	3568	pull-request-available, release-testing
13378788	Incompatible subtask mappings while resuming from unaligned checkpoints	"A user [reported|https://lists.apache.org/x/list.html?user@flink.apache.org:lte=1M:Flink%201.13.0%20reactive%20mode:%20Job%20stop%20and%20cannot%20restore%20from%20checkpoint] that he encountered an internal error while resuming during reactive mode. There isn't an immediate connection to reactive mode, so it's safe to assume that one rescaling case was not covered.

{noformat}
Caused by: java.lang.IllegalStateException: Incompatible subtask mappings: are multiple operators ingesting/producing intermediate results with varying degrees of parallelism?Found RescaleMappings{mappings=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119], [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149], [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179], [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209]]} and RescaleMappings{mappings=[[0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 91, 98, 105, 112, 119, 126, 133, 140, 147, 154, 161, 168, 175, 182, 189, 196, 203], [1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99, 106, 113, 120, 127, 134, 141, 148, 155, 162, 169, 176, 183, 190, 197, 204], [2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86, 93, 100, 107, 114, 121, 128, 135, 142, 149, 156, 163, 170, 177, 184, 191, 198, 205], [3, 10, 17, 24, 31, 38, 45, 52, 59, 66, 73, 80, 87, 94, 101, 108, 115, 122, 129, 136, 143, 150, 157, 164, 171, 178, 185, 192, 199, 206], [4, 11, 18, 25, 32, 39, 46, 53, 60, 67, 74, 81, 88, 95, 102, 109, 116, 123, 130, 137, 144, 151, 158, 165, 172, 179, 186, 193, 200, 207], [5, 12, 19, 26, 33, 40, 47, 54, 61, 68, 75, 82, 89, 96, 103, 110, 117, 124, 131, 138, 145, 152, 159, 166, 173, 180, 187, 194, 201, 208], [6, 13, 20, 27, 34, 41, 48, 55, 62, 69, 76, 83, 90, 97, 104, 111, 118, 125, 132, 139, 146, 153, 160, 167, 174, 181, 188, 195, 202, 209]]}.
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.checkSubtaskMapping(TaskStateAssignment.java:322) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.TaskStateAssignment.getInputMapping(TaskStateAssignment.java:306) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.reDistributeInputChannelStates(StateAssignmentOperation.java:409) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignAttemptState(StateAssignmentOperation.java:193) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.StateAssignmentOperation.assignStates(StateAssignmentOperation.java:139) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreLatestCheckpointedStateInternal(CheckpointCoordinator.java:1566) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1646) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:163) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:138) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphAndRestoreState(AdaptiveScheduler.java:986) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$createExecutionGraphAndRestoreStateAsync$25(AdaptiveScheduler.java:976) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at org.apache.flink.runtime.scheduler.adaptive.BackgroundTask.lambda$new$0(BackgroundTask.java:57) ~[flink-dist_2.12-1.13.0.jar:1.13.0]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642) ~[?:?]
        at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478) ~[?:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
        at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.lang.Thread.run(Thread.java:834) ~[?:?]
{noformat}

Here it seems that the same gate gets input from a range-partitioned and a round-robin partitioned channel at the same time. During the implementation of FLINK-19801, we couldn't find such a case and optimized the implementation accordingly.

We have asked the user to provide his topology."	FLINK	Closed	1	1	3568	pull-request-available
13556214	Implement type inference for Over function	"https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/overOffsets.scala

Functions:
* OVER
* CURRENT_RANGE
* CURRENT_ROW
* UNBOUNDED_ROW
* UNBOUNDED_RANGE"	FLINK	Closed	3	7	3568	pull-request-available
13580266	BlockStatementGrouper uses lots of memory	"For deeply nested {{if else}} statements {{BlockStatementGrouper}} uses loads of memory and fails with OOM quickly.

When running JMs with around 400mb a query like:
{code}
select case when orderid = 0 then 1 when orderid = 1 then 2 when orderid
    = 2 then 3 when orderid = 3 then 4 when orderid = 4 then 5 when orderid = 5 then
    6 when orderid = 6 then 7 when orderid = 7 then 8 when orderid = 8 then 9 when
    orderid = 9 then 10 when orderid = 10 then 11 when orderid = 11 then 12 when orderid
    = 12 then 13 when orderid = 13 then 14 when orderid = 14 then 15 when orderid
    = 15 then 16 when orderid = 16 then 17 when orderid = 17 then 18 when orderid
    = 18 then 19 when orderid = 19 then 20 when orderid = 20 then 21 when orderid
    = 21 then 22 when orderid = 22 then 23 when orderid = 23 then 24 when orderid
    = 24 then 25 when orderid = 25 then 26 when orderid = 26 then 27 when orderid
    = 27 then 28 when orderid = 28 then 29 when orderid = 29 then 30 when orderid
    = 30 then 31 when orderid = 31 then 32 when orderid = 32 then 33 when orderid
    = 33 then 34 when orderid = 34 then 35 when orderid = 35 then 36 when orderid
    = 36 then 37 when orderid = 37 then 38 when orderid = 38 then 39 when orderid
    = 39 then 40 when orderid = 40 then 41 when orderid = 41 then 42 when orderid
    = 42 then 43 when orderid = 43 then 44 when orderid = 44 then 45 when orderid
    = 45 then 46 when orderid = 46 then 47 when orderid = 47 then 48 when orderid
    = 48 then 49 when orderid = 49 then 50 when orderid = 50 then 51 when orderid
    = 51 then 52 when orderid = 52 then 53 when orderid = 53 then 54 when orderid
    = 54 then 55 when orderid = 55 then 56 when orderid = 56 then 57 when orderid
    = 57 then 58 when orderid = 58 then 59 when orderid = 59 then 60 when orderid
    = 60 then 61 when orderid = 61 then 62 when orderid = 62 then 63 when orderid
    = 63 then 64 when orderid = 64 then 65 when orderid = 65 then 66 when orderid
    = 66 then 67 when orderid = 67 then 68 when orderid = 68 then 69 when orderid
    = 69 then 70 when orderid = 70 then 71 when orderid = 71 then 72 when orderid
    = 72 then 73 when orderid = 73 then 74 when orderid = 74 then 75 when orderid
    = 75 then 76 when orderid = 76 then 77 when orderid = 77 then 78 when orderid
    = 78 then 79 when orderid = 79 then 80 when orderid = 80 then 81 when orderid
    = 81 then 82 when orderid = 82 then 83 when orderid = 83 then 84 when orderid
    = 84 then 85 when orderid = 85 then 86 when orderid = 86 then 87 when orderid
    = 87 then 88 when orderid = 88 then 89 when orderid = 89 then 90 when orderid
    = 90 then 91 when orderid = 91 then 92 when orderid = 92 then 93 when orderid
    = 93 then 94 when orderid = 94 then 95 when orderid = 95 then 96 when orderid
    = 96 then 97 when orderid = 97 then 98 when orderid = 98 then 99 when orderid
    = 99 then 100 when orderid = 100 then 101 when orderid = 101 then 102 when orderid
    = 102 then 103 when orderid = 103 then 104 when orderid = 104 then 105 when orderid
    = 105 then 106 when orderid = 106 then 107 when orderid = 107 then 108 when orderid
    = 108 then 109 when orderid = 109 then 110 when orderid = 110 then 111 when orderid
    = 111 then 112 when orderid = 112 then 113 when orderid = 113 then 114 when orderid
    = 114 then 115 when orderid = 115 then 116 when orderid = 116 then 117 when orderid
    = 117 then 118 when orderid = 118 then 119 when orderid = 119 then 120 when orderid
    = 120 then 121 when orderid = 121 then 122 when orderid = 122 then 123 when orderid
    = 123 then 124 when orderid = 124 then 125 when orderid = 125 then 126 when orderid
    = 126 then 127 when orderid = 127 then 128 when orderid = 128 then 129 when orderid
    = 129 then 130 when orderid = 130 then 131 when orderid = 131 then 132 when orderid
    = 132 then 133 when orderid = 133 then 134 when orderid = 134 then 135 when orderid
    = 135 then 136 when orderid = 136 then 137 when orderid = 137 then 138 when orderid
    = 138 then 139 when orderid = 139 then 140 when orderid = 140 then 141 when orderid
    = 141 then 142 when orderid = 142 then 143 when orderid = 143 then 144 when orderid
    = 144 then 145 when orderid = 145 then 146 when orderid = 146 then 147 when orderid
    = 147 then 148 when orderid = 148 then 149 when orderid = 149 then 150 when orderid
    = 150 then 151 when orderid = 151 then 152 when orderid = 152 then 153 when orderid
    = 153 then 154 when orderid = 154 then 155 when orderid = 155 then 156 when orderid
    = 156 then 157 when orderid = 157 then 158 when orderid = 158 then 159 when orderid
    = 159 then 160 when orderid = 160 then 161 when orderid = 161 then 162 when orderid
    = 162 then 163 when orderid = 163 then 164 when orderid = 164 then 165 when orderid
    = 165 then 166 when orderid = 166 then 167 when orderid = 167 then 168 when orderid
    = 168 then 169 when orderid = 169 then 170 when orderid = 170 then 171 when orderid
    = 171 then 172 when orderid = 172 then 173 when orderid = 173 then 174 when orderid
    = 174 then 175 when orderid = 175 then 176 when orderid = 176 then 177 when orderid
    = 177 then 178 when orderid = 178 then 179 when orderid = 179 then 180 when orderid
    = 180 then 181 when orderid = 181 then 182 when orderid = 182 then 183 when orderid
    = 183 then 184 when orderid = 184 then 185 when orderid = 185 then 186 when orderid
    = 186 then 187 when orderid = 187 then 188 when orderid = 188 then 189 when orderid
    = 189 then 190 when orderid = 190 then 191 when orderid = 191 then 192 when orderid
    = 192 then 193 when orderid = 193 then 194 when orderid = 194 then 195 when orderid
    = 195 then 196 when orderid = 196 then 197 when orderid = 197 then 198 when orderid
    = 198 then 199 when orderid = 199 then 200 when orderid = 200 then 201 when orderid
    = 201 then 202 when orderid = 202 then 203 when orderid = 203 then 204 when orderid
    = 204 then 205 when orderid = 205 then 206 when orderid = 206 then 207 when orderid
    = 207 then 208 when orderid = 208 then 209 when orderid = 209 then 210 when orderid
    = 210 then 211 when orderid = 211 then 212 when orderid = 212 then 213 when orderid
    = 213 then 214 when orderid = 214 then 215 when orderid = 215 then 216 when orderid
    = 216 then 217 when orderid = 217 then 218 when orderid = 218 then 219 when orderid
    = 219 then 220 when orderid = 220 then 221 when orderid = 221 then 222 when orderid
    = 222 then 223 when orderid = 223 then 224 when orderid = 224 then 225 when orderid
    = 225 then 226 when orderid = 226 then 227 when orderid = 227 then 228 when orderid
    = 228 then 229 when orderid = 229 then 230 when orderid = 230 then 231 when orderid
    = 231 then 232 when orderid = 232 then 233 when orderid = 233 then 234 when orderid
    = 234 then 235 when orderid = 235 then 236 when orderid = 236 then 237 when orderid
    = 237 then 238 when orderid = 238 then 239 when orderid = 239 then 240 when orderid
    = 240 then 241 when orderid = 241 then 242 when orderid = 242 then 243 when orderid
    = 243 then 244 when orderid = 244 then 245 when orderid = 245 then 246 when orderid
    = 246 then 247 when orderid = 247 then 248 when orderid = 248 then 249 when orderid
    = 249 then 250 else 9999 end case_when_col from sample_data_1;
{code}

fails with an OOM. (Yes, I know the query can be simplified, but it shows the case)."	FLINK	Resolved	3	4	3568	pull-request-available
13387961	All records are processed in the close stage in ContinuousFileReaderOperatorBenchmark	The {{TARGET_COUNT_REACHED_LATCH}} is not correctly reset after the warmup iterations and thus subsequent runs process all records in the {{CLOSE}} stage of the {{ContinuousFileReaderOperator}} testing something different than anticipated.	FLINK	Closed	3	1	3568	pull-request-available
13296470	Support parsing LIKE clause in CREATE TABLE statement	We should support the CREATE TABLE ... LIKE syntax in SqlParser	FLINK	Closed	3	7	3568	pull-request-available
13553409	MATCH_RECOGNIZE AFTER MATCH clause can not be deserialised from a compiled plan	"{code}
        String sql =
                ""insert into MySink""
                        + "" SELECT * FROM\n""
                        + "" MyTable\n""
                        + ""   MATCH_RECOGNIZE(\n""
                        + ""   PARTITION BY vehicle_id\n""
                        + ""   ORDER BY `rowtime`\n""
                        + ""   MEASURES \n""
                        + ""       FIRST(A.`rowtime`) as startTime,\n""
                        + ""       LAST(A.`rowtime`) as endTime,\n""
                        + ""       FIRST(A.engine_temperature) as Initial_Temp,\n""
                        + ""       LAST(A.engine_temperature) as Final_Temp\n""
                        + ""   ONE ROW PER MATCH\n""
                        + ""   AFTER MATCH SKIP TO FIRST B\n""
                        + ""   PATTERN (A+ B)\n""
                        + ""   DEFINE\n""
                        + ""       A as LAST(A.engine_temperature,1) is NULL OR A.engine_temperature > LAST(A.engine_temperature,1),\n""
                        + ""       B as B.engine_temperature < LAST(A.engine_temperature)\n""
                        + ""   )MR;"";
        util.verifyJsonPlan(String.format(sql, afterClause));
{code}

fails with:

{code}
Could not resolve internal system function '$SKIP TO LAST$1'. This is a bug, please file an issue. (through reference chain: org.apache.flink.table.planner.plan.nodes.exec.serde.JsonPlanGraph[""nodes""]->java.util.ArrayList[3]->org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecMatch[""matchSpec""]->org.apache.flink.table.planner.plan.nodes.exec.spec.MatchSpec[""after""])
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13324219	Remove deprecated RuntimeContext#getAllAccumulators	"We could  remove the deprecated:
{code}
RuntimeContext#getAllAcumullators
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13556206	Remove old expression stack leftovers for time functions	"Remove leftovers from https://issues.apache.org/jira/browse/FLINK-13785

There are some parts of the time functions that have not been removed e.g. https://github.com/apache/flink/blob/91d81c427aa6312841ca868d54e8ce6ea721cd60/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/time.scala and some code in https://github.com/apache/flink/blob/b6000f6e589128ae1fd1e0e7d063a1b6ff1fcc20/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/expressions/PlannerExpressionConverter.scala"	FLINK	Closed	3	7	3568	pull-request-available
13360332	Document possible recommended usage of Bounded{One/Multi}Input.endInput and emphasize that they could be called multiple times	"It is too tempting to use these api, especially {{BoundedOneInput.endInput}}, to commit final result before FLIP-147 delivered. And this will cause re-commit after failover as [~gaoyunhaii] has pointed out in FLINK-21132.

I have [pointed|https://github.com/apache/iceberg/issues/2033#issuecomment-784153620] this out in [apache/iceberg#2033|https://github.com/apache/iceberg/issues/2033], please correct me if I was wrong.

cc [~aljoscha] [~pnowojski] [~roman_khachatryan]"	FLINK	Closed	10200	4	3568	auto-deprioritized-major, pull-request-available, stale-minor
13212945	Create CatalogManager to manage multiple catalogs and encapsulate Calcite schema	"Flink allows for more than one registered catalogs. {{CatalogManager}} class is the holding class to manage and encapsulate the catalogs and their interrelations with Calcite.

Following section describes how table resolution should work:

h4. PATH resolution:

First look into DEFAULT PATH: cat or cat.db. Then in the root. This is also the behavior of Calcite. We should mimic this behavior in Flink.
Example:

{noformat}
root:
  |- builtin
      |- default
          |- tab1
          |- tab2
      |- db1
          |- tab1
      |- clashing
          |- tab1
  |- cat1
      |- db1
          |- tab1
          |- tab2
  |- extCat1
      |- tab1
      |- clashing
          |- tab1
      |- extCat2
          |- tab1
          |- tab2
  |- clashing (ExternalCatalog)
      |- tab1
{noformat}
      
There is always a default catalog, initially set to builtin and default database initially set to default.

{noformat}
Assume 
default path = builtin then
  default.tab1 = builtin.default.tab1
  tab1 = error
  cat1.db1.tab1 = cat1.db1.tab1
  clashing.tab1 = builtin.clashing.tab1
default path = extCat1 then
  tab1 = extCat1.tab1
  clashing.tab1 = extCat1.clashing.tab1
default path = extCat1.extCat2 (do not support further nesting) then
  tab1 = extCat1.extCat2.tab1
  clashing.tab1 = clashing.tab1
{noformat}
  

h4. Structure in the Planner(Calcite-specific)

{noformat}
root: CatalogManagerSchema(CatalogManager)
     |- CatalogCalciteSchema(ReadableCatalog)
         |- DatabaseCalciteSchema (ReadableCatalog scoped to DB)
             |- Table
     |- ExternalCatalogSchema
         |- Table
         |- ExternalCatalogSchema
             |- Table
{noformat}

h4. Structure in the API
{noformat}    
CatalogManager:
    |- ReadableCatalog
      |- CatalogTable
    |- ExternalCatalog
      |- ExternalCatalog
      |- ExternalCatalogTable
{noformat}"	FLINK	Resolved	3	7	3568	pull-request-available
13194127	Add a switch to run_test to configure if logs should be checked for errors/excepions	After adding the switch, we should disable log checking for nightly-tests that currently fail (or fix the test).	FLINK	Closed	1	4	3568	pull-request-available
13574746	AggregateQueryOperations produces wrong asSerializableString representation	"A table API query:
{code}
        env.fromValues(1, 2, 3)
                .as(""number"")
                .select(col(""number"").count())
                .insertInto(TEST_TABLE_API)
{code}

produces

{code}
INSERT INTO `default`.`timo_eu_west_1`.`table_api_basic_api` SELECT `EXPR$0` FROM (
    SELECT (COUNT(`number`)) AS `EXPR$0` FROM (
        SELECT `f0` AS `number` FROM (
            SELECT `f0` FROM (VALUES 
                (1),
                (2),
                (3)
            ) VAL$0(`f0`)
        )
    )
    GROUP BY 
)
{code}

which is missing a grouping expression"	FLINK	Closed	3	1	3568	pull-request-available
13382184	Tasks are blocked while broadcasting stream status	"On a cluster I observed symptoms of tasks being blocked for long time, causing long delays with unaligned checkpointing. 99% of those cases were caused by `broadcastEmit` of the stream status

{noformat}
2021-06-04 14:41:44,049 ERROR org.apache.flink.runtime.io.network.buffer.LocalBufferPool   [] - Blocking wait [11059 ms] for an available buffer.
java.lang.Exception: Stracktracegenerator
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:323) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:290) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:338) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:314) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForNewRecord(BufferWritingResultPartition.java:246) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:142) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:104) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.broadcastEmit(ChannelSelectorRecordWriter.java:67) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.writeStreamStatus(RecordWriterOutput.java:136) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.streamstatus.AnnouncedStatus.ensureActive(AnnouncedStatus.java:65) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:103) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:90) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:44) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:38) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:82) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask$AsyncDataOutputToOutput.emitRecord(SourceOperatorStreamTask.java:182) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:110) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.source.SourceOutputWithWatermarks.collect(SourceOutputWithWatermarks.java:101) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.api.connector.source.lib.util.IteratorSourceReader.pollNext(IteratorSourceReader.java:98) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:294) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:69) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:422) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:680) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:635) ~[flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:646) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:619) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566) [flink-dist_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{noformat}

*I have seen this happening both in source and network tasks.* ~80% cases were in the source tasks

{{broadcastEmit}} can easily bypass our non blocking checks. There are two questions:
# why is the stream idling so much? It’s like almost every millisecond it’s broadcasting status
# should we optimise this? Broadcasting CBs and other events is not an issue, as those are events that do not request/require buffers"	FLINK	Closed	2	1	3568	pull-request-available
13316803	Kerberized YARN per-job on Docker test failed to download JDK 8u251	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4514&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529
{code}
+ mkdir -p /usr/java/default
+ curl -Ls https://download.oracle.com/otn-pub/java/jdk/8u251-b08/3d5a2bb8f8d4428bbe94aed7ec7ae784/jdk-8u251-linux-x64.tar.gz -H Cookie: oraclelicense=accept-securebackup-cookie
+ tar --strip-components=1 -xz -C /usr/java/default/

gzip: stdin: not in gzip format
tar: Child returned status 1
{code}"	FLINK	Closed	1	1	3568	pull-request-available
13192203	YarnConfigurationITCase.testFlinkContainerMemory test instability	"Test appeared to fail (by a narrow margin) without a reason randomly:

{noformat}
Failed tests: 
  YarnConfigurationITCase.testFlinkContainerMemory:182 
Expected: is a numeric value within <0.1> of <1.0>
     but: <0.8913043478260869> differed by <0.008695652173913077>
{noformat}

https://api.travis-ci.org/v3/job/442246057/log.txt"	FLINK	Closed	2	1	3568	pull-request-available, test-stability
13398595	Document FLIP-147 capabiliites and limitations	We should document how to enable the checkpointing after tasks finish as well as limitations we are aware of.	FLINK	Closed	2	7	3568	pull-request-available
13362233	 CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14079&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=c2734c79-73b6-521c-e85a-67c7ecae9107

{code:java}
[ERROR] testAsyncCheckpointFailureTriggerJobFailed(org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase)  Time elapsed: 38.623 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 10000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:62)
	at org.apache.flink.test.checkpointing.CheckpointFailureManagerITCase.testAsyncCheckpointFailureTriggerJobFailed(CheckpointFailureManagerITCase.java:103)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
"	FLINK	Closed	3	1	3568	pull-request-available, test-stability
13228910	Convert CatalogTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs	In FLINK-11476, we created CatalogManager to hook up planner with unified catalog APIs. What's missing there is, at the very last step, convert CatalogBaseTable to org.apache.calcite.schema.Table so that planner can use unified catalog APIs, like how {{ExternalTableUtil.fromExternalCatalogTable()}} works to convert the old {{ExternalCatalogTable}} to a Calcite table	FLINK	Closed	3	7	3568	pull-request-available
13409619	Remove scala suffix from respective benchmarks dependencies	With FLINK-24018 few dependencies lost its scala suffix. We should remove it in benchmark dependencies to test against newest artifacts.	FLINK	Closed	3	1	3568	pull-request-available
13404250	Add Flink 1.14 MigrationVersion	"Currently the largest MigrationVersion is 1.13. We need newer versions to add more serializer compatibility tests.
"	FLINK	Closed	3	4	3568	pull-request-available
13231708	Fix failling e2e test test_streaming_sql.sh	"https://travis-ci.org/apache/flink/jobs/535231108

{code}
==============================================================================
Running 'Streaming SQL end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-37856266807
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.9-SNAPSHOT-bin/flink-1.9-SNAPSHOT
Starting cluster.
Starting standalonesession daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Waiting for dispatcher REST endpoint to come up...
Dispatcher REST endpoint is up.
[INFO] 1 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 2 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
[INFO] 3 instance(s) of taskexecutor are already running on travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting taskexecutor daemon on host travis-job-2e07a029-3701-4311-87e2-25d48ae1f7eb.
Starting execution of program
java.lang.AssertionError: Cannot add expression of different type to set:
set type is RecordType(INTEGER NOT NULL correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
expression type is RecordType(INTEGER correct, TIMESTAMP(3) NOT NULL w$start, TIMESTAMP(3) NOT NULL w$end, TIME ATTRIBUTE(ROWTIME) w$rowtime, TIME ATTRIBUTE(PROCTIME) w$proctime) NOT NULL
set is rel#150:LogicalWindowAggregate.NONE(input=HepRelVertex#139,group={},correct=SUM($1),w$start=start('w$),w$end=end('w$),w$rowtime=rowtime('w$),w$proctime=proctime('w$))
expression is LogicalWindowAggregate#167
	at org.apache.calcite.plan.RelOptUtil.verifyTypeEquivalence(RelOptUtil.java:381)
	at org.apache.calcite.plan.hep.HepRuleCall.transformTo(HepRuleCall.java:57)
	at org.apache.calcite.plan.RelOptRuleCall.transformTo(RelOptRuleCall.java:234)
	at org.apache.flink.table.plan.rules.logical.ExtendedAggregateExtractProjectRule.onMatch(ExtendedAggregateExtractProjectRule.java:90)
	at org.apache.calcite.plan.AbstractRelOptPlanner.fireRule(AbstractRelOptPlanner.java:319)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:559)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:418)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:255)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:214)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:201)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlanner(TableEnvImpl.scala:282)
	at org.apache.flink.table.api.TableEnvImpl.runHepPlannerSequentially(TableEnvImpl.scala:248)
	at org.apache.flink.table.api.TableEnvImpl.optimizeNormalizeLogicalPlan(TableEnvImpl.scala:204)
	at org.apache.flink.table.api.StreamTableEnvImpl.optimize(StreamTableEnvImpl.scala:738)
	at org.apache.flink.table.api.StreamTableEnvImpl.translate(StreamTableEnvImpl.scala:787)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:100)
	at org.apache.flink.table.api.java.StreamTableEnvImpl.toAppendStream(StreamTableEnvImpl.scala:83)
	at org.apache.flink.sql.tests.StreamSQLTestProgram.main(StreamSQLTestProgram.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:269)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:742)
	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:272)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:204)
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:983)
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1056)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1056)
{code}"	FLINK	Closed	3	7	3568	pull-request-available
13180226	WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow	"{{WindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow}} failed on Travis.

https://api.travis-ci.org/v3/job/418629694/log.txt"	FLINK	Closed	2	1	3568	test-stability
13486345	BatchExecutionKeyedStateBackend is using incorrect ExecutionConfig when creating serializer	"{{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionKeyedStateBackend#getOrCreateKeyedState}} is using freshly constructed {{ExecutionConfig}}, instead of the one configured by the user from the environment.


{code:java}
    public <N, S extends State, T> S getOrCreateKeyedState(
            TypeSerializer<N> namespaceSerializer, StateDescriptor<S, T> stateDescriptor)
            throws Exception {
        checkNotNull(namespaceSerializer, ""Namespace serializer"");
        checkNotNull(
                keySerializer,
                ""State key serializer has not been configured in the config. ""
                        + ""This operation cannot use partitioned state."");

        if (!stateDescriptor.isSerializerInitialized()) {
            stateDescriptor.initializeSerializerUnlessSet(new ExecutionConfig());
        }
{code}

The correct one could be obtained from {{env.getExecutionConfig()}} in {{org.apache.flink.streaming.api.operators.sorted.state.BatchExecutionStateBackend#createKeyedStateBackend}} "	FLINK	Closed	4	1	3568	pull-request-available
13208537	KafkaITCase.testConcurrentProducerConsumerTopology times out on Travis	"The {{KafkaITCase.testConcurrentProducerConsumerTopology}} times out on Travis.

{code}
20:26:29.579 [ERROR] Errors: 
20:26:29.579 [ERROR]   KafkaITCase.testConcurrentProducerConsumerTopology:73->KafkaConsumerTestBase.runSimpleConcurrentProducerConsumerTopology:824->KafkaTestBase.deleteTestTopic:206->Object.wait:502->Object.wait:-2 Â» TestTimedOut
{code}

The solution might as simple as increasing the timeout.

https://api.travis-ci.org/v3/job/476975725/log.txt"	FLINK	Closed	2	4	3568	pull-request-available, test-stability
13031245	Fails AkkaRpcServiceTest#testTerminationFuture	"{code}
testTerminationFuture(org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest)  Time elapsed: 1.013 sec  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 1000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at akka.remote.Remoting.start(Remoting.scala:179)
	at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:620)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:617)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:634)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:142)
	at akka.actor.ActorSystem$.apply(ActorSystem.scala:119)
	at akka.actor.ActorSystem$.create(ActorSystem.scala:67)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
	at org.apache.flink.runtime.akka.AkkaUtils$.createDefaultActorSystem(AkkaUtils.scala:114)
	at org.apache.flink.runtime.akka.AkkaUtils.createDefaultActorSystem(AkkaUtils.scala)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest.testTerminationFuture(AkkaRpcServiceTest.java:134)
{code} in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest while testing current master 1.2.0 branch "	FLINK	Closed	3	1	3568	test-stability
13371934	JobMasterStopWithSavepointITCase failed due to status is FAILING	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16405&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228&l=4472


{code:java}
[ERROR] Failures: 
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheSuspend:133->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR]   JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate:138->throwingExceptionOnCallbackWithoutRestartsHelper:155 
Expected: <FAILED>
     but: was <FAILING>
[ERROR] Errors: 
[ERROR]   JobMasterStopWithSavepointITCase.suspendWithSavepointWithoutComplicationsShouldSucceedAndLeadJobToFinished:103->stopWithSavepointNormalExecutionHelper:113->setUpJobGraph:307 » IllegalState
[ERROR]   JobMasterStopWithSavepointITCase.testRestartCheckpointCoordinatorIfStopWithSavepointFails:237 » IllegalState
[INFO] 
[ERROR] Tests run: 1645, Failures: 2, Errors: 2, Skipped: 51

{code}
"	FLINK	Resolved	2	1	3568	test-stability
13326530	Clean up the UnilateralSortMerger	"This is a preparation step for [FLIP-140|https://cwiki.apache.org/confluence/display/FLINK/FLIP-140%3A+Introduce+bounded+style+execution+for+keyed+streams]. The purpose of the task is two-folds:
* break down the implementation into a more composable pieces
* introduce a way to produce records in a push-based manner instead of pull-based with additional reading thread."	FLINK	Closed	3	7	3568	pull-request-available
13278176	Preserve logs from BashJavaUtils and make them part of TM logs	"In FLINK-13983 we introduced BashJavaUtils utility to call in taskmanager.sh before starting TM and calculate memory configuration for the JVM process of TM.

Ideally, it would be nice to preserve BashJavaUtils logs and make them part of the TM logs. Currently, logging for BashJavaUtils is configured from the class path and can differ from TM logging. Moreover TM logging can rewrite BashJavaUtils even if we align their loggings (e.g. log4j.appender.file.append=false in default log4j.properties  for Flink)."	FLINK	Closed	1	4	3568	pull-request-available
13597368	TIMESTAMPDIFF can not be string serialized	TIMESTAMPDIFF can not be properly string serialized, because TIMEPOINTUNIT can not be serialized.	FLINK	In Progress	3	1	3568	pull-request-available
13205141	Exception in code generation when ambiguous columns in MATCH_RECOGNIZE	"Query:
{code}
SELECT *
FROM Ticker
MATCH_RECOGNIZE (
  PARTITION BY symbol, price
  ORDER BY proctime
  MEASURES
    A.symbol AS symbol,
    A.price AS price
  PATTERN (A)
  DEFINE
    A AS symbol = 'a'
) AS T
{code}

throws a cryptic exception from the code generation stack that the output arity is wrong. We should add early validation and throw a meaningful exception. 

I've also created a calcite ticket to fix it on calcite's side: [CALCITE-2747]"	FLINK	Closed	3	1	3568	pull-request-available
13185500	Savepoints should be counted as retained checkpoints	"This task is about reverting [FLINK-6328].

The problem is that you can get incorrect results with exactly-once sinks if there is a failure after taking a savepoint but before taking the next checkpoint because the savepoint will also have manifested side effects to the sink.
"	FLINK	Closed	3	1	3568	pull-request-available
13343946	Some Table examples are not built correctly	"Some examples were moved to the {{org.apache.flink.table.examples.scala.basics}} package but the pom.xml was not updated. This means the example jars are not built correctly and do not contain the classes.

Examples that I noticed:
* org.apache.flink.table.examples.scala.basics.StreamTableExample
* org.apache.flink.table.examples.scala.basics.TPCHQuery3Table

We should update the {{includes}} sections e.g.:

{code}
<execution>
	<id>StreamTableExample</id>
	<phase>package</phase>
	<goals>
		<goal>jar</goal>
	</goals>
	<configuration>
		<classifier>StreamTableExample</classifier>

<!--- The sections below should be updated -->

		<archive>
			<manifestEntries>
				<program-class>org.apache.flink.table.examples.scala.StreamTableExample</program-class>
			</manifestEntries>
		</archive>
		<includes>
			<include>org/apache/flink/table/examples/scala/StreamTableExample*</include>
		</includes>
	</configuration>
</execution>
{code}"	FLINK	Closed	2	1	3568	pull-request-available
13355441	Write savepoints in unified format from HeapStateBackend	The aim is to implement a {{HeapKeyValueStateIterator}} which can be used to produce a unified savepoint out of a HeapKeyedStateBackend	FLINK	Closed	3	7	3568	pull-request-available
13242119	Remove expressionBridge from QueryOperations factories	Expression bridge is used to create a schema of QueryOperation. This is no longer necessary with ResolvedExpressions in place.	FLINK	Closed	3	7	3568	pull-request-available
13415595	Document claim & no-claim mode	We should describe how the different restore modes work. It is important to go through the FLIP and include all {{NOTES}} in the written documentation	FLINK	Closed	3	7	3568	pull-request-available
13238698	Improve expression based TableSchema extraction from DataStream/DataSet	"We should improve the extraction of {{TableSchema}} from {{DataStream/DataSet}}. Currently it is split into a few stages:
# Extract types ignoring time attributes via {{FieldInfoUtils#getFieldInfo}}
# Extract the rowtime and proctime positions via {{StreamTableEnvImpl#validateAndExtractTimeAttributes}}
# Adjust the indices from #1 using information from #2

All that could happen in a single pass. This will also deal with the porting/removing of a few methods from {{StreamTableEnvImpl}}."	FLINK	Closed	3	7	3568	pull-request-available
13314702	Tests RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte & RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort fail locally	"The tests:
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorShort
* RocksKeyGroupsRocksSingleStateIteratorTest#testMergeIteratorByte

fail locally (in IDE or from cmd with {{mvn clean install}}) with
{code}
java.lang.UnsatisfiedLinkError: org.rocksdb.ReadOptions.newReadOptions()J
	at org.rocksdb.ReadOptions.newReadOptions(Native Method)
	at org.rocksdb.ReadOptions.<init>(ReadOptions.java:16)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIterator(RocksKeyGroupsRocksSingleStateIteratorTest.java:78)
	at org.apache.flink.contrib.streaming.state.RocksKeyGroupsRocksSingleStateIteratorTest.testMergeIteratorShort(RocksKeyGroupsRocksSingleStateIteratorTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}"	FLINK	Closed	3	1	3568	pull-request-available
13353481	SQLClientSchemaRegistryITCase unstable with InternalServerErrorException: Status 500	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12253&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2021-01-20T00:10:21.3510385Z Jan 20 00:10:21 
2021-01-20T00:10:21.3516246Z Jan 20 00:10:21 [ERROR] testWriting(org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase)  Time elapsed: 0.001 s  <<< ERROR!
2021-01-20T00:10:21.3517459Z Jan 20 00:10:21 java.lang.RuntimeException: Could not build the flink-dist image
2021-01-20T00:10:21.3518178Z Jan 20 00:10:21 	at org.apache.flink.tests.util.flink.FlinkContainer$FlinkContainerBuilder.build(FlinkContainer.java:281)
2021-01-20T00:10:21.3519176Z Jan 20 00:10:21 	at org.apache.flink.tests.util.kafka.SQLClientSchemaRegistryITCase.<init>(SQLClientSchemaRegistryITCase.java:88)
2021-01-20T00:10:21.3519873Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-01-20T00:10:21.3520537Z Jan 20 00:10:21 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-01-20T00:10:21.3521390Z Jan 20 00:10:21 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-01-20T00:10:21.3522080Z Jan 20 00:10:21 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2021-01-20T00:10:21.3522730Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:217)
2021-01-20T00:10:21.3523452Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:266)
2021-01-20T00:10:21.3524237Z Jan 20 00:10:21 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-01-20T00:10:21.3524879Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:263)
2021-01-20T00:10:21.3525527Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-01-20T00:10:21.3526157Z Jan 20 00:10:21 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-01-20T00:10:21.3526754Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-01-20T00:10:21.3527316Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-01-20T00:10:21.3527884Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-01-20T00:10:21.3528462Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-01-20T00:10:21.3529491Z Jan 20 00:10:21 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-01-20T00:10:21.3530220Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
2021-01-20T00:10:21.3530970Z Jan 20 00:10:21 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
2021-01-20T00:10:21.3531649Z Jan 20 00:10:21 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-01-20T00:10:21.3532201Z Jan 20 00:10:21 	at java.lang.Thread.run(Thread.java:748)
2021-01-20T00:10:21.3533545Z Jan 20 00:10:21 Caused by: com.github.dockerjava.api.exception.InternalServerErrorException: Status 500: {""message"":""Get https://registry-1.docker.io/v2/testcontainers/ryuk/manifests/0.3.0: received unexpected HTTP status: 502 Bad Gateway""}
2021-01-20T00:10:21.3534353Z Jan 20 00:10:21 
2021-01-20T00:10:21.3534955Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.execute(DefaultInvocationBuilder.java:247)
2021-01-20T00:10:21.3536388Z Jan 20 00:10:21 	at org.testcontainers.shaded.com.github.dockerjava.core.DefaultInvocationBuilder.lambda$executeAndStream$1(DefaultInvocationBuilder.java:269)
2021-01-20T00:10:21.3537066Z Jan 20 00:10:21 	... 1 more
2021-01-20T00:10:21.3541323Z Jan 20 00:10:21 
{code}"	FLINK	Closed	2	1	3568	test-stability
13280523	SELECT 'ABC'; does not work in sql-client	"A query like {{SELECT 'abc';}} fails in sql-client with blink planner enabled with an error:
{code}
org.apache.flink.table.api.ValidationException: Type CHAR(3) of table field 'EXPR$0' does not match with the physical type STRING of the 'EXPR$0' field of the TableSink consumed type.
{code}

The reason is that those sinks do not properly support new type system. There is no good way to define schema and consumed data type so that they match. We should update the in-memory sinks in sql-client to work with the legacy type system for now until the retract and upsert sinks work properly with the new type system."	FLINK	Closed	1	1	3568	pull-request-available
13381827	Support profiling for Python user-defined functions	We will support profiling to help users to analyze the performance bottleneck in their python udf	FLINK	Resolved	3	4	5059	auto-unassigned, pull-request-available
13377539	Restructure the coders in PyFlink	Now, PyFlink has introduced many top-level coder. The top-level coder is defined as used by `Python Operation` directly. Currently, some top-level coder comes with the semantics of function, such as `TableFunctionCoder`. In fact, we need to remove this semantic and remove many coders such as `TableFunctionCoder`, `AggregateFunctionCoder` and so on. For the data structure type to be processed, there will only be `FlattenRowCoder`(used in udf, udtf.., the type of data will be List), `TopRowCoder`(used in udaf, udtagg, the type of data will be Row or InternalRow), `RawCoder`(used in datastream, the type of data will be raw data type), `ArrowCoder`(used in pandas udf, the type of data will be pandas data structure).	FLINK	Resolved	3	4	5059	pull-request-available
13370337	Fix the bug of same timers are registered multiple times	"The same timer will be registered multiple times. We need to deduplicate same timers
"	FLINK	Resolved	3	7	5059	pull-request-available
13269808	Move the Python SqlDialect to module table	Currently SqlDialect is located in pyflink.common which is not correct as it belongs to the module table actually and so we should move it to module pyflink.table.	FLINK	Closed	3	4	5059	pull-request-available
13362534	Add NamespacedStateView and PerWindowStateDataViewStore	Currently we only support KeyedStateView and PerKeyStateDataViewStore. We need to Add NamespacedStateView and PerWindowStateDataViewStore for support setting namespace.	FLINK	Resolved	3	7	5059	pull-request-available
13417062	StreamingModeDataStreamTests::test_set_stream_env failed on azure	"{code:java}
2021-12-13T02:25:17.0905034Z Dec 13 02:25:17 =================================== FAILURES ===================================
2021-12-13T02:25:17.0905626Z Dec 13 02:25:17 _____________ StreamExecutionEnvironmentTests.test_set_stream_env ______________
2021-12-13T02:25:17.0906084Z Dec 13 02:25:17 
2021-12-13T02:25:17.0906569Z Dec 13 02:25:17 self = <pyflink.datastream.tests.test_stream_execution_environment.StreamExecutionEnvironmentTests testMethod=test_set_stream_env>
2021-12-13T02:25:17.0907047Z Dec 13 02:25:17 
2021-12-13T02:25:17.0907483Z Dec 13 02:25:17     @unittest.skipIf(on_windows(), ""Symbolic link is not supported on Windows, skipping."")
2021-12-13T02:25:17.0909942Z Dec 13 02:25:17     def test_set_stream_env(self):
2021-12-13T02:25:17.0910424Z Dec 13 02:25:17         import sys
2021-12-13T02:25:17.0910816Z Dec 13 02:25:17         python_exec = sys.executable
2021-12-13T02:25:17.0911369Z Dec 13 02:25:17         tmp_dir = self.tempdir
2021-12-13T02:25:17.0911736Z Dec 13 02:25:17         env = self.env
2021-12-13T02:25:17.0912153Z Dec 13 02:25:17         python_exec_link_path = os.path.join(tmp_dir, ""py_exec"")
2021-12-13T02:25:17.0912876Z Dec 13 02:25:17         os.symlink(python_exec, python_exec_link_path)
2021-12-13T02:25:17.0913342Z Dec 13 02:25:17         env.set_python_executable(python_exec_link_path)
2021-12-13T02:25:17.0913799Z Dec 13 02:25:17     
2021-12-13T02:25:17.0914365Z Dec 13 02:25:17         def check_python_exec(i):
2021-12-13T02:25:17.0914944Z Dec 13 02:25:17             import os
2021-12-13T02:25:17.0915541Z Dec 13 02:25:17             assert os.environ[""python""] == python_exec_link_path
2021-12-13T02:25:17.0917263Z Dec 13 02:25:17             return i
2021-12-13T02:25:17.0917659Z Dec 13 02:25:17     
2021-12-13T02:25:17.0918488Z Dec 13 02:25:17         ds = env.from_collection([1, 2, 3, 4, 5])
2021-12-13T02:25:17.0919021Z Dec 13 02:25:17         ds.map(check_python_exec).add_sink(self.test_sink)
2021-12-13T02:25:17.0919717Z Dec 13 02:25:17 >       env.execute(""test set python executable"")
2021-12-13T02:25:17.0920265Z Dec 13 02:25:17 
2021-12-13T02:25:17.0920707Z Dec 13 02:25:17 pyflink/datastream/tests/test_stream_execution_environment.py:546: 
2021-12-13T02:25:17.0921533Z Dec 13 02:25:17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-12-13T02:25:17.0922325Z Dec 13 02:25:17 pyflink/datastream/stream_execution_environment.py:691: in execute
2021-12-13T02:25:17.0922897Z Dec 13 02:25:17     return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
2021-12-13T02:25:17.0924157Z Dec 13 02:25:17 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1286: in __call__
2021-12-13T02:25:17.0924680Z Dec 13 02:25:17     answer, self.gateway_client, self.target_id, self.name)
2021-12-13T02:25:17.0925131Z Dec 13 02:25:17 pyflink/util/exceptions.py:146: in deco
2021-12-13T02:25:17.0925615Z Dec 13 02:25:17     return f(*a, **kw)
2021-12-13T02:25:17.0926311Z Dec 13 02:25:17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2021-12-13T02:25:17.0926793Z Dec 13 02:25:17 
2021-12-13T02:25:17.0927550Z Dec 13 02:25:17 answer = 'xro13244'
2021-12-13T02:25:17.0928239Z Dec 13 02:25:17 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f6f553685f8>
2021-12-13T02:25:17.0929456Z Dec 13 02:25:17 target_id = 'o13195', name = 'execute'
2021-12-13T02:25:17.0929975Z Dec 13 02:25:17 
2021-12-13T02:25:17.0930616Z Dec 13 02:25:17     def get_return_value(answer, gateway_client, target_id=None, name=None):
2021-12-13T02:25:17.0931506Z Dec 13 02:25:17         """"""Converts an answer received from the Java gateway into a Python object.
2021-12-13T02:25:17.0931993Z Dec 13 02:25:17     
2021-12-13T02:25:17.0932493Z Dec 13 02:25:17         For example, string representation of integers are converted to Python
2021-12-13T02:25:17.0933249Z Dec 13 02:25:17         integer, string representation of objects are converted to JavaObject
2021-12-13T02:25:17.0933779Z Dec 13 02:25:17         instances, etc.
2021-12-13T02:25:17.0934191Z Dec 13 02:25:17     
2021-12-13T02:25:17.0934809Z Dec 13 02:25:17         :param answer: the string returned by the Java gateway
2021-12-13T02:25:17.0935350Z Dec 13 02:25:17         :param gateway_client: the gateway client used to communicate with the Java
2021-12-13T02:25:17.0935983Z Dec 13 02:25:17             Gateway. Only necessary if the answer is a reference (e.g., object,
2021-12-13T02:25:17.0936593Z Dec 13 02:25:17             list, map)
2021-12-13T02:25:17.0937254Z Dec 13 02:25:17         :param target_id: the name of the object from which the answer comes from
2021-12-13T02:25:17.0937783Z Dec 13 02:25:17             (e.g., *object1* in `object1.hello()`). Optional.
2021-12-13T02:25:17.0938649Z Dec 13 02:25:17         :param name: the name of the member from which the answer comes from
2021-12-13T02:25:17.0939345Z Dec 13 02:25:17             (e.g., *hello* in `object1.hello()`). Optional.
2021-12-13T02:25:17.0939964Z Dec 13 02:25:17         """"""
2021-12-13T02:25:17.0940478Z Dec 13 02:25:17         if is_error(answer)[0]:
2021-12-13T02:25:17.0940892Z Dec 13 02:25:17             if len(answer) > 1:
2021-12-13T02:25:17.0941376Z Dec 13 02:25:17                 type = answer[1]
2021-12-13T02:25:17.0942007Z Dec 13 02:25:17                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2021-12-13T02:25:17.0942632Z Dec 13 02:25:17                 if answer[1] == REFERENCE_TYPE:
2021-12-13T02:25:17.0943271Z Dec 13 02:25:17                     raise Py4JJavaError(
2021-12-13T02:25:17.0943777Z Dec 13 02:25:17                         ""An error occurred while calling {0}{1}{2}.\n"".
2021-12-13T02:25:17.0944418Z Dec 13 02:25:17 >                       format(target_id, ""."", name), value)
2021-12-13T02:25:17.0945358Z Dec 13 02:25:17 E                   py4j.protocol.Py4JJavaError: An error occurred while calling o13195.execute.
2021-12-13T02:25:17.0946044Z Dec 13 02:25:17 E                   : org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-12-13T02:25:17.0946828Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-12-13T02:25:17.0947676Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-12-13T02:25:17.0948832Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-12-13T02:25:17.0949879Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2021-12-13T02:25:17.0950937Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-12-13T02:25:17.0951856Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-12-13T02:25:17.0952574Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:258)
2021-12-13T02:25:17.0953291Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-12-13T02:25:17.0954025Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-12-13T02:25:17.0954868Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-12-13T02:25:17.0955515Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-12-13T02:25:17.0956286Z Dec 13 02:25:17 E                   	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1389)
2021-12-13T02:25:17.0956987Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)
2021-12-13T02:25:17.0957747Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:68)
2021-12-13T02:25:17.0958744Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$guardCompletionWithContextClassLoader$2(ClassLoadingUtils.java:92)
2021-12-13T02:25:17.0959510Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2021-12-13T02:25:17.0960192Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2021-12-13T02:25:17.0961333Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2021-12-13T02:25:17.0962057Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2021-12-13T02:25:17.0962745Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$1.onComplete(AkkaFutureUtils.java:47)
2021-12-13T02:25:17.0963385Z Dec 13 02:25:17 E                   	at akka.dispatch.OnComplete.internal(Future.scala:300)
2021-12-13T02:25:17.0963953Z Dec 13 02:25:17 E                   	at akka.dispatch.OnComplete.internal(Future.scala:297)
2021-12-13T02:25:17.0964529Z Dec 13 02:25:17 E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:224)
2021-12-13T02:25:17.0965092Z Dec 13 02:25:17 E                   	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:221)
2021-12-13T02:25:17.0965783Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2021-12-13T02:25:17.0966981Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.AkkaFutureUtils$DirectExecutionContext.execute(AkkaFutureUtils.java:65)
2021-12-13T02:25:17.0967839Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68)
2021-12-13T02:25:17.0968667Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284)
2021-12-13T02:25:17.0969372Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284)
2021-12-13T02:25:17.0970092Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284)
2021-12-13T02:25:17.0970725Z Dec 13 02:25:17 E                   	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:621)
2021-12-13T02:25:17.0971571Z Dec 13 02:25:17 E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:24)
2021-12-13T02:25:17.0972299Z Dec 13 02:25:17 E                   	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23)
2021-12-13T02:25:17.0972960Z Dec 13 02:25:17 E                   	at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532)
2021-12-13T02:25:17.0973539Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29)
2021-12-13T02:25:17.0974153Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29)
2021-12-13T02:25:17.0974894Z Dec 13 02:25:17 E                   	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)
2021-12-13T02:25:17.0975538Z Dec 13 02:25:17 E                   	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63)
2021-12-13T02:25:17.0976226Z Dec 13 02:25:17 E                   	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100)
2021-12-13T02:25:17.0976890Z Dec 13 02:25:17 E                   	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
2021-12-13T02:25:17.0977517Z Dec 13 02:25:17 E                   	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)
2021-12-13T02:25:17.0978328Z Dec 13 02:25:17 E                   	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100)
2021-12-13T02:25:17.0978966Z Dec 13 02:25:17 E                   	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49)
2021-12-13T02:25:17.0979629Z Dec 13 02:25:17 E                   	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48)
2021-12-13T02:25:17.0980303Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2021-12-13T02:25:17.0980930Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2021-12-13T02:25:17.0981634Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2021-12-13T02:25:17.0982267Z Dec 13 02:25:17 E                   	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
2021-12-13T02:25:17.0982938Z Dec 13 02:25:17 E                   Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2021-12-13T02:25:17.0983705Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
2021-12-13T02:25:17.0984560Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
2021-12-13T02:25:17.0985362Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:252)
2021-12-13T02:25:17.0986176Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:242)
2021-12-13T02:25:17.0986940Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:233)
2021-12-13T02:25:17.0987690Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:684)
2021-12-13T02:25:17.0988544Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:79)
2021-12-13T02:25:17.0989257Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:444)
2021-12-13T02:25:17.0989869Z Dec 13 02:25:17 E                   	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source)
2021-12-13T02:25:17.0990476Z Dec 13 02:25:17 E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-12-13T02:25:17.0991160Z Dec 13 02:25:17 E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2021-12-13T02:25:17.0991810Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:316)
2021-12-13T02:25:17.0992548Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
2021-12-13T02:25:17.0993369Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:314)
2021-12-13T02:25:17.0994063Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
2021-12-13T02:25:17.0994782Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
2021-12-13T02:25:17.0995498Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
2021-12-13T02:25:17.0996134Z Dec 13 02:25:17 E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
2021-12-13T02:25:17.0996728Z Dec 13 02:25:17 E                   	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
2021-12-13T02:25:17.0997323Z Dec 13 02:25:17 E                   	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2021-12-13T02:25:17.0997922Z Dec 13 02:25:17 E                   	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2021-12-13T02:25:17.0998648Z Dec 13 02:25:17 E                   	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
2021-12-13T02:25:17.0999267Z Dec 13 02:25:17 E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-12-13T02:25:17.0999884Z Dec 13 02:25:17 E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-12-13T02:25:17.1000502Z Dec 13 02:25:17 E                   	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2021-12-13T02:25:17.1001186Z Dec 13 02:25:17 E                   	at akka.actor.Actor.aroundReceive(Actor.scala:537)
2021-12-13T02:25:17.1001737Z Dec 13 02:25:17 E                   	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
2021-12-13T02:25:17.1002318Z Dec 13 02:25:17 E                   	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
2021-12-13T02:25:17.1002914Z Dec 13 02:25:17 E                   	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
2021-12-13T02:25:17.1003468Z Dec 13 02:25:17 E                   	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
2021-12-13T02:25:17.1004032Z Dec 13 02:25:17 E                   	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
2021-12-13T02:25:17.1004770Z Dec 13 02:25:17 E                   	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
2021-12-13T02:25:17.1005426Z Dec 13 02:25:17 E                   	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
2021-12-13T02:25:17.1005900Z Dec 13 02:25:17 E                   	... 4 more
2021-12-13T02:25:17.1006435Z Dec 13 02:25:17 E                   Caused by: java.lang.RuntimeException: Error while waiting for BeamPythonFunctionRunner flush
2021-12-13T02:25:17.1007254Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.invokeFinishBundle(AbstractPythonFunctionOperator.java:361)
2021-12-13T02:25:17.1008300Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.processWatermark(AbstractPythonFunctionOperator.java:222)
2021-12-13T02:25:17.1009150Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.PythonProcessOperator.processWatermark(PythonProcessOperator.java:104)
2021-12-13T02:25:17.1010004Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitWatermark(OneInputStreamTask.java:239)
2021-12-13T02:25:17.1010906Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.findAndOutputNewMinWatermarkAcrossAlignedChannels(StatusWatermarkValve.java:200)
2021-12-13T02:25:17.1011881Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.inputWatermark(StatusWatermarkValve.java:105)
2021-12-13T02:25:17.1012809Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:136)
2021-12-13T02:25:17.1013645Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
2021-12-13T02:25:17.1014454Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
2021-12-13T02:25:17.1015210Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:496)
2021-12-13T02:25:17.1015953Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
2021-12-13T02:25:17.1016812Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:809)
2021-12-13T02:25:17.1017496Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:761)
2021-12-13T02:25:17.1018310Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:958)
2021-12-13T02:25:17.1019030Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:937)
2021-12-13T02:25:17.1019655Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:766)
2021-12-13T02:25:17.1020246Z Dec 13 02:25:17 E                   	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:575)
2021-12-13T02:25:17.1020809Z Dec 13 02:25:17 E                   	at java.lang.Thread.run(Thread.java:748)
2021-12-13T02:25:17.1021413Z Dec 13 02:25:17 E                   Caused by: java.lang.RuntimeException: Failed to close remote bundle
2021-12-13T02:25:17.1022119Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:377)
2021-12-13T02:25:17.1022913Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.flush(BeamPythonFunctionRunner.java:361)
2021-12-13T02:25:17.1023754Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.operators.python.AbstractPythonFunctionOperator.lambda$invokeFinishBundle$2(AbstractPythonFunctionOperator.java:340)
2021-12-13T02:25:17.1024611Z Dec 13 02:25:17 E                   	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-12-13T02:25:17.1025210Z Dec 13 02:25:17 E                   	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-12-13T02:25:17.1025827Z Dec 13 02:25:17 E                   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-12-13T02:25:17.1026462Z Dec 13 02:25:17 E                   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-12-13T02:25:17.1026982Z Dec 13 02:25:17 E                   	... 1 more
2021-12-13T02:25:17.1027656Z Dec 13 02:25:17 E                   Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
2021-12-13T02:25:17.1029249Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
2021-12-13T02:25:17.1029844Z Dec 13 02:25:17 E                       response = task()
2021-12-13T02:25:17.1030674Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
2021-12-13T02:25:17.1031384Z Dec 13 02:25:17 E                       lambda: self.create_worker().do_instruction(request), request)
2021-12-13T02:25:17.1032404Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
2021-12-13T02:25:17.1033046Z Dec 13 02:25:17 E                       getattr(request, request_type), request.instruction_id)
2021-12-13T02:25:17.1033923Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
2021-12-13T02:25:17.1034563Z Dec 13 02:25:17 E                       bundle_processor.process_bundle(instruction_id))
2021-12-13T02:25:17.1035445Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
2021-12-13T02:25:17.1036033Z Dec 13 02:25:17 E                       element.data)
2021-12-13T02:25:17.1036863Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
2021-12-13T02:25:17.1037451Z Dec 13 02:25:17 E                       self.output(decoded_value)
2021-12-13T02:25:17.1038003Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1038848Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1039542Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2021-12-13T02:25:17.1040229Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
2021-12-13T02:25:17.1041187Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
2021-12-13T02:25:17.1041793Z Dec 13 02:25:17 E                       self._output_processor.process_outputs(o, self.process_element(value))
2021-12-13T02:25:17.1042611Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 63, in process_outputs
2021-12-13T02:25:17.1043330Z Dec 13 02:25:17 E                       self._consumer.process(windowed_value.with_value(results))
2021-12-13T02:25:17.1044122Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 131, in process
2021-12-13T02:25:17.1044657Z Dec 13 02:25:17 E                       for value in o.value:
2021-12-13T02:25:17.1045416Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/operations.py"", line 179, in wrapped_func
2021-12-13T02:25:17.1046002Z Dec 13 02:25:17 E                       yield from _emit_results(timestamp, watermark, results)
2021-12-13T02:25:17.1046792Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/input_handler.py"", line 101, in _emit_results
2021-12-13T02:25:17.1047351Z Dec 13 02:25:17 E                       for result in results:
2021-12-13T02:25:17.1048215Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/data_stream.py"", line 271, in process_element
2021-12-13T02:25:17.1048808Z Dec 13 02:25:17 E                       yield self._map_func(value)
2021-12-13T02:25:17.1049619Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/tests/test_stream_execution_environment.py"", line 541, in check_python_exec
2021-12-13T02:25:17.1050206Z Dec 13 02:25:17 E                       assert os.environ[""python""] == python_exec_link_path
2021-12-13T02:25:17.1050911Z Dec 13 02:25:17 E                   AssertionError: assert 'python' == '/tmp/tmp1mas4ii5/py_exec'
2021-12-13T02:25:17.1051732Z Dec 13 02:25:17 E                     - /tmp/tmp1mas4ii5/py_exec
2021-12-13T02:25:17.1052146Z Dec 13 02:25:17 E                     + python
2021-12-13T02:25:17.1052510Z Dec 13 02:25:17 E                   
2021-12-13T02:25:17.1053005Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-12-13T02:25:17.1053659Z Dec 13 02:25:17 E                   	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2021-12-13T02:25:17.1054257Z Dec 13 02:25:17 E                   	at org.apache.beam.sdk.util.MoreFutures.get(MoreFutures.java:60)
2021-12-13T02:25:17.1054970Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.SdkHarnessClient$BundleProcessor$ActiveBundle.close(SdkHarnessClient.java:504)
2021-12-13T02:25:17.1055806Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.DefaultJobBundleFactory$SimpleStageBundleFactory$1.close(DefaultJobBundleFactory.java:555)
2021-12-13T02:25:17.1056648Z Dec 13 02:25:17 E                   	at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.finishBundle(BeamPythonFunctionRunner.java:375)
2021-12-13T02:25:17.1057241Z Dec 13 02:25:17 E                   	... 7 more
2021-12-13T02:25:17.1057839Z Dec 13 02:25:17 E                   Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 1: Traceback (most recent call last):
2021-12-13T02:25:17.1059051Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
2021-12-13T02:25:17.1059629Z Dec 13 02:25:17 E                       response = task()
2021-12-13T02:25:17.1060435Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
2021-12-13T02:25:17.1061175Z Dec 13 02:25:17 E                       lambda: self.create_worker().do_instruction(request), request)
2021-12-13T02:25:17.1062217Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
2021-12-13T02:25:17.1062851Z Dec 13 02:25:17 E                       getattr(request, request_type), request.instruction_id)
2021-12-13T02:25:17.1063842Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
2021-12-13T02:25:17.1064466Z Dec 13 02:25:17 E                       bundle_processor.process_bundle(instruction_id))
2021-12-13T02:25:17.1065345Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
2021-12-13T02:25:17.1065925Z Dec 13 02:25:17 E                       element.data)
2021-12-13T02:25:17.1066750Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
2021-12-13T02:25:17.1067358Z Dec 13 02:25:17 E                       self.output(decoded_value)
2021-12-13T02:25:17.1067912Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1068748Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
2021-12-13T02:25:17.1069441Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2021-12-13T02:25:17.1070226Z Dec 13 02:25:17 E                     File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
2021-12-13T02:25:17.1071169Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
2021-12-13T02:25:17.1071779Z Dec 13 02:25:17 E                       self._output_processor.process_outputs(o, self.process_element(value))
2021-12-13T02:25:17.1072601Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 63, in process_outputs
2021-12-13T02:25:17.1073175Z Dec 13 02:25:17 E                       self._consumer.process(windowed_value.with_value(results))
2021-12-13T02:25:17.1073958Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 131, in process
2021-12-13T02:25:17.1074508Z Dec 13 02:25:17 E                       for value in o.value:
2021-12-13T02:25:17.1075271Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/operations.py"", line 179, in wrapped_func
2021-12-13T02:25:17.1075850Z Dec 13 02:25:17 E                       yield from _emit_results(timestamp, watermark, results)
2021-12-13T02:25:17.1076635Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/fn_execution/datastream/input_handler.py"", line 101, in _emit_results
2021-12-13T02:25:17.1077195Z Dec 13 02:25:17 E                       for result in results:
2021-12-13T02:25:17.1077928Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/data_stream.py"", line 271, in process_element
2021-12-13T02:25:17.1078658Z Dec 13 02:25:17 E                       yield self._map_func(value)
2021-12-13T02:25:17.1079464Z Dec 13 02:25:17 E                     File ""/__w/1/s/flink-python/pyflink/datastream/tests/test_stream_execution_environment.py"", line 541, in check_python_exec
2021-12-13T02:25:17.1080069Z Dec 13 02:25:17 E                       assert os.environ[""python""] == python_exec_link_path
2021-12-13T02:25:17.1080770Z Dec 13 02:25:17 E                   AssertionError: assert 'python' == '/tmp/tmp1mas4ii5/py_exec'
2021-12-13T02:25:17.1081471Z Dec 13 02:25:17 E                     - /tmp/tmp1mas4ii5/py_exec
2021-12-13T02:25:17.1081878Z Dec 13 02:25:17 E                     + python
2021-12-13T02:25:17.1082234Z Dec 13 02:25:17 E                   
2021-12-13T02:25:17.1082893Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:180)
2021-12-13T02:25:17.1083710Z Dec 13 02:25:17 E                   	at org.apache.beam.runners.fnexecution.control.FnApiControlClient$ResponseStreamObserver.onNext(FnApiControlClient.java:160)
2021-12-13T02:25:17.1084561Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:251)
2021-12-13T02:25:17.1085416Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)
2021-12-13T02:25:17.1086210Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)
2021-12-13T02:25:17.1087049Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailableInternal(ServerCallImpl.java:309)
2021-12-13T02:25:17.1087941Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:292)
2021-12-13T02:25:17.1089000Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:782)
2021-12-13T02:25:17.1089933Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
2021-12-13T02:25:17.1090672Z Dec 13 02:25:17 E                   	at org.apache.beam.vendor.grpc.v1p26p0.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
2021-12-13T02:25:17.1091321Z Dec 13 02:25:17 E                   	... 3 more
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28010&view=logs&j=bdd9ea51-4de2-506a-d4d9-f3930e4d2355&t=dd50312f-73b5-56b5-c172-4d81d03e2ef1&l=24236"	FLINK	Open	3	1	5059	stale-assigned, test-stability
13476513	Optimize the Python Execution Mode Documentation	https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/python_execution_mode/	FLINK	Closed	3	4	5059	pull-request-available
13342888	Broken Link in deployment/ha/kubernetes_ha.zh.md	"When executing the script build_docs.sh, it will throw the following exception:
{code:java}
Liquid Exception: Could not find document 'deployment/resource-providers/standalone/kubernetes.md' in tag 'link'. Make sure the document exists and the path is correct. in deployment/ha/kubernetes_ha.zh.md Could not find document 'deployment/resource-providers/standalone/kubernetes.md' in tag 'link'.
{code}"	FLINK	Resolved	3	1	5059	pull-request-available
13243576	Remove legacy python docs	 Batch (DataSet API)/Transformations and Batch (DataSet API)/Zipping Elements have legacy flink python related documentation	FLINK	Closed	3	4	5059	pull-request-available
13346291	Port BatchExecPythonCalc and StreamExecPythonCalc to Java	"https://issues.apache.org/jira/browse/FLINK-20610 will separate the implementation of BatchExecCalc and StreamExecCalc, and port BatchExecCalc and StreamExecCalc to Java. 
{{StreamExecPythonCalc}} extends from {{CommonExecPythonCalc}} and {{CommonExecPythonCalc}} extends from {{CommonPythonBase}}, they are all Scala classes, and involves a lot of code. Java class can't extend Scala interface with default implementation. So I create an issue separately to port them to Java."	FLINK	Closed	3	7	5059	pull-request-available
13344509	Introduce PythonStreamGroupTableAggregateOperator	Adds PythonStreamGroupTableAggregateOperator to support running Python TableAggregateFunction	FLINK	Closed	3	7	5059	pull-request-available
13315070	Extract the implementation logic of Beam in Operations	Extract the implementation logic of Beam in Operations, so that the implementation of general operations and Beam operations can be decoupled	FLINK	Closed	3	4	5059	pull-request-available
13383723	java.lang.ClassCastException when using Python UDF	"Hi,

I'm trying to use Python UDF with logical condition as argument.

 
{code:java}
log = logging.getLogger()

@udf(result_type=DataTypes.BOOLEAN())
def trace(message, condition):
    if condition:
        log.warn(message)
    return condition
table_env.create_temporary_function('trace', trace)

table_env.execute_sql(""""""
CREATE TABLE datagen (
    n int
) WITH (
    'connector' = 'datagen',
    'number-of-rows' = '10'
)
"""""")

result = table_env.sql_query(""""""
SELECT * 
FROM datagen
WHERE trace(n, n < 0)
"""""")
for r in result.execute().collect():
    print(r){code}
 

As a result I'm getting exception:
{code:java}
Py4JJavaError: An error occurred while calling o135.execute.
: java.lang.ClassCastException: class org.apache.calcite.rex.RexInputRef cannot be cast to class org.apache.calcite.rex.RexCall (org.apache.calcite.rex.RexInputRef and org.apache.calcite.rex.RexCall are in unnamed module of loader 'app')
	at org.apache.flink.table.planner.plan.rules.logical.PythonMapMergeRule.matches(PythonMapMergeRule.java:70)
	at org.apache.calcite.plan.hep.HepPlanner.applyRule(HepPlanner.java:538)
	at org.apache.calcite.plan.hep.HepPlanner.applyRules(HepPlanner.java:407)
	at org.apache.calcite.plan.hep.HepPlanner.executeInstruction(HepPlanner.java:243)
	at org.apache.calcite.plan.hep.HepInstruction$RuleInstance.execute(HepInstruction.java:127)
	at org.apache.calcite.plan.hep.HepPlanner.executeProgram(HepPlanner.java:202)
	at org.apache.calcite.plan.hep.HepPlanner.findBestExp(HepPlanner.java:189)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepProgram.optimize(FlinkHepProgram.scala:69)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkHepRuleSetProgram.optimize(FlinkHepRuleSetProgram.scala:87)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
{code}
 "	FLINK	Resolved	2	1	5059	pull-request-available, stale-critical
13437142	Python EmbeddedThreadDependencyTests.test_add_python_file failed on azure	"
{code:java}
Mar 31 10:49:17 =================================== FAILURES ===================================
Mar 31 10:49:17 ______________ EmbeddedThreadDependencyTests.test_add_python_file ______________
Mar 31 10:49:17 
Mar 31 10:49:17 self = <pyflink.table.tests.test_dependency.EmbeddedThreadDependencyTests testMethod=test_add_python_file>
Mar 31 10:49:17 
Mar 31 10:49:17     def test_add_python_file(self):
Mar 31 10:49:17         python_file_dir = os.path.join(self.tempdir, ""python_file_dir_"" + str(uuid.uuid4()))
Mar 31 10:49:17         os.mkdir(python_file_dir)
Mar 31 10:49:17         python_file_path = os.path.join(python_file_dir, ""test_dependency_manage_lib.py"")
Mar 31 10:49:17         with open(python_file_path, 'w') as f:
Mar 31 10:49:17             f.write(""def add_two(a):\n    raise Exception('This function should not be called!')"")
Mar 31 10:49:17         self.t_env.add_python_file(python_file_path)
Mar 31 10:49:17     
Mar 31 10:49:17         python_file_dir_with_higher_priority = os.path.join(
Mar 31 10:49:17             self.tempdir, ""python_file_dir_"" + str(uuid.uuid4()))
Mar 31 10:49:17         os.mkdir(python_file_dir_with_higher_priority)
Mar 31 10:49:17         python_file_path_higher_priority = os.path.join(python_file_dir_with_higher_priority,
Mar 31 10:49:17                                                         ""test_dependency_manage_lib.py"")
Mar 31 10:49:17         with open(python_file_path_higher_priority, 'w') as f:
Mar 31 10:49:17             f.write(""def add_two(a):\n    return a + 2"")
Mar 31 10:49:17         self.t_env.add_python_file(python_file_path_higher_priority)
Mar 31 10:49:17     
Mar 31 10:49:17         def plus_two(i):
Mar 31 10:49:17             from test_dependency_manage_lib import add_two
Mar 31 10:49:17             return add_two(i)
Mar 31 10:49:17     
Mar 31 10:49:17         self.t_env.create_temporary_system_function(
Mar 31 10:49:17             ""add_two"", udf(plus_two, DataTypes.BIGINT(), DataTypes.BIGINT()))
Mar 31 10:49:17         table_sink = source_sink_utils.TestAppendSink(
Mar 31 10:49:17             ['a', 'b'], [DataTypes.BIGINT(), DataTypes.BIGINT()])
Mar 31 10:49:17         self.t_env.register_table_sink(""Results"", table_sink)
Mar 31 10:49:17         t = self.t_env.from_elements([(1, 2), (2, 5), (3, 1)], ['a', 'b'])
Mar 31 10:49:17 >       t.select(expr.call(""add_two"", t.a), t.a).execute_insert(""Results"").wait()
Mar 31 10:49:17 
Mar 31 10:49:17 pyflink/table/tests/test_dependency.py:63: 
Mar 31 10:49:17 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 31 10:49:17 pyflink/table/table_result.py:76: in wait
Mar 31 10:49:17     get_method(self._j_table_result, ""await"")()
Mar 31 10:49:17 .tox/py38-cython/lib/python3.8/site-packages/py4j/java_gateway.py:1321: in __call__

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34001&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=27239"	FLINK	Open	2	1	5059	auto-deprioritized-major, stale-assigned, test-stability
13255641	Add validation check for places where Python ScalarFunction cannot be used	"Currently, there are places where Python ScalarFunction could not be used, for example:
 # Python UDF could not be used in MatchRecognize
 # Python UDFs could not be used in Join condition which take the columns from both the left table and the right table as inputs

We should add validation check for places where it’s not supported."	FLINK	Closed	3	7	5059	pull-request-available
13483752	ProcessDataStreamStreamingTests.test_process_function unstable	"
{code:java}
2022-09-29T02:10:45.3571648Z Sep 29 02:10:45 self = <pyflink.datastream.tests.test_data_stream.ProcessDataStreamStreamingTests testMethod=test_process_function>
2022-09-29T02:10:45.3572279Z Sep 29 02:10:45 
2022-09-29T02:10:45.3572810Z Sep 29 02:10:45     def test_process_function(self):
2022-09-29T02:10:45.3573495Z Sep 29 02:10:45         self.env.set_parallelism(1)
2022-09-29T02:10:45.3574148Z Sep 29 02:10:45         self.env.get_config().set_auto_watermark_interval(2000)
2022-09-29T02:10:45.3580634Z Sep 29 02:10:45         self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
2022-09-29T02:10:45.3583194Z Sep 29 02:10:45         data_stream = self.env.from_collection([(1, '1603708211000'),
2022-09-29T02:10:45.3584515Z Sep 29 02:10:45                                                 (2, '1603708224000'),
2022-09-29T02:10:45.3585957Z Sep 29 02:10:45                                                 (3, '1603708226000'),
2022-09-29T02:10:45.3587132Z Sep 29 02:10:45                                                 (4, '1603708289000')],
2022-09-29T02:10:45.3588094Z Sep 29 02:10:45                                                type_info=Types.ROW([Types.INT(), Types.STRING()]))
2022-09-29T02:10:45.3589090Z Sep 29 02:10:45     
2022-09-29T02:10:45.3589949Z Sep 29 02:10:45         class MyProcessFunction(ProcessFunction):
2022-09-29T02:10:45.3590710Z Sep 29 02:10:45     
2022-09-29T02:10:45.3591856Z Sep 29 02:10:45             def process_element(self, value, ctx):
2022-09-29T02:10:45.3592873Z Sep 29 02:10:45                 current_timestamp = ctx.timestamp()
2022-09-29T02:10:45.3593862Z Sep 29 02:10:45                 current_watermark = ctx.timer_service().current_watermark()
2022-09-29T02:10:45.3594915Z Sep 29 02:10:45                 yield ""current timestamp: {}, current watermark: {}, current_value: {}""\
2022-09-29T02:10:45.3596201Z Sep 29 02:10:45                     .format(str(current_timestamp), str(current_watermark), str(value))
2022-09-29T02:10:45.3597089Z Sep 29 02:10:45     
2022-09-29T02:10:45.3597942Z Sep 29 02:10:45         watermark_strategy = WatermarkStrategy.for_monotonous_timestamps()\
2022-09-29T02:10:45.3599260Z Sep 29 02:10:45             .with_timestamp_assigner(SecondColumnTimestampAssigner())
2022-09-29T02:10:45.3600611Z Sep 29 02:10:45         data_stream.assign_timestamps_and_watermarks(watermark_strategy)\
2022-09-29T02:10:45.3601877Z Sep 29 02:10:45             .process(MyProcessFunction(), output_type=Types.STRING()).add_sink(self.test_sink)
2022-09-29T02:10:45.3603527Z Sep 29 02:10:45         self.env.execute('test process function')
2022-09-29T02:10:45.3604445Z Sep 29 02:10:45         results = self.test_sink.get_results()
2022-09-29T02:10:45.3605684Z Sep 29 02:10:45         expected = [""current timestamp: 1603708211000, current watermark: ""
2022-09-29T02:10:45.3607157Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=1, f1='1603708211000')"",
2022-09-29T02:10:45.3608256Z Sep 29 02:10:45                     ""current timestamp: 1603708224000, current watermark: ""
2022-09-29T02:10:45.3609650Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=2, f1='1603708224000')"",
2022-09-29T02:10:45.3610854Z Sep 29 02:10:45                     ""current timestamp: 1603708226000, current watermark: ""
2022-09-29T02:10:45.3612279Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=3, f1='1603708226000')"",
2022-09-29T02:10:45.3613382Z Sep 29 02:10:45                     ""current timestamp: 1603708289000, current watermark: ""
2022-09-29T02:10:45.3615683Z Sep 29 02:10:45                     ""-9223372036854775808, current_value: Row(f0=4, f1='1603708289000')""]
2022-09-29T02:10:45.3617687Z Sep 29 02:10:45 >       self.assert_equals_sorted(expected, results)
2022-09-29T02:10:45.3618620Z Sep 29 02:10:45 
2022-09-29T02:10:45.3619425Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:986: 
2022-09-29T02:10:45.3620424Z Sep 29 02:10:45 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-09-29T02:10:45.3621886Z Sep 29 02:10:45 pyflink/datastream/tests/test_data_stream.py:66: in assert_equals_sorted
2022-09-29T02:10:45.3622847Z Sep 29 02:10:45     self.assertEqual(expected, actual)
2022-09-29T02:10:45.3624658Z Sep 29 02:10:45 E   AssertionError: Lists differ: [""cur[414 chars]ark: -9223372036854775808, current_value: Row([22 chars]0')""] != [""cur[414 chars]ark: 1603708225999, current_value: Row(f0=4, f[15 chars]0')""]
2022-09-29T02:10:45.3625881Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3626591Z Sep 29 02:10:45 E   First differing element 3:
2022-09-29T02:10:45.3627726Z Sep 29 02:10:45 E   ""curr[44 chars]ark: -9223372036854775808, current_value: Row([21 chars]00')""
2022-09-29T02:10:45.3628758Z Sep 29 02:10:45 E   ""curr[44 chars]ark: 1603708225999, current_value: Row(f0=4, f[14 chars]00')""
2022-09-29T02:10:45.3629276Z Sep 29 02:10:45 E   
2022-09-29T02:10:45.3629842Z Sep 29 02:10:45 E   Diff is 753 characters long. Set self.maxDiff to None to see it.
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=41436&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901"	FLINK	Closed	2	1	5059	pull-request-available, test-stability
13375204	Support `print` to print logs in PyFlink	"Currently, if users want to print logs, they need to use logging module.

{code:python}
@udf(result_type=DataTypes.BIGINT())
def add(i, j):
    import logging
    logging.info(""debug"")
    return i + j
{code}
It will be more convenient to use `print` to print logs.

{code: python}
@udf(result_type=DataTypes.BIGINT())
def add(i, j):
    print(""debug"")
    return i + j
{code}
"	FLINK	Resolved	3	2	5059	pull-request-available
13398666	state TTL might not take effect for pyflink	"Since pyflink has its own data cache on python side, it might still read the data from python side even TTL has expired.

Scripts below could reproduce this:
{code:python}
from pyflink.common.time import Time
from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig, ListStateDescriptor, MapStateDescriptor
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, RuntimeContext, KeyedProcessFunction, \
    EmbeddedRocksDBStateBackend
import time
from datetime import datetime

def test_keyed_process_function_with_state():
    env = StreamExecutionEnvironment.get_execution_environment()
    env.get_config().set_auto_watermark_interval(2000)
    env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
    env.set_state_backend(EmbeddedRocksDBStateBackend())
    data_stream = env.from_collection([(1, 'hi', '1603708211000'),
                                            (3, 'hi', '1603708226000'),
                                            (10, 'hi', '1603708226000'),
                                            (6, 'hello', '1603708293000')],
                                           type_info=Types.ROW([Types.INT(), Types.STRING(),
                                                                Types.STRING()]))


    class MyProcessFunction(KeyedProcessFunction):

        def __init__(self):
            self.value_state = None
            self.list_state = None
            self.map_state = None

        def open(self, runtime_context: RuntimeContext):
            state_ttl_config = StateTtlConfig \
                .new_builder(Time.seconds(1)) \
                .set_update_type(StateTtlConfig.UpdateType.OnCreateAndWrite) \
                .never_return_expired() \
                .build()
            value_state_descriptor = ValueStateDescriptor('value_state', Types.INT())
            value_state_descriptor.enable_time_to_live(state_ttl_config)
            self.value_state = runtime_context.get_state(value_state_descriptor)
            list_state_descriptor = ListStateDescriptor('list_state', Types.INT())
            list_state_descriptor.enable_time_to_live(state_ttl_config)
            self.list_state = runtime_context.get_list_state(list_state_descriptor)
            map_state_descriptor = MapStateDescriptor('map_state', Types.INT(), Types.STRING())
            map_state_descriptor.enable_time_to_live(state_ttl_config)
            self.map_state = runtime_context.get_map_state(map_state_descriptor)

        def process_element(self, value, ctx):
            time.sleep(20)
            current_value = self.value_state.value()
            self.value_state.update(value[0])
            current_list = [_ for _ in self.list_state.get()]
            self.list_state.add(value[0])
            map_entries_string = []
            for k, v in self.map_state.items():
                map_entries_string.append(str(k) + ': ' + str(v))
            map_entries_string = '{' + ', '.join(map_entries_string) + '}'
            self.map_state.put(value[0], value[1])
            current_key = ctx.get_current_key()
            yield ""time: {}, current key: {}, current value state: {}, current list state: {}, "" \
                  ""current map state: {}, current value: {}"".format(str(datetime.now().time()),
                                                                    str(current_key),
                                                                    str(current_value),
                                                                    str(current_list),
                                                                    map_entries_string,
                                                                    str(value))

        def on_timer(self, timestamp, ctx):
            pass


    data_stream.key_by(lambda x: x[1], key_type=Types.STRING()) \
        .process(MyProcessFunction(), output_type=Types.STRING()) \
        .print()
    env.execute('test time stamp assigner with keyed process function')

if __name__ == '__main__':
    test_keyed_process_function_with_state()
{code}


"	FLINK	Resolved	1	7	5059	pull-request-available
13309321	Performance tests for PyFlink UDFs	"Check items as follows:
 * Performance tests for Python UDF
 * Performance tests for Pandas UDF"	FLINK	Closed	1	4	5059	release-testing
13298017	Adds Pipeline of Building Wheel Packages in Azure CI	Adds Pipeline of Building Wheel Packages in Azure CI.	FLINK	Resolved	3	7	5059	pull-request-available
13290730	Use type hints to declare the signature of the methods	[Type Hints|https://www.python.org/dev/peps/pep-0484/] was introduced in Python 3.5 and it would be great if we can declare the signature of the methods using type hints and introduce [type check|https://realpython.com/python-type-checking/] in the python APIs	FLINK	Closed	3	4	5059	pull-request-available
13287121	Optimize the output emit logic to remove unnecessary overhead	We need to optimize the function call chains in process_outputs to improve the performance in Python UDF	FLINK	Closed	3	7	5059	pull-request-available
13355390	Introduce TimeWindow and CountWindow in PyFlink	Introduce TimeWindow, CountWindow and corresponding coders.	FLINK	Resolved	3	7	5059	pull-request-available
13462753	'Run kubernetes pyflink application test' fails while pulling image	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=37103&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=070ff179-953e-5bda-71fa-d6599415701c&l=6592

{code:java}
Jun 23 10:40:35 Flink logs:
Error from server (BadRequest): container ""flink-main-container"" in pod ""flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm"" is waiting to start: image can't be pulled
Jun 23 10:40:35 deployment.apps ""flink-native-k8s-pyflink-application-1"" deleted
Jun 23 10:40:35 clusterrolebinding.rbac.authorization.k8s.io ""flink-role-binding-default"" deleted
Jun 23 10:40:36 pod/flink-native-k8s-pyflink-application-1-5d87889db9-rm8mm condition met
Jun 23 10:40:36 Stopping minikube ...
Jun 23 10:40:36 * Stopping node ""minikube""  ...
Jun 23 10:40:46 * 1 node stopped.
Jun 23 10:40:46 [FAIL] Test script contains errors.
Jun 23 10:40:46 Checking for errors...
Jun 23 10:40:46 No errors in log files.
Jun 23 10:40:46 Checking for exceptions...
Jun 23 10:40:46 No exceptions in log files.
Jun 23 10:40:46 Checking for non-empty .out files...
grep: /home/vsts/work/_temp/debug_files/flink-logs/*.out: No such file or directory
Jun 23 10:40:46 No non-empty .out files.
Jun 23 10:40:46 

{code}"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13284305	Adjust the default value of bundle size and bundle time	"Currently the default value for ""python.fn-execution.bundle.size"" is 1000 and the default value for ""python.fn-execution.bundle.time"" is 1000ms. We should try to find out a meaningful default value which works best in most scenarios."	FLINK	Closed	3	4	5059	pull-request-available
13301469	Support Python UDTF in blink planner under batch mode	Currently, Python UDTF has been supported under flink planner and blink planner(only stream). This jira dedicates to add Python UDTF support for blink planner under batch mode.	FLINK	Closed	3	7	5059	pull-request-available
13284178	Introduce Python Physical Correlate RelNodes  which are containers for Python TableFunction	Dedicated Python Physical Correlate RelNodes should be introduced for Python TableFunction execution. These nodes exists as containers for Python TableFunctions which could be executed in a batch and then we can employ PythonTableFunctionOperator for Python TableFunction execution.	FLINK	Closed	3	7	5059	pull-request-available
13319183	The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct	The links of the connector sql jar of Kafka 0.10 and 0.11 are extinct. I will fix it as soon as possible.	FLINK	Closed	3	1	5059	pull-request-available
13414660	Add many kinds of checks in ML Python API	Add many kinds of checks in ML Python API. These checks include pytest, flask8 and mypy.	FLINK	Resolved	3	7	5059	pull-request-available
13284697	Add rules to transpose the  join condition of Python Correlate node	Because the conditions can’t be executed in Python correlate node, this rule will transpose the conditions after the Python correlate node if the join type is inner join.	FLINK	Closed	3	7	5059	pull-request-available
13412212	Python test_data_stream.py hangs on azure	"{code:java}
Nov 17 04:36:57 pyflink/datastream/tests/test_data_stream.py ........................... [ 33%]
Nov 17 04:52:58 ....................==============================================================================
Nov 17 04:52:58 Process produced no output for 900 seconds.
Nov 17 04:52:58 ==============================================================================
Nov 17 04:52:58 ==============================================================================
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26627&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295"	FLINK	Closed	3	1	5059	test-stability
13362524	Support Python UDAF in Tumbling Window	Support Python UDAF in Tumbling Window	FLINK	Resolved	3	7	5059	pull-request-available
13255640	Add rules to push down the Python ScalarFunctions contained in the join condition of Correlate node	The Python ScalarFunctions contained in the join condition of Correlate node should be extracted to make sure the TableFunction works well.	FLINK	Closed	3	7	5059	pull-request-available
13427404	PyFlinkEmbeddedSubInterpreterTests. test_udf_without_arguments failed on azure	"
{code:java}
2022-02-08T02:55:16.0701246Z Feb 08 02:55:16 =================================== FAILURES ===================================
2022-02-08T02:55:16.0702483Z Feb 08 02:55:16 ________ PyFlinkEmbeddedSubInterpreterTests.test_udf_without_arguments _________
2022-02-08T02:55:16.0703190Z Feb 08 02:55:16 
2022-02-08T02:55:16.0703959Z Feb 08 02:55:16 self = <pyflink.table.tests.test_udf.PyFlinkEmbeddedSubInterpreterTests testMethod=test_udf_without_arguments>
2022-02-08T02:55:16.0704967Z Feb 08 02:55:16 
2022-02-08T02:55:16.0705639Z Feb 08 02:55:16     def test_udf_without_arguments(self):
2022-02-08T02:55:16.0706641Z Feb 08 02:55:16         one = udf(lambda: 1, result_type=DataTypes.BIGINT(), deterministic=True)
2022-02-08T02:55:16.0707595Z Feb 08 02:55:16         two = udf(lambda: 2, result_type=DataTypes.BIGINT(), deterministic=False)
2022-02-08T02:55:16.0713079Z Feb 08 02:55:16     
2022-02-08T02:55:16.0714866Z Feb 08 02:55:16         table_sink = source_sink_utils.TestAppendSink(['a', 'b'],
2022-02-08T02:55:16.0716495Z Feb 08 02:55:16                                                       [DataTypes.BIGINT(), DataTypes.BIGINT()])
2022-02-08T02:55:16.0717411Z Feb 08 02:55:16         self.t_env.register_table_sink(""Results"", table_sink)
2022-02-08T02:55:16.0718059Z Feb 08 02:55:16     
2022-02-08T02:55:16.0719148Z Feb 08 02:55:16         t = self.t_env.from_elements([(1, 2), (2, 5), (3, 1)], ['a', 'b'])
2022-02-08T02:55:16.0719974Z Feb 08 02:55:16 >       t.select(one(), two()).execute_insert(""Results"").wait()
2022-02-08T02:55:16.0720697Z Feb 08 02:55:16 
2022-02-08T02:55:16.0721294Z Feb 08 02:55:16 pyflink/table/tests/test_udf.py:252: 
2022-02-08T02:55:16.0722119Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0722943Z Feb 08 02:55:16 pyflink/table/table_result.py:76: in wait
2022-02-08T02:55:16.0723686Z Feb 08 02:55:16     get_method(self._j_table_result, ""await"")()
2022-02-08T02:55:16.0725024Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1322: in __call__
2022-02-08T02:55:16.0726044Z Feb 08 02:55:16     answer, self.gateway_client, self.target_id, self.name)
2022-02-08T02:55:16.0726824Z Feb 08 02:55:16 pyflink/util/exceptions.py:146: in deco
2022-02-08T02:55:16.0727569Z Feb 08 02:55:16     return f(*a, **kw)
2022-02-08T02:55:16.0728326Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0728995Z Feb 08 02:55:16 
2022-02-08T02:55:16.0729717Z Feb 08 02:55:16 answer = 'x'
2022-02-08T02:55:16.0730447Z Feb 08 02:55:16 gateway_client = <py4j.java_gateway.GatewayClient object at 0x7fadb5dc97f0>
2022-02-08T02:55:16.0731465Z Feb 08 02:55:16 target_id = 'o26503', name = 'await'
2022-02-08T02:55:16.0732045Z Feb 08 02:55:16 
2022-02-08T02:55:16.0732763Z Feb 08 02:55:16     def get_return_value(answer, gateway_client, target_id=None, name=None):
2022-02-08T02:55:16.0733699Z Feb 08 02:55:16         """"""Converts an answer received from the Java gateway into a Python object.
2022-02-08T02:55:16.0734508Z Feb 08 02:55:16     
2022-02-08T02:55:16.0735205Z Feb 08 02:55:16         For example, string representation of integers are converted to Python
2022-02-08T02:55:16.0736228Z Feb 08 02:55:16         integer, string representation of objects are converted to JavaObject
2022-02-08T02:55:16.0736974Z Feb 08 02:55:16         instances, etc.
2022-02-08T02:55:16.0737508Z Feb 08 02:55:16     
2022-02-08T02:55:16.0738185Z Feb 08 02:55:16         :param answer: the string returned by the Java gateway
2022-02-08T02:55:16.0739074Z Feb 08 02:55:16         :param gateway_client: the gateway client used to communicate with the Java
2022-02-08T02:55:16.0739994Z Feb 08 02:55:16             Gateway. Only necessary if the answer is a reference (e.g., object,
2022-02-08T02:55:16.0740723Z Feb 08 02:55:16             list, map)
2022-02-08T02:55:16.0741491Z Feb 08 02:55:16         :param target_id: the name of the object from which the answer comes from
2022-02-08T02:55:16.0742350Z Feb 08 02:55:16             (e.g., *object1* in `object1.hello()`). Optional.
2022-02-08T02:55:16.0743175Z Feb 08 02:55:16         :param name: the name of the member from which the answer comes from
2022-02-08T02:55:16.0744315Z Feb 08 02:55:16             (e.g., *hello* in `object1.hello()`). Optional.
2022-02-08T02:55:16.0744973Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0745608Z Feb 08 02:55:16         if is_error(answer)[0]:
2022-02-08T02:55:16.0746484Z Feb 08 02:55:16             if len(answer) > 1:
2022-02-08T02:55:16.0747162Z Feb 08 02:55:16                 type = answer[1]
2022-02-08T02:55:16.0747943Z Feb 08 02:55:16                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2022-02-08T02:55:16.0748809Z Feb 08 02:55:16                 if answer[1] == REFERENCE_TYPE:
2022-02-08T02:55:16.0749555Z Feb 08 02:55:16                     raise Py4JJavaError(
2022-02-08T02:55:16.0750313Z Feb 08 02:55:16                         ""An error occurred while calling {0}{1}{2}.\n"".
2022-02-08T02:55:16.0751291Z Feb 08 02:55:16                         format(target_id, ""."", name), value)
2022-02-08T02:55:16.0752004Z Feb 08 02:55:16                 else:
2022-02-08T02:55:16.0752642Z Feb 08 02:55:16                     raise Py4JError(
2022-02-08T02:55:16.0753484Z Feb 08 02:55:16                         ""An error occurred while calling {0}{1}{2}. Trace:\n{3}\n"".
2022-02-08T02:55:16.0754479Z Feb 08 02:55:16                         format(target_id, ""."", name, value))
2022-02-08T02:55:16.0755171Z Feb 08 02:55:16             else:
2022-02-08T02:55:16.0755914Z Feb 08 02:55:16                 raise Py4JError(
2022-02-08T02:55:16.0756575Z Feb 08 02:55:16                     ""An error occurred while calling {0}{1}{2}"".
2022-02-08T02:55:16.0757325Z Feb 08 02:55:16 >                   format(target_id, ""."", name))
2022-02-08T02:55:16.0758178Z Feb 08 02:55:16 E               py4j.protocol.Py4JError: An error occurred while calling o26503.await
2022-02-08T02:55:16.0758918Z Feb 08 02:55:16 
2022-02-08T02:55:16.0760185Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/protocol.py:336: Py4JError
2022-02-08T02:55:16.0761380Z Feb 08 02:55:16 ----------------------------- Captured stdout call -----------------------------
2022-02-08T02:55:16.0762060Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0762752Z Feb 08 02:55:16 # A fatal error has been detected by the Java Runtime Environment:
2022-02-08T02:55:16.0763425Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0764075Z Feb 08 02:55:16 #  SIGSEGV (0xb) at pc=0x00007f417f38bc06, pid=19997, tid=0x00007f4159d73700
2022-02-08T02:55:16.0764867Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0766133Z Feb 08 02:55:16 # JRE version: OpenJDK Runtime Environment (8.0_292-b10) (build 1.8.0_292-8u292-b10-0ubuntu1~16.04.1-b10)
2022-02-08T02:55:16.0767491Z Feb 08 02:55:16 # Java VM: OpenJDK 64-Bit Server VM (25.292-b10 mixed mode linux-amd64 compressed oops)
2022-02-08T02:55:16.0768305Z Feb 08 02:55:16 # Problematic frame:
2022-02-08T02:55:16.0769026Z Feb 08 02:55:16 # C  [libpython3.7m.so.1.0+0x1f5c06]  _PyObject_Malloc+0x76
2022-02-08T02:55:16.0769662Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0770699Z Feb 08 02:55:16 # Core dump written. Default location: /__w/2/s/flink-python/core or core.19997
2022-02-08T02:55:16.0771460Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0772156Z Feb 08 02:55:16 # An error report file with more information is saved as:
2022-02-08T02:55:16.0773210Z Feb 08 02:55:16 # /__w/2/s/flink-python/hs_err_pid19997.log
2022-02-08T02:55:16.0773840Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0774570Z Feb 08 02:55:16 # If you would like to submit a bug report, please visit:
2022-02-08T02:55:16.0775367Z Feb 08 02:55:16 #   http://bugreport.java.com/bugreport/crash.jsp
2022-02-08T02:55:16.0776264Z Feb 08 02:55:16 # The crash happened outside the Java Virtual Machine in native code.
2022-02-08T02:55:16.0777074Z Feb 08 02:55:16 # See problematic frame for where to report the bug.
2022-02-08T02:55:16.0777677Z Feb 08 02:55:16 #
2022-02-08T02:55:16.0778665Z Feb 08 02:55:16 ------------------------------ Captured log call -------------------------------
2022-02-08T02:55:16.0779519Z Feb 08 02:55:16 ERROR    root:java_gateway.py:1056 Exception while sending command.
2022-02-08T02:55:16.0780512Z Feb 08 02:55:16 Traceback (most recent call last):
2022-02-08T02:55:16.0781811Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1224, in send_command
2022-02-08T02:55:16.0782801Z Feb 08 02:55:16     raise Py4JNetworkError(""Answer from Java side is empty"")
2022-02-08T02:55:16.0783648Z Feb 08 02:55:16 py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2022-02-08T02:55:16.0784455Z Feb 08 02:55:16 
2022-02-08T02:55:16.0785200Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0786054Z Feb 08 02:55:16 
2022-02-08T02:55:16.0786654Z Feb 08 02:55:16 Traceback (most recent call last):
2022-02-08T02:55:16.0787982Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1038, in send_command
2022-02-08T02:55:16.0789321Z Feb 08 02:55:16     response = connection.send_command(command)
2022-02-08T02:55:16.0790649Z Feb 08 02:55:16   File ""/__w/2/s/flink-python/.tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1229, in send_command
2022-02-08T02:55:16.0791618Z Feb 08 02:55:16     ""Error while receiving"", e, proto.ERROR_ON_RECEIVE)
2022-02-08T02:55:16.0792405Z Feb 08 02:55:16 py4j.protocol.Py4JNetworkError: Error while receiving
2022-02-08T02:55:16.0793242Z Feb 08 02:55:16 ______ PyFlinkStreamUserDefinedFunctionTests.test_execute_from_json_plan _______
2022-02-08T02:55:16.0793915Z Feb 08 02:55:16 
2022-02-08T02:55:16.0794707Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayClient object at 0x7fadb5dc97f0>
2022-02-08T02:55:16.0795354Z Feb 08 02:55:16 
2022-02-08T02:55:16.0796067Z Feb 08 02:55:16     def _get_connection(self):
2022-02-08T02:55:16.0796782Z Feb 08 02:55:16         if not self.is_connected:
2022-02-08T02:55:16.0797537Z Feb 08 02:55:16             raise Py4JNetworkError(""Gateway is not connected."")
2022-02-08T02:55:16.0798233Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0798882Z Feb 08 02:55:16 >           connection = self.deque.pop()
2022-02-08T02:55:16.0799589Z Feb 08 02:55:16 E           IndexError: pop from an empty deque
2022-02-08T02:55:16.0800212Z Feb 08 02:55:16 
2022-02-08T02:55:16.0801304Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:982: IndexError
2022-02-08T02:55:16.0802023Z Feb 08 02:55:16 
2022-02-08T02:55:16.0802720Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0803443Z Feb 08 02:55:16 
2022-02-08T02:55:16.0804235Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayConnection object at 0x7faecec6ecc0>
2022-02-08T02:55:16.0804911Z Feb 08 02:55:16 
2022-02-08T02:55:16.0805463Z Feb 08 02:55:16     def start(self):
2022-02-08T02:55:16.0806319Z Feb 08 02:55:16         """"""Starts the connection by connecting to the `address` and the `port`
2022-02-08T02:55:16.0807034Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0807593Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0808269Z Feb 08 02:55:16 >           self.socket.connect((self.address, self.port))
2022-02-08T02:55:16.0809116Z Feb 08 02:55:16 E           ConnectionRefusedError: [Errno 111] Connection refused
2022-02-08T02:55:16.0809804Z Feb 08 02:55:16 
2022-02-08T02:55:16.0810912Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1132: ConnectionRefusedError
2022-02-08T02:55:16.0811653Z Feb 08 02:55:16 
2022-02-08T02:55:16.0812378Z Feb 08 02:55:16 During handling of the above exception, another exception occurred:
2022-02-08T02:55:16.0813232Z Feb 08 02:55:16 pyflink/testing/test_case_utils.py:137: in setUp
2022-02-08T02:55:16.0814075Z Feb 08 02:55:16     self.t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
2022-02-08T02:55:16.0815068Z Feb 08 02:55:16 pyflink/table/environment_settings.py:267: in in_streaming_mode
2022-02-08T02:55:16.0816018Z Feb 08 02:55:16     get_gateway().jvm.EnvironmentSettings.inStreamingMode())
2022-02-08T02:55:16.0826059Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1712: in __getattr__
2022-02-08T02:55:16.0826592Z Feb 08 02:55:16     ""\n"" + proto.END_COMMAND_PART)
2022-02-08T02:55:16.0827339Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:1036: in send_command
2022-02-08T02:55:16.0827853Z Feb 08 02:55:16     connection = self._get_connection()
2022-02-08T02:55:16.0828568Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:984: in _get_connection
2022-02-08T02:55:16.0829089Z Feb 08 02:55:16     connection = self._create_connection()
2022-02-08T02:55:16.0830171Z Feb 08 02:55:16 .tox/py37-cython/lib/python3.7/site-packages/py4j/java_gateway.py:990: in _create_connection
2022-02-08T02:55:16.0830673Z Feb 08 02:55:16     connection.start()
2022-02-08T02:55:16.0831220Z Feb 08 02:55:16 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2022-02-08T02:55:16.0831828Z Feb 08 02:55:16 
2022-02-08T02:55:16.0832255Z Feb 08 02:55:16 self = <py4j.java_gateway.GatewayConnection object at 0x7faecec6ecc0>
2022-02-08T02:55:16.0832661Z Feb 08 02:55:16 
2022-02-08T02:55:16.0833006Z Feb 08 02:55:16     def start(self):
2022-02-08T02:55:16.0833469Z Feb 08 02:55:16         """"""Starts the connection by connecting to the `address` and the `port`
2022-02-08T02:55:16.0833921Z Feb 08 02:55:16         """"""
2022-02-08T02:55:16.0834369Z Feb 08 02:55:16         try:
2022-02-08T02:55:16.0834798Z Feb 08 02:55:16             self.socket.connect((self.address, self.port))
2022-02-08T02:55:16.0835281Z Feb 08 02:55:16             self.stream = self.socket.makefile(""rb"")
2022-02-08T02:55:16.0835988Z Feb 08 02:55:16             self.is_connected = True
2022-02-08T02:55:16.0836367Z Feb 08 02:55:16     
2022-02-08T02:55:16.0836734Z Feb 08 02:55:16             self._authenticate_connection()
2022-02-08T02:55:16.0837200Z Feb 08 02:55:16         except Py4JAuthenticationError:
2022-02-08T02:55:16.0837722Z Feb 08 02:55:16             logger.exception(""Cannot authenticate with gateway server."")
2022-02-08T02:55:16.0838161Z Feb 08 02:55:16             raise
2022-02-08T02:55:16.0838559Z Feb 08 02:55:16         except Exception as e:
2022-02-08T02:55:16.0839045Z Feb 08 02:55:16             msg = ""An error occurred while trying to connect to the Java ""\
2022-02-08T02:55:16.0839682Z Feb 08 02:55:16                 ""server ({0}:{1})"".format(self.address, self.port)
2022-02-08T02:55:16.0840168Z Feb 08 02:55:16             logger.exception(msg)
2022-02-08T02:55:16.0840581Z Feb 08 02:55:16 >           raise Py4JNetworkError(msg, e)
2022-02-08T02:55:16.0841178Z Feb 08 02:55:16 E           py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:41141)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30878&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24930"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13476330	Optimize the Python DataStream Window Documentation	"https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/datastream/operators/windows/
"	FLINK	Resolved	3	4	5059	pull-request-available
13264789	UDF cannot be in the join condition in blink planner	"Currently, UDF cannot be in the join condition in blink planner, for the following example:

val util = batchTestUtil()
val left = util.addTableSource[(Int, Int, String)](""Table3"",'a, 'b, 'c)
val right = util.addTableSource[(Int, Int, Int, String, Long)](""Table5"", 'd, 'e, 'f, 'g, 'h)
util.addFunction(""Func"", Func0)
val result = left
 .leftOuterJoin(right, ""a === d && Func(a) === a + d"")
 .select(""a, d"")
util.verifyExplain(result)

The following exception will be thrown:

java.util.NoSuchElementException: No value presentjava.util.NoSuchElementException: No value present
 at java.util.Optional.get(Optional.java:135) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:475) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.lambda$visit$0(QueryOperationConverter.java:470) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:472) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.lambda$visit$0(QueryOperationConverter.java:470) at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:472) at org.apache.flink.table.planner.plan.QueryOperationConverter$JoinExpressionVisitor.visit(QueryOperationConverter.java:463) at org.apache.flink.table.expressions.CallExpression.accept(CallExpression.java:121) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:228) at org.apache.flink.table.planner.plan.QueryOperationConverter$SingleRelVisitor.visit(QueryOperationConverter.java:139) at org.apache.flink.table.operations.JoinQueryOperation.accept(JoinQueryOperation.java:128) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:136) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:116) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:61) at org.apache.flink.table.operations.JoinQueryOperation.accept(JoinQueryOperation.java:128) at org.apache.flink.table.planner.plan.QueryOperationConverter.lambda$defaultMethod$0(QueryOperationConverter.java:135) at java.util.Collections$SingletonList.forEach(Collections.java:4822) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:135) at org.apache.flink.table.planner.plan.QueryOperationConverter.defaultMethod(QueryOperationConverter.java:116) at org.apache.flink.table.operations.utils.QueryOperationDefaultVisitor.visit(QueryOperationDefaultVisitor.java:46) at org.apache.flink.table.operations.ProjectQueryOperation.accept(ProjectQueryOperation.java:75) at org.apache.flink.table.planner.calcite.FlinkRelBuilder.queryOperation(FlinkRelBuilder.scala:151) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$1.apply(BatchPlanner.scala:81) at org.apache.flink.table.planner.delegation.BatchPlanner$$anonfun$1.apply(BatchPlanner.scala:79) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.Iterator$class.foreach(Iterator.scala:891) at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.AbstractTraversable.map(Traversable.scala:104) at org.apache.flink.table.planner.delegation.BatchPlanner.explain(BatchPlanner.scala:79) at org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnvironmentImpl.java:296) at org.apache.flink.table.planner.utils.TableTestUtilBase.doVerifyExplain(TableTestBase.scala:404) at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExplain(TableTestBase.scala:304) at org.apache.flink.table.planner.utils.TableTestUtilBase.verifyExplain(TableTestBase.scala:301) at org.apache.flink.table.planner.plan.batch.table.validation.JoinValidationTest.testOuterJoinWithPythonFunctionInCondition(JoinValidationTest.scala:130) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)"	FLINK	Closed	3	1	5059	pull-request-available
13290688	Avoid codegen user-defined function for Python UDF	Currently we make use of codegen to generate PythonScalarFunction and PythonTableFunction, but it is unnecessary. We can directly create a static PythonScalarFunction and PythonTableFunction.	FLINK	Closed	3	4	5059	pull-request-available
13280640	Improve Python API Tutorial doc	Adds the content of preparing input data in the Python API Tutorial doc	FLINK	Closed	3	4	5059	pull-request-available
13474822	Cannot run PyFlink 1.16 on MacOS with M1 chip	"I have tested it with 2 m1 machines. i will reproduce the bug 100%.

1.m1 machine
macos bigsur 11.5.1 & jdk8 * & jdk11 & python 3.8 & python 3.9
1.m1 machine
macos monterey 12.1 & jdk8 * & jdk11 & python 3.8 & python 3.9

reproduce step:
1.python -m pip install -r flink-python/dev/dev-requirements.txt
2.cd flink-python; python setup.py sdist bdist_wheel; cd apache-flink-libraries; python setup.py sdist; cd ..;
3.python -m pip install apache-flink-libraries/dist/*.tar.gz
4.python -m pip install dist/*.whl

when run [word_count.py|https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/table_api_tutorial/] it will cause


{code:java}
<frozen importlib._bootstrap>:219: RuntimeWarning: apache_beam.coders.coder_impl.StreamCoderImpl size changed, may indicate binary incompatibility. Expected 24 from C header, got 32 from PyObject
Traceback (most recent call last):
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 129, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/chucheng/GitLab/pyflink-demo/table/streaming/word_count.py"", line 49, in word_count
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1637, in _open
    startup_loopback_server()
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/table/table_environment.py"", line 1628, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 44, in <module>
    from pyflink.fn_execution.beam import beam_sdk_worker_main  # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_sdk_worker_main.py"", line 21, in <module>
    import pyflink.fn_execution.beam.beam_operations # noqa # pylint: disable=unused-import
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_operations.py"", line 27, in <module>
    from pyflink.fn_execution.state_impl import RemoteKeyedStateBackend, RemoteOperatorStateBackend
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/state_impl.py"", line 33, in <module>
    from pyflink.fn_execution.beam.beam_coders import FlinkCoder
  File ""/Users/chucheng/venv/lib/python3.8/site-packages/pyflink/fn_execution/beam/beam_coders.py"", line 27, in <module>
    from pyflink.fn_execution.beam import beam_coder_impl_fast as beam_coder_impl
  File ""pyflink/fn_execution/beam/beam_coder_impl_fast.pyx"", line 1, in init pyflink.fn_execution.beam.beam_coder_impl_fast
KeyError: '__pyx_vtable__'
{code}


"	FLINK	Closed	3	1	5059	pull-request-available
13321213	Improve the Python documentation about sql	"Now, there are a few documentations are written for java/scala users, which is not convenient for python users to read.
We need to use a more pythonic description in docs."	FLINK	Closed	3	4	5059	pull-request-available
13316130	Build manylinux1 with better compatibility instead of manylinux2014 Python Wheel Packages	Build manylinux1 with better compatibility instead of manylinux2014 Python Wheel Packages	FLINK	Closed	3	4	5059	pull-request-available
13446366	CheckPubSubEmulatorTest.testPull failed with AssertionError	"
{code:java}
2022-05-23T10:08:12.4382583Z May 23 10:08:12 [ERROR] org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull  Time elapsed: 24.092 s  <<< FAILURE!
2022-05-23T10:08:12.4383262Z May 23 10:08:12 java.lang.AssertionError: expected:<1> but was:<0>
2022-05-23T10:08:12.4383939Z May 23 10:08:12 	at org.junit.Assert.fail(Assert.java:89)
2022-05-23T10:08:12.4384555Z May 23 10:08:12 	at org.junit.Assert.failNotEquals(Assert.java:835)
2022-05-23T10:08:12.4385199Z May 23 10:08:12 	at org.junit.Assert.assertEquals(Assert.java:647)
2022-05-23T10:08:12.4385829Z May 23 10:08:12 	at org.junit.Assert.assertEquals(Assert.java:633)
2022-05-23T10:08:12.4386614Z May 23 10:08:12 	at org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest.testPull(CheckPubSubEmulatorTest.java:78)
2022-05-23T10:08:12.4389064Z May 23 10:08:12 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-05-23T10:08:12.4389735Z May 23 10:08:12 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-05-23T10:08:12.4390436Z May 23 10:08:12 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-05-23T10:08:12.4391073Z May 23 10:08:12 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-05-23T10:08:12.4391708Z May 23 10:08:12 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-05-23T10:08:12.4392415Z May 23 10:08:12 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-05-23T10:08:12.4393116Z May 23 10:08:12 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-05-23T10:08:12.4393811Z May 23 10:08:12 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-05-23T10:08:12.4394491Z May 23 10:08:12 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-05-23T10:08:12.4395111Z May 23 10:08:12 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-05-23T10:08:12.4395718Z May 23 10:08:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:08:12.4396384Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-05-23T10:08:12.4397042Z May 23 10:08:12 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-05-23T10:08:12.4397697Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-05-23T10:08:12.4398392Z May 23 10:08:12 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-05-23T10:08:12.4399020Z May 23 10:08:12 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-05-23T10:08:12.4399622Z May 23 10:08:12 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-05-23T10:08:12.4400236Z May 23 10:08:12 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-05-23T10:08:12.4400851Z May 23 10:08:12 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-05-23T10:08:12.4401466Z May 23 10:08:12 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-05-23T10:08:12.4402292Z May 23 10:08:12 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2022-05-23T10:08:12.4402950Z May 23 10:08:12 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2022-05-23T10:08:12.4403713Z May 23 10:08:12 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2022-05-23T10:08:12.4404416Z May 23 10:08:12 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2022-05-23T10:08:12.4405003Z May 23 10:08:12 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-05-23T10:08:12.4405602Z May 23 10:08:12 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-05-23T10:08:12.4406166Z May 23 10:08:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-05-23T10:08:12.4406792Z May 23 10:08:12 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-05-23T10:08:12.4407415Z May 23 10:08:12 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-05-23T10:08:12.4408134Z May 23 10:08:12 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-05-23T10:08:12.4408839Z May 23 10:08:12 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-05-23T10:08:12.4409588Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-05-23T10:08:12.4410406Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-05-23T10:08:12.4411234Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-05-23T10:08:12.4412084Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-05-23T10:08:12.4412930Z May 23 10:08:12 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-05-23T10:08:12.4413733Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-05-23T10:08:12.4414435Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-05-23T10:08:12.4415214Z May 23 10:08:12 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-05-23T10:08:12.4416039Z May 23 10:08:12 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-05-23T10:08:12.4416826Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-05-23T10:08:12.4417633Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-05-23T10:08:12.4418426Z May 23 10:08:12 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:128)
2022-05-23T10:08:12.4419181Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-05-23T10:08:12.4419878Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-05-23T10:08:12.4420538Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-05-23T10:08:12.4421196Z May 23 10:08:12 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
2022-05-23T10:08:12.4421667Z May 23 10:08:12 
2022-05-23T10:08:12.8012357Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8014745Z May 23 10:08:12 [INFO] Results:
2022-05-23T10:08:12.8015399Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8015938Z May 23 10:08:12 [ERROR] Failures: 
2022-05-23T10:08:12.8016703Z May 23 10:08:12 [ERROR]   CheckPubSubEmulatorTest.testPull:78 expected:<1> but was:<0>
2022-05-23T10:08:12.8017540Z May 23 10:08:12 [INFO] 
2022-05-23T10:08:12.8018140Z May 23 10:08:12 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=35954&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a
"	FLINK	Closed	3	1	5059	stale-assigned, test-stability
13363797	Add apache-flink-libraries module	Since release-1.11, pyflink has introduced cython and provided corresponding wheel packages for different platforms and multiple Python versions. Due to the large size of each wheel package, the entire project space in PyPI has grown very fast, and we need to frequently apply to PyPI for more project space. Please refer to [https://github.com/pypa/pypi-support/issues/831] for more details. The reason why each wheel package is so big is that each wheel package packs several jar packages such as `flink-dist` into it. We'd like to extract these the jar packages into another python project [apache-flink-libraries|https://pypi.org/project/apache-flink-libraries], and then let [apache-flink|https://pypi.org/project/apache-flink/] depends on [apache-flink-libraries|https://pypi.org/project/apache-flink-libraries]. As apache-flink-libraries only contains jar files and so there is only one package for each version. We still need to release multiple wheel packages of  [apache-flink|https://pypi.org/project/apache-flink/]. However, the size will be very small as it doesn't contain jar files any more.	FLINK	Resolved	3	2	5059	pull-request-available
13303561	Create a dedicated Python directory in release directory to place Python-related source and binary packages	"We introduced cross platform wheel packages in 1.11.
It is confused that we put all wheel packages and the corresponding signature verification files in the root directory of the release.
So we plan to put python-related files in a dedicated sub directory in the release directory."	FLINK	Closed	3	4	5059	pull-request-available
13298014	Add Cython support for primitive data types	Support Primitive DataTypes in Cython	FLINK	Closed	3	7	5059	pull-request-available
13263915	Support primitive data types in Python user-defined functions	This jira is a sub-task of FLINK-14388. In this jira, only primitive types are dedicated to be supported for python UDF. 	FLINK	Closed	3	2	5059	pull-request-available
13346697	Fix the deserialized Row losing the field_name information in PyFlink	"Now, the deserialized Row loses the field_name information.
{code:java}
@udf(result_type=DataTypes.STRING())
def get_string_element(my_list):
    my_string = 'xxx'
    for element in my_list:
        if element.integer_element == 2:  # element lost the field_name information
            my_string = element.string_element
    return my_string

t = t_env.from_elements(
    [(""1"", [Row(3, ""flink"")]), (""3"", [Row(2, ""pyflink"")]), (""2"", [Row(2, ""python"")])],
    DataTypes.ROW(
        [DataTypes.FIELD(""Key"", DataTypes.STRING()),
         DataTypes.FIELD(""List_element"",
                         DataTypes.ARRAY(DataTypes.ROW(
                             [DataTypes.FIELD(""integer_element"", DataTypes.INT()),
                              DataTypes.FIELD(""string_element"", DataTypes.STRING())])))]))
print(t.select(get_string_element(t.List_element)).to_pandas())
{code}
element lost the field_name information"	FLINK	Closed	3	1	5059	pull-request-available
13391803	python tests hang on Azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20898&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=4fad9527-b9a5-5015-1b70-8356e5c91490&l=22829
"	FLINK	Closed	1	1	5059	pull-request-available, test-stability
13342030	Error happens in TaskExecutor when closing JobMaster connection if there was a python UDF	"When a TaskExecutor successfully finished running a python UDF task and disconnecting from JobMaster, errors below will happen. This error, however, seems not affect job execution at the moment.

{code:java}
2020-11-20 17:05:21,932 INFO  org.apache.beam.runners.fnexecution.logging.GrpcLoggingService [] - 1 Beam Fn Logging clients still connected during shutdown.
2020-11-20 17:05:21,938 WARN  org.apache.beam.sdk.fn.data.BeamFnDataGrpcMultiplexer        [] - Hanged up for unknown endpoint.
2020-11-20 17:05:22,126 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Source: Custom Source -> select: (f0) -> select: (add_one(f0) AS a) -> to: Tuple2 -> Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51) switched from RUNNING to FINISHED.
2020-11-20 17:05:22,126 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for Source: Custom Source -> select: (f0) -> select: (add_one(f0) AS a) -> to: Tuple2 -> Sink: Streaming select table sink (1/1)#0 (b0c2104dd8f87bb1caf0c83586c22a51).
2020-11-20 17:05:22,128 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task Source: Custom Source -> select: (f0) -> select: (add_one(f0) AS a) -> to: Tuple2 -> Sink: Streaming select table sink (1/1)#0 b0c2104dd8f87bb1caf0c83586c22a51.
2020-11-20 17:05:22,156 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ACTIVE, resource profile: ResourceProfile{cpuCores=1.0000000000000000, taskHeapMemory=384.000mb (402653174 bytes), taskOffHeapMemory=0 bytes, managedMemory=512.000mb (536870920 bytes), networkMemory=128.000mb (134217730 bytes)}, allocationId: b67c3307dcf93757adfb4f0f9f7b8c7b, jobId: d05f32162f38ec3ec813c4621bc106d9).
2020-11-20 17:05:22,157 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Remove job d05f32162f38ec3ec813c4621bc106d9 from job leader monitoring.
2020-11-20 17:05:22,157 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job d05f32162f38ec3ec813c4621bc106d9.
2020-11-20 17:05:23,064 ERROR org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?
java.lang.NoClassDefFoundError: org/apache/beam/vendor/grpc/v1p26p0/io/netty/util/concurrent/GlobalEventExecutor$2
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.startThread(GlobalEventExecutor.java:227) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor.execute(GlobalEventExecutor.java:215) ~[blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:498) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:604) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.DefaultPromise.setSuccess(DefaultPromise.java:96) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1089) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [blob_p-bd7a5d615512eb8a2e856e7c1630a0c22fca7cf3-ff27946fda7e2b8cb24ea56d505b689e:1.12-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_261]
Caused by: java.lang.ClassNotFoundException: org.apache.beam.vendor.grpc.v1p26p0.io.netty.util.concurrent.GlobalEventExecutor$2
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_261]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_261]
        at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:63) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.util.ChildFirstClassLoader.loadClassWithoutExceptionHandling(ChildFirstClassLoader.java:72) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:49) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_261]
        ... 11 more
{code}
"	FLINK	Closed	3	1	5059	pull-request-available
13362526	Support Python UDAF in Sliding Window	Support Python UDAF in Sliding Window	FLINK	Resolved	3	7	5059	pull-request-available
13285667	Move the Python scalar operators and table operators to separate package	"Currently both the Python scalar operators and table operators are under the same package org.apache.flink.table.runtime.operators.python. There are already many operators under this package. After introducing the aggregate function support and Vectorized Python function support in the future, there will be more and more operators under the same package. 

We could improve it by the following package structure: org.apache.flink.table.runtime.operators.python.scalar
 org.apache.flink.table.runtime.operators.python.table
org.apache.flink.table.runtime.operators.python.aggregate (in the future)
org.apache.flink.table.runtime.operators.python.scalar.arrow (in the future)

As these classes are internal, it's safe to do so and there are no backwards compatibility issues."	FLINK	Closed	3	4	5059	pull-request-available
13362527	Support Python UDAF in Session Window	Support Python UDAF in Session Window	FLINK	Resolved	3	7	5059	pull-request-available
13328938	Support Pandas Batch Over Window Aggregation	We will add Batch Physical Pandas Over Window RelNode, Python Operation and PandasOverWindowFunctionCoder to support Pandas Batch Over Window Aggregation	FLINK	Closed	3	7	5059	pull-request-available
13329317	Support Pandas Stream Over Window Aggregation	We will add Stream Physical Pandas Over Window RelNode and StreamArrowPythonOverWindowAggregateFunctionOperator to support Pandas Stream Over Window Aggregation	FLINK	Closed	3	7	5059	pull-request-available
13259492	Support Blink planner for Python UDF	Currently, the Python UDF only works in the legacy planner, we should also support it in the Blink planner.	FLINK	Closed	3	7	5059	pull-request-available
13335414	Add Pandas UDAF Doc	Add Pandas UDAF Doc	FLINK	Closed	3	7	5059	pull-request-available
13429602	Support Vector and Matrix in ML Python API	We will add class of DenseVector, SparseVector, DenseMatrix and SparseMatrix	FLINK	Closed	3	7	5059	pull-request-available
13431610	StreamExecutionEnvironmentTests::test_add_python_file failed with ModuleNotFoundError	"
{code:java}
2022-03-02T16:33:43.6649755Z Mar 02 16:33:43 Traceback (most recent call last):
2022-03-02T16:33:43.6650839Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
2022-03-02T16:33:43.6651405Z Mar 02 16:33:43     response = task()
2022-03-02T16:33:43.6652257Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
2022-03-02T16:33:43.6652846Z Mar 02 16:33:43     lambda: self.create_worker().do_instruction(request), request)
2022-03-02T16:33:43.6653655Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 606, in do_instruction
2022-03-02T16:33:43.6654200Z Mar 02 16:33:43     return getattr(self, request_type)(
2022-03-02T16:33:43.6654969Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
2022-03-02T16:33:43.6655549Z Mar 02 16:33:43     bundle_processor.process_bundle(instruction_id))
2022-03-02T16:33:43.6656337Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 999, in process_bundle
2022-03-02T16:33:43.6656936Z Mar 02 16:33:43     input_op_by_transform_id[element.transform_id].process_encoded(
2022-03-02T16:33:43.6657773Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/.tox/py38/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
2022-03-02T16:33:43.6658326Z Mar 02 16:33:43     self.output(decoded_value)
2022-03-02T16:33:43.6658829Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
2022-03-02T16:33:43.6659460Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
2022-03-02T16:33:43.6660261Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
2022-03-02T16:33:43.6661064Z Mar 02 16:33:43   File ""apache_beam/runners/worker/operations.py"", line 319, in apache_beam.runners.worker.operations.Operation.process
2022-03-02T16:33:43.6661902Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 132, in process
2022-03-02T16:33:43.6662440Z Mar 02 16:33:43     self._output_processor.process_outputs(o, self.process_element(value))
2022-03-02T16:33:43.6663200Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 63, in process_outputs
2022-03-02T16:33:43.6663750Z Mar 02 16:33:43     self._consumer.process(windowed_value.with_value(results))
2022-03-02T16:33:43.6664475Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/beam/beam_operations_slow.py"", line 131, in process
2022-03-02T16:33:43.6664969Z Mar 02 16:33:43     for value in o.value:
2022-03-02T16:33:43.6665754Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/datastream/operations.py"", line 179, in wrapped_func
2022-03-02T16:33:43.6666280Z Mar 02 16:33:43     yield from _emit_results(timestamp, watermark, results)
2022-03-02T16:33:43.6667010Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/fn_execution/datastream/input_handler.py"", line 101, in _emit_results
2022-03-02T16:33:43.6667492Z Mar 02 16:33:43     for result in results:
2022-03-02T16:33:43.6668154Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/datastream/data_stream.py"", line 271, in process_element
2022-03-02T16:33:43.6668626Z Mar 02 16:33:43     yield self._map_func(value)
2022-03-02T16:33:43.6669335Z Mar 02 16:33:43   File ""/__w/2/s/flink-python/pyflink/datastream/tests/test_stream_execution_environment.py"", line 350, in plus_two_map
2022-03-02T16:33:43.6669848Z Mar 02 16:33:43     from test_dep1 import add_two
2022-03-02T16:33:43.6670561Z Mar 02 16:33:43 ModuleNotFoundError: No module named 'test_dep1'
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32448&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901
"	FLINK	Closed	2	1	5059	stale-assigned, test-stability
13298800	Failed to download conda when running python tests	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7549&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=14487301-07d2-5d56-5690-6dfab9ffd4d9
This pipeline failed to download conda

If this issue starts appearing more often we should come up with some solution for those kinds of problems.

{code}
CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/anaconda/noarch/babel-2.8.0-py_0.tar.bz2>
Elapsed: -

An HTTP error occurred when trying to retrieve this URL.
HTTP errors are often intermittent, and a simple retry will get you on your way.


conda install sphinx failed         please try to exec the script again.        if failed many times, you can try to exec in the form of sudo ./lint-python.sh -f
PYTHON exited with EXIT CODE: 1.
{code}
"	FLINK	Closed	1	1	5059	pull-request-available, test-stability
13400910	Remove unnecessary info in Loopback mode	If the job runs in loopback mode, it will print unnecessary info `apache_beam.typehints.native_type_compatibility - INFO - Using Any for unsupported type: typing.Sequence[~T]` in the console. We need to remove this confusing info.	FLINK	Closed	3	4	5059	pull-request-available
13429606	Add common params interface in ML Python API	We will add the common used params interface of HasDistanceMeasure,HasFeaturesCol,HasGlobalBatchSize,HasHandleInvalid,HasInputCols,HasLabelCol,HasLearningRate,HasMaxIter,HasMultiClass,HasOutputCols,HasPredictionCol,HasRawPredictionCol,HasReg,HasSeed,HasTol and HasWeightCol	FLINK	Resolved	3	7	5059	pull-request-available
13351642	Config Python Operator Use Managed Memory In Python DataStream	"Now the way to set `Python DataStream Operator` to use managed memory is to set a hook in the `execute` method of `Python StreamExecutionEnvironment` to traverse the `StreamGraph` and set the `Python Operator` to use managed memory.
But when the user’s job uses `from_data_stream` to convert the `DataStream` to a `Table`, the `TableEnvironment.execute` method is used at the end rather than `StreamExecutionEnvironment.execute`, so the `Python DataStream` related operators will not have `Managed Memory` set."	FLINK	Closed	3	1	5059	pull-request-available
13370562	Fix the bug of shared resource among Python Operators of the same slot is not released	"The problem was discussed here:
https://issues.apache.org/jira/browse/FLINK-20663?focusedCommentId=17317706&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17317706

In general, the python process should be terminated when the shared resource is released as many times as requested. However, this reference counting logic has been implemented twice. Consequently, the shared resource may be requested multiple times but only released once."	FLINK	Resolved	2	1	5059	pull-request-available
13368986	"Python Test failed with ""OSError: [Errno 12] Cannot allocate memory"""	"https://dev.azure.com/sewen0794/Flink/_build/results?buildId=249&view=logs&j=fba17979-6d2e-591d-72f1-97cf42797c11&t=443dc6bf-b240-56df-6acf-c882d4b238da&l=21533

Python Test failed with ""OSError: [Errno 12] Cannot allocate memory"" in Azure Pipeline. I am not sure if it is caused by insufficient machine memory on Azure.
"	FLINK	Resolved	3	1	5059	pull-request-available, test-stability
13337486	PyFlinkStreamUserDefinedTableFunctionTests.test_table_function_with_sql_query is unstable	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8401&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3

{code}
=================================== FAILURES ===================================
_ PyFlinkStreamUserDefinedTableFunctionTests.test_table_function_with_sql_query _

self = <pyflink.table.tests.test_udtf.PyFlinkStreamUserDefinedTableFunctionTests testMethod=test_table_function_with_sql_query>

    def test_table_function_with_sql_query(self):
        self._register_table_sink(
            ['a', 'b', 'c'],
            [DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.BIGINT()])
    
        self.t_env.create_temporary_system_function(
            ""multi_emit"", udtf(MultiEmit(), result_types=[DataTypes.BIGINT(), DataTypes.BIGINT()]))
    
        t = self.t_env.from_elements([(1, 1, 3), (2, 1, 6), (3, 2, 9)], ['a', 'b', 'c'])
        self.t_env.register_table(""MyTable"", t)
        t = self.t_env.sql_query(
            ""SELECT a, x, y FROM MyTable LEFT JOIN LATERAL TABLE(multi_emit(a, b)) as T(x, y)""
            "" ON TRUE"")
        actual = self._get_output(t)
>       self.assert_equals(actual, [""1,1,0"", ""2,2,0"", ""3,3,0"", ""3,3,1""])

pyflink/table/tests/test_udtf.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'pyflink.table.tests.test_udtf.PyFlinkStreamUserDefinedTableFunctionTests'>
actual = JavaObject id=o37759, expected = ['1,1,0', '2,2,0', '3,3,0', '3,3,1']

{code}"	FLINK	Closed	2	1	5059	pull-request-available, test-stability
13341196	Add check of unsupported result type in Pandas UDAF	Currently the return type of Pandas UDAF should be a primitive data type, and the returned scalar can be either a python primitive type, e.g., {{int}} or {{float}} or a numpy data type, e.g., {{numpy.int64}} or {{numpy.float64}}. {{Any}} should ideally be a specific scalar type accordingly. We will add related DataType check and throw a more readable exception for unsupported DataTypes. What's more, we will add related notes in docs.	FLINK	Closed	3	4	5059	pull-request-available
13326791	Add Batch Physical Pandas Group Aggregate Rule and RelNode	Add Batch Physical Pandas Group Aggregate Rule and RelNode	FLINK	Closed	3	7	5059	pull-request-available
13446790	Throw exception when UDAF used in sliding window does not implement merge method in PyFlink	"We use the pane state to optimize the result of calculating the window state, which requires udaf to implement the merge method. However, due to the lack of detection of whether the merge method of udaf is implemented, the user's output result did not meet his expectations and there is no exception. Below is an example of a UDAF that implements the merge method:

{code:python}
class SumAggregateFunction(AggregateFunction):

    def get_value(self, accumulator):
        return accumulator[0]

    def create_accumulator(self):
        return [0]

    def accumulate(self, accumulator, *args):
        accumulator[0] = accumulator[0] + args[0]

    def retract(self, accumulator, *args):
        accumulator[0] = accumulator[0] - args[0]

    def merge(self, accumulator, accumulators):
        for other_acc in accumulators:
            accumulator[0] = accumulator[0] + other_acc[0]

    def get_accumulator_type(self):
        return DataTypes.ARRAY(DataTypes.BIGINT())

    def get_result_type(self):
        return DataTypes.BIGINT()
{code}
"	FLINK	Closed	3	4	5059	pull-request-available
13298053	The PyFlink Job runs into infinite loop if the Python UDF imports job code	"If the UDF file imports job code directly or indirectly, the PyFlink Job will run into a infinite loop as follows:
 - submit job
 - execute job
 - launch UDF worker
 - import UDF
 - (If the job file is depended by UDF or imported as the top level module) import job code
 - (If the job code is executed outside the ""*if __name__ == '__main__':*"") launch gateway server and submit job to local executor
 - execute job in local mode
 - launch UDF worker
 - import UDF
 - import job code
 ...
This infinite loop will create new Java processes and Python processes endlessly until the resources on the machine are exhausted. We should fix it ASAP."	FLINK	Closed	3	1	5059	pull-request-available
13287119	Optimize the cost of function call  in ScalarFunctionOpertation	Currently, there are too many extra function calls cost in  ScalarFunctionOpertation.We need to optimize it to improve performance in Python UDF.	FLINK	Closed	3	7	5059	pull-request-available
13419297	Build wheels failed	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=28552&view=logs&j=33dd8067-7758-552f-a1cf-a8b8ff0e44cd&t=bf344275-d244-5694-d05a-7ad127794669
"	FLINK	Resolved	1	1	5059	pull-request-available, test-stability
13344741	Add Python building blocks to make sure the basic functionality of Stream Group Table Aggregation could work 	Add Python building blocks to make sure the basic functionality of Stream Group Table Aggregation could work	FLINK	Closed	3	7	5059	pull-request-available
13411043	Fix the wrong position mappings in the Python UDTF	"The failed example:
{code:python}
        @udtf(result_types=[DataTypes.STRING(), DataTypes.STRING()])
        def StoTraceMqSourcePlugUDTF(s: str):
            import json
            try:
                data = json.loads(s)
            except Exception as e:
                return None
            source_code = ""trace""
            try:
                shipment_no = data['shipMentNo']
            except Exception as e:
                return None
            yield source_code, shipment_no

        class StoTraceFindNameUDTF(TableFunction):
            def eval(self, shipment_no):
                yield shipment_no, shipment_no

        sto_trace_find_name = udtf(StoTraceFindNameUDTF(),
                                   result_types=[DataTypes.STRING(), DataTypes.STRING()])

        # self.env.set_parallelism(1)
        self.t_env.create_temporary_system_function(
            ""StoTraceMqSourcePlugUDTF"", StoTraceMqSourcePlugUDTF)
        self.t_env.create_temporary_system_function(
            ""sto_trace_find_name"", sto_trace_find_name
        )
        source_table = self.t_env.from_elements([(
            '{""shipMentNo"":""84210186879""}',)],
            ['biz_context'])
        # self.t_env.execute_sql(source_table)
        self.t_env.register_table(""source_table"", source_table)

        t = self.t_env.sql_query(
            ""SELECT biz_context, source_code, shipment_no FROM source_table LEFT JOIN LATERAL TABLE(StoTraceMqSourcePlugUDTF(biz_context)) as T(source_code, shipment_no)""
            "" ON TRUE"")
        self.t_env.register_table(""Table2"", t)
        t = self.t_env.sql_query(
            ""SELECT source_code, shipment_no, shipment_name, shipment_type FROM Table2 LEFT JOIN LATERAL TABLE(sto_trace_find_name(shipment_no)) as T(shipment_name, shipment_type)""
            "" ON TRUE""
        )
        print(t.to_pandas())
{code}
In the failed example, the input arguments of the second Python Table Function has the wrong positions mapping.
"	FLINK	Resolved	3	1	5059	pull-request-available
13258762	Optimize Python UDFs with parameters of constant values	We need support Python UDFs with parameters of constant values. It should be noticed that the constant parameters are not needed to be transferred between the Java operator and the Python worker.	FLINK	Closed	3	7	5059	pull-request-available
13284310	Add Python building blocks to make sure the basic functionality of Python TableFunction could work	"We need to add a few Python building blocks such as TableFunctionOperation, TableFunctionRowCoder, etc for Python TableFunction execution. TableFunctionOperation is subclass of Operation in Beam and TableFunctionRowCoder, etc are subclasses of Coder in Beam. These classes will be registered into the Beam’s portability framework to make sure they take effects.

This PR makes sure that a basic end to end Python UDTF could be executed."	FLINK	Closed	3	7	5059	pull-request-available
13397554	Lookback mode doesn't work when mixing use of Python Table API and Python DataStream API	"For the following program:
{code}
import logging
import time

from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment, CoMapFunction
from pyflink.table import StreamTableEnvironment, DataTypes, Schema


def test_chaining():
    env = StreamExecutionEnvironment.get_execution_environment()
    t_env = StreamTableEnvironment.create(stream_execution_environment=env)
    t_env.get_config().get_configuration().set_boolean(""python.operator-chaining.enabled"", False)

    # 1. create source Table
    t_env.execute_sql(""""""
        CREATE TABLE datagen (
            id INT,
            data STRING
        ) WITH (
            'connector' = 'datagen',
            'rows-per-second' = '1000000',
            'fields.id.kind' = 'sequence',
            'fields.id.start' = '1',
            'fields.id.end' = '1000'
        )
    """""")

    # 2. create sink Table
    t_env.execute_sql(""""""
        CREATE TABLE print (
            id BIGINT,
            data STRING,
            flag STRING
        ) WITH (
            'connector' = 'blackhole'
        )
    """""")

    t_env.execute_sql(""""""
        CREATE TABLE print_2 (
            id BIGINT,
            data STRING,
            flag STRING
        ) WITH (
            'connector' = 'blackhole'
        )
    """""")

    # 3. query from source table and perform calculations
    # create a Table from a Table API query:
    source_table = t_env.from_path(""datagen"")

    ds = t_env.to_append_stream(
        source_table,
        Types.ROW([Types.INT(), Types.STRING()]))

    ds1 = ds.map(lambda i: (i[0] * i[0], i[1]))
    ds2 = ds.map(lambda i: (i[0], i[1][2:]))

    class MyCoMapFunction(CoMapFunction):

        def map1(self, value):
            print('hahah')
            return value

        def map2(self, value):
            print('hahah')
            return value

    ds3 = ds1.connect(ds2).map(MyCoMapFunction(), output_type=Types.TUPLE([Types.LONG(), Types.STRING()]))

    ds4 = ds3.map(lambda i: (i[0], i[1], ""left""),
                  output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING()]))

    ds5 = ds3.map(lambda i: (i[0], i[1], ""right""))\
             .map(lambda i: i,
                  output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING()]))

    schema = Schema.new_builder() \
        .column(""f0"", DataTypes.BIGINT()) \
        .column(""f1"", DataTypes.STRING()) \
        .column(""f2"", DataTypes.STRING()) \
        .build()

    result_table_3 = t_env.from_data_stream(ds4, schema)
    statement_set = t_env.create_statement_set()
    statement_set.add_insert(""print"", result_table_3)

    result_table_4 = t_env.from_data_stream(ds5, schema)
    statement_set.add_insert(""print_2"", result_table_4)

    statement_set.execute().wait()


if __name__ == ""__main__"":

    start_ts = time.time()
    test_chaining()
    end_ts = time.time()
    print(""--- %s seconds ---"" % (end_ts - start_ts))
{code}

Lookback mode doesn't work."	FLINK	Resolved	1	1	5059	pull-request-available
13401492	BatchPandasUDAFITTests.test_over_window_aggregate_function fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=24170&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=23011

{code}
Sep 15 20:40:43 cls = <class 'pyflink.table.tests.test_pandas_udaf.BatchPandasUDAFITTests'>
Sep 15 20:40:43 actual = JavaObject id=o8666
Sep 15 20:40:43 expected = ['+I[1, 4.3333335, 13, 5.5, 3.0, 3.0, 4.3333335, 8.0, 5.0, 5.0]', '+I[1, 4.3333335, 5, 4.3333335, 3.0, 3.0, 2.5, 4.333....0, 4.0, 2.0]', '+I[2, 2.0, 9, 2.0, 4.0, 4.0, 2.0, 2.0, 4.0, 4.0]', '+I[3, 2.0, 3, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0]']
Sep 15 20:40:43 
Sep 15 20:40:43     @classmethod
Sep 15 20:40:43     def assert_equals(cls, actual, expected):
Sep 15 20:40:43         if isinstance(actual, JavaObject):
Sep 15 20:40:43             actual_py_list = cls.to_py_list(actual)
Sep 15 20:40:43         else:
Sep 15 20:40:43             actual_py_list = actual
Sep 15 20:40:43         actual_py_list.sort()
Sep 15 20:40:43         expected.sort()
Sep 15 20:40:43         assert len(actual_py_list) == len(expected)
Sep 15 20:40:43 >       assert all(x == y for x, y in zip(actual_py_list, expected))
Sep 15 20:40:43 E       AssertionError: assert False
Sep 15 20:40:43 E        +  where False = all(<generator object PyFlinkTestCase.assert_equals.<locals>.<genexpr> at 0x7f792d98b900>)
{code}"	FLINK	Resolved	1	1	5059	pull-request-available, test-stability
13348210	Fix NamesTest due to code style refactor	"Due to the [FLINK-20651|https://issues.apache.org/jira/browse/FLINK-20651], the NameTest failed

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=11403&view=results]

I will fix it asap"	FLINK	Closed	3	1	5059	pull-request-available, test-stability
13355692	StreamPandasConversionTests Fails	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12699&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3]

 
=================================== FAILURES =================================== 
_______________ StreamPandasConversionTests.test_empty_to_pandas _______________ 
 
self = <pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_empty_to_pandas> 
 
 def test_empty_to_pandas(self): 
> table = self.t_env.from_pandas(self.pdf, self.data_type) 
 
pyflink/table/tests/test_pandas_conversion.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyflink/table/table_environment.py:1462: in from_pandas 
 arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False) 
pyarrow/types.pxi:1315: in pyarrow.lib.Schema.from_pandas 
 ??? 
.tox/py37-cython/lib/python3.7/site-packages/pyarrow/pandas_compat.py:519: in dataframe_to_types 
 type_ = pa.lib._ndarray_to_arrow_type(values, type_) 
pyarrow/array.pxi:53: in pyarrow.lib._ndarray_to_arrow_type 
 ??? 
pyarrow/array.pxi:64: in pyarrow.lib._ndarray_to_type 
 ??? 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 
> ??? 
E pyarrow.lib.ArrowTypeError: Did not pass numpy.dtype object 
 
pyarrow/error.pxi:108: ArrowTypeError 
_________________ StreamPandasConversionTests.test_from_pandas _________________ 
 
self = <pyflink.table.tests.test_pandas_conversion.StreamPandasConversionTests testMethod=test_from_pandas> 
 
 def test_from_pandas(self): 
> table = self.t_env.from_pandas(self.pdf, self.data_type, 5) 
 
pyflink/table/tests/test_pandas_conversion.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"	FLINK	Closed	3	1	5059	pull-request-available, test-stability
13425079	BatchArrowPythonOverWindowAggregateFunctionOperatorTest. testFinishBundleTriggeredByTime failed in Azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=30261&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199

{code:java}
2022-01-27T05:23:36.0047569Z Jan 27 05:23:35 [ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.226 s <<< FAILURE! - in org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest
2022-01-27T05:23:36.0060644Z Jan 27 05:23:36 [ERROR] org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByTime  Time elapsed: 0.11 s  <<< FAILURE!
2022-01-27T05:23:36.0061804Z Jan 27 05:23:36 java.lang.AssertionError: expected:<4> but was:<3>
2022-01-27T05:23:36.0062505Z Jan 27 05:23:36 	at org.junit.Assert.fail(Assert.java:89)
2022-01-27T05:23:36.0063140Z Jan 27 05:23:36 	at org.junit.Assert.failNotEquals(Assert.java:835)
2022-01-27T05:23:36.0065604Z Jan 27 05:23:36 	at org.junit.Assert.assertEquals(Assert.java:647)
2022-01-27T05:23:36.0066132Z Jan 27 05:23:36 	at org.junit.Assert.assertEquals(Assert.java:633)
2022-01-27T05:23:36.0067101Z Jan 27 05:23:36 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:80)
2022-01-27T05:23:36.0068344Z Jan 27 05:23:36 	at org.apache.flink.table.runtime.util.RowDataHarnessAssertor.assertOutputEquals(RowDataHarnessAssertor.java:60)
2022-01-27T05:23:36.0070375Z Jan 27 05:23:36 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.ArrowPythonAggregateFunctionOperatorTestBase.assertOutputEquals(ArrowPythonAggregateFunctionOperatorTestBase.java:62)
2022-01-27T05:23:36.0071970Z Jan 27 05:23:36 	at org.apache.flink.table.runtime.operators.python.aggregate.arrow.batch.BatchArrowPythonOverWindowAggregateFunctionOperatorTest.testFinishBundleTriggeredByTime(BatchArrowPythonOverWindowAggregateFunctionOperatorTest.java:161)
2022-01-27T05:23:36.0073109Z Jan 27 05:23:36 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-01-27T05:23:36.0073968Z Jan 27 05:23:36 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-01-27T05:23:36.0074876Z Jan 27 05:23:36 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-01-27T05:23:36.0075802Z Jan 27 05:23:36 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-01-27T05:23:36.0076471Z Jan 27 05:23:36 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-01-27T05:23:36.0077209Z Jan 27 05:23:36 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-01-27T05:23:36.0077932Z Jan 27 05:23:36 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-01-27T05:23:36.0078998Z Jan 27 05:23:36 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-01-27T05:23:36.0079682Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-27T05:23:36.0080368Z Jan 27 05:23:36 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-01-27T05:23:36.0081041Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-01-27T05:23:36.0081723Z Jan 27 05:23:36 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-01-27T05:23:36.0082444Z Jan 27 05:23:36 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-01-27T05:23:36.0083105Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-01-27T05:23:36.0083742Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-01-27T05:23:36.0084381Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-01-27T05:23:36.0085033Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-01-27T05:23:36.0088283Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-01-27T05:23:36.0089997Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-01-27T05:23:36.0090530Z Jan 27 05:23:36 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-01-27T05:23:36.0091040Z Jan 27 05:23:36 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-01-27T05:23:36.0091519Z Jan 27 05:23:36 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-01-27T05:23:36.0092048Z Jan 27 05:23:36 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-01-27T05:23:36.0092660Z Jan 27 05:23:36 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-01-27T05:23:36.0093264Z Jan 27 05:23:36 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-01-27T05:23:36.0093905Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-01-27T05:23:36.0094595Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-01-27T05:23:36.0095296Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-01-27T05:23:36.0096026Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-01-27T05:23:36.0096943Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-01-27T05:23:36.0097583Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-01-27T05:23:36.0098182Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-01-27T05:23:36.0098820Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-01-27T05:23:36.0099630Z Jan 27 05:23:36 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-01-27T05:23:36.0100293Z Jan 27 05:23:36 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
2022-01-27T05:23:36.0100980Z Jan 27 05:23:36 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
2022-01-27T05:23:36.0101734Z Jan 27 05:23:36 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
2022-01-27T05:23:36.0102424Z Jan 27 05:23:36 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-01-27T05:23:36.0103295Z Jan 27 05:23:36 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
2022-01-27T05:23:36.0104098Z Jan 27 05:23:36 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-01-27T05:23:36.0104680Z Jan 27 05:23:36 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-01-27T05:23:36.0105397Z Jan 27 05:23:36 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-01-27T05:23:36.0106138Z Jan 27 05:23:36 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}

"	FLINK	Open	3	1	5059	stale-assigned, test-stability
13270862	Dependency shading of table modules test fails on Travis	"e2e - misc cron job fails on Travis. The messages are as follows:

{code}
==============================================================================
Running 'Dependency shading of table modules test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-41270732894
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-java/target/flink-table-api-java-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-scala/target/flink-table-api-scala_2.11-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-java-bridge/target/flink-table-api-java-bridge_2.11-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-api-scala-bridge/target/flink-table-api-scala-bridge_2.11-1.10-SNAPSHOT.jar jar.
Success: There are no unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-planner/target/flink-table-planner_2.11-1.10-SNAPSHOT.jar jar.
Failure: There are unwanted dependencies in the /home/travis/build/apache/flink/flink-end-to-end-tests/../flink-table/flink-table-planner-blink/target/flink-table-planner-blink_2.11-1.10-SNAPSHOT.jar jar:       -> com.esotericsoftware.kryo                          not found
[FAIL] Test script contains errors.
Checking for errors...
No errors in log files.
Checking for exceptions...
No exceptions in log files.
Checking for non-empty .out files...
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/log/*.out: No such file or directory
No non-empty .out files.

[FAIL] 'Dependency shading of table modules test' failed after 0 minutes and 30 seconds! Test exited with exit code 1
{code}

See https://api.travis-ci.org/v3/job/617187809/log.txt for full message"	FLINK	Closed	1	1	5351	pull-request-available, test-stability
13277861	Set default planner for SQL Client to Blink planner	"As discussed in the mailing list [1], we will change the default planner to Blink planner for SQL CLI. 

[1]: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Set-default-planner-for-SQL-Client-to-Blink-planner-in-1-10-release-td36379.html
"	FLINK	Resolved	3	4	5351	pull-request-available
13323550	Support to only read changelogs of specific database and table for canal-json format	"Usually, users use Canal to synchronize binlog data from various MySQL databases and tables into a single Kafka topic. However, currently, canal-json can't support this case, because it requires the canal data in the topic should be in the same data format. 

This issue propose to introduce a new option ""canal-json.database"" and ""canal-json.table"" to filter out the specific data. It would be great to support table list or table pattern in case of all the tables have the same schema. "	FLINK	Closed	3	2	5351	pull-request-available
13213879	Support multiple languages for the framework of flink docs	"A more detailed description can be found in the proposed doc: https://docs.google.com/document/d/1R1-uDq-KawLB8afQYrczfcoQHjjIhq6tvUksxrfhBl0/edit#

This step aims to integrate the mulitple-language-plugin for flink docs to support Chinese. All the $pagename.zh.md should be created first in this JIRA but keep the original English contents. A link between English version and Chinese version should also be considered.

"	FLINK	Closed	3	7	5351	pull-request-available
13339476	Outdated SQL docs on aggregate functions' merge	"In the java docs as well as the user docs, the {{merge}} method of an aggregation UDF is described as optional, e.g.
{quote}Merges a group of accumulator instances into one accumulator instance. This function must be implemented for data stream session window grouping aggregates and data set grouping aggregates.{quote}

However, it seems that nowadays this method is required in more cases (I stumbled on this for a HOP window in streaming):
{code}
StreamExecGlobalGroupAggregate.scala
      .needMerge(mergedAccOffset, mergedAccOnHeap, mergedAccExternalTypes)
StreamExecGroupWindowAggregateBase.scala
      generator.needMerge(mergedAccOffset = 0, mergedAccOnHeap = false)
StreamExecIncrementalGroupAggregate.scala
      .needMerge(mergedAccOffset, mergedAccOnHeap = true, mergedAccExternalTypes)
StreamExecLocalGroupAggregate.scala
      .needMerge(mergedAccOffset = 0, mergedAccOnHeap = true)
{code}"	FLINK	Closed	3	1	5351	pull-request-available
13219793	Introduce CodeGeneratorContext to maintain reusable statements for code generation	The CodeGeneratorContext will keep all the reusable statements which will be the basic class for code generation. In the future, we will introduce FunctionCodeGeneration, AggregateCodeGeneration, etc... and they will depend on the CodeGeneratorContext to store reusable statements.	FLINK	Closed	3	2	5351	pull-request-available
13260693	Support to apply watermark assigner according to the WatermarkSpec in TableSourceTable	"Apply watermark assigner according to the {{WatermarkSpec}} in {{TableSourceTable}} when {{StreamExecTableSourceScan#translateToPlan}}. Ignore TableSource’s {{DefinedRowtimeAttributes}} if {{WatermarkSpec}} exists. 
"	FLINK	Resolved	3	7	5351	pull-request-available
13247477	Change default planner to legacy planner instead of any one	As discussed in FLINK-13399, we will change the default behavior of the {{EnvironmentSettings}} to use old planner instead of any planner. This will enable us to have both planner in the classpath. This will also enable users/connectors to have both planner in dependency and without  using {{EnvironmentSettings}}.	FLINK	Resolved	1	4	5351	pull-request-available
13248892	Fix some transformation names are not set in blink planner	"Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion."	FLINK	Resolved	3	1	5351	pull-request-available
13217573	Add a language switch to the sidebar for Flink documents	"Add a language switch similar to the project webpage. 

We didn't add the switch in the first version of supporting Chinese language, because we want to expose the switch when we satisfied the translation coverage."	FLINK	Closed	3	7	5351	pull-request-available
13315761	"KafkaTableITCase.testKafkaDebeziumChangelogSource failed with ""Topic 'changelog_topic' already exists"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=4350&view=logs&j=4be4ed2b-549a-533d-aa33-09e28e360cc8&t=0db94045-2aa0-53fa-f444-0130d6933518

{code}
2020-07-08T21:14:04.1626423Z [ERROR] Failures: 
2020-07-08T21:14:04.1629804Z [ERROR]   KafkaTableITCase.testKafkaDebeziumChangelogSource:66->KafkaTestBase.createTestTopic:197 Create test topic : changelog_topic failed, org.apache.kafka.common.errors.TopicExistsException: Topic 'changelog_topic' already exists.
2020-07-08T21:14:04.1630642Z [ERROR] Errors: 
2020-07-08T21:14:04.1630986Z [ERROR]   KafkaTableITCase.testKafkaDebeziumChangelogSource:83  Failed to write debezium...
{code}"	FLINK	Closed	3	4	5351	pull-request-available, test-stability
13232183	Large number of broken links	"{code}
[2019-05-07 11:35:10] ERROR `/zh/dev/stream/side_output.html' not found.
[2019-05-07 11:35:11] ERROR `/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.8.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.7.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.6.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/release-notes/flink-1.5.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/levels_of_abstraction.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/table_api.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/program_dataflow.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/parallel_dataflow.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/windows.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/event_ingestion_processing_time.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/state_partitioning.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/tasks_chains.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/processes.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/tasks_slots.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/slot_sharing.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/checkpoints.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/linking_with_flink.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/linking.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/apis/streaming/event_timestamps_watermarks.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/apis/streaming/event_timestamp_extractors.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/apis/streaming/event_time.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/dev/table/(/dev/table/sourceSinks.html' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/checkpoint_tuning.svg' not found.
[2019-05-07 11:35:15] ERROR `/zh/fig/local_recovery.png' not found.
{code}"	FLINK	Closed	2	1	5351	pull-request-available
13290444	Improve default flush strategy for Elasticsearch sink to make it work out-of-box	"Currently, Elasticsearch sink provides 3 flush options: 

{code:java}
'connector.bulk-flush.max-actions' = '42'
'connector.bulk-flush.max-size' = '42 mb'
'connector.bulk-flush.interval' = '60000'
{code}

All of them are optional and have no default value in Flink side [1]. But flush actions and flush size have a default value {{1000}} and {{5mb}} in Elasticsearch client [2]. This results in some surprising behavior that no results are outputed by default, see user report [3]. Because it has to wait for 1000 records however there is no so many records in the testing. 

This will also be a potential ""problem"" in production. Because if it's a low throughout job, soem data may take a very long time to be visible in the elasticsearch. 

So we propose to have a default flush '1s' interval  and '1000' rows and '2mb' size for ES sink flush. This only applies to new ES sink options:

{code}
'sink.bulk-flush.max-actions' = '1000'
'sink.bulk-flush.max-size' = '2mb'
'sink.bulk-flush.interval' = '1s'
{code}


[1]: https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java#L357-L356
[2]: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk-processor.html
[3]: http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Should-I-use-a-Sink-or-Connector-Or-Both-td33352.html"	FLINK	Closed	2	4	5351	usability
13529111	Move query SqlNode conversion logic to SqlQueryConverter	"Introduce {{SqlQueryConverter}} and move the conversion logic of query {{SqlNode}} -> {{PlannerQueryOption}} to it.

Note that the conversion is complex for query SqlNodes because it's unclear what the specific query SqlNode classes are. But they should all belong to the {{SqlKind.QUERY}}. Therefore, we can introduce a new mapping path for the SqlKind. 

{code:java}
public interface SqlNodeConverter<S extends SqlNode> {
    /**
     * Returns the {@link SqlKind SqlKinds} of {@link SqlNode SqlNodes} that the {@link
     * SqlNodeConverter} supports to convert.
     *
     * <p>If a {@link SqlNodeConverter} return s a non-empty SqlKinds, the conversion framework
     * prefer to match SqlKind of SqlNode instead of matching class of SqlNode.
     *
     * @see SqlQueryConverter
     */
    default Optional<EnumSet<SqlKind>> supportedSqlKinds() {
        return Optional.empty();
    }

   ...

}
{code}



"	FLINK	Closed	3	7	5351	pull-request-available
13274920	"Run sql appear error of ""Zero-length character strings have no serializable string representation""."	"*The sql is:*
 CREATE TABLE `INT8_TBL` (
 q1 BIGINT,
 q2 BIGINT
 ) WITH (
 'format.field-delimiter'=',',
 'connector.type'='filesystem',
 'format.derive-schema'='true',
 'connector.path'='/defender_test_data/daily_regression_batch_postgres_1.10/test_bigint/sources/INT8_TBL.csv',
 'format.type'='csv'
 );

SELECT '' AS five, q1 AS plus, -q1 AS xm FROM INT8_TBL;

*The error detail is :*
 2019-12-17 15:35:07,026 ERROR org.apache.flink.table.client.SqlClient - SQL Client must stop. Unexpected exception. This is a bug. Please consider filing an issue.
 org.apache.flink.table.api.TableException: Zero-length character strings have no serializable string representation.
 at org.apache.flink.table.types.logical.CharType.asSerializableString(CharType.java:116)
 at org.apache.flink.table.descriptors.DescriptorProperties.putTableSchema(DescriptorProperties.java:218)
 at org.apache.flink.table.catalog.CatalogTableImpl.toProperties(CatalogTableImpl.java:75)
 at org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSink(TableFactoryUtil.java:85)
 at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryAndPersistInternal(LocalExecutor.java:688)
 at org.apache.flink.table.client.gateway.local.LocalExecutor.executeQueryAndPersist(LocalExecutor.java:488)
 at org.apache.flink.table.client.cli.CliClient.callSelect(CliClient.java:601)
 at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:385)
 at java.util.Optional.ifPresent(Optional.java:159)
 at org.apache.flink.table.client.cli.CliClient.submitSQLFile(CliClient.java:271)
 at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:125)
 at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104)
 at org.apache.flink.table.client.SqlClient.main(SqlClient.java:180)

*The input data is:*
 123,456
 123,4567890123456789
 4567890123456789,123
 4567890123456789,4567890123456789
 4567890123456789,-4567890123456789"	FLINK	Reopened	4	1	5351	auto-deprioritized-major, stale-assigned
13335154	Implement cumulative windowing for window aggregate operator	Support cumulative windows for existing window aggregate operator, i.e. {{WindowOperator}}.	FLINK	Closed	3	7	5351	pull-request-available
13528392	Introduce ExecutableOperation for operations to execute	"Colocating the execution logic within the Operation, just like how RunnableCommand and V2CommandExec do in Spark. We can introduce a class like:

{code:java}
public interface ExecutableOperation {

    TableResultInternal execute(Context ctx);

    interface Context {
        CatalogManager getCatalogManager();

        FunctionCatalog getFunctionCatalog();

        ResourceManager getResourceManager();

        Configuration getConfiguration();
    }
}
{code}

Many base interfaces can extend it (AlterOperation, CreateOperation, DropOperation, etc.). This approach improves code readability (not spread code across different classes) and make supporting a new statement by just adding an Operation class instead of 3 classes (Operation class, Executor class, and the mapping class)."	FLINK	Closed	3	7	5351	pull-request-available
13233143	Fix AsyncLookupJoin doesn't close all generated ResultFutures	"There is a fragile test in AsyncLookupJoinITCase, that not all the udfs are closed at the end.

{code:java}
02:40:48.787 [ERROR] Tests run: 22, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 47.098 s <<< FAILURE! - in org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase
02:40:48.791 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=HEAP](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase)  Time elapsed: 1.266 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<2>
	at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)

02:40:48.794 [ERROR] testAsyncJoinTemporalTableWithUdfFilter[StateBackend=ROCKSDB](org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase)  Time elapsed: 1.033 s  <<< FAILURE!
java.lang.AssertionError: expected:<0> but was:<2>
	at org.apache.flink.table.runtime.stream.sql.AsyncLookupJoinITCase.testAsyncJoinTemporalTableWithUdfFilter(AsyncLookupJoinITCase.scala:268)
{code}
"	FLINK	Closed	3	1	5351	pull-request-available
13274925	Rename WatermarkSepc#getWatermarkExpressionString to getWatermarkExpr	"Currently, the expression getter methods in {{org.apache.flink.table.api.WatermarkSpec}} and {{org.apache.flink.table.api.TableColumn}} are not aligned, one is {{getWatermarkExpressionString}}, the other is {{getExpr}}. 

I would suggest to rename 
{{WatermarkSepc#getWatermarkExpressionString}} to {{WatermarkSepc#getWatermarkExpr}}.

"	FLINK	Resolved	3	4	5351	pull-request-available
13225682	Introduce unbounded streaming inner/left/right/full join operator	"This operator is responsible for unbounded streaming inner/left/right/full join, and will be optimized in following cases:
# If the join keys (with equality condition) are also primary key, we will have a more efficient state layout
# If the inputs have primary keys, but join keys are not primary key, we can also come up with an efficient state layout
# Inputs don't have primary keys, this will go to default implementation
"	FLINK	Closed	3	4	5351	pull-request-available
13528990	Move execution logic of CreateOperation out from TableEnvironmentImpl	This should implement {{ExecutableOperation}} for all the {{CreateOperation}}s to move the execution logic out from {{TableEnvironmentImpl#executeInternal()}}.	FLINK	Closed	3	7	5351	pull-request-available
13277001	"Improve ""Connect to External Systems"" documentation page"	"1. Remove documentation for format schema, which is not necessary any more and is deprecated.
2. Add DDL documentation for ""Table Schema"" and ""Rowtime Attribute"" sections.
3. Update the comments in DDL for better rendering (do not wrap line)."	FLINK	Resolved	3	4	5351	pull-request-available
13528952	Move SqlCreateView conversion logic to SqlCreateViewConverter	"Introduce {{SqlCreateViewConverter}} and move the conversion logic of SqlCreateView -> CreateViewOperation to it.

"	FLINK	In Progress	3	7	5351	pull-request-available, stale-assigned
13273224	Improve schema derivation for Avro format	"For JSON, CSV and OldCsv, we already supported {{derive.schema=true}} to get the schema from table schema. But for Avro format, a user has to pass an Avro schema file or define the format schema explicitly via {{avro.schema}}.

We can think of if we can drop {{avro.schema}} and make {{derive.schema=true}} as the default behavior."	FLINK	Closed	3	7	5351	pull-request-available
13490592	CatalogPropertiesUtil supports de/serializing column comment	We should consider adding {{schema.${i}.comment}} to {{CatalogPropertiesUtil, in order to support comment's de/serialization.}}	FLINK	Closed	3	7	5351	pull-request-available
13244943	Add documentation for all the new features of blink planner	"This is an umbrella issue to track documentations for blink planner. All new features introduced by blink planner, or behavior different with flink planner should be documented.

Structure and Tasks are proposed in the google doc: https://docs.google.com/document/d/1xcI77x-15CbSPOdluRaFzx7jf2_V2SBOrlTyDPhIUHE/edit#

Subtasks will be added later."	FLINK	Closed	3	4	5351	pull-request-available
13007621	Add possibility to get column names	"For debugging and maybe for visualization in future (e.g. in a shell) it would be good to have the possibilty to get the names of {{Table}} columns. At the moment the user has no idea how the table columns are named; if they need to be matched with POJO fields for example.

My suggestion:

{code}
Schema s = table.schema();
TypeInformation<?> type = s.getType(1);
TypeInformation<?> type = s.getType(""col"");
String s = s.getColumnName(1);
String[] s = s.getColumnNames();
{code}"	FLINK	Resolved	3	2	5351	starter
13289945	Fix IndexOutOfBoundsException for DISTINCT AGG with constants	"When I use lisgagg with distinct and over window.
{code:java}
//代码占位符
""select listagg(distinct product, '|') over(partition by user order by proctime rows between 200 preceding and current row) as product, user from "" + testTable
{code}
I got the follwing exception
{code:java}
//代码占位符

Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index: 3, Size: 3 at java.util.ArrayList.rangeCheck(ArrayList.java:657) at java.util.ArrayList.get(ArrayList.java:433) at java.util.Collections$UnmodifiableList.get(Collections.java:1311) at org.apache.flink.table.types.logical.RowType.getTypeAt(RowType.java:174) at org.apache.flink.table.planner.codegen.GenerateUtils$.generateFieldAccess(GenerateUtils.scala:635) at org.apache.flink.table.planner.codegen.GenerateUtils$.generateFieldAccess(GenerateUtils.scala:620) at org.apache.flink.table.planner.codegen.GenerateUtils$.generateInputAccess(GenerateUtils.scala:524) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen$$anonfun$10.apply(DistinctAggCodeGen.scala:374) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen$$anonfun$10.apply(DistinctAggCodeGen.scala:374) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.mutable.ArrayOps$ofInt.map(ArrayOps.scala:234) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.generateKeyExpression(DistinctAggCodeGen.scala:374) at org.apache.flink.table.planner.codegen.agg.DistinctAggCodeGen.accumulate(DistinctAggCodeGen.scala:192) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$12.apply(AggsHandlerCodeGenerator.scala:871) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator$$anonfun$12.apply(AggsHandlerCodeGenerator.scala:871) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.genAccumulate(AggsHandlerCodeGenerator.scala:871) at org.apache.flink.table.planner.codegen.agg.AggsHandlerCodeGenerator.generateAggsHandler(AggsHandlerCodeGenerator.scala:329) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.createBoundedOverProcessFunction(StreamExecOverAggregate.scala:425) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.translateToPlanInternal(StreamExecOverAggregate.scala:255) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.translateToPlanInternal(StreamExecOverAggregate.scala:56) at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecOverAggregate.translateToPlan(StreamExecOverAggregate.scala:56) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54) at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39)
{code}
But It worked with 
{code:java}
//代码占位符
select listagg(distinct product) over(partition by user order by proctime rows between 200 preceding and current row) as product, user from "" + testTable
{code}
 

The exception will be throw  at the below code. 
{code:java}
//代码占位符
private def generateKeyExpression(
    ctx: CodeGeneratorContext,
    generator: ExprCodeGenerator): GeneratedExpression = {
  val fieldExprs = distinctInfo.argIndexes.map(generateInputAccess(
    ctx,
    generator.input1Type,
    generator.input1Term,
    _,
    nullableInput = false,
    deepCopy = inputFieldCopy))
{code}
 

The distinctInfo.argIndexs is  [1, 3] .  But the index 3 is a logical index. It will be replaced by  '|' . And should not  generate Input Access for  index 3 "	FLINK	Closed	2	1	5351	pull-request-available
13251436	Fix hardcode Scala version dependency in hive connector	"FLINK-13688 introduced a {{flink-test-utils}} dependency in {{flink-connector-hive}}. However, the Scala version of the artifactId is hardcoded, this result in recent CRON jobs failed. 

Here is an instance: https://api.travis-ci.org/v3/job/573092374/log.txt


{code}
11:46:09.078 [INFO] --- maven-enforcer-plugin:3.0.0-M1:enforce (enforce-versions) @ flink-connector-hive_2.12 ---
11:46:09.134 [WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies failed with message:
Found Banned Dependency: com.typesafe.akka:akka-slf4j_2.11:jar:2.5.21
Found Banned Dependency: com.typesafe.akka:akka-actor_2.11:jar:2.5.21
Found Banned Dependency: com.typesafe:ssl-config-core_2.11:jar:0.3.7
Found Banned Dependency: org.scala-lang.modules:scala-java8-compat_2.11:jar:0.7.0
Found Banned Dependency: com.typesafe.akka:akka-protobuf_2.11:jar:2.5.21
Found Banned Dependency: org.apache.flink:flink-clients_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-streaming-java_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: com.typesafe.akka:akka-stream_2.11:jar:2.5.21
Found Banned Dependency: com.github.scopt:scopt_2.11:jar:3.5.0
Found Banned Dependency: org.apache.flink:flink-test-utils_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-runtime_2.11:jar:1.10-SNAPSHOT
Found Banned Dependency: org.apache.flink:flink-runtime_2.11:test-jar:tests:1.10-SNAPSHOT
Found Banned Dependency: org.scala-lang.modules:scala-parser-combinators_2.11:jar:1.1.1
Found Banned Dependency: com.twitter:chill_2.11:jar:0.7.6
Found Banned Dependency: org.clapper:grizzled-slf4j_2.11:jar:1.3.2
Found Banned Dependency: org.apache.flink:flink-optimizer_2.11:jar:1.10-SNAPSHOT
Use 'mvn dependency:tree' to locate the source of the banned dependencies.
{code}
"	FLINK	Resolved	1	1	5351	pull-request-available
13266725	Data types defined in DDL will lose precision and nullability when converting to properties	"Currently, data types defined in DDL will be converted to {{TypeInformation}} and use {{TypeStringUtils}} to serialize/deserialize which will lose the precision and nullablitiy information. 

We can use {{LogicalType#asSerializableString}} and {{LogicalTypeParser}} to serialize/deserialize data types which keeps all the information. But we need to figure out how to keep compability with previous versions."	FLINK	Resolved	2	7	5351	pull-request-available
13319117	Debezium-json format throws Exception when PG table's IDENTITY config is not FULL	"If the cdc data comes from a PG table and the PG table's IDENTITY config is not FULL，

the UPDATE changelog's update message will be null lead a NPE error.

We can add document to remind user set proper  IDENTITY for PG table in debezium doc

and improve the Exception message.

 

It's reported in [http://apache-flink.147419.n8.nabble.com/flink-1-11-cdc-td5298.html]"	FLINK	Closed	3	1	5351	pull-request-available
13287576	NullPointerException in GroupAggProcessFunction.close()	"CI run: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5586&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=48867695-c47f-5af3-2f21-7845611247b9

{code}
java.lang.NullPointerException: null
	at org.apache.flink.table.runtime.aggregate.GroupAggProcessFunction.close(GroupAggProcessFunction.scala:182) ~[flink-table_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:627) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:565) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:483) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]

{code}"	FLINK	Resolved	3	1	5351	pull-request-available, test-stability
13314196	Fix StateMigrationException because RetractableTopNFunction#ComparatorWrapper might be incompatible	"We found that in SQL jobs using ""Top-N"" functionality provided by the blink planner, the job state cannot be retrieved because of ""incompatible"" state serializers (in fact they are compatible).

The error log is displayed like below
{panel:title=taskmanager.log}
2020-06-30 09:19:32.089 [Rank(strategy=[RetractStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[appkey, serverid], orderBy=[quantity DESC], select=[appkey, serverid,  quantity]) (1/1)] INFO  org.apache.flink.runtime.taskmanager.Task  - Rank(strategy=[RetractStrategy], rankType=[ROW_NUMBER], rankRange=[rankStart=1, rankEnd=100], partitionBy=[appkey, serverid], orderBy=[quantity DESC], select=[appkey, serverid, oid, quantity]) (1/1) (bd4d2e4327efac57dc70e220b8de460b) switched from RUNNING to FAILED.
java.lang.RuntimeException: Error while getting state
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:62)
        at org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:144)
        at org.apache.flink.table.runtime.operators.rank.RetractableTopNFunction.open(RetractableTopNFunction.java:115)
        at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
        at org.apache.flink.streaming.api.operators.KeyedProcessOperator.open(KeyedProcessOperator.java:57)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:990)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flink.util.StateMigrationException: The new state serializer cannot be incompatible.
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.updateRestoredStateMetaInfo(RocksDBKeyedStateBackend.java:543)
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:491)
        at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:652)
        at org.apache.flink.runtime.state.KeyedStateFactory.createInternalState(KeyedStateFactory.java:47)
        at org.apache.flink.runtime.state.ttl.TtlStateFactory.createStateAndWrapWithTtlIfEnabled(TtlStateFactory.java:72)
        at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getOrCreateKeyedState(AbstractKeyedStateBackend.java:279)
        at org.apache.flink.runtime.state.AbstractKeyedStateBackend.getPartitionedState(AbstractKeyedStateBackend.java:328)
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getPartitionedState(DefaultKeyedStateStore.java:124)
        at org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:60)
        ... 13 more{panel}
 
After careful debugging, it is found to be an issue with the compatibility check of type serializers.
 
In short, during checkpointing, Flink serializes _SortedMapSerializer_ by creating a _SortedMapSerializerSnapshot_ object, and the original comparator is encapsulated within the object (here we call it _StreamExecSortComparator$579_).
 
At restoration, the object is read and restored as normal. However, during the construction of RetractableTopNFunction instance, another Comparator is provided by Flink as an argument (we call it _StreamExecSortComparator$626_), and it is later used in the _ValueStateDescriptor_ which acts like a key to the state store.
 
Here comes the problem: when the newly-restored Flink program tries to access state (_getState_) through the previously mentioned _ValueStateDescriptor_, the State Backend firstly detects whether the provided comparator in state descriptor is compatible with the one in snapshot, eventually the logic goes to the _equals_ method at _RetractableTopNFunction.ComparatorWrapper_ class.
 
In the equals method, here is a code snippet:
{code:java}
return generatedRecordComparator.getClassName().equals(oGeneratedComparator.getClassName()) &&
      generatedRecordComparator.getCode().equals(oGeneratedComparator.getCode()) &&
      Arrays.equals(generatedRecordComparator.getReferences(), oGeneratedComparator.getReferences());
{code}
After debugging, we found that the class name of comparator within snapshot is _StreamExecSortComparator$579_, and the class name of comparator provided in the new job is _StreamExecSortComparator$626_, hence this method always returns false, even though actually they are indeed compatible (acts the same). Also, because the code in each generator is generated independently, the corresponding varaibles within the two comparators are highly likely to be different (_isNullA$581_ vs _isNullA$682_).
 
Hence we believe that the implementation of equals method has serious flaws, and should be addressed in later releases."	FLINK	Closed	3	1	5351	pull-request-available
13238749	Introduce PartitionableTableSource for partition pruning	Many data sources are partitionable storage, e.g. HIVE, HDFS, Druid. And many queries just need to read a small subset of the total data. We can use partition information to prune or skip over files irrelevant to the user's queries. Both query optimization time and execution time can be reduced obviously, especially for a large partitioned table.	FLINK	Closed	3	2	5351	pull-request-available
13262596	MapType doesn't accept any subclass of java.util.Map	"Currently the conversion class of MapType is {{java.util.Map}}, but {{java.util.Map}} is an interface not a concrete class. So when verifying an instance of {{HashMap}} for MapType, it fails. 

For example:


{code:java}
		Map<String, Integer> map = new HashMap<>();
		map.put(""key1"", 1);
		map.put(""key2"", 2);
		map.put(""key3"", 3);
		assertEquals(
			""{key1=1, key2=2, key3=3}"",
			new ValueLiteralExpression(
				map,
				DataTypes.MAP(DataTypes.STRING(), DataTypes.INT()))
				.toString());
{code}

throws exception:

{code}
org.apache.flink.table.api.ValidationException: Data type 'MAP<STRING, INT>' does not support a conversion from class 'java.util.HashMap'.

	at org.apache.flink.table.expressions.ValueLiteralExpression.validateValueDataType(ValueLiteralExpression.java:236)
	at org.apache.flink.table.expressions.ValueLiteralExpression.<init>(ValueLiteralExpression.java:66)
{code}

It's easy to fix this by considering whether it's a subclass of Map. But I'm wondering what the default conversion class should be? "	FLINK	Closed	3	1	5351	pull-request-available
13336263	Replace 'collection' connector by 'values' connector for temporal join plan tests	Currently, both COLLECTION and VALUES connectors are `LookupTableSoure`, we can add a non lookup table source connector to cover  scan-only source.	FLINK	Closed	3	7	5351	pull-request-available
13307160	LIMIT queries are failed when adding sleeping time of async checkpoint	When we change the timing of operations (sleep after emit first record and sleep for async operation of checkpoint) with this [commit|https://github.com/apache/flink/commit/c05a0d865989c9959047cebcf2cd68b3838cc699], the test {{org.apache.flink.table.planner.runtime.stream.sql.AggregateITCase#testDifferentTypesSumWithRetract}} in flink-table-planner-blink is failed. 	FLINK	Closed	1	1	5351	pull-request-available
13267674	Improve some code of computed column in planner	"The following code can be improved which is introduced by FLINK-14665.

1. FlinkRelOptUtilTest#testToString shouldn't ignore attributes of RelNodes. The original expcted string makes sense. We should fix it to keep the result unchanged. 
2. The test {{CatalogTableITCase#testInsertSinkTableWithUnmatchedFields}} should fail and shouldn't verify the expected results (no results).
3. {{SqlToOperationConverterTest#testCreateTableWithComputedColumn}} change the udf calling to non-qualified way to verify the qualifying works well. "	FLINK	Resolved	3	7	5351	pull-request-available
13276527	ScalarOperatorsTest failed in travis	"The travis of release-1.9 failed with the following error:
{code:java}
14:50:19.796 [ERROR] ScalarOperatorsTest>ExpressionTestBase.evaluateExprs:161 Wrong result for: [CASE WHEN (CASE WHEN f2 = 1 THEN CAST('' as INT) ELSE 0 END) is null THEN 'null' ELSE 'not null' END] optimized to: [_UTF-16LE'not null':VARCHAR(8) CHARACTER SET ""UTF-16LE""] expected:<n[]ull> but was:<n[ot n]ull>
{code}
instance: [https://api.travis-ci.org/v3/job/629636107/log.txt]"	FLINK	Resolved	3	1	5351	pull-request-available
13301764	Support JSON serialization and deseriazation schema for RowData type	Add support {{JsonRowDataDeserializationSchema}} and {{JsonRowDataSerializationSchema}} for the new data structure {{RowData}}.	FLINK	Closed	3	7	5351	pull-request-available
13306250	PostgresCatalogITCase . testGroupByInsert() fails on CI	"{{org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase . testGroupByInsert}}


Error:
{code}
2020-05-20T16:36:33.9647037Z org.apache.flink.table.api.ValidationException: 
2020-05-20T16:36:33.9647354Z Field types of query result and registered TableSink mypg.postgres.primitive_table2 do not match.

2020-05-20T16:36:33.9648233Z Query schema: [int: INT NOT NULL, EXPR$1: VARBINARY(2147483647) NOT NULL, short: SMALLINT NOT NULL, EXPR$3: BIGINT, EXPR$4: FLOAT, EXPR$5: DOUBLE, EXPR$6: DECIMAL(10, 5), EXPR$7: BOOLEAN, EXPR$8: VARCHAR(2147483647), EXPR$9: CHAR(1) NOT NULL, EXPR$10: CHAR(1) NOT NULL, EXPR$11: VARCHAR(20), EXPR$12: TIMESTAMP(5), EXPR$13: DATE, EXPR$14: TIME(0), EXPR$15: DECIMAL(38, 18)]

2020-05-20T16:36:33.9650272Z Sink schema: [int: INT, bytea: VARBINARY(2147483647), short: SMALLINT, long: BIGINT, real: FLOAT, double_precision: DOUBLE, numeric: DECIMAL(10, 5), decimal: DECIMAL(10, 1), boolean: BOOLEAN, text: VARCHAR(2147483647), char: CHAR(1), character: CHAR(3), character_varying: VARCHAR(20), timestamp: TIMESTAMP(5), date: DATE, time: TIME(0), default_numeric: DECIMAL(38, 18)]

2020-05-20T16:36:33.9651218Z 	at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyImplicitCast(TableSinkUtils.scala:100)
2020-05-20T16:36:33.9651689Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:218)
2020-05-20T16:36:33.9652136Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:193)
2020-05-20T16:36:33.9652936Z 	at scala.Option.map(Option.scala:146)
2020-05-20T16:36:33.9653593Z 	at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:193)
2020-05-20T16:36:33.9653993Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:152)
2020-05-20T16:36:33.9654428Z 	at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:152)
2020-05-20T16:36:33.9654841Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-05-20T16:36:33.9655221Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-05-20T16:36:33.9655759Z 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-05-20T16:36:33.9656072Z 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-05-20T16:36:33.9656413Z 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-05-20T16:36:33.9656890Z 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-05-20T16:36:33.9657211Z 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
2020-05-20T16:36:33.9657525Z 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2020-05-20T16:36:33.9657878Z 	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:152)
2020-05-20T16:36:33.9658350Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1217)
2020-05-20T16:36:33.9658784Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:663)
2020-05-20T16:36:33.9659391Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:750)
2020-05-20T16:36:33.9659856Z 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:653)
2020-05-20T16:36:33.9660507Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil$.execInsertSqlAndWaitResult(TableEnvUtil.scala:27)
2020-05-20T16:36:33.9661115Z 	at org.apache.flink.table.planner.runtime.utils.TableEnvUtil.execInsertSqlAndWaitResult(TableEnvUtil.scala)
2020-05-20T16:36:33.9661583Z 	at org.apache.flink.connector.jdbc.catalog.PostgresCatalogITCase.testGroupByInsert(PostgresCatalogITCase.java:88)
{code}

Full log: https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/25/logs/93"	FLINK	Closed	1	1	5351	pull-request-available, test-stability
13342076	Optimize the exception message of FileSystemTableSink when missing format dependencies	"Current when the format factory failed to load, the following exception would be thrown:
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.sink'.

Table options are:

'auto-compaction'='true'
'connector'='filesystem'
'format'='csv'
'path'='file:///tmp/compaction'
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:166)
  at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:362)
  at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:220)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:164)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.Iterator$class.foreach(Iterator.scala:891)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:164)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1261)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:674)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:757)
  at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:664)
  at FileCompactionTest.main(FileCompactionTest.java:147)
Caused by: org.apache.flink.table.api.ValidationException: Please implement at least one of the following formats: BulkWriter.Factory, SerializationSchema, FileSystemFormatFactory.
  at org.apache.flink.table.filesystem.FileSystemTableSink.<init>(FileSystemTableSink.java:124)
  at org.apache.flink.table.filesystem.FileSystemTableFactory.createDynamicTableSink(FileSystemTableFactory.java:83)
  at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:163)
  ... 18 more
{code}
 

We might directly advice users to check if the format dependency is added or if there are package conflicts so that users would fix this issue faster.

 "	FLINK	Closed	2	1	5351	pull-request-available
13366625	Introduce SOURCE_WATERMARK built-infunction to preserve watermark from source	The SOURCE_WATERMARK function doesn't have concrete implementation. The {{eval()}} function should throw a meaningful exception message to indicate users the function should only be used in DDL to generate a watermark preserved from source system.	FLINK	Closed	3	7	5351	pull-request-available
13372979	Redundant CAST in plan when selecting window start and window end in window agg	"Add the following test case to {{org.apache.flink.table.planner.plan.stream.sql.agg.WindowAggregateTest}} to reproduce this bug.

{code:scala}
@Test
def testSessionFunction(): Unit = {
  val sql =
    """"""
      |SELECT
      |    COUNT(*),
      |    SESSION_START(proctime, INTERVAL '15' MINUTE),
      |    SESSION_END(proctime, INTERVAL '15' MINUTE)
      |FROM MyTable
      |    GROUP BY SESSION(proctime, INTERVAL '15' MINUTE)
    """""".stripMargin
  util.verifyExecPlan(sql)
}
{code}

The produced plan is
{code}
Calc(select=[EXPR$0, CAST(w$start) AS EXPR$1, CAST(w$end) AS EXPR$2])
+- GroupWindowAggregate(window=[SessionGroupWindow('w$, proctime, 900000)], properties=[w$start, w$end, w$proctime], select=[COUNT(*) AS EXPR$0, start('w$) AS w$start, end('w$) AS w$end, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[single])
      +- Calc(select=[proctime])
         +- WatermarkAssigner(rowtime=[rowtime], watermark=[(rowtime - 1000:INTERVAL SECOND)])
            +- Calc(select=[PROCTIME() AS proctime, rowtime])
               +- TableSourceScan(table=[[default_catalog, default_database, MyTable, project=[rowtime]]], fields=[rowtime])
{code}

This is because the nullability indicated by {{PlannerWindowStart#getResultType}} and {{SqlGroupedWindowFunction#WindowStartEndReturnTypeInference}} are different. Actually time attribute and window start / end should always be not null."	FLINK	Closed	3	1	5351	pull-request-available
13327691	TableSourceITCase.testStreamScanParallelism fails on private Azure accounts	"Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8381&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac
or
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8379&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac
or
https://dev.azure.com/rmetzger/Flink/_build/results?buildId=8369&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=58eb9526-6bcb-5835-ae76-f5bd5f6df6ac
(this change is already merged to master, so it is unlikely to cause the error)
{code}
2020-09-15T13:51:34.6773312Z org.apache.flink.api.common.InvalidProgramException: The implementation of the CollectionInputFormat is not serializable. The object probably contains or references non serializable fields.
2020-09-15T13:51:34.6774140Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:151)
2020-09-15T13:51:34.6774634Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:126)
2020-09-15T13:51:34.6775136Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:71)
2020-09-15T13:51:34.6775728Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.clean(StreamExecutionEnvironment.java:1913)
2020-09-15T13:51:34.6776617Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.addSource(StreamExecutionEnvironment.java:1601)
2020-09-15T13:51:34.6777322Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createInput(StreamExecutionEnvironment.java:1493)
2020-09-15T13:51:34.6778029Z 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createInput(StreamExecutionEnvironment.java:1483)
2020-09-15T13:51:34.6778887Z 	at org.apache.flink.table.factories.utils.TestCollectionTableFactory$CollectionTableSource.getDataStream(TestCollectionTableFactory.scala:159)
2020-09-15T13:51:34.6779659Z 	at org.apache.flink.table.factories.utils.TestCollectionTableFactory$CollectionTableSource.getDataStream(TestCollectionTableFactory.scala:134)
2020-09-15T13:51:34.6780394Z 	at org.apache.flink.table.plan.nodes.datastream.StreamTableSourceScan.translateToPlan(StreamTableSourceScan.scala:105)
2020-09-15T13:51:34.6781058Z 	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.translateInput(DataStreamSink.scala:189)
2020-09-15T13:51:34.6781655Z 	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.writeToSink(DataStreamSink.scala:84)
2020-09-15T13:51:34.6782266Z 	at org.apache.flink.table.plan.nodes.datastream.DataStreamSink.translateToPlan(DataStreamSink.scala:59)
2020-09-15T13:51:34.6782951Z 	at org.apache.flink.table.planner.StreamPlanner.org$apache$flink$table$planner$StreamPlanner$$translateToCRow(StreamPlanner.scala:274)
2020-09-15T13:51:34.6783640Z 	at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:119)
2020-09-15T13:51:34.6784227Z 	at org.apache.flink.table.planner.StreamPlanner$$anonfun$translate$1.apply(StreamPlanner.scala:116)
2020-09-15T13:51:34.6784799Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-09-15T13:51:34.6785345Z 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
2020-09-15T13:51:34.6785828Z 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-09-15T13:51:34.6786285Z 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-09-15T13:51:34.6786760Z 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-09-15T13:51:34.6787210Z 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-09-15T13:51:34.6787681Z 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
2020-09-15T13:51:34.6788168Z 	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
2020-09-15T13:51:34.6788648Z 	at org.apache.flink.table.planner.StreamPlanner.translate(StreamPlanner.scala:116)
2020-09-15T13:51:34.6789286Z 	at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.scala:178)
2020-09-15T13:51:34.6790031Z 	at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toAppendStream(StreamTableEnvironmentImpl.scala:103)
2020-09-15T13:51:34.6790705Z 	at org.apache.flink.table.api.bridge.scala.TableConversions.toAppendStream(TableConversions.scala:78)
2020-09-15T13:51:34.6791362Z 	at org.apache.flink.table.runtime.stream.table.TableSourceITCase.testStreamScanParallelism(TableSourceITCase.scala:118)
2020-09-15T13:51:34.6791907Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-15T13:51:34.6792350Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-15T13:51:34.6792906Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-15T13:51:34.6793391Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-15T13:51:34.6793869Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-15T13:51:34.6794406Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-15T13:51:34.6795396Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-15T13:51:34.6796052Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-15T13:51:34.6796565Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-15T13:51:34.6797051Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-15T13:51:34.6797481Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-15T13:51:34.6797896Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-15T13:51:34.6798465Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-15T13:51:34.6799768Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-15T13:51:34.6800273Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-15T13:51:34.6800728Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-15T13:51:34.6801197Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-15T13:51:34.6801656Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-15T13:51:34.6802119Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-15T13:51:34.6802599Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-15T13:51:34.6803183Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-15T13:51:34.6803687Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-15T13:51:34.6804117Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-15T13:51:34.6804589Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-15T13:51:34.6805148Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-15T13:51:34.6805721Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-15T13:51:34.6806253Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-15T13:51:34.6806844Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-15T13:51:34.6807445Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-15T13:51:34.6807972Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-15T13:51:34.6808487Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-15T13:51:34.6809041Z Caused by: java.lang.RuntimeException: Row arity of record (3) does not match this serializers field length (1).
2020-09-15T13:51:34.6809610Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:181)
2020-09-15T13:51:34.6810199Z 	at org.apache.flink.api.java.typeutils.runtime.RowSerializer.serialize(RowSerializer.java:58)
2020-09-15T13:51:34.6810784Z 	at org.apache.flink.api.java.io.CollectionInputFormat.writeObject(CollectionInputFormat.java:90)
2020-09-15T13:51:34.6811287Z 	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
2020-09-15T13:51:34.6811746Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-15T13:51:34.6814374Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-15T13:51:34.6814842Z 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1140)
2020-09-15T13:51:34.6815329Z 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
2020-09-15T13:51:34.6815850Z 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
2020-09-15T13:51:34.6816355Z 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
2020-09-15T13:51:34.6816826Z 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
2020-09-15T13:51:34.6817345Z 	at org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:586)
2020-09-15T13:51:34.6817871Z 	at org.apache.flink.api.java.ClosureCleaner.clean(ClosureCleaner.java:133)
2020-09-15T13:51:34.6818350Z 	... 59 more
{code}

I don't understand why this is failing only in my personal azure account (on branches with different changes)."	FLINK	Closed	2	1	5351	pull-request-available, test-stability
13243703	SplitAggregateITCase.testMinMaxWithRetraction failed on Travis	"{{SplitAggregateITCase.testMinMaxWithRetraction}} failed on Travis with

{code}
Failures: 
10:50:43.355 [ERROR]   SplitAggregateITCase.testMinMaxWithRetraction:195 expected:<List(2,2,2,1, 5,1,4,2, 6,2,2,1)> but was:<List(2,1,2,1, 5,1,4,2, 6,2,2,1)>
{code}

https://api.travis-ci.org/v3/job/554991853/log.txt"	FLINK	Resolved	1	1	5351	test-stability
13274303	Improve Kafka connector properties make append update-mode as default	Currently, {{update-mode}} is a required properties of Kafka. However, Kafka only support append {{update-mode}}. It is weird and un-user-friendly to make users set such a properties mandatory. We can make this properties as optional and {{append}} by default.	FLINK	Resolved	2	4	5351	pull-request-available
13307388	"Translate ""Python Table API Installation"" page into Chinese"	"The page url is https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/python/installation.html

The markdown file is located in flink/docs/dev/table/python/installation.zh.md"	FLINK	Closed	3	7	5351	pull-request-available
12987796	Add a basic streaming Table API example	Although the Table API does not offer much streaming features yet, there should be a runnable example showing how to convert, union, filter and project streams with the Table API.	FLINK	Resolved	3	2	5351	starter
13352233	USE DATABASE & USE CATALOG fails with quoted identifiers containing characters to be escaped in Flink SQL client	I have a database which name is mod, when I use `use mod` to switch to the db,the system throw an exception, I surround it with backticks ,it is still not well	FLINK	Closed	3	1	5351	pull-request-available
13237194	Introduce new Interfaces for source and sink to make Blink runner work	"In order to support Blink batch and temporal table join, we need some new source&sink interfaces.

1. Introduce {{InputFormatTableSource}}
 - add {{isBounded}} interface to {{StreamTableSource}}
 - {{InputFormatTableSource}} extends {{StreamTableSource}} and expose {{getInputFormat}}
 - removes {{BatchTableSource}} and {{StreamTableSource}} in blink planner
 - support it in blink and flink planner

2. Introduce {{OutputFormatTableSink}}
 - {{OutputFormatTableSink}} extends {{StreamTableSink}} expose {{getOutputFormat}}
 - removes {{BatchTableSink}} in blink planner
 - support it in blink and flink planner

3. Introduce {{LookupableTableSource}}
 - removes {{LookupableTableSource}} and {{LookupConfig}} in blink planner
 - support it only in blink planner

4. Expose {{getTableStats}} in {{TableSource}}
 - support it in blink and flink planner"	FLINK	Closed	3	2	5351	pull-request-available
13316393	Remove deprecated classes in flink-connector-jdbc	We have refactored the class structure of flink-connector-jdbc module and kept the old API classes in {{org.apache.flink.api.java.io.jdbc}} compatible purpose. Now, it's safe to remove these classes. 	FLINK	Closed	3	4	5351	pull-request-available
13226867	Support unbounded aggregate in streaming table runtime	"This ticket is aiming to support unbounded aggregate in streaming runtime. This should includes:

1. GroupAggFunction: function that support unbounded aggregate without optimizations
2. MiniBatchGroupAggFunction: function that support unbounded aggregate with minibatch optimization
3. MiniBatchLocalGroupAggFunction & MiniBatchGlobalGroupAggFunction:  function that support unbounded aggregate with local combine optimization"	FLINK	Closed	3	2	5351	pull-request-available
13290446	Improve default flush strategy for JDBC sink to make it work out-of-box	"Currently, old JDBC sink provides 2 flush options:

{code}
'connector.write.flush.max-rows' = '5000', -- default is 5000
'connector.write.flush.interval' = '2s', -- no default value
{code}

That means if flush interval is not set, the buffered output rows may not be flushed to database for a long time. That is a surprising behavior because no results are outputed by default. 

So we propose to have a default flush '1s' interval and '100' rows for JDBC sink flush. This only applies to new JDBC sink options:

{code}
'sink.buffer-flush.max-rows' = '100'
'sink.buffer-flush.interval' = '1s'
{code}



"	FLINK	Closed	2	4	5351	pull-request-available
13305641	Align the behavior between the new and legacy HBase table source	The legacy HBase table source, i.e. {{HBaseTableSource}}, supports projection push down. In order to make the user experience consistent. We should align the behavior and add tests. 	FLINK	Closed	3	7	5351	pull-request-available
13287969	"Annoying ""Cannot find FunctionDefinition"" messages with SQL for f_proctime or ="	"When running the following SQL query
{code}
SELECT
    D1.col1 AS A,
    D1.col2 AS B,
    D1.col3 AS C,
    D1.col4 AS D,
    D1.col5 AS E,
    D2.col1 AS F,
    D2.col2 AS G,
    D2.col3 AS H,
    D2.col4 AS I,
    D2.col5 AS J,
    D3.col1 AS K,
    D3.col2 AS L,
    D3.col3 AS M,
    D3.col4 AS N,
    D3.col5 AS O,
    D4.col1 AS P,
    D4.col2 AS Q,
    D4.col3 AS R,
    D4.col4 AS S,
    D4.col5 AS T,
    D5.col1 AS U,
    D5.col2 AS V,
    D5.col3 AS W,
    D5.col4 AS X,
    D5.col5 AS Y
FROM
    fact_table,
    LATERAL TABLE (dimension_table1(f_proctime)) AS D1,
    LATERAL TABLE (dimension_table2(f_proctime)) AS D2,
    LATERAL TABLE (dimension_table3(f_proctime)) AS D3,
    LATERAL TABLE (dimension_table4(f_proctime)) AS D4,
    LATERAL TABLE (dimension_table5(f_proctime)) AS D5
WHERE
    fact_table.dim1     = D1.id
    AND fact_table.dim2 = D2.id
    AND fact_table.dim3 = D3.id
    AND fact_table.dim4 = D4.id
    AND fact_table.dim5 = D5.id
{code}

with the Blink planner, it prints a log of bogus warnings about unknown functions for things like {{f_proctime}} or {{=}} at INFO level which should be DEBUG level at least in order not to bother the users with it. The messages I got are:

{code}
13:33:59,590 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,641 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,644 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,647 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,650 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition f_proctime from any loaded modules
13:33:59,662 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,665 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,666 INFO  org.apache.flink.table.module.ModuleManager                   - Got FunctionDefinition and from module core
13:33:59,667 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,668 INFO  org.apache.flink.table.module.ModuleManager                   - Got FunctionDefinition and from module core
13:33:59,669 INFO  org.apache.flink.table.module.ModuleManager                   - Cannot find FunctionDefinition = from any loaded modules
13:33:59,670 INFO  org.apache.flink.table.module.ModuleManager                   - Got FunctionDefinition and from module core
{code}"	FLINK	Resolved	3	1	5351	pull-request-available
13337366	Refactor and merge SupportsComputedColumnPushDown and SupportsWatermarkPushDown interfaces	"As discussed in mailing list [1], the existing SupportsComputedColumnPushDown and SupportsWatermarkPushDown are confusing and hard to implement for connectors. The 
{{SupportsComputedColumnPushDown}} only used for watermark push down. Therefore, 
 combining them into a single interface {{SupportsWatermarkPushDown}} would be better and also work. The proposed interface looks like this:


{code:java}
public interface SupportsWatermarkPushDown {
    
    void applyWatermark(org.apache.flink.table.sources.wmstrategies.WatermarkStrategy<RowData> watermarkStrategy);
    
}
{code}



[1]: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Merge-SupportsComputedColumnPushDown-and-SupportsWatermarkPushDown-td44387.html"	FLINK	Closed	3	7	5351	pull-request-available
13375756	BytesHashMap has many verbose logs	"I ran a query which contains operators using BytesHashMap, and the logs file contain many following logs

{code}
2021-04-29 14:00:47,420 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 758, 24838144 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:48,322 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 672, 22020096 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:48,345 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:49,611 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 634, 20774912 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:49,703 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 757, 24805376 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:50,582 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 658, 21561344 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:50,621 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 762, 24969216 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:51,837 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 636, 20840448 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:51,862 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 758, 24838144 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:52,836 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 649, 21266432 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:52,899 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 761, 24936448 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,064 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 617, 20217856 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,148 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 754, 24707072 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,871 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 673, 22052864 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:54,959 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 765, 25067520 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:55,912 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 641, 21004288 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:56,017 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 761, 24936448 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:56,924 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 639, 20938752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:57,047 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 759, 24870912 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:57,727 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 666, 21823488 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:57,768 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:58,541 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 665, 21790720 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:58,589 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 766, 25100288 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:59,373 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 657, 21528576 in bytes, init allocating 32 for bucket area.
2021-04-29 14:00:59,422 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:00,201 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 673, 22052864 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:00,248 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 766, 25100288 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,015 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 665, 21790720 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,080 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 764, 25034752 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,626 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 689, 22577152 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:01,698 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 767, 25133056 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:02,017 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 721, 23625728 in bytes, init allocating 32 for bucket area.
2021-04-29 14:01:02,108 INFO  org.apache.flink.table.runtime.util.collections.binary.BytesMap [] - reset BytesHashMap with record memory segments 773, 25329664 in bytes, init allocating 32 for bucket area.
{code}"	FLINK	Closed	3	1	5351	pull-request-available
12987794	Create a batch SQL example	"Currently there is no runnable code example in `flink-table` showing a working batch SQL query with the Table API.

A Scala and Java example should be added."	FLINK	Resolved	3	2	5351	starter
13353502	Implement mini-batch optimized slicing window aggregate operator	"We have supported cumulative windows in FLINK-19605. However, the current cumulative window is not efficient, because the slices are not shared. 

We leverages the slicing ideas proposed in FLINK-7001 and this design doc [1]. The slicing is an optimized implementation for hopping, cumulative, tumbling windows. Besides of that, we introduced ManagedMemory based mini-batch optimization for the slicing window aggregate operator, this can tremendously reduce the accessing of state and get the higher throughtput without latency loss.  

[1]: https://docs.google.com/document/d/1ziVsuW_HQnvJr_4a9yKwx_LEnhVkdlde2Z5l6sx5HlY/edit#"	FLINK	Closed	3	7	5351	pull-request-available
13375757	TUMBLE TVF should throw helpful exception when specifying second interval parameter	"Currently, the following query can run and no exception is thrown. 

However, the second interval parameter (i.e. the offset parameter) is not supported yet. We should throw a exception for this. 


{code:sql}
select 
  date_format(window_end, 'yyyy-MM-dd') as date_str,
  date_format(window_end, 'HH:mm') as time_str,
  count(distinct user_id) as uv
from table(tumble(table user_behavior, descriptor(ts), interval '10' minute, interval '1' day))
group by window_start, window_end;
{code}"	FLINK	Closed	3	1	5351	pull-request-available
13295288	Refactor retraction rules to support inferring ChangelogMode	"Current retraction machanism only support 2 message kinds (+ and -). However, since FLIP-95, we will introduce more message kinds to users (insert/delete/update_before/update_after). 

In order to support that, we should first refactor current retraction rules to support ChangelogMode inference. In previous, every node will be attached with a AccMode trait after retraction rule. In the proposed design, we will infer ChangelogMode trait for every node. 

Design documentation: https://docs.google.com/document/d/1n_iXIQsKT3uiBqENR8j8RdjRhZfzMhhB66QZvx2rFjE/edit?ts=5e8419c1#"	FLINK	Closed	3	7	5351	pull-request-available
13306796	Improve interface of ScanFormatFactory and SinkFormatFactory	"There is some problem with current ScanForamtFactory and SinkFormatFactory interfaces:
1) {{ScanFormat#createScanFormat}} only accepts {{ScanTableSource.Context}}, which means it can’t work in lookup source.
2) The naming of {{ScanFormat}} also indicates it is only used in scan source. But a lookup source should be able to work with format too.
3) It’s confusing that {{ScanFormatFactory#createScanFormat}} and {{ScanFormat#createScanFormat}} (create itself?)

The proposed changes:

1. Have a common interface DynamicTableSource.Context, and make Context of ScanTableSource and LookupTableSource extend it, and rename them to LookupContext and ScanContext
2. Change parameter of ScanFormat.createScanFormat from ScanTableSource.Context to DynamicTableSource.Context
3. Rename ScanFormat.createScanFormat to DecodingFormat#createRuntimeDecoder()
4. Rename SinkFormat.createSinkFormat to EncodingFormat#createRuntimeEncoder()
5. Rename ScanFormatFactory to DecodingFormatFactory
6. Rename SinkFormatFactory to EncodingFormatFactory
"	FLINK	Closed	1	7	5351	pull-request-available
13229178	Support Temporal Table Join in blink planner	"Support translate following ""FOR SYSTEM_TIME AS OF"" query into {{StreamExecTemporalTableJoin}}.

{code:sql}
SELECT
  o.amout, o.currency, r.rate, o.amount * r.rate
FROM
  Orders AS o
  JOIN LatestRates FOR SYSTEM_TIME AS OF o.proctime AS r
  ON r.currency = o.currency
{code}

This is an extension to current temporal join (FLINK-9738) using a standard syntax introduced in Calcite 1.19."	FLINK	Closed	3	2	5351	pull-request-available
13379431	UpsertKafkaTableITCase.testAggregate fail due to ConcurrentModificationException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18151&view=logs&j=c5612577-f1f7-5977-6ff6-7432788526f7&t=53f6305f-55e6-561c-8f1e-3a1dde2c77df&l=6613


{code:java}
2021-05-19T21:28:02.8689083Z May 19 21:28:02 [ERROR] testAggregate[format = avro](org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase)  Time elapsed: 2.067 s  <<< ERROR!
2021-05-19T21:28:02.8708337Z May 19 21:28:02 java.util.ConcurrentModificationException
2021-05-19T21:28:02.8710333Z May 19 21:28:02 	at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)
2021-05-19T21:28:02.8712083Z May 19 21:28:02 	at java.util.HashMap$ValueIterator.next(HashMap.java:1474)
2021-05-19T21:28:02.8712680Z May 19 21:28:02 	at java.util.AbstractCollection.toArray(AbstractCollection.java:141)
2021-05-19T21:28:02.8713142Z May 19 21:28:02 	at java.util.ArrayList.addAll(ArrayList.java:583)
2021-05-19T21:28:02.8716029Z May 19 21:28:02 	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.lambda$getResults$0(TestValuesRuntimeFunctions.java:114)
2021-05-19T21:28:02.8717007Z May 19 21:28:02 	at java.util.HashMap$Values.forEach(HashMap.java:981)
2021-05-19T21:28:02.8718041Z May 19 21:28:02 	at org.apache.flink.table.planner.factories.TestValuesRuntimeFunctions.getResults(TestValuesRuntimeFunctions.java:114)
2021-05-19T21:28:02.8719339Z May 19 21:28:02 	at org.apache.flink.table.planner.factories.TestValuesTableFactory.getResults(TestValuesTableFactory.java:184)
2021-05-19T21:28:02.8720309Z May 19 21:28:02 	at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestUtils.waitingExpectedResults(KafkaTableTestUtils.java:82)
2021-05-19T21:28:02.8721311Z May 19 21:28:02 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.wordFreqToUpsertKafka(UpsertKafkaTableITCase.java:440)
2021-05-19T21:28:02.8730402Z May 19 21:28:02 	at org.apache.flink.streaming.connectors.kafka.table.UpsertKafkaTableITCase.testAggregate(UpsertKafkaTableITCase.java:73)
2021-05-19T21:28:02.8731390Z May 19 21:28:02 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-05-19T21:28:02.8732095Z May 19 21:28:02 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-05-19T21:28:02.8732935Z May 19 21:28:02 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-05-19T21:28:02.8733726Z May 19 21:28:02 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-05-19T21:28:02.8734598Z May 19 21:28:02 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-05-19T21:28:02.8735450Z May 19 21:28:02 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-05-19T21:28:02.8736313Z May 19 21:28:02 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-05-19T21:28:02.8737329Z May 19 21:28:02 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-05-19T21:28:02.8738165Z May 19 21:28:02 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-05-19T21:28:02.8738989Z May 19 21:28:02 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-05-19T21:28:02.8739741Z May 19 21:28:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-19T21:28:02.8740563Z May 19 21:28:02 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-05-19T21:28:02.8741340Z May 19 21:28:02 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-05-19T21:28:02.8742077Z May 19 21:28:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-19T21:28:02.8742802Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-05-19T21:28:02.8743594Z May 19 21:28:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-05-19T21:28:02.8744811Z May 19 21:28:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-05-19T21:28:02.8745580Z May 19 21:28:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-19T21:28:02.8746330Z May 19 21:28:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-19T21:28:02.8747222Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-19T21:28:02.8748007Z May 19 21:28:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-19T21:28:02.8748791Z May 19 21:28:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-19T21:28:02.8749564Z May 19 21:28:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-19T21:28:02.8750251Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-05-19T21:28:02.8785419Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-05-19T21:28:02.8786427Z May 19 21:28:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-19T21:28:02.8787364Z May 19 21:28:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-19T21:28:02.8788096Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-19T21:28:02.8788855Z May 19 21:28:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-19T21:28:02.8789859Z May 19 21:28:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-19T21:28:02.8790792Z May 19 21:28:02 	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
2021-05-19T21:28:02.8791626Z May 19 21:28:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-19T21:28:02.8792399Z May 19 21:28:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-05-19T21:28:02.8793090Z May 19 21:28:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-05-19T21:28:02.8794167Z May 19 21:28:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-19T21:28:02.8794864Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-05-19T21:28:02.8795467Z May 19 21:28:02 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-05-19T21:28:02.8796145Z May 19 21:28:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-05-19T21:28:02.8796795Z May 19 21:28:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-05-19T21:28:02.8797701Z May 19 21:28:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-05-19T21:28:02.8798488Z May 19 21:28:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-05-19T21:28:02.8799258Z May 19 21:28:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-05-19T21:28:02.8799967Z May 19 21:28:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-05-19T21:28:02.8800706Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-05-19T21:28:02.8801594Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-05-19T21:28:02.8802426Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-05-19T21:28:02.8803344Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-05-19T21:28:02.8804358Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-05-19T21:28:02.8805238Z May 19 21:28:02 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-05-19T21:28:02.8806153Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-05-19T21:28:02.8807081Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-05-19T21:28:02.8808226Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-05-19T21:28:02.8809039Z May 19 21:28:02 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-05-19T21:28:02.8809548Z May 19 21:28:02 
{code}
"	FLINK	Closed	3	1	5351	test-stability
13341962	Fix ArrayIndexOutOfBounds for RetractableTopNFunction	"We can get the bug by adding the case into the UnitTest {{RetractableTopNFunctionTest}}


{code:java}
// Some comments here
	@Test
	public void testCornerCase() throws Exception {
		AbstractTopNFunction func = createFunction(RankType.ROW_NUMBER, new ConstantRankRange(1, 3), false,
				false);
		OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(func);
		testHarness.open();
		testHarness.processElement(insertRecord(""a"", 1L, 1));
		testHarness.processElement(insertRecord(""a"", 2L, 2));
		testHarness.processElement(insertRecord(""a"", 3L, 2));
		testHarness.processElement(insertRecord(""a"", 4L, 2));
		testHarness.processElement(insertRecord(""a"", 5L, 3));
		testHarness.processElement(insertRecord(""a"", 6L, 4));
		testHarness.processElement(updateBeforeRecord(""a"", 2L, 2));
		testHarness.close();
	}
{code}
"	FLINK	Closed	3	1	5351	pull-request-available
13296762	Introduce a new HBase connector with new property keys	"This new hbase connector should use new interfaces proposed by FLIP-95, e.g. DynamicTableSource, DynamicTableSink, and Factory.

The new proposed keys :
||Old key||New key||Note||
|connector.type|connector| |
|connector.version|N/A|merged into 'connector' key|
|connector.table-name|table-name| |
|connector.zookeeper.quorum|zookeeper.quorum| |
|connector.zookeeper.znode.parent|zookeeper.znode-parent| |
|connector.write.buffer-flush.max-size|sink.buffer-flush.max-size| |
|connector.write.buffer-flush.max-rows|sink.buffer-flush.max-rows| |
|connector.write.buffer-flush.interval|sink.buffer-flush.interval| |
 
 "	FLINK	Closed	3	7	5351	pull-request-available
13357041	Fix Cumulative and Hopping window should accumulate late events belonging to the cleaned slice	"Currently, the CUMULATE window drops elements belonging to the cleaned slices. This will lead to more inaccurate result than without slicing optimization.

Hopping window has the similar problem, you can reproduce the problem with the following test in {{WindowAggregateJsonITCase}}.


{code:scala}
    @Test
    public void testEventTimeHopWindow2() throws Exception {
        createTestValuesSinkTable(""MySink"", ""name STRING"", ""cnt BIGINT"");
        String jsonPlan =
                tableEnv.getJsonPlan(
                        ""insert into MySink select\n""
                                + ""  name,\n""
                                + ""  COUNT(*)\n""
                                + ""FROM TABLE(\n""
                                + ""   HOP(TABLE MyTable, DESCRIPTOR(rowtime), INTERVAL '5' SECOND, INTERVAL '10' SECOND))\n""
                                + ""GROUP BY name, window_start, window_end"");
        tableEnv.executeJsonPlan(jsonPlan).await();

        List<String> result = TestValuesTableFactory.getResults(""MySink"");
        assertResult(
                Arrays.asList(
                        ""+I[a, 1]"",
                        ""+I[a, 4]"",
                        ""+I[a, 6]"",
                        ""+I[b, 1]"",
                        ""+I[b, 1]"",
                        ""+I[b, 1]"",
                        ""+I[b, 1]"",
                        ""+I[b, 2]"",
                        ""+I[b, 2]"",
                        ""+I[null, 1]"",
                        ""+I[null, 1]""),
                result);
    }
{code}"	FLINK	Closed	2	7	5351	pull-request-available
13319033	Adding flink-table-api-java-bridge_2.11 to a Flink job kills the IDE logging	"Steps to reproduce:
- Set up a Flink project using a Maven archetype
- Add ""flink-table-api-java-bridge_2.11"" as a dependency
- Running Flink won't produce any log output

Probable cause:
""flink-table-api-java-bridge_2.11"" has a dependency to ""org.apache.flink:flink-streaming-java_2.11:test-jar:tests:1.11.0"", which contains a ""log4j2-test.properties"" file.

When I disable Log4j2 debugging (with ""-Dlog4j2.debug""), I see the following line:
{code}
DEBUG StatusLogger Reconfiguration complete for context[name=3d4eac69] at URI jar:file:/Users/robert/.m2/repository/org/apache/flink/flink-streaming-java_2.11/1.11.0/flink-streaming-java_2.11-1.11.0-tests.jar!/log4j2-test.properties (org.apache.logging.log4j.core.LoggerContext@568bf312) with optional ClassLoader: null
{code}

"	FLINK	Closed	3	1	5351	pull-request-available
13287407	Generic type can not be matched when convert table to stream.	"The query result schema printed by table.printSchema():
{noformat}
 |-- deviceId: BIGINT
 |-- channel: STRING
 |-- schemaId: BIGINT
 |-- productId: BIGINT
 |-- schema: LEGACY('RAW', 'ANY<com.yunmo.iot.schema.Schema>')
{noformat}
then excuting table.toRetractStream[DeviceSchema].print(), exception throwed:
{noformat}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Field types of query result and registered TableSink do not match.
 Query schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: RAW('com.yunmo.iot.schema.Schema', ?)]
 Sink schema: [deviceId: BIGINT, channel: STRING, schemaId: BIGINT, productId: BIGINT, schema: LEGACY('RAW', 'ANY<com.yunmo.iot.schema.Schema>')]{noformat}
The com.yunmo.iot.schema.Schema is a generic type.

The schema field of Query schema change from LEGACY('RAW' to RAW, but the Sink schema still a LEGACY('RAW'"	FLINK	Resolved	3	1	5351	pull-request-available
13288307	Improve exception message when reading an unbounded source in batch mode	"We can just ignore watermark in batch mode. 

cc [~jark]"	FLINK	Resolved	3	4	5351	pull-request-available
13285715	ExpressionReducer shouldn't escape the reduced string value	"ExpressionReducer shouldn't escape the reduced string value, the escaping should only happen in code generation, otherwise the output result is inccorect. 

The problem is this line I guess: https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala#L142

Here is a simple example to reproduce the problem:

{code:java}
  val smallTupleData3: Seq[(Int, Long, String)] = {
    val data = new mutable.MutableList[(Int, Long, String)]
    data.+=((1, 1L, ""你好""))
    data.+=((2, 2L, ""你好""))
    data.+=((3, 2L, ""你好世界""))
    data
  }

  @Test
  def test(): Unit = {
    val t = env.fromCollection(smallTupleData3)
      .toTable(tEnv, 'a, 'b, 'c)
    tEnv.createTemporaryView(""MyTable"", t)
    val sqlQuery = s""select * from MyTable where c = '你好'""

    val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]
    val sink = new TestingAppendSink
    result.addSink(sink)
    env.execute()
    println(sink.getAppendResults.mkString(""\n""))
  }
{code}

The output:

{code:java}
1,1,\u4F60\u597D
2,2,\u4F60\u597D
{code}

This is also mentioned in user mailing list: http://apache-flink.147419.n8.nabble.com/ParquetTableSource-blink-table-planner-tp1696p1720.html
"	FLINK	Resolved	2	1	5351	pull-request-available
13273411	Introduce TypeTransformation interface and basic transformations	"Currently, the default DataType derived from properties is the default conversion now. However, some connectors/formats are still using sql Timestamp. But bridging DataType into sql Timestamp  is not simple, because DataType is nested. So we propose to introduce a TypeTransformation which transform one data type to another, this is also a useful tool for FLIP-65. 

The proposal including:
- Remove {{CallContext}} from the exiting {{TypeTransformation}}.
- add a package {{o.a.f.table.types.inference.transforms}}
- add commonly used transform classes there, e.g. {{timeToSqlTypes}}
- add a class `o.a.f.table.types.inference.TypeTransforms` for listing all available transforms"	FLINK	Resolved	3	4	5351	pull-request-available
13218671	Support Code Generation for RexNode	Introduce {{ExprCodeGenerator}} to support generate codes from RexNode.	FLINK	Closed	3	2	5351	pull-request-available
13275663	"Correct the terminology of ""Time-windowed Join"" to ""Interval Join"" in Table API & SQL"	"Currently, in the docuementation, we call the joins with time conditions as ""Time-windowed Join"". However, it is called ""Interval Join"" in DataStream. We should align the terminology in Flink project. 

From my point of view, ""Interval Join"" is more suitable, because it joins a time interval range of right stream[1]. And ""Windowed Join"" should be joins data in the same window, this is also described in DataStream API. 

For Table API & SQL, the ""Time-windowed Join"" is the ""Interval Join"" in DataStream. And we miss the new feature ""Windowed Join"" in Table API & SQL.

I propose to correct the terminology in docs before 1.10 is release. 


[1]: https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/joining.html#interval-join
[2]: https://ci.apache.org/projects/flink/flink-docs-master/dev/stream/operators/joining.html#window-join

Discussion thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Correct-the-terminology-of-quot-Time-windowed-Join-quot-to-quot-Interval-Join-quot-in-Table-L-td36202.html#a36208
Voting thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Rename-terminology-quot-Time-windowed-Join-quot-to-quot-Interval-Join-quot-in-Table-API-amp-SQL-td36370.html"	FLINK	Closed	1	4	5351	pull-request-available
13231910	Synchronize the latest documentation changes into Chinese documents	There are several commits to documentations have not been synchronized to Chinese documents, i.e. `xx.zh.md`. This pull request will synchronize the latest changes into Chinese documents.	FLINK	Closed	4	7	5351	pull-request-available
13248467	Enable reuse forks for integration tests in blink planner	As discussed in https://github.com/apache/flink/pull/9180 , we find that with enabling reuse forks we can save ~20min (50min -> 30min) for blink planner test. 	FLINK	Resolved	3	7	5351	pull-request-available
13307941	Redesign Table & SQL Connectors pages	A lot of contents in https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connect.html#overview is out-dated. There are also many frictions on the Descriptor API and YAML file. I would propose to remove them in the new Overview page, we should encourage users to use DDL for now. We can add them back once Descriptor API and YAML API is ready again. 	FLINK	Closed	1	7	5351	pull-request-available
13312938	CollectionExecutorTest failed to compiled in release-1.11	 !image-2020-06-23-10-00-45-519.png! 	FLINK	Closed	1	4	5351	pull-request-available
13250410	Fix TableFactory doesn't work with DDL when containing TIMESTAMP/DATE/TIME types	"Currently, in blink planner, we will convert DDL to {{TableSchema}} with new type system, i.e. DataTypes.TIMESTAMP()/DATE()/TIME() whose underlying TypeInformation are  Types.LOCAL_DATETIME/LOCAL_DATE/LOCAL_TIME. 

However, this makes the existing connector implementations (Kafka, ES, CSV, etc..) don't work because they only accept the old TypeInformations (Types.SQL_TIMESTAMP/SQL_DATE/SQL_TIME).

A simple solution is encode DataTypes.TIMESTAMP() as ""TIMESTAMP"" when translating to properties. And will be converted back to the old TypeInformation: Types.SQL_TIMESTAMP. This would fix all factories at once.
"	FLINK	Resolved	2	1	5351	pull-request-available
13328904	[umbrella] Migrate Filesystem and Hive to new source interface	"Runtime implements to FLIP-27:
 * Migrate Filesystem source
 * Migrate Hive source"	FLINK	Closed	10200	2	6732	auto-deprioritized-major, auto-deprioritized-minor
13446540	Introduce a filesystem catalog for table store	"The Table Store has store the schema and other information on the file system so that it can actually provide a FileSystemCatalog for users to use.

This catalog is provided in JIRA:
- It supports database related management
- It supports the creation and deletion of tables
- It supports table changes, but currently can only modify the options and other information, the underlying does not yet support the schema column information modification

Currently this Catalog only supports filestore, logstore capabilities are still to be developed."	FLINK	Closed	3	2	6732	pull-request-available
13468839	Planner free in flink-table-store-codegen	"We currently have the table-planner bundled into flink-table-store-codegen, which causes:

* bundle jar is too big, 20+MB
* Dependence on planner code will make it difficult to be compatible with multiple versions of Flink"	FLINK	Closed	3	7	6732	pull-request-available
13336670	Introduce File streaming compaction operators	Introduce CompactCoordinator and CompactOperator.	FLINK	Closed	3	7	6732	pull-request-available
13428241	Introduce log store	"Introduce log store:
 * Introduce log store interfaces
 * Implement Kafka log store"	FLINK	Closed	3	7	6732	pull-request-available
13303983	Prepare Hive partitioned streaming source	"* Refactor ContinuousFileReaderOperator to read generic split
 * HiveTableInputFormat implements CheckpointableInputFormat"	FLINK	Closed	3	7	6732	pull-request-available
13293693	HiveModuleTest failed to compile on release-1.10	"The cron task of release-1.10 failed to compile with the following exception:
{code}
23:36:45.190 [ERROR] /home/travis/build/apache/flink/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/table/module/hive/HiveModuleTest.java:[158,45] constructor HiveModule in class org.apache.flink.table.module.hive.HiveModule cannot be applied to given types;
 required: java.lang.String
 found: no arguments
 reason: actual and formal argument lists differ in length
{code}

instance: [https://api.travis-ci.org/v3/job/666450476/log.txt]"	FLINK	Resolved	3	1	6732	pull-request-available
13280094	Cannot use generic types as the result of an AggregateFunction in Blink planner	"It is not possible to use a GenericTypeInfo for a result type of an {{AggregateFunction}} in a retract mode with state cleaning disabled.

{code}

  @Test
  def testGenericTypes(): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val setting = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()
    val tEnv = StreamTableEnvironment.create(env, setting)
    val t = env.fromElements(1, 2, 3).toTable(tEnv, 'a)

    val results = t
      .select(new GenericAggregateFunction()('a))
      .toRetractStream[Row]

    val sink = new TestingRetractSink
    results.addSink(sink).setParallelism(1)
    env.execute()
  }

class RandomClass(var i: Int)

class GenericAggregateFunction extends AggregateFunction[java.lang.Integer, RandomClass] {
  override def getValue(accumulator: RandomClass): java.lang.Integer = accumulator.i

  override def createAccumulator(): RandomClass = new RandomClass(0)

  override def getResultType: TypeInformation[java.lang.Integer] = new GenericTypeInfo[Integer](classOf[Integer])

  override def getAccumulatorType: TypeInformation[RandomClass] = new GenericTypeInfo[RandomClass](
    classOf[RandomClass])

  def accumulate(acc: RandomClass, value: Int): Unit = {
    acc.i = value
  }

  def retract(acc: RandomClass, value: Int): Unit = {
    acc.i = value
  }

  def resetAccumulator(acc: RandomClass): Unit = {
    acc.i = 0
  }
}
{code}

The code above fails with:

{code}
Caused by: java.lang.UnsupportedOperationException: BinaryGeneric cannot be compared
	at org.apache.flink.table.dataformat.BinaryGeneric.equals(BinaryGeneric.java:77)
	at GroupAggValueEqualiser$17.equalsWithoutHeader(Unknown Source)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:177)
	at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
	at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:85)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:170)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)
	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:702)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:527)
	at java.lang.Thread.run(Thread.java:748)
{code}

This is related to FLINK-13702"	FLINK	Resolved	3	1	6732	pull-request-available
13274910	ack flink-hadoop-compatibility and flink-orc into flink-hive	flink-connector-hive should contain flink-hadoop-compatibility to reduce users' efforts in figuring out dependency jars, and flink-orc for adding orc dependency which is missing in hive 2.2.x.	FLINK	Closed	3	1	6732	pull-request-available
13298802	Table with processing time attribute can not be read from Hive catalog	"DDL:
{code}
CREATE TABLE PROD_LINEITEM (
  L_ORDERKEY       INTEGER,
  L_PARTKEY        INTEGER,
  L_SUPPKEY        INTEGER,
  L_LINENUMBER     INTEGER,
  L_QUANTITY       DOUBLE,
  L_EXTENDEDPRICE  DOUBLE,
  L_DISCOUNT       DOUBLE,
  L_TAX            DOUBLE,
  L_CURRENCY       STRING,
  L_RETURNFLAG     STRING,
  L_LINESTATUS     STRING,
  L_ORDERTIME      TIMESTAMP(3),
  L_SHIPINSTRUCT   STRING,
  L_SHIPMODE       STRING,
  L_COMMENT        STRING,
  WATERMARK FOR L_ORDERTIME AS L_ORDERTIME - INTERVAL '5' MINUTE,
  L_PROCTIME       AS PROCTIME()
) WITH (
  'connector.type' = 'kafka',
  'connector.version' = 'universal',
  'connector.topic' = 'Lineitem',
  'connector.properties.zookeeper.connect' = 'not-needed',
  'connector.properties.bootstrap.servers' = 'kafka:9092',
  'connector.startup-mode' = 'earliest-offset',
  'format.type' = 'csv',
  'format.field-delimiter' = '|'
);
{code}

Query:
{code}
SELECT * FROM prod_lineitem;
{code}

Result:
{code}
[ERROR] Could not execute SQL statement. Reason:
java.lang.AssertionError: Conversion to relational algebra failed to preserve datatypes:
validated type:
RecordType(INTEGER L_ORDERKEY, INTEGER L_PARTKEY, INTEGER L_SUPPKEY, INTEGER L_LINENUMBER, DOUBLE L_QUANTITY, DOUBLE L_EXTENDEDPRICE, DOUBLE L_DISCOUNT, DOUBLE L_TAX, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_CURRENCY, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_RETURNFLAG, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_LINESTATUS, TIME ATTRIBUTE(ROWTIME) L_ORDERTIME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPINSTRUCT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPMODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_COMMENT, TIMESTAMP(3) NOT NULL L_PROCTIME) NOT NULL
converted type:
RecordType(INTEGER L_ORDERKEY, INTEGER L_PARTKEY, INTEGER L_SUPPKEY, INTEGER L_LINENUMBER, DOUBLE L_QUANTITY, DOUBLE L_EXTENDEDPRICE, DOUBLE L_DISCOUNT, DOUBLE L_TAX, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_CURRENCY, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_RETURNFLAG, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_LINESTATUS, TIME ATTRIBUTE(ROWTIME) L_ORDERTIME, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPINSTRUCT, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_SHIPMODE, VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"" L_COMMENT, TIME ATTRIBUTE(PROCTIME) NOT NULL L_PROCTIME) NOT NULL
rel:
LogicalProject(L_ORDERKEY=[$0], L_PARTKEY=[$1], L_SUPPKEY=[$2], L_LINENUMBER=[$3], L_QUANTITY=[$4], L_EXTENDEDPRICE=[$5], L_DISCOUNT=[$6], L_TAX=[$7], L_CURRENCY=[$8], L_RETURNFLAG=[$9], L_LINESTATUS=[$10], L_ORDERTIME=[$11], L_SHIPINSTRUCT=[$12], L_SHIPMODE=[$13], L_COMMENT=[$14], L_PROCTIME=[$15])
  LogicalWatermarkAssigner(rowtime=[L_ORDERTIME], watermark=[-($11, 300000:INTERVAL MINUTE)])
    LogicalProject(L_ORDERKEY=[$0], L_PARTKEY=[$1], L_SUPPKEY=[$2], L_LINENUMBER=[$3], L_QUANTITY=[$4], L_EXTENDEDPRICE=[$5], L_DISCOUNT=[$6], L_TAX=[$7], L_CURRENCY=[$8], L_RETURNFLAG=[$9], L_LINESTATUS=[$10], L_ORDERTIME=[$11], L_SHIPINSTRUCT=[$12], L_SHIPMODE=[$13], L_COMMENT=[$14], L_PROCTIME=[PROCTIME()])
      LogicalTableScan(table=[[hcat, default, prod_lineitem, source: [KafkaTableSource(L_ORDERKEY, L_PARTKEY, L_SUPPKEY, L_LINENUMBER, L_QUANTITY, L_EXTENDEDPRICE, L_DISCOUNT, L_TAX, L_CURRENCY, L_RETURNFLAG, L_LINESTATUS, L_ORDERTIME, L_SHIPINSTRUCT, L_SHIPMODE, L_COMMENT)]]])
{code}"	FLINK	Closed	1	1	6732	pull-request-available
13480921	Support create table-store table with 'connector'='table-store'	"Support create table-store table with 'connector'='table-store': 

sink to table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'datagen',
    'fields.word.length' = '1'
);
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',  //should rename 'catalog-database'？
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word; {code}
source from table-store:
{code:java}
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH(
  'connector' = 'table-store',
  'catalog-name' = 'test-catalog',
  'default-database' = 'test-db',
  'catalog-table' = 'test-tb',
  'warehouse'='file:/tmp/table_store'
);
CREATE TEMPORARY TABLE word_table (
    word STRING
) WITH (
    'connector' = 'print'
);
INSERT INTO word_table SELECT word FROM word_count;{code}"	FLINK	Closed	4	4	6732	pull-request-available
13218523	Introduce MemorySegmentWritable to let Segments direct copy to internal bytes	"Blink new binary format is based on MemorySegment.

Introduce MemorySegmentWritable to let DataOutputView direct copy to internal bytes
{code:java}
/**
 * Provides the interface for write(Segment).
 */
public interface MemorySegmentWritable {

 /**
 * Writes {@code len} bytes from memory segment {@code segment} starting at offset {@code off}, in order,
 * to the output.
 *
 * @param segment memory segment to copy the bytes from.
 * @param off the start offset in the memory segment.
 * @param len The number of bytes to copy.
 * @throws IOException if an I/O error occurs.
 */
 void write(MemorySegment segment, int off, int len) throws IOException;
}{code}
 

If we want to write a Memory Segment to DataOutputView, we need to copy bytes to byte[] and then write it in, which is less effective.

If we let AbstractPagedOutputView have a write(MemorySegment) interface, we can copy it directly.

We need to ensure this in network serialization, batch operator calculation serialization, Streaming State serialization to avoid new byte[] and copy."	FLINK	Closed	3	2	6732	pull-request-available
13301079	sink.rolling-policy.time.interval default value should be bigger	"Otherwise there is a lot of small files.

We should also consider sin.rolling-policy.file.size"	FLINK	Closed	3	7	6732	pull-request-available
13220996	Introduce code generated typed sort to blink table	"Introduce SortCodeGenerator (CodeGen efficient computation and comparison of  NormalizedKey, idea based on FLINK-5734 ):

support sort by primitive type, string, decimal...

support sort by ArrayType

support sort by RowType(Nested Struct)

 

 "	FLINK	Closed	3	2	6732	pull-request-available
13293470	[Umbrella] Introduce datagen, print, blackhole connectors	"Discussion: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Introduce-TableFactory-for-StatefulSequenceSource-td39116.html]

Introduce:
 * DataGeneratorSource
 * DataGenTableSourceFactory
 * PrintTableSinkFactory
 * BlackHoleTableSinkFactory"	FLINK	Closed	3	2	6732	pull-request-available
13441392	Record schema id in ManifestFileMeta and DataFileMeta	For future schema evolution, the schema id should be recorded inside the data, and when the schema changes, the previous schema can still be used to restore the existing data.	FLINK	Closed	3	7	6732	pull-request-available
13267666	Optimize mapred.HadoopInputSplit to not serialize conf when split is not configurable	"JobConf may very big, contains hundreds of configurations, if it is serialized by every split, that will significantly reduce performance.

Consider thousands of splits, the akka thread of JobMaster will all on the serialization of conf. That may will lead to various akka timeouts too."	FLINK	Closed	3	7	6732	pull-request-available
13277313	Avoid failing when required memory calculation not accurate in BinaryHashTable	"In BinaryHashBucketArea.insertToBucket.

When BinaryHashTable.buildTableFromSpilledPartition.""Build in memory hash table"", it requires memory can put all records, if not, will fail.

Because the linked hash conflict solution, the required memory calculation are not accurate, in this case, we should apply for insufficient memory from heap.

And must be careful, the steal memory should not return to table."	FLINK	Resolved	3	1	6732	pull-request-available
13517528	Kerberos in HiveCatalog is not work	We should read kerberos keytab from catalog options and doAs for hive metastore client.	FLINK	Closed	3	1	6732	pull-request-available
13338771	Introduce PartitionFieldExtractor to extract partition field from split	"Filesystem: extract partition field from path

Hive: there is partition information in split"	FLINK	Closed	3	7	6732	pull-request-available
13289961	Introduce flink-sql-connector-hive modules to provide hive uber jars	Discussed in: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Introduce-flink-connector-hive-xx-modules-td38440.html]	FLINK	Resolved	3	2	6732	pull-request-available
13227639	Support e2e limit, sortLimit, rank, union in blink batch	"Support limit and add limit it cases to blink batch.

Support sortLimit and add sortLimit it cases to blink batch.

Support rank and add rank it cases to blink batch.

Support union and add union it cases to blink batch."	FLINK	Closed	3	2	6732	pull-request-available
13445952	Add comment to schema	We can add comment to schema for table. See `CatalogBaseTable.getComment`.	FLINK	Closed	3	7	6732	pull-request-available
13289943	Integrate parquet columnar row reader to hive	Use parquet columnar row reader in hive.	FLINK	Resolved	3	7	6732	pull-request-available
13228767	Support e2e over window in blink batch	Supporting all major over window operations	FLINK	Closed	3	2	6732	pull-request-available
13479781	log.system can be congiured by dynamic options	"Now log.system default value is null, it can not be configured by dynamic option.
We can let default value is 'none'."	FLINK	Closed	3	4	6732	pull-request-available
13485655	Let write buffer spillable	Column format and remote DFS may greatly affect the performance of compaction. We can change the writeBuffer to spillable to improve the performance.	FLINK	Closed	3	4	6732	pull-request-available
13503368	Enable write-buffer-spillable by default only for object storage	"After a lot of tests, it is found that the participation of spillable does not improve HDFS greatly, but will bring some jitters.
In this jira, spillable is enabled only when the object is stored by default, so that the performance can be improved without affecting hdfs."	FLINK	Closed	3	4	6732	pull-request-available
13471959	Supports predicate testing for new columns	"The currently added column, if there is a filter on it, will cause an error in the RowDataToObjectArrayConverter because the number of columns is not correct
We can make BinaryTableStats supports evolution from shorter rowData."	FLINK	Closed	3	4	6732	pull-request-available
13337825	Integrate file compaction to filesystem connector	Integrate file compaction to Filesystem connector.	FLINK	Closed	3	7	6732	pull-request-available
13479438	Support LookupTableSource for table store	At present, the bottom layer of Table Store is LSM structure, and it has full and incremental reading, which can have certain lookup capability. We can unlock Table Store to implement LookupTableSource.	FLINK	Closed	3	4	6732	pull-request-available
13441395	Make DataFileMeta and ManifestFileMeta schemaless	"Currently these meta need key and value schema to serialize and deserialize
 * schema binding makes the construction of serializers difficult and prone to problems
 * Not conducive to schema evolution, different schema can not be mixed together, will increase the difficulty of management."	FLINK	Closed	3	1	6732	pull-request-available
13436369	Introduce parallelism setter for table store	"Support:
 * scan.parallelism
 * sink.parallelism"	FLINK	Closed	3	7	6732	pull-request-available
13509170	Introduce Table.copy from dynamic options	"At present, our processing of dynamic options is relatively independent. In FileStoreTableFactory, this is not conducive to other engines configuring dynamic options.

We should propose an interface on the Table, and dynamic options can be configured at any time."	FLINK	Closed	3	4	6732	pull-request-available
13434713	Init document for table store and introduce quickstart	"* Just like other sub project of Apache Flink, table store should build docs like Flink ML and Flink Stateful Functions.
 * Introduce index page and quickstart page."	FLINK	Closed	3	7	6732	pull-request-available
13468498	Replace ValueKind to RowKind in table store	"In FLINK-28244
We need store changelog of a table, the row kind should contains UPDATE_BEFORE and UPDATE_AFTER, but if we use ValueKind, it dose not contain UPDATE kinds.
We can just use RowKind."	FLINK	Closed	3	4	6732	pull-request-available
13311655	flink-sql-connector-hive modules should merge hive-exec dependencies	Since hive-exec is a bundle jar, we should merge the bundle dependencies from hive-exec.	FLINK	Closed	1	1	6732	pull-request-available
13274700	Better error message when insert partition with values	"Now, we not support insert partition with values like:

Insert into mytable partition (date='2019-08-08') values ('jason', 25)

Will throw a exception:

schema not match.

We should improve error message to tell user we not support this pattern."	FLINK	Resolved	3	1	6732	pull-request-available
13473527	Check warehouse path in CatalogFactory	"* Not exist, automatic creating the directory.
* Exist but it is not directory, throw exception.
* Exist and it is a directory, pass..."	FLINK	Closed	1	1	6732	pull-request-available
13472856	Cannot create sink for Temporary table in table store catalog	"Using Flink 1.14. Temporary table in table store catalog will use the Factory provided by catalog.
We need to do some delegate work like HiveDynamicTableFactory."	FLINK	Closed	1	1	6732	pull-request-available
13292452	BytesColumnVector should init buffer in Hive 3.x	The failed test is {{TableEnvHiveConnectorTest#testDifferentFormats}} when hive 3.x.	FLINK	Resolved	3	1	6732	pull-request-available
13241007	Support intersect all and minus all to blink planner	"Now, we just support intersect and minus, See ReplaceIntersectWithSemiJoinRule and ReplaceMinusWithAntiJoinRule, replace intersect with null aware semi-join and distinct aggregate.

We need support intersect all and minus all too.

Presto and Spark already support them:

[https://github.com/prestodb/presto/issues/4918]

https://issues.apache.org/jira/browse/SPARK-21274

I think them have a good rewrite design and we can follow them:

1.For intersect all

Input Query
{code:java}
SELECT c1 FROM ut1 INTERSECT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
  SELECT c1
    FROM (
         SELECT replicate_row(min_count, c1)
         FROM (
              SELECT c1,
                     IF (vcol1_cnt > vcol2_cnt, vcol2_cnt, vcol1_cnt) AS min_count
              FROM (
                   SELECT   c1, count(vcol1) as vcol1_cnt, count(vcol2) as vcol2_cnt
                   FROM (
                        SELECT c1, true as vcol1, null as vcol2 FROM ut1
                        UNION ALL
                        SELECT c1, null as vcol1, true as vcol2 FROM ut2
                        ) AS union_all
                   GROUP BY c1
                   HAVING vcol1_cnt >= 1 AND vcol2_cnt >= 1
                  )
              )
          )
{code}
2.For minus all:

Input Query
{code:java}
SELECT c1 FROM ut1 EXCEPT ALL SELECT c1 FROM ut2
{code}
Rewritten Query
{code:java}
 SELECT c1
    FROM (
     SELECT replicate_rows(sum_val, c1)
       FROM (
         SELECT c1, sum_val
           FROM (
             SELECT c1, sum(vcol) AS sum_val
               FROM (
                 SELECT 1L as vcol, c1 FROM ut1
                 UNION ALL
                 SELECT -1L as vcol, c1 FROM ut2
              ) AS union_all
            GROUP BY union_all.c1
          )
        WHERE sum_val > 0
       )
   )
{code}"	FLINK	Closed	3	2	6732	pull-request-available
13341284	Partition commit is delayed when records keep coming	"When set partition-commit.delay=0, Users expect partitions to be committed immediately.

However, if the record of this partition continues to flow in, the bucket for the partition will be activated, and no inactive bucket will appear.

We need to consider listening to bucket created."	FLINK	Closed	3	1	6732	pull-request-available
13336016	Refactor table streaming file sink	"Refactor:
 * Extract some classes
 * Make writer more generic
 * Provides StreamingSink util class"	FLINK	Closed	3	7	6732	pull-request-available
13432569	KafkaWriterITCase.testMetadataPublisher  failed on azure	"{code:java}
022-03-07T13:43:34.3882626Z Mar 07 13:43:34 [ERROR] org.apache.flink.connector.kafka.sink.KafkaWriterITCase.testMetadataPublisher  Time elapsed: 0.205 s  <<< FAILURE!
2022-03-07T13:43:34.3883743Z Mar 07 13:43:34 java.lang.AssertionError: 
2022-03-07T13:43:34.3884867Z Mar 07 13:43:34 
2022-03-07T13:43:34.3885412Z Mar 07 13:43:34 Expecting actual:
2022-03-07T13:43:34.3886464Z Mar 07 13:43:34   [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.3887361Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.3888222Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.3888833Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.3892032Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.3893140Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.3893849Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.3895077Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.3895779Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.3896423Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.3897164Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.3897792Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.3949208Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.3950956Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.3952287Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.3954341Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.3955834Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.3957048Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.3958287Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.3959519Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.3960798Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.3961973Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.3963302Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.3964563Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.3966941Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.3968246Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.3969452Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.3970656Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.3971853Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.3974163Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.3975441Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.3976380Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.3977278Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.3978197Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.3979120Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.3980051Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.3981017Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.3981952Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.3982975Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.3983882Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.3984940Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.3985838Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.3986702Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.3987661Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.3988564Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.3989444Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.3990347Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.3991206Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.3992100Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.3993091Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.3994383Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.3995399Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.3996287Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.3997270Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.3998095Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.3998915Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.3999752Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4000838Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4001693Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4002979Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4003852Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4004693Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4005626Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4006473Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4007269Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4008109Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4008948Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4009771Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4010608Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4011436Z Mar 07 13:43:34     ""testMetadataPublisher-0@69""]
2022-03-07T13:43:34.4011993Z Mar 07 13:43:34 to be equal to:
2022-03-07T13:43:34.4012951Z Mar 07 13:43:34   [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4013825Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4014670Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4015693Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4016554Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4017408Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4018257Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4019101Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4019961Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4020826Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4021702Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4022572Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4023663Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4024553Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4025531Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4026421Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4027296Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4028132Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4029000Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4029870Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4030733Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4031578Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4032456Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4033487Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4034372Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4035335Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4036217Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4037098Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4037963Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4038783Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4039621Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4040479Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4041774Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4042684Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4043595Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4044311Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4045166Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4045886Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4046625Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4047419Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4048409Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4049167Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4049936Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4050682Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4051494Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4052280Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4053217Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4053988Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4054891Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4055671Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4056416Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4057149Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4058037Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4059007Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4059996Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4060761Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4061524Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4062243Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4063142Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4063867Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4064594Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4065485Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4066207Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4066926Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4067666Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4068423Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4069208Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4069987Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4070747Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4071503Z Mar 07 13:43:34     ""testMetadataPublisher-0@69"",
2022-03-07T13:43:34.4072260Z Mar 07 13:43:34     ""testMetadataPublisher-0@70"",
2022-03-07T13:43:34.4073205Z Mar 07 13:43:34     ""testMetadataPublisher-0@71"",
2022-03-07T13:43:34.4073954Z Mar 07 13:43:34     ""testMetadataPublisher-0@72"",
2022-03-07T13:43:34.4074687Z Mar 07 13:43:34     ""testMetadataPublisher-0@73"",
2022-03-07T13:43:34.4075576Z Mar 07 13:43:34     ""testMetadataPublisher-0@74"",
2022-03-07T13:43:34.4076438Z Mar 07 13:43:34     ""testMetadataPublisher-0@75"",
2022-03-07T13:43:34.4077280Z Mar 07 13:43:34     ""testMetadataPublisher-0@76"",
2022-03-07T13:43:34.4078103Z Mar 07 13:43:34     ""testMetadataPublisher-0@77"",
2022-03-07T13:43:34.4078906Z Mar 07 13:43:34     ""testMetadataPublisher-0@78"",
2022-03-07T13:43:34.4079740Z Mar 07 13:43:34     ""testMetadataPublisher-0@79"",
2022-03-07T13:43:34.4080568Z Mar 07 13:43:34     ""testMetadataPublisher-0@80"",
2022-03-07T13:43:34.4081382Z Mar 07 13:43:34     ""testMetadataPublisher-0@81"",
2022-03-07T13:43:34.4082183Z Mar 07 13:43:34     ""testMetadataPublisher-0@82"",
2022-03-07T13:43:34.4083434Z Mar 07 13:43:34     ""testMetadataPublisher-0@83"",
2022-03-07T13:43:34.4084219Z Mar 07 13:43:34     ""testMetadataPublisher-0@84"",
2022-03-07T13:43:34.4085164Z Mar 07 13:43:34     ""testMetadataPublisher-0@85"",
2022-03-07T13:43:34.4085981Z Mar 07 13:43:34     ""testMetadataPublisher-0@86"",
2022-03-07T13:43:34.4086783Z Mar 07 13:43:34     ""testMetadataPublisher-0@87"",
2022-03-07T13:43:34.4087594Z Mar 07 13:43:34     ""testMetadataPublisher-0@88"",
2022-03-07T13:43:34.4088399Z Mar 07 13:43:34     ""testMetadataPublisher-0@89"",
2022-03-07T13:43:34.4089197Z Mar 07 13:43:34     ""testMetadataPublisher-0@90"",
2022-03-07T13:43:34.4090016Z Mar 07 13:43:34     ""testMetadataPublisher-0@91"",
2022-03-07T13:43:34.4091073Z Mar 07 13:43:34     ""testMetadataPublisher-0@92"",
2022-03-07T13:43:34.4091883Z Mar 07 13:43:34     ""testMetadataPublisher-0@93"",
2022-03-07T13:43:34.4092685Z Mar 07 13:43:34     ""testMetadataPublisher-0@94"",
2022-03-07T13:43:34.4093638Z Mar 07 13:43:34     ""testMetadataPublisher-0@95"",
2022-03-07T13:43:34.4094429Z Mar 07 13:43:34     ""testMetadataPublisher-0@96"",
2022-03-07T13:43:34.4095340Z Mar 07 13:43:34     ""testMetadataPublisher-0@97"",
2022-03-07T13:43:34.4096112Z Mar 07 13:43:34     ""testMetadataPublisher-0@98"",
2022-03-07T13:43:34.4096909Z Mar 07 13:43:34     ""testMetadataPublisher-0@99""]
2022-03-07T13:43:34.4097650Z Mar 07 13:43:34 when recursively comparing field by field, but found the following difference:
2022-03-07T13:43:34.4098330Z Mar 07 13:43:34 
2022-03-07T13:43:34.4098903Z Mar 07 13:43:34 Top level actual and expected objects differ:
2022-03-07T13:43:34.4099837Z Mar 07 13:43:34 - actual value  : [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4100698Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4101474Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4102387Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4103366Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4104138Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4105064Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4105984Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4106818Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4107589Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4108447Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4109302Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4110170Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4111034Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4111918Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4113454Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4114503Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4115478Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4116351Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4117223Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4118093Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4118957Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4119823Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4120664Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4121523Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4122384Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4123353Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4124229Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4125165Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4126006Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4126874Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4127731Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4129264Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4130106Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4130969Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4131840Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4132709Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4133674Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4134532Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4135442Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4136523Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4137388Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4138251Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4139110Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4139970Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4140817Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4141681Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4142544Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4143507Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4144371Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4145318Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4146161Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4147023Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4147874Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4148716Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4149571Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4150426Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4151259Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4152128Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4153090Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4153968Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4154903Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4155764Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4156608Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4157477Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4158334Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4159195Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4160059Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4160917Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4161745Z Mar 07 13:43:34     ""testMetadataPublisher-0@69""]
2022-03-07T13:43:34.4162674Z Mar 07 13:43:34 - expected value: [""testMetadataPublisher-0@0"",
2022-03-07T13:43:34.4163702Z Mar 07 13:43:34     ""testMetadataPublisher-0@1"",
2022-03-07T13:43:34.4164568Z Mar 07 13:43:34     ""testMetadataPublisher-0@2"",
2022-03-07T13:43:34.4165546Z Mar 07 13:43:34     ""testMetadataPublisher-0@3"",
2022-03-07T13:43:34.4166403Z Mar 07 13:43:34     ""testMetadataPublisher-0@4"",
2022-03-07T13:43:34.4167243Z Mar 07 13:43:34     ""testMetadataPublisher-0@5"",
2022-03-07T13:43:34.4168103Z Mar 07 13:43:34     ""testMetadataPublisher-0@6"",
2022-03-07T13:43:34.4168961Z Mar 07 13:43:34     ""testMetadataPublisher-0@7"",
2022-03-07T13:43:34.4169823Z Mar 07 13:43:34     ""testMetadataPublisher-0@8"",
2022-03-07T13:43:34.4170689Z Mar 07 13:43:34     ""testMetadataPublisher-0@9"",
2022-03-07T13:43:34.4171559Z Mar 07 13:43:34     ""testMetadataPublisher-0@10"",
2022-03-07T13:43:34.4172403Z Mar 07 13:43:34     ""testMetadataPublisher-0@11"",
2022-03-07T13:43:34.4173359Z Mar 07 13:43:34     ""testMetadataPublisher-0@12"",
2022-03-07T13:43:34.4174233Z Mar 07 13:43:34     ""testMetadataPublisher-0@13"",
2022-03-07T13:43:34.4175394Z Mar 07 13:43:34     ""testMetadataPublisher-0@14"",
2022-03-07T13:43:34.4176266Z Mar 07 13:43:34     ""testMetadataPublisher-0@15"",
2022-03-07T13:43:34.4177121Z Mar 07 13:43:34     ""testMetadataPublisher-0@16"",
2022-03-07T13:43:34.4177964Z Mar 07 13:43:34     ""testMetadataPublisher-0@17"",
2022-03-07T13:43:34.4178833Z Mar 07 13:43:34     ""testMetadataPublisher-0@18"",
2022-03-07T13:43:34.4179694Z Mar 07 13:43:34     ""testMetadataPublisher-0@19"",
2022-03-07T13:43:34.4180556Z Mar 07 13:43:34     ""testMetadataPublisher-0@20"",
2022-03-07T13:43:34.4181413Z Mar 07 13:43:34     ""testMetadataPublisher-0@21"",
2022-03-07T13:43:34.4182411Z Mar 07 13:43:34     ""testMetadataPublisher-0@22"",
2022-03-07T13:43:34.4183347Z Mar 07 13:43:34     ""testMetadataPublisher-0@23"",
2022-03-07T13:43:34.4184211Z Mar 07 13:43:34     ""testMetadataPublisher-0@24"",
2022-03-07T13:43:34.4185151Z Mar 07 13:43:34     ""testMetadataPublisher-0@25"",
2022-03-07T13:43:34.4186023Z Mar 07 13:43:34     ""testMetadataPublisher-0@26"",
2022-03-07T13:43:34.4186901Z Mar 07 13:43:34     ""testMetadataPublisher-0@27"",
2022-03-07T13:43:34.4187764Z Mar 07 13:43:34     ""testMetadataPublisher-0@28"",
2022-03-07T13:43:34.4188609Z Mar 07 13:43:34     ""testMetadataPublisher-0@29"",
2022-03-07T13:43:34.4189479Z Mar 07 13:43:34     ""testMetadataPublisher-0@30"",
2022-03-07T13:43:34.4190338Z Mar 07 13:43:34     ""testMetadataPublisher-0@31"",
2022-03-07T13:43:34.4191198Z Mar 07 13:43:34     ""testMetadataPublisher-0@32"",
2022-03-07T13:43:34.4192060Z Mar 07 13:43:34     ""testMetadataPublisher-0@33"",
2022-03-07T13:43:34.4193009Z Mar 07 13:43:34     ""testMetadataPublisher-0@34"",
2022-03-07T13:43:34.4193872Z Mar 07 13:43:34     ""testMetadataPublisher-0@35"",
2022-03-07T13:43:34.4194809Z Mar 07 13:43:34     ""testMetadataPublisher-0@36"",
2022-03-07T13:43:34.4195680Z Mar 07 13:43:34     ""testMetadataPublisher-0@37"",
2022-03-07T13:43:34.4196549Z Mar 07 13:43:34     ""testMetadataPublisher-0@38"",
2022-03-07T13:43:34.4197410Z Mar 07 13:43:34     ""testMetadataPublisher-0@39"",
2022-03-07T13:43:34.4198275Z Mar 07 13:43:34     ""testMetadataPublisher-0@40"",
2022-03-07T13:43:34.4199110Z Mar 07 13:43:34     ""testMetadataPublisher-0@41"",
2022-03-07T13:43:34.4199970Z Mar 07 13:43:34     ""testMetadataPublisher-0@42"",
2022-03-07T13:43:34.4200826Z Mar 07 13:43:34     ""testMetadataPublisher-0@43"",
2022-03-07T13:43:34.4201685Z Mar 07 13:43:34     ""testMetadataPublisher-0@44"",
2022-03-07T13:43:34.4202544Z Mar 07 13:43:34     ""testMetadataPublisher-0@45"",
2022-03-07T13:43:34.4203506Z Mar 07 13:43:34     ""testMetadataPublisher-0@46"",
2022-03-07T13:43:34.4204356Z Mar 07 13:43:34     ""testMetadataPublisher-0@47"",
2022-03-07T13:43:34.4205303Z Mar 07 13:43:34     ""testMetadataPublisher-0@48"",
2022-03-07T13:43:34.4206169Z Mar 07 13:43:34     ""testMetadataPublisher-0@49"",
2022-03-07T13:43:34.4207030Z Mar 07 13:43:34     ""testMetadataPublisher-0@50"",
2022-03-07T13:43:34.4207895Z Mar 07 13:43:34     ""testMetadataPublisher-0@51"",
2022-03-07T13:43:34.4208754Z Mar 07 13:43:34     ""testMetadataPublisher-0@52"",
2022-03-07T13:43:34.4209606Z Mar 07 13:43:34     ""testMetadataPublisher-0@53"",
2022-03-07T13:43:34.4210468Z Mar 07 13:43:34     ""testMetadataPublisher-0@54"",
2022-03-07T13:43:34.4211327Z Mar 07 13:43:34     ""testMetadataPublisher-0@55"",
2022-03-07T13:43:34.4212182Z Mar 07 13:43:34     ""testMetadataPublisher-0@56"",
2022-03-07T13:43:34.4213139Z Mar 07 13:43:34     ""testMetadataPublisher-0@57"",
2022-03-07T13:43:34.4214002Z Mar 07 13:43:34     ""testMetadataPublisher-0@58"",
2022-03-07T13:43:34.4214920Z Mar 07 13:43:34     ""testMetadataPublisher-0@59"",
2022-03-07T13:43:34.4215784Z Mar 07 13:43:34     ""testMetadataPublisher-0@60"",
2022-03-07T13:43:34.4216653Z Mar 07 13:43:34     ""testMetadataPublisher-0@61"",
2022-03-07T13:43:34.4217523Z Mar 07 13:43:34     ""testMetadataPublisher-0@62"",
2022-03-07T13:43:34.4218387Z Mar 07 13:43:34     ""testMetadataPublisher-0@63"",
2022-03-07T13:43:34.4219251Z Mar 07 13:43:34     ""testMetadataPublisher-0@64"",
2022-03-07T13:43:34.4220089Z Mar 07 13:43:34     ""testMetadataPublisher-0@65"",
2022-03-07T13:43:34.4222481Z Mar 07 13:43:34     ""testMetadataPublisher-0@66"",
2022-03-07T13:43:34.4223512Z Mar 07 13:43:34     ""testMetadataPublisher-0@67"",
2022-03-07T13:43:34.4224379Z Mar 07 13:43:34     ""testMetadataPublisher-0@68"",
2022-03-07T13:43:34.4225326Z Mar 07 13:43:34     ""testMetadataPublisher-0@69"",
2022-03-07T13:43:34.4226187Z Mar 07 13:43:34     ""testMetadataPublisher-0@70"",
2022-03-07T13:43:34.4227046Z Mar 07 13:43:34     ""testMetadataPublisher-0@71"",
2022-03-07T13:43:34.4227888Z Mar 07 13:43:34     ""testMetadataPublisher-0@72"",
2022-03-07T13:43:34.4228748Z Mar 07 13:43:34     ""testMetadataPublisher-0@73"",
2022-03-07T13:43:34.4229755Z Mar 07 13:43:34     ""testMetadataPublisher-0@74"",
2022-03-07T13:43:34.4230618Z Mar 07 13:43:34     ""testMetadataPublisher-0@75"",
2022-03-07T13:43:34.4231483Z Mar 07 13:43:34     ""testMetadataPublisher-0@76"",
2022-03-07T13:43:34.4232344Z Mar 07 13:43:34     ""testMetadataPublisher-0@77"",
2022-03-07T13:43:34.4233292Z Mar 07 13:43:34     ""testMetadataPublisher-0@78"",
2022-03-07T13:43:34.4234161Z Mar 07 13:43:34     ""testMetadataPublisher-0@79"",
2022-03-07T13:43:34.4235139Z Mar 07 13:43:34     ""testMetadataPublisher-0@80"",
2022-03-07T13:43:34.4236004Z Mar 07 13:43:34     ""testMetadataPublisher-0@81"",
2022-03-07T13:43:34.4236856Z Mar 07 13:43:34     ""testMetadataPublisher-0@82"",
2022-03-07T13:43:34.4237713Z Mar 07 13:43:34     ""testMetadataPublisher-0@83"",
2022-03-07T13:43:34.4238554Z Mar 07 13:43:34     ""testMetadataPublisher-0@84"",
2022-03-07T13:43:34.4239420Z Mar 07 13:43:34     ""testMetadataPublisher-0@85"",
2022-03-07T13:43:34.4240278Z Mar 07 13:43:34     ""testMetadataPublisher-0@86"",
2022-03-07T13:43:34.4241143Z Mar 07 13:43:34     ""testMetadataPublisher-0@87"",
2022-03-07T13:43:34.4242004Z Mar 07 13:43:34     ""testMetadataPublisher-0@88"",
2022-03-07T13:43:34.4242931Z Mar 07 13:43:34     ""testMetadataPublisher-0@89"",
2022-03-07T13:43:34.4243787Z Mar 07 13:43:34     ""testMetadataPublisher-0@90"",
2022-03-07T13:43:34.4244657Z Mar 07 13:43:34     ""testMetadataPublisher-0@91"",
2022-03-07T13:43:34.4245634Z Mar 07 13:43:34     ""testMetadataPublisher-0@92"",
2022-03-07T13:43:34.4246508Z Mar 07 13:43:34     ""testMetadataPublisher-0@93"",
2022-03-07T13:43:34.4247365Z Mar 07 13:43:34     ""testMetadataPublisher-0@94"",
2022-03-07T13:43:34.4248219Z Mar 07 13:43:34     ""testMetadataPublisher-0@95"",
2022-03-07T13:43:34.4249064Z Mar 07 13:43:34     ""testMetadataPublisher-0@96"",
2022-03-07T13:43:34.4249919Z Mar 07 13:43:34     ""testMetadataPublisher-0@97"",
2022-03-07T13:43:34.4250774Z Mar 07 13:43:34     ""testMetadataPublisher-0@98"",
2022-03-07T13:43:34.4251627Z Mar 07 13:43:34     ""testMetadataPublisher-0@99""]
2022-03-07T13:43:34.4252463Z Mar 07 13:43:34 actual and expected values are collections of different size, actual size=70 when expected size=100
2022-03-07T13:43:34.4253310Z Mar 07 13:43:34 
2022-03-07T13:43:34.4254001Z Mar 07 13:43:34 The recursive comparison was performed with this configuration:
2022-03-07T13:43:34.4255270Z Mar 07 13:43:34 - no overridden equals methods were used in the comparison (except for java types)
2022-03-07T13:43:34.4256419Z Mar 07 13:43:34 - these types were compared with the following comparators:
2022-03-07T13:43:34.4257510Z Mar 07 13:43:34   - java.lang.Double -> DoubleComparator[precision=1.0E-15]
2022-03-07T13:43:34.4258541Z Mar 07 13:43:34   - java.lang.Float -> FloatComparator[precision=1.0E-6]
2022-03-07T13:43:34.4259606Z Mar 07 13:43:34   - java.nio.file.Path -> lexicographic comparator (Path natural order)
2022-03-07T13:43:34.4261366Z Mar 07 13:43:34 - actual and expected objects and their fields were compared field by field recursively even if they were not of the same type, this allows for example to compare a Person to a PersonDto (call strictTypeChecking(true) to change that behavior).
2022-03-07T13:43:34.4262493Z Mar 07 13:43:34 
2022-03-07T13:43:34.4263436Z Mar 07 13:43:34 	at org.apache.flink.connector.kafka.sink.KafkaWriterITCase.testMetadataPublisher(KafkaWriterITCase.java:236)
2022-03-07T13:43:34.4264640Z Mar 07 13:43:34 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-03-07T13:43:34.4265699Z Mar 07 13:43:34 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-03-07T13:43:34.4266770Z Mar 07 13:43:34 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-03-07T13:43:34.4267730Z Mar 07 13:43:34 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-03-07T13:43:34.4268684Z Mar 07 13:43:34 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
2022-03-07T13:43:34.4269757Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2022-03-07T13:43:34.4271099Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2022-03-07T13:43:34.4272304Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
2022-03-07T13:43:34.4273543Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
2022-03-07T13:43:34.4274806Z Mar 07 13:43:34 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
2022-03-07T13:43:34.4276066Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
2022-03-07T13:43:34.4277323Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
2022-03-07T13:43:34.4278583Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2022-03-07T13:43:34.4279873Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2022-03-07T13:43:34.4281114Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2022-03-07T13:43:34.4282354Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2022-03-07T13:43:34.4283570Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
2022-03-07T13:43:34.4284671Z Mar 07 13:43:34 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
2022-03-07T13:43:34.4285944Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
2022-03-07T13:43:34.4287198Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4288409Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
2022-03-07T13:43:34.4289624Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
2022-03-07T13:43:34.4290798Z Mar 07 13:43:34 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
2022-03-07T13:43:34.4291999Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2022-03-07T13:43:34.4293287Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4294482Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4295692Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4296804Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4298169Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4299357Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4300494Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4301851Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4303669Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2022-03-07T13:43:34.4305318Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2022-03-07T13:43:34.4306715Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-03-07T13:43:34.4307899Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4309109Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4310224Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4311342Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4312547Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4313807Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4315052Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4316417Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4317981Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2022-03-07T13:43:34.4319355Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2022-03-07T13:43:34.4320552Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4321766Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2022-03-07T13:43:34.4322967Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2022-03-07T13:43:34.4324075Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2022-03-07T13:43:34.4325327Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2022-03-07T13:43:34.4326373Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2022-03-07T13:43:34.4327462Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2022-03-07T13:43:34.4328690Z Mar 07 13:43:34 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2022-03-07T13:43:34.4330020Z Mar 07 13:43:34 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2022-03-07T13:43:34.4330850Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
2022-03-07T13:43:34.4331809Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
2022-03-07T13:43:34.4332901Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
2022-03-07T13:43:34.4333858Z Mar 07 13:43:34 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32628&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=15a22db7-8faa-5b34-3920-d33c9f0ca23c&l=36036"	FLINK	Closed	2	1	6732	pull-request-available, test-stability
13238666	Introduce BaseArray and BaseMap to reduce conversion overhead to blink	"Currently, in internal data format of flink, the array is only BinaryArray, and the map is only BinaryMap. If the user writes a UDAF with arrays as parameters and return values, it will lead to frequent conversion between Java arrays and BinaryArrays (each conversion is equivalent to the entire array of copys), which is very time-consuming.

In order to avoid copy in conversion, BaseArray and BaseMap are introduced as internal formats.

BaseArray is the parent of GenericArray and BinaryArray, providing various read and write operations on an array.

GenericArray is a wrapper class for Java arrays, which internally wraps a Java array. This array stores some elements of internal data format.

Conversion can be avoided when the element type is a primitive type or a type that is consistent internally format and externally format. (Detail see: DataFormatConverters)

After our benchmark, the performance of UDAF using primitive Array has been improved by 10 times."	FLINK	Closed	3	1	6732	pull-request-available
13480780	Can not overwrite from empty input	There is currently no data, which will not trigger an overwrite, which causes the semantics of the sql to not be correct	FLINK	Closed	3	1	6732	pull-request-available
13239440	Use default conversion class LocalDate/LocalTime/LocalDateTime for DateType/TimeType/TimestampType in blink	"Now we still use java.sql.Timestamp to be default conversion class for TimestampType.

We should use design of new Type System to use java.time.Local***.

NOTE, This may affect user behaviour:

eg: UDF with eval(Object o), now will pass a java.time.Local*** instead of java.sql.**.

Compatibility method: use DataType.bridgedTo(java.sql.**.class);"	FLINK	Closed	3	4	6732	pull-request-available
13436296	Introduce write/query table document for table store	"* Batch or streaming
 * file store continuous and log store
 * retention
 * parallelism
 * memory"	FLINK	Closed	3	7	6732	pull-request-available
13273797	Move DDL to first tab in table connector page	Since we have a good support for DDL in tableEnv.sqlUpdate and SQL-CLI, I think it is time to highlight DDL in the document.	FLINK	Resolved	3	7	6732	pull-request-available
13434387	Remove change tracking in TableStore	"Remove CHANGE_TRACKING, the default overwrite does not generate changes to the log system.
Add hint CHANGE_TRACKING=false is an unfriendly way to  to support overwrite, and by default overwrite does not generate changes, so users have to turn it off.
Later we have the ability to generate changes for overwrite, and then add an overwrite.change-tracking.mode to configure it."	FLINK	Closed	3	7	6732	pull-request-available
13433018	FileStoreSourceSplitReader should deal with value count	"There is a keyAsRecord in FileStoreSourceSplitReader, but this should only be keyAsRecord.

When keyAsRecord, it should emit the same number of records as value count."	FLINK	Closed	3	7	6732	pull-request-available
13326581	StreamingFileWriter should register Listener before the initialization of buckets	"In [http://apache-flink.147419.n8.nabble.com/StreamingFileSink-hive-metadata-td6898.html]

The feedback of User indicates that some partitions have not been committed since the job failed.

This maybe due to FLINK-18110, in FLINK-18110, it has fixed Buckets, but forgot fixing {{StreamingFileWriter}} , it should register Listener before the initialization of buckets, otherwise, will loose listening too."	FLINK	Closed	1	1	6732	pull-request-available
13474643	Unclean shade in flink-table-store-dist	"java.lang.NoSuchFieldError: callback
  at org.apache.flink.table.store.kafka.KafkaSinkFunction.open (KafkaSinkFunction.java:71)

Error when using table store with kafka sql-jar.

We should shade more for flink-connector-kafka."	FLINK	Closed	1	1	6732	pull-request-available
13527201	Flink 1.16 should implement new LookupFunction	Only implements new LookupFunction, retry lookup join can work.	FLINK	Closed	3	1	6732	pull-request-available
13473756	Throw better exception when file not found in reading	"When reading a file, if it is found that the file does not exist, it directly throws a file not found exception, which is often difficult for users to understand.
We can make it more clear in the exception message, e.g.
The file cannot be found, this may be because the read is too slow and the previous snapshot expired, you can configure a larger snapshot.time-retained or speed up your read.


Caused by: java.io.FileNotFoundException: File does not exist: 
at org.apache.flink.table.store.file.utils.FileUtils.getFileSize(FileUtils.java:94) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]
at org.apache.flink.table.store.file.data.DataFileReader$DataFileRecordReader.<init>(DataFileReader.java:86) ~[flink-table-store-dist-0.2.jar:0.2-SNAPSHOT]"	FLINK	Closed	1	4	6732	pull-request-available
13441353	Record schema on filesystem path	"We can store the schema on the path of the table store, which includes type, options, etc.

This schema should be in a format that supports evolution, which means that the fields contain id information."	FLINK	Closed	3	7	6732	pull-request-available
13483164	Connection leak in orc reader	"1. OrcFileStatsExtractor forget closing reader.
2. HadoopReadOnlyFileSystem forget closing fsDataInputStream.

We need a pocket test to assert all connections are closed. "	FLINK	Closed	1	1	6732	pull-request-available
13513150	Flink-table-runtime free for flink-table-store-codegen	"To remove the dependence of flink-table-store-core  on flink-table-runtime, flink-table-store-codegen is the first step.

"	FLINK	Closed	3	7	6732	pull-request-available
13229516	avoid to call retractExpressions method of max/min function in retractable aggregate code-gen	after {{FlinkRelMdModifiedMonotonicity}} introduced, a max/min function whose result value is modified  increasing/decreasing could ignore retraction message. We could choose regular max/min function instead of retract max/min function in code-gen. Currently, this requires the regular max/min function must implements {{retractExpressions}} method and do not throw any exceptions. A better approach is the retractable aggregate operator does not call {{retractExpressions}} method for max/min function.	FLINK	Closed	3	2	6732	auto-unassigned, pull-request-available
13221026	Introduce BytesHashMap to batch hash agg	"Introduce bytes based hash table.
It can be used for performing aggregations where the aggregated values are fixed-width.
Because the data is stored in continuous memory, AggBuffer of variable length cannot be applied to this HashMap. The KeyValue form in hash map is designed to reduce the cost of key fetching in lookup.

Add a test to do a complete hash agg. When HashMap has enough memory, pure hash AGG is performed; when memory is insufficient, it degenerates into sort agg."	FLINK	Closed	3	2	6732	pull-request-available
13365473	Refactor SlicingWindowAggOperatorBuilder to accept serializer instead of LogicalType	Now SlicingWindowAggOperatorBuilder accept LogicalTypes, it is better to avoid LogicalTypes in runtime operators and functions.	FLINK	Closed	3	4	6732	pull-request-available
13510712	Supports lookup a partial-update table	"The lookup uses streaming read for reading table. (In TableStreamingReader)
- We should support lookup a partial-update table with full compaction.
- But partial-update table without full compaction, we should throw exception.

"	FLINK	Closed	3	4	6732	pull-request-available
13268467	Multi vectorized read version support for hive orc read	"The vector api of Hive 1.x version are totally different from hive 2+.

So we need introduce more efforts to support vectorized read for hive 1.x."	FLINK	Closed	3	7	6732	pull-request-available
13225216	Introduce HashJoinOperator and LongHashJoinGenerator to blink	"Introduce HashJoinOperator to support all join type.

Introduce LongHashJoinGenerator to do some performance optimization when key is long using LongHybridHashTable.

HashJoinOperator and LongHashJoinOperator can be used in both shuffle hash join and broadcast hash join."	FLINK	Closed	3	2	6732	pull-request-available
13473453	Modify changelog-file to changelog-producer	The changelog-file is limited, we can have more changelog producer types.	FLINK	Closed	1	4	6732	pull-request-available
13527173	Fix Parquet stats extractor	"Some bugs in Parquet stats extractor:
 # Decimal Supports
 # Timestamp Supports
 # Null nullCounts supports"	FLINK	Closed	3	1	6732	pull-request-available
13266016	Move flink-orc to flink-formats from flink-connectors	"We already have the parent model of formats. we have put other formats(flink-avro, flink-json, flink-parquet, flink-json, flink-csv, flink-sequence-file) to flink-formats. flink-orc is a format too. So we can move it to flink-formats.
 
In theory, there should be no compatibility problem, only the parent model needs to be changed, and no other changes are needed. 
 
Discuss thread: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Move-flink-orc-to-flink-formats-from-flink-connectors-td34438.html]
Vote thread: [http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Move-flink-orc-to-flink-formats-from-flink-connectors-td34496.html]"	FLINK	Resolved	3	1	6732	pull-request-available
13450565	Create flink-table-store-connector-base to shade all flink dependencies	For Hive and other readers, they currently need to shade a bunch of dependencies, which is not very friendly, we can have a common module, and connector depends on this one module.	FLINK	Closed	3	7	6732	pull-request-available
13253199	Hive functions can not work in blink planner stream mode	"In flink, specifying the StreamTableEnvironment through the EnvironmentSetting using the blink planner, when using the UDAF in hive in the table API, the error is reported.

The hive function should been make by correct constants and argTypes. Otherwise it will throw an exception. (See HiveAggSqlFunction)
In this isTableAggregate, it just want to check the aggregate function class type, so the better way is get the function instead of make a function.


{code:java}
Caused by: java.lang.NullPointerException
	at java.util.Arrays.stream(Arrays.java:5004)
	at java.util.stream.Stream.of(Stream.java:1000)
	at org.apache.flink.table.types.utils.TypeConversions.fromLogicalToDataType(TypeConversions.java:67)
	at org.apache.flink.table.planner.functions.utils.HiveFunctionUtils.invokeSetArgs(HiveFunctionUtils.java:59)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:68)
	at org.apache.flink.table.planner.functions.utils.HiveAggSqlFunction.makeFunction(HiveAggSqlFunction.java:47)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$$anonfun$isTableAggregate$2.apply(AggregateUtil.scala:750)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.utils.AggregateUtil$.isTableAggregate(AggregateUtil.scala:750)
	at org.apache.flink.table.planner.plan.utils.RelExplainUtil$.streamGroupAggregationToString(RelExplainUtil.scala:346)
	at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecGroupAggregate.explainTerms(StreamExecGroupAggregate.scala:109)
	at org.apache.calcite.rel.AbstractRelNode.explain(AbstractRelNode.java:307)
	at org.apache.calcite.rel.AbstractRelNode.computeDigest(AbstractRelNode.java:388)
	at org.apache.calcite.rel.AbstractRelNode.recomputeDigest(AbstractRelNode.java:351)
	at org.apache.calcite.rel.AbstractRelNode.onRegister(AbstractRelNode.java:345)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.registerImpl(VolcanoPlanner.java:1668)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.register(VolcanoPlanner.java:846)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:868)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.ensureRegistered(VolcanoPlanner.java:1939)
	at org.apache.calcite.plan.volcano.VolcanoRuleCall.transformTo(VolcanoRuleCall.java:129)
	... 60 more
{code}"	FLINK	Resolved	3	1	6732	pull-request-available
13470918	Basic schema evolution for table store	Currently Flink 1.15 does not have the DDL for modifying table schema, but we can expose the most basic modifications of Schema at Catalog level and Spark read side, such as adding fields.	FLINK	Closed	3	7	6732	pull-request-available
13422186	Add basic structures of file store in table-store	"Add basic structures of file store in table-store:
 * Add OffsetRowData
 * Add KeyValue and KeyValueSerializer
 * Add MemTable and Accumulator"	FLINK	Closed	3	7	6732	pull-request-available
13370535	Partition insert with union all will fail	INSERT INTO partitioned_sink (e,a,g,f,c,d) SELECT e,a,456,123,c,d FROM MyTable GROUP BY a,b,c,d,e UNION ALL SELECT e,a,789,456,c,d FROM MyTable GROUP BY a,b,c,d,e	FLINK	Closed	3	1	6732	pull-request-available
13527954	Introduce write-once hash lookup store	"Introduce interface for lookup changelog producer:
{code:java}
/**
 * A key-value store for lookup, key-value store should be single binary file written once and ready
 * to be used. This factory provide two interfaces:
 *
 * <ul>
 *   <li>Writer: written once to prepare binary file.
 *   <li>Reader: lookup value by key bytes.
 * </ul>
 */
public interface LookupStoreFactory {

    LookupStoreWriter createWriter(File file) throws IOException;

    LookupStoreReader createReader(File file) throws IOException;
}
 {code}
We can convert remote columnar data to local lookup store, and ready to be used to lookup."	FLINK	Closed	3	7	6732	pull-request-available
13244334	ExpressionReducer with udf bug in blink	When a udf is reduced by ExpressionReducer, there will be a code gen exception.	FLINK	Resolved	3	1	6732	pull-request-available
13470016	Add CatalogLock support for HiveCatalog in table store	In HiveCatalog, we can set CatalogLock.Factory to TableStoreSink to support multiple concurrency writers for object storage.	FLINK	Closed	3	7	6732	pull-request-available
13305806	Should throw a readable exception when group by Map type	"We use flink 1.10.0 ,  blink planner,  to  submit a batch sql job to read from a hive table which contains map type fields, and then aggregate.   the sql as below:

```
 create view aaa
 as select * from table1 where event_id = '0103002' and `day`='2020-05-13'
 and `hour`='13';
 create view view_1
 as
 select
 `day`,
 a.rtime as itime,
 a.uid as uid,
 trim(BOTH a.event.log_1['scene']) as refer_list,
 T.s as abflags,
 a.hdid as hdid,
 a.country as country
 from aaa as a
 left join LATERAL TABLE(splitByChar(trim(BOTH a.event.log_1['abflag]),
 ',')) as T(s) on true;

{color:#172b4d}CREATE VIEW view_6 as {color}
 {color:#172b4d} SELECT{color}
 {color:#172b4d} `uid`,{color}
 {color:#172b4d} `refer_list`,{color}
 {color:#172b4d} `abflag`,{color}
 {color:#172b4d}        last_value(country){color}
 {color:#172b4d} FROM view_1{color}
 {color:#172b4d} where `refer_list` in ('WELOG_NEARBY', 'WELOG_FOLLOW', 'WELOG_POPULAR'){color}
 {color:#172b4d} GROUP BY  `uid`, `refer_list`, abflag;{color}
 insert into ............
 ``` 

when submit the job, the exception occurs as below:
 org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:335)
         at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205)
         at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:138)
         at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:664)
         at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)
         at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:895)
         at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:968)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:422)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)
         at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
         at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:968)
 Caused by: java.lang.RuntimeException: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at sg.bigo.streaming.sql.StreamingSqlRunner.main(StreamingSqlRunner.java:143)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
         at java.lang.reflect.Method.invoke(Method.java:498)
         at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321)
         ... 11 more
 Caused by: scala.MatchError: MAP (of class org.apache.flink.table.types.logical.LogicalTypeRoot)
         at org.apache.flink.table.planner.codegen.CodeGenUtils$.hashCodeForType(CodeGenUtils.scala:212)
         at org.apache.flink.table.planner.codegen.HashCodeGenerator$.$anonfun$generateCodeBody$1(HashCodeGenerator.scala:97)
         at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
         at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  
 and then we found the method hashCodeForType  in the CodeGenUtils class do not match MAP type.  and we fix it as below
```
 def hashCodeForType(
 ctx: CodeGeneratorContext, t: LogicalType, term: String): String = t.getTypeRoot match

{ case BOOLEAN => s""$\\{className[JBoolean]}

.hashCode($term)""
 case MAP => s""$\{className[BaseMap]}.getHashCode($term)""  //the code we add
 case TINYINT => s""$\{className[JByte]}.hashCode($term)""
 ```


 then the job can be sumitted, it run for a while, another exception occurs:
 java.lang.RuntimeException: Could not instantiate generated class 'HashAggregateWithKeys$1543'
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:67)
 at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:46)
 at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:48)
 at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:156)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:433)
 at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)
 at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
 at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
 at java.lang.Thread.run(Thread.java:745)
 Caused by: org.apache.flink.util.FlinkRuntimeException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:68)
 at org.apache.flink.table.runtime.generated.GeneratedClass.compile(GeneratedClass.java:78)
 at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
 ... 8 more
 Caused by: org.apache.flink.shaded.guava18.com.google.common.util.concurrent.UncheckedExecutionException: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache.get(LocalCache.java:3937)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
 at org.apache.flink.table.runtime.generated.CompileUtils.compile(CompileUtils.java:66)
 ... 10 more
 Caused by: org.apache.flink.api.common.InvalidProgramException: Table program cannot be compiled. This is a bug. Please file an issue.
 at org.apache.flink.table.runtime.generated.CompileUtils.doCompile(CompileUtils.java:81)
 at org.apache.flink.table.runtime.generated.CompileUtils.lambda$compile$1(CompileUtils.java:66)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
 at org.apache.flink.shaded.guava18.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
 ... 13 more
 Caused by: org.codehaus.commons.compiler.CompileException: Line 459, Column 57: A method named ""compareTo"" is not declared in any enclosing class nor any supertype, nor through a static import
 at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12124)
 at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8997)
 at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5060)
 at org.codehaus.janino.UnitCompiler.access$9100(UnitCompiler.java:215)
 at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4421)
 at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4394)
 at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5062)
 at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4394)
 at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5575)
 at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2580)
 at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:215)
  "	FLINK	Closed	4	1	6732	pull-request-available
13431399	Introduce sequence.field to custom the order of the records	"In general, the order of the record can be generated directly by the store itself.
But for some cases, its order may be broken, and then a field can be specified to control the sequence number information. (For example, which one should be returned when querying multiple records of data under one primary key)"	FLINK	Closed	3	4	6732	pull-request-available
13280673	LastValueAggFunctionWithOrderTest compilation error due to incompatible types	"{{LastValueAggFunctionWithOrderTest}} failed to compile in latest release-1.10 nightly build with below error:
{code}
03:02:41.792 [INFO] -------------------------------------------------------------
03:02:41.792 [ERROR] COMPILATION ERROR : 
03:02:41.792 [INFO] -------------------------------------------------------------
03:02:41.792 [ERROR] /home/travis/build/apache/flink/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunctionWithOrderTest.java:[78,37] incompatible types: inference variable T has incompatible bounds
    equality constraints: N,java.lang.Byte,N,T,T,T,T,org.apache.flink.table.dataformat.BinaryString,T,T
    lower bounds: java.lang.Byte,org.apache.flink.table.dataformat.BinaryString
{code}

https://api.travis-ci.org/v3/job/639573134/log.txt"	FLINK	Closed	2	1	6732	pull-request-available
13347529	Clean useless codes: Never push calcProgram to correlate	projectProgram in StreamPhysicalCorrelateBase never be used.	FLINK	Closed	3	4	6732	pull-request-available
13565616	Upgrade Flink CI Docker container to Ubuntu 22.04	"The current CI Docker image is based on Ubuntu 16.04. We already use 20.04 for the e2e tests. We can update the Docker image to a newer version to be on par with what we need in GitHub Actions (FLINK-33923).

This issue can cover the following topics:
 * Update to 22.04
 ** OpenSSL 1.0.0 dependency should be added for netty-tcnative support
 ** Use Python3 instead of Python 2.7 (python symlink needs to be added due to FLINK-34195) 
 * Removal of Maven (FLINK-33501 makes us rely on the Maven wrapper)"	FLINK	Reopened	3	1	6847	github-actions, pull-request-available
13532778	Enable Precondition in DefaultLeaderElectionService.close after the MultipleComponentLeaderElectionDriverAdapter is removed	"Currently, we have a cyclic dependency in the leader election code where the {{DefaultLeaderElection.close()}} (pre FLIP-285 code: {{{}DefaultLeaderElectionService.stop(){}}}) call will trigger the deregistration of the {{LeaderElectionEventHandler}} (which is the very same {{DefaultLeaderElectionService}} that is in the midst of being closed). The deregistration is forwarded to the {{DefaultMultipleComponentLeaderElectionService}} that will also trigger an {{onRevokeLeadership()}} call on the {{{}LeaderElectionEventHandler{}}}.

There is an intended Precondition in {{DefaultLeaderElectionService.onRevokeLeadership()}} introduced in FLINK-31773 that should be valid if the service has uni-directed ownership of the driver (i.e. the driver doesn't call the service again). This Precondition is commented out and marked with this Jira issue FLINK-31814 right now due to the cyclic dependency described above.

We can add the Precondition after {{MultipleComponentLeaderElectionDriverAdapter}} is removed (and, therefore, the cyclic dependency resolved)."	FLINK	Resolved	3	7	6847	pull-request-available
13262557	Add metric for managed memory	"If a user wants to get memory used in time, as there's no manage memory's metrics, it couldn't get it.

*Propose*
 * add default memory type in MemoryManager

 
{code:java}
public static final MemoryType DEFAULT_MEMORY_TYPE = MemoryType.OFF_HEAP;
{code}
 * add getManagedMemoryTotal in TaskExecutor:

 
{code:java}
public long getManagedMemoryTotal() {
    return this.taskSlotTable.getAllocatedSlots().stream().mapToLong(
        slot -> slot.getMemoryManager().getMemorySizeByType(MemoryManager.DEFAULT_MEMORY_TYPE)
    ).sum();
}
{code}
 
 * add getManagedMemoryUsed in TaskExecutor:

 
{code:java}
public long getManagedMemoryUsed() {
    return this.taskSlotTable.getAllocatedSlots().stream().mapToLong(
        slot -> slot.getMemoryManager().getMemorySizeByType(MemoryManager.DEFAULT_MEMORY_TYPE) - slot.getMemoryManager().availableMemory(MemoryManager.DEFAULT_MEMORY_TYPE)
    ).sum();
}
{code}
 
 * add instantiateMemoryManagerMetrics in MetricUtils

 
{code:java}
public static void instantiateMemoryManagerMetrics(MetricGroup statusMetricGroup, TaskExecutor taskExecutor) {
    checkNotNull(statusMetricGroup);
    MetricGroup memoryManagerGroup = statusMetricGroup.addGroup(""Managed"").addGroup(""Memory"");
    memoryManagerGroup.<Long, Gauge<Long>>gauge(""TotalCapacity"", taskExecutor::getManagedMemoryTotal);
    memoryManagerGroup.<Long, Gauge<Long>>gauge(""MemoryUsed"", taskExecutor::getManagedMemoryUsed);
}
{code}
 * register it in TaskManagerRunner#startTaskManager

 

 "	FLINK	Closed	3	7	6847	pull-request-available
13527392	NettyClientServerSslTest.testValidSslConnectionAdvanced timed out	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46883&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8242

{code}
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) is running.
--------------------------------------------------------------------------------
05:15:10,904 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port: 42935, ssl enabled: true, memory segment size (bytes): 1024, transport type: AUTO, number of server threads: 1 (manual), number of client threads>
05:15:10,916 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
05:15:12,149 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 1245 ms). Listening on SocketAddress /127.0.0.1:42935.
05:15:12,150 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
05:15:13,249 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 1099 ms).
05:15:14,588 [                main] ERROR org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest [] - 
--------------------------------------------------------------------------------
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) failed with:
org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandshakeTimeoutException: handshake timed out after 1000ms
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$7.run(SslHandler.java:2115)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:153)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)
{code}"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13471299	Job suspended because ZK session times out	"
{code:java}
2022-07-12T02:28:07.2243130Z Jul 12 02:28:07 [ERROR] LocalRecoveryITCase.executeTest  Time elapsed: 29.154 s  <<< FAILURE!
2022-07-12T02:28:07.2243760Z Jul 12 02:28:07 java.lang.AssertionError: Job completed with illegal application status: UNKNOWN.
2022-07-12T02:28:07.2244333Z Jul 12 02:28:07 	at org.junit.Assert.fail(Assert.java:89)
2022-07-12T02:28:07.2245097Z Jul 12 02:28:07 	at org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.testSlidingTimeWindow(EventTimeWindowCheckpointingITCase.java:529)
2022-07-12T02:28:07.2245983Z Jul 12 02:28:07 	at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:84)
2022-07-12T02:28:07.2246802Z Jul 12 02:28:07 	at org.apache.flink.test.checkpointing.LocalRecoveryITCase.executeTest(LocalRecoveryITCase.java:66)
2022-07-12T02:28:07.2247680Z Jul 12 02:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2022-07-12T02:28:07.2248340Z Jul 12 02:28:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2022-07-12T02:28:07.2249573Z Jul 12 02:28:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2022-07-12T02:28:07.2250788Z Jul 12 02:28:07 	at java.lang.reflect.Method.invoke(Method.java:498)
2022-07-12T02:28:07.2251836Z Jul 12 02:28:07 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
2022-07-12T02:28:07.2253000Z Jul 12 02:28:07 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2022-07-12T02:28:07.2253985Z Jul 12 02:28:07 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
2022-07-12T02:28:07.2254718Z Jul 12 02:28:07 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2022-07-12T02:28:07.2255385Z Jul 12 02:28:07 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-12T02:28:07.2256041Z Jul 12 02:28:07 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2022-07-12T02:28:07.2256709Z Jul 12 02:28:07 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
2022-07-12T02:28:07.2257481Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T02:28:07.2258155Z Jul 12 02:28:07 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
2022-07-12T02:28:07.2258837Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
2022-07-12T02:28:07.2259514Z Jul 12 02:28:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
2022-07-12T02:28:07.2260373Z Jul 12 02:28:07 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
2022-07-12T02:28:07.2261087Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T02:28:07.2261755Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T02:28:07.2262398Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T02:28:07.2263045Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T02:28:07.2263680Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T02:28:07.2264302Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T02:28:07.2265018Z Jul 12 02:28:07 	at org.junit.runners.Suite.runChild(Suite.java:128)
2022-07-12T02:28:07.2265581Z Jul 12 02:28:07 	at org.junit.runners.Suite.runChild(Suite.java:27)
2022-07-12T02:28:07.2266314Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
2022-07-12T02:28:07.2266941Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
2022-07-12T02:28:07.2267694Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
2022-07-12T02:28:07.2268376Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
2022-07-12T02:28:07.2269147Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
2022-07-12T02:28:07.2269785Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
2022-07-12T02:28:07.2270449Z Jul 12 02:28:07 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
2022-07-12T02:28:07.2271039Z Jul 12 02:28:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
2022-07-12T02:28:07.2271611Z Jul 12 02:28:07 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
2022-07-12T02:28:07.2272316Z Jul 12 02:28:07 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
2022-07-12T02:28:07.2273057Z Jul 12 02:28:07 	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
2022-07-12T02:28:07.2273906Z Jul 12 02:28:07 	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
2022-07-12T02:28:07.2274912Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)
2022-07-12T02:28:07.2275865Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)
2022-07-12T02:28:07.2276771Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)
2022-07-12T02:28:07.2277820Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)
2022-07-12T02:28:07.2278699Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)
2022-07-12T02:28:07.2279479Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
2022-07-12T02:28:07.2280208Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
2022-07-12T02:28:07.2281076Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
2022-07-12T02:28:07.2281914Z Jul 12 02:28:07 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
2022-07-12T02:28:07.2282738Z Jul 12 02:28:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
2022-07-12T02:28:07.2283578Z Jul 12 02:28:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
2022-07-12T02:28:07.2284417Z Jul 12 02:28:07 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
2022-07-12T02:28:07.2285196Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
2022-07-12T02:28:07.2285914Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
2022-07-12T02:28:07.2286603Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
2022-07-12T02:28:07.2287377Z Jul 12 02:28:07 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=38057&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7"	FLINK	Resolved	3	11500	6847	pull-request-available, test-stability
13219310	Standby per job mode Dispatchers don't know job's JobSchedulingStatus	"At the moment, it can happen that standby {{Dispatchers}} in per job mode will restart a terminated job after they gained leadership. The problem is that we currently clear the {{RunningJobsRegistry}} once a job has reached a globally terminal state. After the leading {{Dispatcher}} terminates, a standby {{Dispatcher}} will gain leadership. Without having the information from the {{RunningJobsRegistry}} it cannot tell whether the job has been executed or whether the {{Dispatcher}} needs to re-execute the job. At the moment, the {{Dispatcher}} will assume that there was a fault and hence re-execute the job. This can lead to duplicate results.

I think we need some way to tell standby {{Dispatchers}} that a certain job has been successfully executed. One trivial solution could be to not clean up the {{RunningJobsRegistry}} but then we will clutter ZooKeeper."	FLINK	Closed	3	1	6847	pull-request-available
13542763	Renames contenderID into componentId	"We introduced {{contenderID}} in a lot of places with FLINK-26522. The original multi-component leader election classes of FLINK-24038 used {{componentId}}.

Revisiting that naming made me realize that it's actually wrong. A contender is a specific instance of a component that participates in the leader election. A component, in this sense, is the more abstract concept. {{contenderID}} refers to an ID for the specific contender instance but the IDs we're sharing are actually referring to a Flink component and therefore, are the same between different contenders which compete for leadership for the same component. This contradicts the definition of an identifier."	FLINK	Resolved	3	7	6847	pull-request-available
13375384	Avoid discarding checkpoints in case of failure	"Both {{StateHandleStore}} implementations (i.e. [KubernetesStateHandleStore:157|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesStateHandleStore.java#L157] and [ZooKeeperStateHandleStore:170|https://github.com/apache/flink/blob/c6997c97c575d334679915c328792b8a3067cfb5/flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java#L170]) discard checkpoints if the checkpoint metadata wasn't written to the backend. 

This does not cover the cases where the data was actually written to the backend but the call failed anyway (e.g. due to network issues). In such a case, we might end up having a pointer in the backend pointing to a checkpoint that was discarded.

Instead of discarding the checkpoint data in this case, we might want to keep it for this specific use case. Otherwise, we might run into Exceptions when recovering from the Checkpoint later on. We might want to add a warning to the user pointing to the possibly orphaned checkpoint data."	FLINK	Closed	2	1	6847	pull-request-available
13553705	.scalafmt.conf cannot be found in Test packaging/licensing job	https://github.com/XComp/flink/actions/runs/6473584177/job/17581941684#step:8:4327	FLINK	Resolved	3	7	6847	github-actions, test-stability
13567382	GHA e2e test failure due to no space left on device error	"https://github.com/apache/flink/actions/runs/7763815214

{code}
AdaptiveScheduler / E2E (group 2)
Process completed with exit code 1.
AdaptiveScheduler / E2E (group 2)
You are running out of disk space. The runner will stop working when the machine runs out of disk space. Free space left: 35 MB
{code}

We're only seeing it in GHA which makes me think that it's rather a GHA-related issue."	FLINK	Resolved	3	1	6847	github-actions, pull-request-available, test-stability
13529601	Update japicmp configuration	"Update the japicmp reference version and wipe exclusions / enable API compatibility checks for {{@PublicEvolving}} APIs on the corresponding SNAPSHOT branch with the {{update_japicmp_configuration.sh}} script (see below).

For a new major release (x.y.0), run the same command also on the master branch for updating the japicmp reference version and removing out-dated exclusions in the japicmp configuration.

Make sure that all Maven artifacts are already pushed to Maven Central. Otherwise, there's a risk that CI fails due to missing reference artifacts.
{code:bash}
tools $ NEW_VERSION=$RELEASE_VERSION releasing/update_japicmp_configuration.sh
tools $ cd ..$ git add *$ git commit -m ""Update japicmp configuration for $RELEASE_VERSION"" {code}"	FLINK	Resolved	3	7	6847	pull-request-available
13434297	FileSystemJobResultStore#constructDirtyPath might lost the scheme	" 
{code:java}
/**
     * Given a job ID, construct the path for a dirty entry corresponding to it in the job result
     * store.
     *
     * @param jobId The job ID to construct a dirty entry path from.
     * @return A path for a dirty entry for the given the Job ID.
     */
    private Path constructDirtyPath(JobID jobId) {
        return new Path(this.basePath.getPath(), jobId.toString() + DIRTY_FILE_EXTENSION);
    } {code}
 

Just like above piece of code, we are using {{{}this.basePath.getPath(){}}}, not directly use {{this.basePath}} when create a new Path. I am afraid this will cause scheme lost and cause issue when some filesystem implementation tries to stat the path."	FLINK	Resolved	1	1	6847	pull-request-available
13590912	Disabling japicmp plugin for deprecated APIs	The Apache Flink 2.0 release allows for the removal of public API. The japicmp plugin usually checks for these kind of changes. To avoid adding explicit excludes for each change, this Jira issue suggest to disable the API check for APIs that are marked as deprecated.	FLINK	Resolved	3	4	6847	pull-request-available
13429099	Making ZK-related logs available in tests	"Recently, we had a few incidents where it appears that ZooKeeper wasn't behaving as expected. It might help to have to the ZooKeeper logs available in these cases.

We have multiple options:
 * Introduce an extension to change the ZK log level for specific tests
 * Lower the ZK log level again and make the logs being written to the standard log files
 * Lower the ZK log level again and move the ZK logs into a dedicated file to avoid spoiling the Flink logs"	FLINK	Resolved	3	4	6847	pull-request-available, test-stability
13355146	Expose extended exception history through REST handler and adapt UI accordingly	"A first idea of a UI change was proposed by [~vthinkxie] in the parent task:

!https://issues.apache.org/jira/secure/attachment/13019553/13019553_%E6%88%AA%E5%B1%8F2021-01-28+%E4%B8%8B%E5%8D%884.47.46.png|width=476,height=245!

The {{JobExceptionsHandler}} response should be extended introducing a new (deprecated) field {{otherFailures}}. This way we avoid breaking the API."	FLINK	Closed	3	7	6847	pull-request-available
13395798	Test Kafka table connector with new runtime provider	The runtime provider of Kafka table connector has been replaced with new KafkaSource and KafkaSink. The table connector requires to be tested to make sure nothing is surprised to Table/SQL API users. 	FLINK	Closed	1	7	6847	release-testing
13432280	AdaptiveSchedulerClusterITCase.testAutomaticScaleUp failed on azure	"{code:java}
Mar 03 13:38:24 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.854 s <<< FAILURE! - in org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase
Mar 03 13:38:24 [ERROR] org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp  Time elapsed: 16.035 s  <<< ERROR!
Mar 03 13:38:24 java.util.concurrent.TimeoutException: Condition was not met in given timeout.
Mar 03 13:38:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:167)
Mar 03 13:38:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Mar 03 13:38:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:137)
Mar 03 13:38:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.waitUntilParallelismForVertexReached(AdaptiveSchedulerClusterITCase.java:267)
Mar 03 13:38:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:147)
Mar 03 13:38:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 03 13:38:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 03 13:38:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 03 13:38:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 03 13:38:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 03 13:38:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 03 13:38:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 03 13:38:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Mar 03 13:38:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Mar 03 13:38:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Mar 03 13:38:24 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Mar 03 13:38:24 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 03 13:38:24 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Mar 03 13:38:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Mar 03 13:38:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Mar 03 13:38:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Mar 03 13:38:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
Mar 03 13:38:24 	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
Mar 03 13:38:24 	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32488&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9618"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13336186	Logger cannot be initialized due to timeout: LoggerInitializationException is thrown	"The actual failure is an error in instantiating the logger:
{code:java}
java.lang.Exception: Could not create actor system
	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:263)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:341)
	at org.apache.flink.runtime.minicluster.MiniCluster.createLocalRpcService(MiniCluster.java:792)
	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:273)
	at org.apache.flink.runtime.testutils.MiniClusterResource.startMiniCluster(MiniClusterResource.java:180)
	at org.apache.flink.runtime.testutils.MiniClusterResource.before(MiniClusterResource.java:91)
	at org.apache.flink.test.util.MiniClusterWithClientResource.before(MiniClusterWithClientResource.java:51)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:46)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: akka.ConfigurationException: Could not start logger due to [akka.ConfigurationException: Logger specified in config can't be loaded [akka.event.slf4j.Slf4jLogger] due to [akka.event.Logging$LoggerInitializationException: Logger log1-Slf4jLogger did not respond with LoggerInitialized, sent instead [TIMEOUT]]]
	at akka.event.LoggingBus$class.startDefaultLoggers(Logging.scala:147)
	at akka.event.EventStream.startDefaultLoggers(EventStream.scala:22)
	at akka.actor.LocalActorRefProvider.init(ActorRefProvider.scala:662)
	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:874)
	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:870)
	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:870)
	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:891)
	at akka.actor.RobustActorSystem$.internalApply(RobustActorSystem.scala:96)
	at akka.actor.RobustActorSystem$.apply(RobustActorSystem.scala:70)
	at akka.actor.RobustActorSystem$.create(RobustActorSystem.scala:55)
	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:125)
	at org.apache.flink.runtime.akka.AkkaUtils.createActorSystem(AkkaUtils.scala)
	at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:276)
	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:260)
	... 17 more

{code}
 

Here is an instance:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7877&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=8d78fe4f-d658-5c70-12f8-4921589024c3&l=20270]
{code:java}
2020-10-19T18:06:30.6704453Z =================================== FAILURES ===================================
2020-10-19T18:06:30.6705106Z ________________ DataStreamTests.test_key_by_on_connect_stream _________________
2020-10-19T18:06:30.6705465Z 
2020-10-19T18:06:30.6705934Z self = <pyflink.datastream.tests.test_data_stream.DataStreamTests testMethod=test_key_by_on_connect_stream>
2020-10-19T18:06:30.6707720Z 
2020-10-19T18:06:30.6708558Z     def test_key_by_on_connect_stream(self):
2020-10-19T18:06:30.6710053Z         ds1 = self.env.from_collection([('a', 0), ('b', 0), ('c', 1), ('d', 1), ('e', 2)],
2020-10-19T18:06:30.6710691Z                                        type_info=Types.ROW([Types.STRING(), Types.INT()])) \
2020-10-19T18:06:30.6711381Z             .key_by(MyKeySelector(), key_type_info=Types.INT())
2020-10-19T18:06:30.6717965Z         ds2 = self.env.from_collection([('a', 0), ('b', 0), ('c', 1), ('d', 1), ('e', 2)],
2020-10-19T18:06:30.6718712Z                                        type_info=Types.ROW([Types.STRING(), Types.INT()]))
2020-10-19T18:06:30.6719278Z     
2020-10-19T18:06:30.6719679Z         class AssertKeyCoMapFunction(CoMapFunction):
2020-10-19T18:06:30.6720136Z             def __init__(self):
2020-10-19T18:06:30.6720528Z                 self.pre1 = None
2020-10-19T18:06:30.6720881Z                 self.pre2 = None
2020-10-19T18:06:30.6721195Z     
2020-10-19T18:06:30.6721510Z             def map1(self, value):
2020-10-19T18:06:30.6722215Z                 if value[0] == 'b':
2020-10-19T18:06:30.6722849Z                     assert self.pre1 == 'a'
2020-10-19T18:06:30.6723621Z                 if value[0] == 'd':
2020-10-19T18:06:30.6724247Z                     assert self.pre1 == 'c'
2020-10-19T18:06:30.6724652Z                 self.pre1 = value[0]
2020-10-19T18:06:30.6725002Z                 return value
2020-10-19T18:06:30.6730397Z     
2020-10-19T18:06:30.6730856Z             def map2(self, value):
2020-10-19T18:06:30.6731728Z                 if value[0] == 'b':
2020-10-19T18:06:30.6732399Z                     assert self.pre2 == 'a'
2020-10-19T18:06:30.6733025Z                 if value[0] == 'd':
2020-10-19T18:06:30.6733769Z                     assert self.pre2 == 'c'
2020-10-19T18:06:30.6734159Z                 self.pre2 = value[0]
2020-10-19T18:06:30.6734521Z                 return value
2020-10-19T18:06:30.6735084Z     
2020-10-19T18:06:30.6735376Z         ds1.connect(ds2)\
2020-10-19T18:06:30.6735862Z             .key_by(MyKeySelector(), MyKeySelector(), key_type_info=Types.INT())\
2020-10-19T18:06:30.6736369Z             .map(AssertKeyCoMapFunction())\
2020-10-19T18:06:30.6736761Z             .add_sink(self.test_sink)
2020-10-19T18:06:30.6737079Z     
2020-10-19T18:06:30.6737718Z >       self.env.execute('key_by_test')
2020-10-19T18:06:30.6737996Z 
2020-10-19T18:06:30.6738342Z pyflink/datastream/tests/test_data_stream.py:206: 
2020-10-19T18:06:30.6738880Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-10-19T18:06:30.6739598Z pyflink/datastream/stream_execution_environment.py:621: in execute
2020-10-19T18:06:30.6740205Z     return JobExecutionResult(self._j_stream_execution_environment.execute(j_stream_graph))
2020-10-19T18:06:30.6741118Z .tox/py35-cython/lib/python3.5/site-packages/py4j/java_gateway.py:1286: in __call__
2020-10-19T18:06:30.6741690Z     answer, self.gateway_client, self.target_id, self.name)
2020-10-19T18:06:30.6745640Z pyflink/util/exceptions.py:147: in deco
2020-10-19T18:06:30.6746059Z     return f(*a, **kw)
2020-10-19T18:06:30.6746516Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2020-10-19T18:06:30.6746870Z 
2020-10-19T18:06:30.6747564Z answer = 'xro4008'
2020-10-19T18:06:30.6748011Z gateway_client = <py4j.java_gateway.GatewayClient object at 0x7f3087c37748>
2020-10-19T18:06:30.6748717Z target_id = 'o3660', name = 'execute'
2020-10-19T18:06:30.6749062Z 
2020-10-19T18:06:30.6749495Z     def get_return_value(answer, gateway_client, target_id=None, name=None):
2020-10-19T18:06:30.6750188Z         """"""Converts an answer received from the Java gateway into a Python object.
2020-10-19T18:06:30.6750598Z     
2020-10-19T18:06:30.6751006Z         For example, string representation of integers are converted to Python
2020-10-19T18:06:30.6751559Z         integer, string representation of objects are converted to JavaObject
2020-10-19T18:06:30.6752020Z         instances, etc.
2020-10-19T18:06:30.6752313Z     
2020-10-19T18:06:30.6752693Z         :param answer: the string returned by the Java gateway
2020-10-19T18:06:30.6753379Z         :param gateway_client: the gateway client used to communicate with the Java
2020-10-19T18:06:30.6753980Z             Gateway. Only necessary if the answer is a reference (e.g., object,
2020-10-19T18:06:30.6754411Z             list, map)
2020-10-19T18:06:30.6754857Z         :param target_id: the name of the object from which the answer comes from
2020-10-19T18:06:30.6755430Z             (e.g., *object1* in `object1.hello()`). Optional.
2020-10-19T18:06:30.6755969Z         :param name: the name of the member from which the answer comes from
2020-10-19T18:06:30.6756522Z             (e.g., *hello* in `object1.hello()`). Optional.
2020-10-19T18:06:30.6756888Z         """"""
2020-10-19T18:06:30.6757327Z         if is_error(answer)[0]:
2020-10-19T18:06:30.6757721Z             if len(answer) > 1:
2020-10-19T18:06:30.6758122Z                 type = answer[1]
2020-10-19T18:06:30.6758610Z                 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
2020-10-19T18:06:30.6759288Z                 if answer[1] == REFERENCE_TYPE:
2020-10-19T18:06:30.6759725Z                     raise Py4JJavaError(
2020-10-19T18:06:30.6760202Z                         ""An error occurred while calling {0}{1}{2}.\n"".
2020-10-19T18:06:30.6760710Z >                       format(target_id, ""."", name), value)
2020-10-19T18:06:30.6761374Z E                   py4j.protocol.Py4JJavaError: An error occurred while calling o3660.execute.
2020-10-19T18:06:30.6762068Z E                   : java.lang.Exception: Could not create actor system
2020-10-19T18:06:30.6762857Z E                   	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:263)
2020-10-19T18:06:30.6763949Z E                   	at org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils$AkkaRpcServiceBuilder.createAndStart(AkkaRpcServiceUtils.java:341)
2020-10-19T18:06:30.6765129Z E                   	at org.apache.flink.runtime.metrics.util.MetricUtils.startMetricRpcService(MetricUtils.java:152)
2020-10-19T18:06:30.6766058Z E                   	at org.apache.flink.runtime.metrics.util.MetricUtils.startLocalMetricsRpcService(MetricUtils.java:142)
2020-10-19T18:06:30.6766915Z E                   	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:275)
2020-10-19T18:06:30.6767883Z E                   	at org.apache.flink.client.program.PerJobMiniClusterFactory.submitJob(PerJobMiniClusterFactory.java:74)
2020-10-19T18:06:30.6768798Z E                   	at org.apache.flink.client.deployment.executors.LocalExecutor.execute(LocalExecutor.java:81)
2020-10-19T18:06:30.6769938Z E                   	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1867)
2020-10-19T18:06:30.6770886Z E                   	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1763)
2020-10-19T18:06:30.6771888Z E                   	at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment.java:74)
2020-10-19T18:06:30.6772685Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-19T18:06:30.6773464Z E                   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-19T18:06:30.6774309Z E                   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-19T18:06:30.6775064Z E                   	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-19T18:06:30.6775849Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
2020-10-19T18:06:30.6776765Z E                   	at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
2020-10-19T18:06:30.6777692Z E                   	at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
2020-10-19T18:06:30.6778577Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
2020-10-19T18:06:30.6779591Z E                   	at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
2020-10-19T18:06:30.6780471Z E                   	at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
2020-10-19T18:06:30.6781193Z E                   	at java.lang.Thread.run(Thread.java:748)
2020-10-19T18:06:30.6783433Z E                   Caused by: akka.ConfigurationException: Could not start logger due to [akka.ConfigurationException: Logger specified in config can't be loaded [akka.event.slf4j.Slf4jLogger] due to [akka.event.Logging$LoggerInitializationException: Logger log1-Slf4jLogger did not respond with LoggerInitialized, sent instead [TIMEOUT]]]
2020-10-19T18:06:30.6784902Z E                   	at akka.event.LoggingBus$class.startDefaultLoggers(Logging.scala:147)
2020-10-19T18:06:30.6785646Z E                   	at akka.event.EventStream.startDefaultLoggers(EventStream.scala:22)
2020-10-19T18:06:30.6786417Z E                   	at akka.actor.LocalActorRefProvider.init(ActorRefProvider.scala:662)
2020-10-19T18:06:30.6787228Z E                   	at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:874)
2020-10-19T18:06:30.6787974Z E                   	at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:870)
2020-10-19T18:06:30.6788680Z E                   	at akka.actor.ActorSystemImpl._start(ActorSystem.scala:870)
2020-10-19T18:06:30.6789462Z E                   	at akka.actor.ActorSystemImpl.start(ActorSystem.scala:891)
2020-10-19T18:06:30.6790182Z E                   	at akka.actor.RobustActorSystem$.internalApply(RobustActorSystem.scala:96)
2020-10-19T18:06:30.6790915Z E                   	at akka.actor.RobustActorSystem$.apply(RobustActorSystem.scala:70)
2020-10-19T18:06:30.6791819Z E                   	at akka.actor.RobustActorSystem$.create(RobustActorSystem.scala:55)
2020-10-19T18:06:30.6792597Z E                   	at org.apache.flink.runtime.akka.AkkaUtils$.createActorSystem(AkkaUtils.scala:125)
2020-10-19T18:06:30.6793524Z E                   	at org.apache.flink.runtime.akka.AkkaUtils.createActorSystem(AkkaUtils.scala)
2020-10-19T18:06:30.6794301Z E                   	at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:276)
2020-10-19T18:06:30.6795256Z E                   	at org.apache.flink.runtime.clusterframework.BootstrapTools.startLocalActorSystem(BootstrapTools.java:260)
2020-10-19T18:06:30.6796016Z E                   	... 20 more
2020-10-19T18:06:30.6796252Z 
2020-10-19T18:06:30.6797013Z .tox/py35-cython/lib/python3.5/site-packages/py4j/protocol.py:328: Py4JJavaError
{code}"	FLINK	Closed	2	1	6847	pull-request-available, test-stability
13561821	Move Slack Invite URL into config.toml	Instead of 4 locations, we want to update only one location with the invite link. Additionally, we should add documentation on how to update the link.	FLINK	Resolved	3	1	6847	pull-request-available
13532133	Disable changelog backend for the StatefulJobSavepointMigrationITCase and StatefulJobWBroadcastStateMigrationITCase with RocksDB and checkpoints	"In FLINK-31593 we discovered an instability when generating the migration test data for 1.17 for {{StatefulJobSavepointMigrationITCase}} and {{StatefulJobWBroadcastStateMigrationITCase}}. According to the discussion in FLINK-31593, we concluded that it's caused by a non-determinism that's happening in the changelog backend code. As a workaround, we're going to disable the changelog backend in these tests for now.

We're not touching 1.16 because it didn't appear in that branch. The non-determinism seems to kick in only when generating the checkpoint files. 

For 1.17, we're gonna create a backport for consistency reasons because we have to enable it for the data generation."	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13570336	Align retry mechanisms of FutureUtils	The retry mechanisms of FutureUtils include quite a bit of redundant code which makes it hard to understand and to extend. The logic should be aligned properly.	FLINK	Open	3	11500	6847	pull-request-available
13335243	Add Metaspace metric	We want to expose the currently used Metaspace memory as well that should be provided through the metrics system.	FLINK	Closed	3	7	6847	pull-request-available
13543018	Deprecate API that uses Flink's Time implementation (related to FLINK-14638)	The plan is to resolve FLINK-14038 with Flink 2.0. As a preparation, we have to deprecate related @Public API .	FLINK	Closed	3	7	6847	pull-request-available, stale-assigned
13505237	BlobServerCleanupTest utilizes failsWithin with a quite low timeout which might result in test instabilities	{{BlobServerCleanupTest}} utilizes {{failsWithin}} using a low timeout threshold (100ms) in several locations. This might result in test instabilities (e.g. [here|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43221&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=8801]). We shouldn't rely on timeouts as stated in the Flink coding guidelines.	FLINK	Resolved	3	1	6847	pull-request-available, starter
13581918	Move CheckpointStatsTracker out of ExecutionGraph into Scheduler	The scheduler needs to know about the CheckpointStatsTracker to allow listening to checkpoint failures and completion.	FLINK	Resolved	3	7	6847	pull-request-available
13362910	YARNSessionCapacitySchedulerITCase.testStartYarnSessionClusterInQaTeamQueue fail because of NullPointerException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14265&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=62110053-334f-5295-a0ab-80dd7e2babbf

{code:java}
2021-03-07T23:00:44.6390668Z [ERROR] testStartYarnSessionClusterInQaTeamQueue(org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase)  Time elapsed: 7.338 s  <<< ERROR!
2021-03-07T23:00:44.6391415Z java.lang.NullPointerException: 
2021-03-07T23:00:44.6403594Z java.lang.NullPointerException
2021-03-07T23:00:44.6404575Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2021-03-07T23:00:44.6405710Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2021-03-07T23:00:44.6406830Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2021-03-07T23:00:44.6407970Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2021-03-07T23:00:44.6409075Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2021-03-07T23:00:44.6412848Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2021-03-07T23:00:44.6417313Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2021-03-07T23:00:44.6421872Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2021-03-07T23:00:44.6423676Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2021-03-07T23:00:44.6424387Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2021-03-07T23:00:44.6424997Z 	at java.security.AccessController.doPrivileged(Native Method)
2021-03-07T23:00:44.6425608Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-03-07T23:00:44.6426513Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2021-03-07T23:00:44.6427351Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2021-03-07T23:00:44.6427767Z 
2021-03-07T23:00:44.6428196Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2021-03-07T23:00:44.6428975Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2021-03-07T23:00:44.6429888Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2021-03-07T23:00:44.6442419Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2021-03-07T23:00:44.6445364Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
2021-03-07T23:00:44.6644429Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateRuntimeException(RPCUtil.java:85)
2021-03-07T23:00:44.6658468Z 	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:122)
2021-03-07T23:00:44.6669171Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:291)
2021-03-07T23:00:44.6680027Z 	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
2021-03-07T23:00:44.6690713Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-07T23:00:44.6701085Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-07T23:00:44.6708626Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
2021-03-07T23:00:44.6709488Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
2021-03-07T23:00:44.6710261Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
2021-03-07T23:00:44.6711051Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
2021-03-07T23:00:44.6711864Z 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
2021-03-07T23:00:44.6729939Z 	at com.sun.proxy.$Proxy111.getApplications(Unknown Source)
2021-03-07T23:00:44.6746044Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:528)
2021-03-07T23:00:44.6747093Z 	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplications(YarnClientImpl.java:505)
2021-03-07T23:00:44.6748256Z 	at org.apache.flink.yarn.YarnTestBase$CleanupYarnApplication.close(YarnTestBase.java:293)
2021-03-07T23:00:44.6749122Z 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:274)
2021-03-07T23:00:44.6750975Z 	at org.apache.flink.yarn.YARNSessionCapacitySchedulerITCase.testStartYarnSessionClusterInQaTeamQueue(YARNSessionCapacitySchedulerITCase.java:164)
2021-03-07T23:00:44.6751907Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-07T23:00:44.6753574Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-07T23:00:44.6754504Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-07T23:00:44.6755270Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-07T23:00:44.6757569Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-07T23:00:44.6758434Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-07T23:00:44.6759463Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-07T23:00:44.6760279Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-07T23:00:44.6761082Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-03-07T23:00:44.6762689Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-03-07T23:00:44.6763589Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6764601Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6765295Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-03-07T23:00:44.6765968Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-07T23:00:44.6766601Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-07T23:00:44.6767442Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-07T23:00:44.6768248Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-07T23:00:44.6768968Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-07T23:00:44.6769645Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-07T23:00:44.6770337Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-07T23:00:44.6771022Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-07T23:00:44.6771746Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-07T23:00:44.6772483Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2021-03-07T23:00:44.6773298Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2021-03-07T23:00:44.6774046Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6774773Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-07T23:00:44.6775435Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-07T23:00:44.6776067Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-07T23:00:44.6776794Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-03-07T23:00:44.6777694Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-03-07T23:00:44.6778543Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-03-07T23:00:44.6779362Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-03-07T23:00:44.6780201Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-07T23:00:44.6781085Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-07T23:00:44.6781878Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-07T23:00:44.6782634Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-03-07T23:00:44.6783545Z Caused by: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
2021-03-07T23:00:44.6784609Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics.getAggregateAppResourceUsage(RMAppAttemptMetrics.java:128)
2021-03-07T23:00:44.6785745Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport(RMAppAttemptImpl.java:900)
2021-03-07T23:00:44.6786821Z 	at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.createAndGetApplicationReport(RMAppImpl.java:660)
2021-03-07T23:00:44.6787886Z 	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplications(ClientRMService.java:930)
2021-03-07T23:00:44.6794073Z 	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplications(ApplicationClientProtocolPBServiceImpl.java:273)
2021-03-07T23:00:44.6796335Z 	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:507)
2021-03-07T23:00:44.6798877Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
2021-03-07T23:00:44.6801068Z 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
2021-03-07T23:00:44.6803385Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:847)
2021-03-07T23:00:44.6805566Z 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:790)
2021-03-07T23:00:44.6807902Z 	at java.security.AccessController.doPrivileged(Native Method)
2021-03-07T23:00:44.6810554Z 	at javax.security.auth.Subject.doAs(Subject.java:422)
2021-03-07T23:00:44.6813030Z 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)
2021-03-07T23:00:44.6815324Z 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2486)
2021-03-07T23:00:44.6817630Z 
2021-03-07T23:00:44.6819989Z 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
2021-03-07T23:00:44.6820646Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
2021-03-07T23:00:44.6821270Z 	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
2021-03-07T23:00:44.6822571Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
2021-03-07T23:00:44.6823429Z 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
2021-03-07T23:00:44.6825060Z 	at com.sun.proxy.$Proxy110.getApplications(Unknown Source)
2021-03-07T23:00:44.6827746Z 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:288)
2021-03-07T23:00:44.6828489Z 	... 50 more
2021-03-07T23:00:44.6828728Z 
2021-03-07T23:01:15.2299964Z [INFO] Running org.apache.flink.yarn.YARNITCase
2021-03-07T23:01:20.5439420Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5442231Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
2021-03-07T23:01:20.5442949Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5443688Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
2021-03-07T23:01:20.5444176Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2021-03-07T23:01:20.5444733Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2021-03-07T23:01:20.5471230Z Mar 07, 2021 11:01:20 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2021-03-07T23:01:20.5473577Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2021-03-07T23:01:20.6368864Z Mar 07, 2021 11:01:20 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getC
{code}
"	FLINK	Closed	3	1	6847	pull-request-available, test-stability
13530212	docker-build.sh build fails on Linux machines	"Building the Flink website on Linux fails due to how Docker is used as a Daemon running under root in Linux (see [this blog|https://jtreminio.com/blog/running-docker-containers-as-current-host-user/#ok-so-what-actually-works] for more details).

Building the website will fail when copying the artifacts because they are owned by the root user. 

{code}
./docker-build.sh build                                                                                     
latest: Pulling from jakejarvis/hugo-extended                                                                                                
Digest: sha256:7d7eb41d7949b5ed338c27926098b84e152e7e1d8ad8f1955c29b383a2336548                                                              
Status: Image is up to date for jakejarvis/hugo-extended:latest                                                                              
docker.io/jakejarvis/hugo-extended:latest                                                                                                    
Start building sites …                                                                                                                       
hugo v0.111.3-5d4eb5154e1fed125ca8e9b5a0315c4180dab192+extended linux/amd64 BuildDate=2023-03-12T11:40:50Z VendorInfo=docker                 
Error: Error building site: open /src/target/news/2014/08/26/release-0.6.html: permission denied                                             
Total in 153 ms                                                                                                                              
mv: cannot move 'docs/target/2014' to 'content/2014': Permission denied                                                                      
mv: cannot move 'docs/target/2015' to 'content/2015': Permission denied                                                                      
mv: cannot move 'docs/target/2016' to 'content/2016': Permission denied                                                                      
mv: cannot move 'docs/target/2017' to 'content/2017': Permission denied                                                                      
mv: cannot move 'docs/target/2018' to 'content/2018': Permission denied                                                                      
mv: cannot move 'docs/target/2019' to 'content/2019': Permission denied                                                                      
mv: cannot move 'docs/target/2020' to 'content/2020': Permission denied                                                                      
mv: cannot move 'docs/target/2021' to 'content/2021': Permission denied                                                                      
mv: cannot move 'docs/target/2022' to 'content/2022': Permission denied                                                                                                                                                                                                                   
mv: cannot move 'docs/target/2023' to 'content/2023': Permission denied                                                                      
mv: cannot move 'docs/target/categories' to 'content/categories': Permission denied
[...]
{code}"	FLINK	Resolved	3	1	6847	pull-request-available
13432901	Missing close in FileSystemJobResultStore	{{FileSystemJobResultStore.createDirtyResultInternal}} does not close the opened {{OutputStream}}	FLINK	Resolved	1	1	6847	pull-request-available
13541200	EmbeddedLeaderService doesn't handle the leader events properly in edge cases	"The leadership is granted when registering the first contender. This sets the leadership flag within the EmbeddedLeaderService (see [EmbeddedLeaderService:312ff|https://github.com/apache/flink/blob/033aca7566a0a561410b3c0e1ae8dca856cd26ce/flink-runtime/src/main/java/org/apache/flink/runtime/highavailability/nonha/embedded/EmbeddedLeaderService.java#L312]: the grantLeadershipCall is triggered afterwards informing the contender about its leadership). In the meantime, close can be called on the contender which deregisters the contender again calling revoke on the contender without having been able to gain the leadership.

This issue was introduced by FLINK-30765."	FLINK	In Progress	4	7	6847	pull-request-available, stale-assigned
13354616	YARNSessionFIFOSecuredITCase cannot connect to BlobServer	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12483&view=logs&j=f450c1a5-64b1-5955-e215-49cb1ad5ec88&t=ea63c80c-957f-50d1-8f67-3671c14686b9

{code}
java.io.IOException: Could not connect to BlobServer at address 29c91476178c/172.21.0.2:44412
java.io.IOException: Could not connect to BlobServer at address 29c91476178c/172.21.0.2:44412
	at org.apache.flink.runtime.blob.BlobClient.<init>(BlobClient.java:102) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.blob.BlobClient.downloadFromBlobServer(BlobClient.java:137) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:538)
	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)


{code}"	FLINK	Closed	3	1	6847	pull-request-available, test-stability
13318775	ZooKeeperLeaderRetrievalService does not invalidate leader in case of SUSPENDED connection	The {{ZooKeeperLeaderRetrievalService}} does not invalidate the leader if the ZooKeeper connection gets SUSPENDED. This means that a {{TaskManager}} won't cancel its running tasks even though it might miss a leader change. I think we should at least make it configurable whether in such a situation the leader listener should be informed about the lost leadership. Otherwise, we might run into the situation where an old and a newly recovered instance of a {{Task}} can run at the same time.	FLINK	Closed	3	1	6847	pull-request-available
13411256	Dropdown menu is not properly shown in UI	"FLINK-21867 introduced a new dropdown menu to browse through concurrently failed {{Executions}}. This feature is disabled due to ngzorro modules not being imported properly in {{release-1.14}}.

Additionally, the tooltip is not printed correctly.

These two issues are fixed on {{master}} already due to [903185d|https://github.com/apache/flink/commit/903185d72c97dd93c777eeb90cb81a7b1c7465e7]"	FLINK	Closed	4	1	6847	pull-request-available
13509846	TaskManagerRunnerTest fails with 239 exit code (i.e. FatalExitExceptionHandler was called) NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1	"We're again experiencing 239 exit code being caused by {{FatalExitExceptionHandler}} due class loading issues:
{code}
04:53:03,365 [flink-akka.remote.default-remote-dispatcher-8] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.remote.default-remote-dispatcher-8' produced an uncaught exception. Stopping the process...
java.lang.NoClassDefFoundError: akka/remote/transport/netty/NettyFutureBridge$$anon$1
        at akka.remote.transport.netty.NettyFutureBridge$.apply(NettyTransport.scala:65) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.remote.transport.netty.NettyTransport.$anonfun$associate$1(NettyTransport.scala:566) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:63) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:100) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:100) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:49) ~[flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:48) [flink-rpc-akka_b340b753-81f5-4e09-b083-5f8c92589fad.jar:1.16-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Caused by: java.lang.ClassNotFoundException: akka.remote.transport.netty.NettyFutureBridge$$anon$1
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382) ~[?:1.8.0_292]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_292]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentOnly(ComponentClassLoader.java:149) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:112) ~[flink-core-1.16-SNAPSHOT.jar:1.16-SNAPSHOT]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_292]
        ... 16 more
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43694&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8319

I created this as a follow-up of FLINK-26037 becasue we repurposed it and fixed a bug in FLINK-26037. But it looks like both are being caused by the same issue."	FLINK	Closed	3	7	6847	pull-request-available, test-stability
13449803	KeyedStateCheckpointingITCase.KeyedStateCheckpointingITCase ends up in infinite failover loop	"-We observed several situations already where log files reached a file size of over 120G. This caused the worker's disk usage to reach 100% resulting in the worker machine to go ""offline"", i.e. not being available to pick up new tasks.-

The initially observed excessive log spilling is due to a TaskManager failing fatally which results in the requested number of slots never becoming available and the test job ending up in an infinite failover/restart loop. See further details in the comment section."	FLINK	Closed	1	1	6847	test-stability
13342676	Rework command line interface documentation page	The command line interface documentation page is quite out-dated and not very easy to read. A large part is simply the help message from the CLI which is wall of text. Ideally, we can loosen the page a bit up and update the examples.	FLINK	Closed	3	7	6847	pull-request-available
13592119	AdaptiveScheduler#hasDesiredResources doesn't rely on all available slots which causes problems in Executing state	"FLINK-36014 aligned the triggering of the execution graph creation in {{WaitingForResources}} and rescaling in {{Executing}} state. Before that change, only {{WaitingForResources}} relied on this method. Relying on free slots was good enough because in {{WaitingForResources}} state, there are no slots allocated, yet.

Using this method for {{Executing}} state now as well changes this premise because there are slots allocated while checking the slot availability that would become available after the restart. Hence, considering these currently allocated slots as well in the slot availability check is good enough. This will not break the premise for the {{WaitingForResources}} state.

{{RescaleOnCheckpointITCase}} fails because of that issue:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=62105&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=86f654fa-ab48-5c1a-25f4-7e7f6afb9bba&l=11287

{code}
Sep 13 17:16:55 ""ForkJoinPool-1-worker-25"" #28 daemon prio=5 os_prio=0 tid=0x00007f973f0c2800 nid=0x31a1 waiting on condition [0x00007f97089fc000]
Sep 13 17:16:55    java.lang.Thread.State: TIMED_WAITING (sleeping)
Sep 13 17:16:55 	at java.lang.Thread.sleep(Native Method)
Sep 13 17:16:55 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:152)
Sep 13 17:16:55 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Sep 13 17:16:55 	at org.apache.flink.test.scheduling.UpdateJobResourceRequirementsITCase.waitForRunningTasks(UpdateJobResourceRequirementsITCase.java:219)
Sep 13 17:16:55 	at org.apache.flink.test.scheduling.RescaleOnCheckpointITCase.testRescaleOnCheckpoint(RescaleOnCheckpointITCase.java:139)
Sep 13 17:16:55 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Sep 13 17:16:55 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[...]
{code}"	FLINK	Resolved	1	7	6847	pull-request-available
13548177	Silence curls in test code	"We use {{curl}} in several locations to download artifacts. Usually, the a progress bar is printed which spams the console output of the test execution. This issue is about cleaning this up.

Parameters to consider (depending on the usecase):
 * {{\-L}}/{{\-\-location}} redirects the curl command and retries if the server reported that the artifact was moved
 * {{\-O}}/{{\-\-remote-name}} writes output to file matching the remote name (which was extracted from the URL) instead of stdout; alternative: {{\-o}}/{{\-\-output}} writes output to a file with the given name instead of stdout
* {{\-f}}/{{\-\-fail}} makes curl command fail with non-0 exit code for HTTP error codes
* {{\-s \-S}}/{{\-\-silent \-\-show-error}} doesn't print progress bar but shows error
* {{\-r}}/{{\-\-retry}} Retries certain errors

{{curl}} uses a default config file {{${user.home}/.curlrc}}. But one could make it more explicit using {{\-K}}/{{\-\-config}}"	FLINK	In Progress	3	4	6847	pull-request-available, stale-assigned, starter
13541013	Add fallback error handler to DefaultLeaderElectionService	"The FLIP-285 work separated the driver lifecycle from the contender lifecycle. Now, a contender can be removed but the driver could still be running. Error could be produced on the driver's side. The {{DefaultLeaderElectionService}} would try to forward the error to the contender. With not contender being registered, the error would be swallowed.

We should add a fallback error handler for this specific case."	FLINK	Resolved	3	7	6847	pull-request-available
13262750	Add metric for network memory	This issue refers to Step 2 in the implementation proposal of [FLIP-102|https://cwiki.apache.org/confluence/display/FLINK/FLIP-102%3A+Add+More+Metrics+to+TaskManager]	FLINK	Closed	3	7	6847	pull-request-available
13534968	CI fails in preparing the e2e test runs due openssl unavailable	"We experience build failures due to the openssl download URL causing a 404:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48653&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=d6e79740-7cf7-5407-2e69-ca34c9be0efb]

It is indeed due to the URL having changed slightly:
 * old:   [http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.deb]
 * new: [http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.12_amd64.deb]"	FLINK	Closed	2	1	6847	test-stability
13523183	ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding failed	"{{ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding}} failed:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45731&view=logs&j=b0a398c0-685b-599c-eb57-c8c2a771138e&t=747432ad-a576-5911-1e2a-68c6bedc248a&l=7436

{code}
Feb 06 02:32:27 [ERROR] org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding  Time elapsed: 29.997 s  <<< FAILURE!
Feb 06 02:32:27 java.lang.AssertionError: 
Feb 06 02:32:27 
Feb 06 02:32:27 Expected: Expected failure cause is <org.apache.flink.util.FlinkRuntimeException: testUnExpectedErrorForwarding>
Feb 06 02:32:27      but: The throwable <org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss> does not contain the expected failure cause <org.apache.flink.util.FlinkRuntimeException: testUnExpectedErrorForwarding>
Feb 06 02:32:27 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Feb 06 02:32:27 	at org.junit.Assert.assertThat(Assert.java:964)
Feb 06 02:32:27 	at org.junit.Assert.assertThat(Assert.java:930)
Feb 06 02:32:27 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testUnExpectedErrorForwarding(ZooKeeperLeaderElectionTest.java:724)

{code}"	FLINK	Closed	2	1	6847	pull-request-available, test-stability
13405915	checkpoint directory is not configurable through the Flink configuration passed into the StreamExecutionEnvironment	"FLINK-19463 introduced the separation of {{StateBackend}} and {{{}CheckpointStorage{}}}. Before that, both were included in the same interface implementation [AbstractFileStateBackend|https://github.com/apache/flink/blob/0a76daba0a428a322f0273d7dc6a70966f62bf26/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/AbstractFileStateBackend.java]. {{FsStateBackend}} was used as a default implementation pre-1.13.

pre-{{{}1.13{}}} initialized the checkpoint directory when instantiating the state backend (see [FsStateBackendFactory|https://github.com/apache/flink/blob/release-1.12/flink-runtime/src/main/java/org/apache/flink/runtime/state/filesystem/FsStateBackendFactory.java#L46]). Starting from {{1.13}} loading the {{CheckpointStorage}} is done by the {{CheckpointStorageLoader.load}} method that is called in various places:
 * Savepoint Disposal (through {{{}Checkpoints.loadCheckpointStorage{}}}) where it only relies on the configuration passed in by the cluster configuration (no application checkpoint storage is passed)
 * {{SchedulerBase}} initialization (through DefaultExecutionGraphBuilder) where it’s based on the cluster’s configuration but also the application configuration (i.e. the {{{}JobGraph{}}}’s setting) that would be considered if {{CheckpointConfig#configure}} would have the checkpoint storage included
 * {{StreamTask}} on the {{{}TaskManager{}}}’s side where it’s based on the configuration passed in by the {{JobVertex}} for the application’s {{CheckpointStorage}} and the {{{}TaskManager{}}}’s configuration (coming from the session cluster) for the fallback {{CheckpointStorage}}

The issue is that we don't set the checkpoint directory in the {{{}CheckpointConfig{}}}. Hence, it's not going to get picked up as a job-related property. Flink always uses the fallback provided by the session cluster configuration."	FLINK	Closed	3	1	6847	pull-request-available
13568155	FineGrainedSlotManagerTest fails fatally (exit code 239)	"https://github.com/apache/flink/actions/runs/7866453350/job/21460921911#step:10:8959

{code}
Error: 02:28:53 02:28:53.220 [ERROR] Process Exit Code: 239
Error: 02:28:53 02:28:53.220 [ERROR] Crashed tests:
Error: 02:28:53 02:28:53.220 [ERROR] org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest
Error: 02:28:53 02:28:53.220 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Error: 02:28:53 02:28:53.220 [ERROR] Command was /bin/sh -c cd '/root/flink/flink-runtime' && '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java' '-XX:+UseG1GC' '-Xms256m' '-XX:+IgnoreUnrecognizedVMOptions' '--add-opens=java.base/java.util=ALL-UNNAMED' '--add-opens=java.base/java.lang=ALL-UNNAMED' '--add-opens=java.base/java.net=ALL-UNNAMED' '--add-opens=java.base/java.io=ALL-UNNAMED' '--add-opens=java.base/java.util.concurrent=ALL-UNNAMED' '-Xmx768m' '-jar' '/root/flink/flink-runtime/target/surefire/surefirebooter-20240212022332296_94.jar' '/root/flink/flink-runtime/target/surefire' '2024-02-12T02-21-39_495-jvmRun3' 'surefire-20240212022332296_88tmp' 'surefire_26-20240212022332296_91tmp'
Error: 02:28:53 02:28:53.220 [ERROR] Error occurred in starting fork, check output in log
Error: 02:28:53 02:28:53.220 [ERROR] Process Exit Code: 239
Error: 02:28:53 02:28:53.220 [ERROR] Crashed tests:
Error: 02:28:53 02:28:53.221 [ERROR] org.apache.flink.runtime.resourcemanager.ResourceManagerTaskExecutorTest
Error: 02:28:53 02:28:53.221 [ERROR] 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:456)
[...]
{code}

The fatal error is triggered most likely within the {{FineGrainedSlotManagerTest}}:
{code}
02:26:39,362 [   pool-643-thread-1] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'pool-643-thread-1' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4bbc0b10 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a45cd9a[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.uniHandleStage(CompletableFuture.java:851) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.handleAsync(CompletableFuture.java:2178) ~[?:1.8.0_392]
        at org.apache.flink.runtime.resourcemanager.slotmanager.DefaultSlotStatusSyncer.allocateSlot(DefaultSlotStatusSyncer.java:138) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.allocateSlotsAccordingTo(FineGrainedSlotManager.java:722) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.checkResourceRequirements(FineGrainedSlotManager.java:645) ~[classes/:?]
        at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager.lambda$null$12(FineGrainedSlotManager.java:603) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_392]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_392]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_392]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_392]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_392]
Caused by: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4bbc0b10 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a45cd9a[Shutting down, pool size = 1, active threads = 1, queued tasks = 1, completed tasks = 194]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_392]
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:326) ~[?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:533) ~[?:1.8.0_392]
        at java.util.concurrent.ScheduledThreadPoolExecutor.execute(ScheduledThreadPoolExecutor.java:622) ~[?:1.8.0_392]
        at java.util.concurrent.Executors$DelegatedExecutorService.execute(Executors.java:668) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture$UniCompletion.claim(CompletableFuture.java:543) ~[?:1.8.0_392]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:826) ~[?:1.8.0_392]
        ... 14 more
{code}"	FLINK	In Progress	2	1	6847	pull-request-available, test-stability
13562467	Custom Action: Select workflow configuration	"During experiments, we noticed that the GHA UI isn't capable of utilizing a random count of compositions of workflows. If we get into the 3rd level of composite workflow, the job name will be cut off in the left menu which makes navigating the jobs harder (because you have duplicate of the same job, e.g. Compile, belonging to different job profiles).

As a workaround, we came up with Flink CI workflow profiles to configure the CI template yaml that is used in every job. A profile configuration can be specified through a JSON file that lives in the {{.github/workflow}} folder. "	FLINK	Closed	3	7	6847	github-actions, pull-request-available
13558344	Remove scala-2.12 system variable	We're only relying on Scala 2.12 right now. Now need to have the scala version specified. This makes the build system scripts easier.	FLINK	Resolved	3	11500	6847	pull-request-available
13376732	Running Kerberized YARN application on Docker test (default input) fails with no resources	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17558&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=8745

{code}
May 05 01:29:04 Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 120000 ms
May 05 01:29:04 	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
May 05 01:29:04 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
May 05 01:29:04 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
May 05 01:29:04 	... 4 more
{code}"	FLINK	Closed	1	1	6847	pull-request-available, test-stability
13380257	ExceptionUtilsITCase.testIsMetaspaceOutOfMemoryError is unstable	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18292&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=a99e99c7-21cd-5a1f-7274-585e62b72f56

{code}
May 25 00:56:38 java.lang.AssertionError: 
May 25 00:56:38 
May 25 00:56:38 Expected: is """"
May 25 00:56:38      but: was ""The system is out of resources.\nConsult the following stack trace for details.""
May 25 00:56:38 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
May 25 00:56:38 	at org.junit.Assert.assertThat(Assert.java:956)
May 25 00:56:38 	at org.junit.Assert.assertThat(Assert.java:923)
May 25 00:56:38 	at org.apache.flink.runtime.util.ExceptionUtilsITCase.run(ExceptionUtilsITCase.java:94)
May 25 00:56:38 	at org.apache.flink.runtime.util.ExceptionUtilsITCase.testIsMetaspaceOutOfMemoryError(ExceptionUtilsITCase.java:70)
May 25 00:56:38 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
May 25 00:56:38 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
May 25 00:56:38 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
May 25 00:56:38 	at java.lang.reflect.Method.invoke(Method.java:498)
May 25 00:56:38 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
May 25 00:56:38 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
May 25 00:56:38 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
May 25 00:56:38 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
May 25 00:56:38 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
May 25 00:56:38 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
May 25 00:56:38 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
May 25 00:56:38 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
May 25 00:56:38 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
May 25 00:56:38 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
May 25 00:56:38 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
May 25 00:56:38 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
May 25 00:56:38 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
May 25 00:56:38 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
May 25 00:56:38 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
May 25 00:56:38 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
May 25 00:56:38 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
May 25 00:56:38 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
May 25 00:56:38 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
May 25 00:56:38 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
May 25 00:56:38 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
May 25 00:56:38 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
May 25 00:56:38 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
May 25 00:56:38 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
May 25 00:56:38 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
May 25 00:56:38 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
May 25 00:56:38 

{code}"	FLINK	Resolved	2	1	6847	pull-request-available, stale-assigned, test-stability
13529795	Update reference data for Migration Tests	"# Update {{CURRENT_VERSION in TypeSerializerUpgradeTestBase}}  with the new version. This will likely fail some tests because snapshots are missing for that version. Generate them, for example in {{TypeSerializerUpgradeTestBase.}} 
 # (major/minor only) Update migration tests in master to cover migration from new version: (search for usages of FlinkV{{{}ersion{}}})
 ** AbstractOperatorRestoreTestBase
 ** CEPMigrationTest
 ** BucketingSinkMigrationTest
 ** FlinkKafkaConsumerBaseMigrationTest
 ** ContinuousFileProcessingMigrationTest
 ** WindowOperatorMigrationTest
 ** StatefulJobSavepointMigrationITCase
 ** StatefulJobWBroadcastStateMigrationITCase"	FLINK	Resolved	3	7	6847	pull-request-available
13540598	Replaces error handling functionality with onError method in MultipleComponentLeaderElectionDriver.Listener interface	The communication between the {{DefaultLeaderElectionService}} and the driver can be improved in a way that errors are forwarded by the driver through the {{Listener}} interface rather than holding the service's error handler.	FLINK	Resolved	3	7	6847	pull-request-available
13554410	"The release profile of Flink does include enforcing the Java version only in a ""soft"" way"	"The vote on 1.18.0-rc2 [revealed|https://lists.apache.org/thread/fbdl2w6wjmwk55y94ml91bpnhmh4rnm0] that we don't do proper JDK version enforcement in {{release}} profile of the Maven build.

This is added to ensure that Java 8 is used when releasing the artifacts. The current configuration sets the version as a ""soft"" requirement (see [documentation on version|https://cwiki.apache.org/confluence/display/MAVENOLD/Dependency+Mediation+and+Conflict+Resolution#DependencyMediationandConflictResolution-DependencyVersionRanges] which is referenced in the [Maven Enforcer Plugin website|https://maven.apache.org/enforcer/maven-enforcer-plugin/]).

We should use square brackets to enforce the JDK version. The same can be applied to the Maven version."	FLINK	Resolved	3	1	6847	pull-request-available
13433283	ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification failed on azure	"
{code:java}
Mar 10 09:16:30 [ERROR] org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification  Time elapsed: 20.752 s  <<< ERROR!
Mar 10 09:16:30 java.lang.NullPointerException
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.lambda$null$9(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:292)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:161)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Mar 10 09:16:30 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:137)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.lambda$testNewLeaderAfterReconnectTriggersListenerNotification$10(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:288)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testWithQueueLeaderElectionListener(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:313)
Mar 10 09:16:30 	at org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalConnectionHandlingTest.testNewLeaderAfterReconnectTriggersListenerNotification(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:250)
Mar 10 09:16:30 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 10 09:16:30 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 10 09:16:30 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 10 09:16:30 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 10 09:16:30 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
Mar 10 09:16:30 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
Mar 10 09:16:30 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
Mar 10 09:16:30 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Mar 10 09:16:30 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32815&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7544"	FLINK	Resolved	3	1	6847	pull-request-available, test-stability
13539651	ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement and DefaultLeaderElectionService.onGrantLeadership fell into dead lock	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49750&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8]

 

there are 2 threads one locked {{0x00000000e3a8a1e8}} and waiting for {{0x00000000e3a89c18}}

{noformat}

2023-06-08T01:18:54.5609123Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25-EventThread"" #956 daemon prio=5 os_prio=0 tid=0x00007f9374253800 nid=0x6a4e waiting for monitor entry [0x00007f94b63e1000]
2023-06-08T01:18:54.5609820Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5610557Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.runInLeaderEventThread(DefaultLeaderElectionService.java:425)
2023-06-08T01:18:54.5611459Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5612198Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onGrantLeadership(DefaultLeaderElectionService.java:300)
2023-06-08T01:18:54.5613110Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:153)
2023-06-08T01:18:54.5614070Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$$Lambda$1649/586959400.accept(Unknown Source)
2023-06-08T01:18:54.5615014Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)
2023-06-08T01:18:54.5616259Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1640/1393625763.run(Unknown Source)
2023-06-08T01:18:54.5617137Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1633/2012730699.execute(Unknown Source)
2023-06-08T01:18:54.5618047Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)
2023-06-08T01:18:54.5618994Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2023-06-08T01:18:54.5620071Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:711)
2023-06-08T01:18:54.5621198Z Jun 08 01:18:54 	- locked <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5622072Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:597)
2023-06-08T01:18:54.5622991Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.access$600(LeaderLatch.java:64)
2023-06-08T01:18:54.5623988Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:648)
2023-06-08T01:18:54.5624965Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926)
2023-06-08T01:18:54.5626218Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)
2023-06-08T01:18:54.5627369Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
2023-06-08T01:18:54.5628353Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187)
2023-06-08T01:18:54.5629281Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:666)
2023-06-08T01:18:54.5630124Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
{noformat}
and another locked {{0x00000000e3a89c18}} and waits for {{0x00000000e3a8a1e8}}
{noformat}
2023-06-08T01:18:54.5738286Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25"" #620 daemon prio=5 os_prio=0 tid=0x00007f953874f000 nid=0x682e waiting for monitor entry [0x00007f95461d4000]
2023-06-08T01:18:54.5738959Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5739645Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:203)
2023-06-08T01:18:54.5740731Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5741591Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:190)
2023-06-08T01:18:54.5742609Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.close(ZooKeeperLeaderElectionDriver.java:135)
2023-06-08T01:18:54.5743491Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.close(DefaultLeaderElectionService.java:217)
2023-06-08T01:18:54.5744427Z Jun 08 01:18:54 	- locked <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5745200Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement(ZooKeeperLeaderElectionTest.java:346)
2023-06-08T01:18:54.5746206Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-06-08T01:18:54.5746829Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-06-08T01:18:54.5747552Z Jun 08 01:18:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-06-08T01:18:54.5748207Z Jun 08 01:18:54 	at java.lang.reflect.Method.invoke(Method.java:498)
...
{noformat}"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13541392	Merge AbstractZooKeeperHaServices and ZooKeeperMultipleComponentLeaderElectionHaServices	{{AbstractZooKeeperHaServices}} isn't needed anymore with the legacy ZK leader election being gone.	FLINK	Resolved	3	7	6847	pull-request-available
13545478	ProcessFailureCancelingITCase.testCancelingOnProcessFailure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51798&view=logs&j=a596f69e-60d2-5a4b-7d39-dc69e4cdaed3&t=712ade8c-ca16-5b76-3acd-14df33bc1cb1&l=7610

{code}
Jul 29 01:03:36 01:03:36.670 [ERROR] org.apache.flink.test.recovery.ProcessFailureCancelingITCase.testCancelingOnProcessFailure  Time elapsed: 9.138 s  <<< ERROR!
Jul 29 01:03:36 java.lang.IllegalStateException: The DefaultLeaderElectionService should have been stopped before closing the instance.
Jul 29 01:03:36 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jul 29 01:03:36 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.close(DefaultLeaderElectionService.java:291)
Jul 29 01:03:36 	at org.apache.flink.runtime.highavailability.AbstractHaServices.closeAndCleanupAllData(AbstractHaServices.java:201)
Jul 29 01:03:36 	at org.apache.flink.test.recovery.ProcessFailureCancelingITCase.testCancelingOnProcessFailure(ProcessFailureCancelingITCase.java:249)
Jul 29 01:03:36 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13544182	Run Kubernetes test is unstable on AZP	"This test 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51447&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=43ba8ce7-ebbf-57cd-9163-444305d74117&l=6213

fails with

{noformat}
2023-07-19T17:14:49.8144730Z Jul 19 17:14:49 deployment.apps/flink-task-manager created
2023-07-19T17:15:03.7983703Z Jul 19 17:15:03 job.batch/flink-job-cluster condition met
2023-07-19T17:15:04.0937620Z error: Internal error occurred: error executing command in container: http: invalid Host header
2023-07-19T17:15:04.0988752Z sort: cannot read: '/home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-11919909188/out/kubernetes_wc_out*': No such file or directory
2023-07-19T17:15:04.1017388Z Jul 19 17:15:04 FAIL WordCount: Output hash mismatch.  Got d41d8cd98f00b204e9800998ecf8427e, expected e682ec6622b5e83f2eb614617d5ab2cf.
{noformat}"	FLINK	Resolved	1	1	6847	pull-request-available, test-stability
13364104	JobMasterTest.testReconnectionAfterDisconnect hangs on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14500&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7030a106-e977-5851-a05e-535de648c9c9&l=8884

{code}

{code}"	FLINK	Closed	3	1	6847	pull-request-available, test-stability
13529598	Publish the Dockerfiles for the new release	"Note: the official Dockerfiles fetch the binary distribution of the target Flink version from an Apache mirror. After publishing the binary release artifacts, mirrors can take some hours to start serving the new artifacts, so you may want to wait to do this step until you are ready to continue with the ""Promote the release"" steps in the follow-up Jira.

Follow the [release instructions in the flink-docker repo|https://github.com/apache/flink-docker#release-workflow] to build the new Dockerfiles and send an updated manifest to Docker Hub so the new images are built and published.

 
----
h3. Expectations
 * Dockerfiles in [flink-docker|https://github.com/apache/flink-docker] updated for the new Flink release and pull request opened on the Docker official-images with an updated manifest"	FLINK	Closed	3	7	6847	pull-request-available
13428247	"Rolling log filenames cause end-to-end test to fail (example test failure ""Running HA (hashmap, async)"")"	"{code:java}
Feb 14 01:31:29 Killed TM @ 255483
Feb 14 01:31:29 Starting new TM.
Feb 14 01:31:42 Killed TM @ 258722
Feb 14 01:31:42 Starting new TM.
Feb 14 01:32:00 Checking for non-empty .out files...
Feb 14 01:32:00 No non-empty .out files.
Feb 14 01:32:00 FAILURE: A JM did not take over.
Feb 14 01:32:00 One or more tests FAILED.
Feb 14 01:32:00 Stopping job timeout watchdog (with pid=250820)
Feb 14 01:32:00 Killing JM watchdog @ 252644
Feb 14 01:32:00 Killing TM watchdog @ 253262
Feb 14 01:32:00 [FAIL] Test script contains errors.
Feb 14 01:32:00 Checking of logs skipped.
Feb 14 01:32:00 
Feb 14 01:32:00 [FAIL] 'Running HA (hashmap, async) end-to-end test' failed after 2 minutes and 51 seconds! Test exited with exit code 1
Feb 14 01:32:00 
01:32:00 ##[group]Environment Information
Feb 14 01:32:01 Searching for .dump, .dumpstream and related files in '/home/vsts/work/1/s'
dmesg: read kernel buffer failed: Operation not permitted
Feb 14 01:32:06 Stopping taskexecutor daemon (pid: 259377) on host fv-az313-602.
Feb 14 01:32:07 Stopping standalonesession daemon (pid: 256528) on host fv-az313-602.
Feb 14 01:32:08 Stopping zookeeper...
Feb 14 01:32:08 Stopping zookeeper daemon (pid: 251023) on host fv-az313-602.
Feb 14 01:32:09 Skipping taskexecutor daemon (pid: 251636), because it is not running anymore on fv-az313-602.
Feb 14 01:32:09 Skipping taskexecutor daemon (pid: 255483), because it is not running anymore on fv-az313-602.
Feb 14 01:32:09 Skipping taskexecutor daemon (pid: 258722), because it is not running anymore on fv-az313-602.
The STDIO streams did not close within 10 seconds of the exit event from process '/usr/bin/bash'. This may indicate a child process inherited the STDIO streams and has not yet exited.
##[error]Bash exited with code '1'.
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31347&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=8020"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13357985	Missing snapshot version compatibility for 1.12	While browsing the documentation I realized that there's no savepoint compatibility entry in {{docs/ops/upgrading.md}} for Flink 1.12.	FLINK	Closed	2	4	6847	pull-request-available
13544715	JobMasterTest.testRetrievingCheckpointStats fails with NPE on AZP	"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=51452&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8654
fails with NPE as
{noformat}
Jul 20 01:01:33 01:01:33.491 [ERROR] org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats  Time elapsed: 0.036 s  <<< ERROR!
Jul 20 01:01:33 java.lang.NullPointerException
Jul 20 01:01:33 	at org.apache.flink.runtime.jobmaster.JobMasterTest.testRetrievingCheckpointStats(JobMasterTest.java:2132)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 20 01:01:33 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 20 01:01:33 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 20 01:01:33 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 20 01:01:33 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Jul 20 01:01:33 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Jul 20 01:01:33 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
...
{noformat}"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13345827	Remove Scheduler 	After FLINK-17760 and FLINK-20595 we can remove the {{Scheduler}} interface and its implementations {{ProgrammedSlotProvider}} and {{SchedulerImpl}} since they are no longer needed.	FLINK	Closed	3	7	6847	pull-request-available
13541360	Remove obsolete LeaderContender.getDescription method	{{LeaderContender.getDescription}} was barely used (only for the log output in the ZK driver implementation). With the {{contenderID}} becoming a more fundamental property of the {{DefaultLeaderElectionService}} we can get rid of the {{getDescription}} method.	FLINK	Resolved	3	7	6847	pull-request-available
13534922	DirectExecutorService doesn't follow the ExecutorService contract throwing a RejectedExecutionException in case it's already shut down	We experienced an issue where we tested behavior using the {{DirectExecutorService}} with the {{ExecutorService}} being shutdown already. The tests succeeded. The production code failed, though, because we used a singleThreadExecutor for which the task execution failed after the executor was stopped. We should add this behavior to the {{DirectExecutorService}} as well to make the unit tests be closer to the production code.	FLINK	Resolved	3	1	6847	pull-request-available
13426572	Reorganizes tests around Dispatcher cleanup	FLINK-25432 introduced new interfaces for the local and global cleanup of job-related data. This enables us reorganize tests (more specifically {{DispatcherCleanupResourcesTest}}).	FLINK	Resolved	3	7	6847	pull-request-available
13300487	freeSlot in TaskExecutor.closeJobManagerConnection cause ConcurrentModificationException	"TaskExecutor may freeSlot when closeJobManagerConnection. freeSlot will modify the TaskSlotTable.slotsPerJob. this modify will cause ConcurrentModificationException.
{code:java}
Iterator<AllocationID> activeSlots = taskSlotTable.getActiveSlots(jobId);

final FlinkException freeingCause = new FlinkException(""Slot could not be marked inactive."");

while (activeSlots.hasNext()) {
 AllocationID activeSlot = activeSlots.next();

 try {
 if (!taskSlotTable.markSlotInactive(activeSlot, taskManagerConfiguration.getTimeout())) {
 freeSlotInternal(activeSlot, freeingCause);
 }
 } catch (SlotNotFoundException e) {
 log.debug(""Could not mark the slot {} inactive."", jobId, e);
 }
}
{code}
 error log：
{code:java}
2020-04-21 23:37:11,363 ERROR org.apache.flink.runtime.rpc.akka.AkkaRpcActor                - Caught exception while executing runnable in main thread.
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437)
    at java.util.HashMap$KeyIterator.next(HashMap.java:1461)
    at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable$TaskSlotIterator.hasNext(TaskSlotTable.java:698)
    at org.apache.flink.runtime.taskexecutor.slot.TaskSlotTable$AllocationIDIterator.hasNext(TaskSlotTable.java:652)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.closeJobManagerConnection(TaskExecutor.java:1314)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$1300(TaskExecutor.java:149)
    at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobLeaderListenerImpl.lambda$jobManagerLostLeadership$1(TaskExecutor.java:1726)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
    at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
    at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
    at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
{code}"	FLINK	Closed	3	1	6847	pull-request-available
13419214	Introduce common interfaces for cleaning up local and global job data	"We want to combine the job-specific cleanup of the different resources and provide a common {{ResourceCleaner}} taking care of the actual cleanup of all resources.

This needs to be integrated into the {{Dispatcher}}."	FLINK	Resolved	3	7	6847	pull-request-available
13562463	Makes copying test jars being done later	"We experienced an issue in GHA which is due to the fact how test resources are pre-computed in GHA:
{code:java}
This fixes the following error when compiling flink-clients:
Error: 2.054 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.2.0:copy-dependencies (copy-dependencies) on project flink-clients: Artifact has not been packaged yet. When used on reactor artifact, copy should be executed after packaging: see MDEP-187. -> [Help 1] {code}
We need to move this goal to a later phase.

The reason why this popped up is (as far as I remember) that we do only do test-compile in GitHub Actions."	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
13553700	misc module: YARN tests are flaky	"https://github.com/XComp/flink/actions/runs/6473584177/job/17581942919

{code}
2023-10-10T23:16:09.3548634Z Oct 10 23:16:09 23:16:09.354 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.664 s <<< FAILURE! - in org.apache.flink.yarn.YarnPrioritySchedulingITCase
2023-10-10T23:16:09.3564980Z Oct 10 23:16:09 23:16:09.354 [ERROR] org.apache.flink.yarn.YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority  Time elapsed: 1.226 s  <<< ERROR!
2023-10-10T23:16:09.3565608Z Oct 10 23:16:09 java.lang.RuntimeException: Runner failed with exception.
2023-10-10T23:16:09.3566290Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase.startWithArgs(YarnTestBase.java:949)
2023-10-10T23:16:09.3566954Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnPrioritySchedulingITCase.lambda$yarnApplication_submissionWithPriority_shouldRespectPriority$0(YarnPrioritySchedulingITCase.java:45)
2023-10-10T23:16:09.3567646Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:09.3568447Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority(YarnPrioritySchedulingITCase.java:41)
2023-10-10T23:16:09.3569187Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:09.3569805Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:09.3570485Z Oct 10 23:16:09 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:09.3571052Z Oct 10 23:16:09 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:09.3571527Z Oct 10 23:16:09 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:09.3572075Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:09.3572716Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:09.3573350Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:09.3573954Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:09.3574665Z Oct 10 23:16:09 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:09.3575378Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:09.3576139Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:09.3576852Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:09.3577539Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:09.3578225Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:09.3578898Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:09.3579568Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:09.3580243Z Oct 10 23:16:09 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:09.3580917Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:09.3581584Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3582276Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:09.3582952Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:09.3583651Z Oct 10 23:16:09 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:09.3584282Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:09.3584914Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3585589Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3586368Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3587030Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3587749Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3588550Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3589237Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3590109Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3591187Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:09.3592081Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:09.3592840Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3593551Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3594182Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3594898Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3595600Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3596324Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3596994Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3597858Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3598950Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:09.3599832Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:09.3600565Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3601268Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:09.3601893Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:09.3602519Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:09.3603281Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:09.3604234Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:09.3604926Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:09.3605850Z Oct 10 23:16:09 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:09.3606648Z Oct 10 23:16:09 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:09.3607160Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:09.3607702Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:09.3608332Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:09.3608855Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:09.3609416Z Oct 10 23:16:09 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:09.3610332Z Oct 10 23:16:09 Caused by: org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn session cluster
2023-10-10T23:16:09.3611024Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:479)
2023-10-10T23:16:09.3611663Z Oct 10 23:16:09 	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:604)
2023-10-10T23:16:09.3612282Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnTestBase$Runner.run(YarnTestBase.java:1154)
2023-10-10T23:16:09.3613022Z Oct 10 23:16:09 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:09.3613849Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:09.3614501Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:09.3615153Z Oct 10 23:16:09 	at org.apache.flink.yarn.YarnClusterDescriptor.deploySessionCluster(YarnClusterDescriptor.java:472)
2023-10-10T23:16:09.3615632Z Oct 10 23:16:09 	... 2 more
2023-10-10T23:16:09.3615862Z Oct 10 23:16:09 
2023-10-10T23:16:10.1130633Z Oct 10 23:16:10 23:16:10.112 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-10-10T23:16:11.1935140Z Oct 10 23:16:11 23:16:11.192 [INFO] Running org.apache.flink.yarn.YARNApplicationITCase
2023-10-10T23:16:17.2611617Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2612300Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2612901Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2613468Z INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class
2023-10-10T23:16:17.2614031Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2614540Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2615047Z Oct 10, 2023 11:16:13 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2615720Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2616208Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2617123Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2617787Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2618489Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2619296Z Oct 10, 2023 11:16:13 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2619952Z INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2620582Z Oct 10, 2023 11:16:14 PM com.google.inject.servlet.GuiceFilter setPipeline
2023-10-10T23:16:17.2621470Z WARNING: Multiple Servlet injectors detected. This is a warning indicating that you have more than one GuiceFilter running in your web application. If this is deliberate, you may safely ignore this message. If this is NOT deliberate however, your application may not work as expected.
2023-10-10T23:16:17.2622217Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2622759Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
2023-10-10T23:16:17.2623380Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2623919Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2624481Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2625294Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2625850Z Oct 10, 2023 11:16:14 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2626484Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2627039Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2627774Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2628487Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2629143Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2629800Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2630486Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2631081Z Oct 10, 2023 11:16:14 PM com.google.inject.servlet.GuiceFilter setPipeline
2023-10-10T23:16:17.2631822Z WARNING: Multiple Servlet injectors detected. This is a warning indicating that you have more than one GuiceFilter running in your web application. If this is deliberate, you may safely ignore this message. If this is NOT deliberate however, your application may not work as expected.
2023-10-10T23:16:17.2632613Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2633238Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class
2023-10-10T23:16:17.2633853Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2634431Z INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class
2023-10-10T23:16:17.2638068Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register
2023-10-10T23:16:17.2638660Z INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class
2023-10-10T23:16:17.2639242Z Oct 10, 2023 11:16:14 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate
2023-10-10T23:16:17.2639820Z INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'
2023-10-10T23:16:17.2640381Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2641137Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2641897Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2642619Z INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2643219Z Oct 10, 2023 11:16:14 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider
2023-10-10T23:16:17.2644145Z INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope ""Singleton""
2023-10-10T23:16:17.2646774Z Oct 10 23:16:17 Formatting using clusterid: testClusterID
2023-10-10T23:16:18.7266553Z Oct 10 23:16:18 23:16:18.726 [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 7.525 s <<< FAILURE! - in org.apache.flink.yarn.YARNApplicationITCase
2023-10-10T23:16:18.7276855Z Oct 10 23:16:18 23:16:18.726 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion  Time elapsed: 0.242 s  <<< ERROR!
2023-10-10T23:16:18.7278287Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7279023Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7279745Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7280549Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion$1(YARNApplicationITCase.java:72)
2023-10-10T23:16:18.7281269Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7282068Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion(YARNApplicationITCase.java:70)
2023-10-10T23:16:18.7282892Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7283646Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7284329Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7284895Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7285448Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7286066Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7286781Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7287482Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7288165Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7288983Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7289779Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7290658Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7291445Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7292193Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7292954Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7293781Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7294515Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7295309Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7296068Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7296820Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7298389Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7299074Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7299710Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7300336Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7300981Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7301613Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7302172Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7302727Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7303360Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7304014Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7304630Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7305425Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7306495Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7307582Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7308509Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7309149Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7309816Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7310378Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7310936Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7311567Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7312301Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7312921Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7313708Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7314698Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7315498Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7316174Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7316814Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7317364Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7317914Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7318538Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7319185Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7319804Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7320583Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7321309Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7321766Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7322238Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7335962Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7336477Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7336970Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7337680Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7338554Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7339149Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7339757Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7340170Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7340437Z Oct 10 23:16:18 
2023-10-10T23:16:18.7344548Z Oct 10 23:16:18 23:16:18.732 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithRemoteUserJar  Time elapsed: 0.554 s  <<< ERROR!
2023-10-10T23:16:18.7345418Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7346043Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7346765Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7347401Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithRemoteUserJar$2(YARNApplicationITCase.java:86)
2023-10-10T23:16:18.7347951Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7348549Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithRemoteUserJar(YARNApplicationITCase.java:84)
2023-10-10T23:16:18.7349174Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7349713Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7350446Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7350937Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7351416Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7351965Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7352606Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7353236Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7353840Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7354480Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7355203Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7355951Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7356667Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7357354Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7358040Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7358719Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7359434Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7360111Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7360797Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7361504Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7362189Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7362863Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7363762Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7364405Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7365043Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7365673Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7366233Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7366788Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7367690Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7368345Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7368964Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7369752Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7370829Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7371914Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7372743Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7373385Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7374007Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7374561Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7375120Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7375746Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7376395Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7377016Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7377868Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7378859Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7379707Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7380337Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7380964Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7381523Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7382136Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7382756Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7383402Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7384019Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7384791Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7385545Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7386000Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7386477Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7386946Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7399865Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7400385Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7401100Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7401860Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7402463Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7403060Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7404050Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7404263Z Oct 10 23:16:18 
2023-10-10T23:16:18.7404867Z Oct 10 23:16:18 23:16:18.737 [ERROR] org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion  Time elapsed: 0.006 s  <<< ERROR!
2023-10-10T23:16:18.7405830Z Oct 10 23:16:18 org.apache.flink.client.deployment.ClusterDeploymentException: Couldn't deploy Yarn Application Cluster
2023-10-10T23:16:18.7406446Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:523)
2023-10-10T23:16:18.7407060Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.deployApplication(YARNApplicationITCase.java:109)
2023-10-10T23:16:18.7407923Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.lambda$testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion$0(YARNApplicationITCase.java:62)
2023-10-10T23:16:18.7408561Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnTestBase.runTest(YarnTestBase.java:303)
2023-10-10T23:16:18.7409274Z Oct 10 23:16:18 	at org.apache.flink.yarn.YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion(YARNApplicationITCase.java:60)
2023-10-10T23:16:18.7410056Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-10-10T23:16:18.7410601Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-10-10T23:16:18.7411193Z Oct 10 23:16:18 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-10-10T23:16:18.7411685Z Oct 10 23:16:18 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
2023-10-10T23:16:18.7412290Z Oct 10 23:16:18 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
2023-10-10T23:16:18.7412842Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
2023-10-10T23:16:18.7413472Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
2023-10-10T23:16:18.7414100Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
2023-10-10T23:16:18.7414702Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
2023-10-10T23:16:18.7415340Z Oct 10 23:16:18 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
2023-10-10T23:16:18.7416112Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
2023-10-10T23:16:18.7416861Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
2023-10-10T23:16:18.7417556Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
2023-10-10T23:16:18.7418232Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
2023-10-10T23:16:18.7418920Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
2023-10-10T23:16:18.7419588Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
2023-10-10T23:16:18.7420253Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
2023-10-10T23:16:18.7420937Z Oct 10 23:16:18 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
2023-10-10T23:16:18.7421621Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
2023-10-10T23:16:18.7422293Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7422964Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
2023-10-10T23:16:18.7423638Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
2023-10-10T23:16:18.7424278Z Oct 10 23:16:18 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
2023-10-10T23:16:18.7424956Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
2023-10-10T23:16:18.7425602Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7426235Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7426860Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7427420Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7428035Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7428687Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7429351Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7430139Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7431226Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
2023-10-10T23:16:18.7432309Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
2023-10-10T23:16:18.7433117Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7433795Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7434496Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7435049Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7435604Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7436234Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7436874Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7437499Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7438284Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7439272Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
2023-10-10T23:16:18.7440071Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
2023-10-10T23:16:18.7440681Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7441310Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
2023-10-10T23:16:18.7441865Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
2023-10-10T23:16:18.7442412Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
2023-10-10T23:16:18.7443068Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
2023-10-10T23:16:18.7448970Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
2023-10-10T23:16:18.7449605Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
2023-10-10T23:16:18.7450536Z Oct 10 23:16:18 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
2023-10-10T23:16:18.7451260Z Oct 10 23:16:18 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
2023-10-10T23:16:18.7451723Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
2023-10-10T23:16:18.7452269Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
2023-10-10T23:16:18.7452735Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
2023-10-10T23:16:18.7453187Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
2023-10-10T23:16:18.7453670Z Oct 10 23:16:18 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2023-10-10T23:16:18.7454381Z Oct 10 23:16:18 Caused by: org.apache.flink.configuration.IllegalConfigurationException: The number of requested virtual cores for application master 1 exceeds the maximum number of virtual cores 0 available in the Yarn Cluster.
2023-10-10T23:16:18.7455128Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.isReadyForDeployment(YarnClusterDescriptor.java:380)
2023-10-10T23:16:18.7455775Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployInternal(YarnClusterDescriptor.java:609)
2023-10-10T23:16:18.7456391Z Oct 10 23:16:18 	at org.apache.flink.yarn.YarnClusterDescriptor.deployApplicationCluster(YarnClusterDescriptor.java:516)
2023-10-10T23:16:18.7456805Z Oct 10 23:16:18 	... 63 more
2023-10-10T23:16:18.7457010Z Oct 10 23:16:18 
2023-10-10T23:16:19.0886760Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0887052Z Oct 10 23:16:19 23:16:19.087 [INFO] Results:
2023-10-10T23:16:19.0887292Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0887538Z Oct 10 23:16:19 23:16:19.087 [ERROR] Errors: 
2023-10-10T23:16:19.0890525Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion:70->YarnTestBase.runTest:303->lambda$testApplicationClusterWithLocalUserJarAndDisableUserJarInclusion$1:72->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0892075Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion:60->YarnTestBase.runTest:303->lambda$testApplicationClusterWithLocalUserJarAndFirstUserJarInclusion$0:62->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0893304Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNApplicationITCase.testApplicationClusterWithRemoteUserJar:84->YarnTestBase.runTest:303->lambda$testApplicationClusterWithRemoteUserJar$2:86->deployApplication:109 » ClusterDeployment
2023-10-10T23:16:19.0894114Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YARNITCase.testPerJobWithProvidedLibDirs » Remote File /flink-provided-lib/fli...
2023-10-10T23:16:19.0895062Z Oct 10 23:16:19 23:16:19.087 [ERROR]   YarnPrioritySchedulingITCase.yarnApplication_submissionWithPriority_shouldRespectPriority:41->YarnTestBase.runTest:303->lambda$yarnApplication_submissionWithPriority_shouldRespectPriority$0:45->YarnTestBase.startWithArgs:949 » Runtime
2023-10-10T23:16:19.0895643Z Oct 10 23:16:19 23:16:19.087 [INFO] 
2023-10-10T23:16:19.0895944Z Oct 10 23:16:19 23:16:19.087 [ERROR] Tests run: 27, Failures: 0, Errors: 5, Skipped: 0
{code}"	FLINK	Closed	3	7	6847	github-actions, test-stability
13520299	Merge DispatcherRunnerLeaderElectionLifecycleManager into DefaultDispatcherRunner	"{{DispatcherRunnerLeaderElectionLifeCycleManager}} handles the lifecycle of the {{{}DefaultDispatcherRunner{}}}'s leader election. The leader election itself is then called within the {{{}DefaultDispatcherRunner{}}}.

Merging both would make the code less complex and prepare the proper refactoring of the LeaderElectionService interface(s)."	FLINK	Resolved	3	7	6847	pull-request-available
13353254	Broken job restart for job with disjoint graph	"Building on top of bugs:

https://issues.apache.org/jira/browse/FLINK-21028

 and https://issues.apache.org/jira/browse/FLINK-21029 : 

I tried to stop a Flink application on YARN via savepoint which didn't succeed due to a possible bug/racecondition in shutdown (Bug 21028). Due to some reason, Flink attempted to restart the pipeline after the failure in shutdown (21029). The bug here:

As I mentioned: My jobgraph is disjoint and the pipelines are fully isolated. Lets say the original error occured in a single task of pipeline1. Flink then restarted the entire pipeline1, but pipeline2 was shutdown successfully and switched the state to FINISHED.

My job thus was in kind of an invalid state after the attempt to stopping: One of two pipelines was running, the other was FINISHED. I guess this is kind of a bug in the restarting behavior that only all connected components of a graph are restarted, but the others aren't..."	FLINK	Closed	1	1	6847	pull-request-available
13559188	Upgrading Flink's default ZooKeeper version to 3.8.3	"Flink currently supports ZooKeeper 3.7.1 as its default version which was added in Flink 1.17 (FLINK-29420). The ZooKeeper project released 3.8 and 3.9 in the meantime. 3.7 will reach its EOL with a new minor release. ZooKeeper 3.8 is already provided by \{{flink-shaded}} and, therefore, allows most likely an easy switch.

This issue is about chaning to ZooKeeper 3.8.3. A final discussion should be initiated in the mailing list before merging the change."	FLINK	In Progress	3	4	6847	pull-request-available
13557890	Move Java 17-specific code into its own module to please Intellij	"FLINK-32380 introduced support for Java records (which are introduced in Java 14). Test classes were added and included via Maven profiles. This works on the command line. Intellij runs into issues though when build {{flink-core}}.

[~snuyanzin] filed [IDEA-338060|https://youtrack.jetbrains.com/issue/IDEA-338060/How-to-make-IntellijIdea-aware-of-maven-compiler-plugin-testExcludes] to cover this issue.

A workaround on the Flink side is to have a dedicated module for Java 17 code (more specifically the test classes which use Java record). This would make {{flink-core}} compilable again. The newly introduced module can be enabled for the {{java17}} profile."	FLINK	Resolved	3	4	6847	pull-request-available
13562469	Template: Add CI template for running Flink's test suite	We want to have a template that runs the entire Flink test suite.	FLINK	Closed	3	7	6847	github-actions
13540393	KubernetesTestFixture doesn't implement the checkAndUpdateConfigMapFunction properly	"[FlinkKubeClient.checkAndUpdateConfigMap|https://github.com/apache/flink/blob/ab3eb40d920fa609f49164a0bbb5fcbb3004a808/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/FlinkKubeClient.java#L163] expects an error to be forwarded through the {{CompletableFuture}} instead of throwing a {{RuntimeException}}.

The actual implementation implements it accordingly in [Fabric8FlinkKubeClient:313|https://github.com/apache/flink/blob/025a95b627faf8ec8b725a7784d1279b41e10ba7/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/Fabric8FlinkKubeClient.java#L313] where a {{CompletionException}} is thrown within the {{CompletableFuture}}'s {{supplyAsync}} call resulting the future to fail.

{{KubernetesTestFixture}} doesn't make the returned future complete exceptionally but throws a {{CompletionException}} (see [KubernetesTestFixture:172|https://github.com/apache/flink/blob/6fc5f789869433688eb5f62494f1a4404e0dd11b/flink-kubernetes/src/test/java/org/apache/flink/kubernetes/highavailability/KubernetesTestFixture.java#L172]).

This results in inconsistent test behavior."	FLINK	Resolved	3	1	6847	pull-request-available
13569823	Adaptive Scheduler restores from empty state if JM fails during restarting state	"If a JobManager failover occurs while the Job is in a Restarting state, the HA metadata is deleted (as if it was a globally terminal state) and the job restarts from an empty state after the JM comes back up:

Jobmanager killed after killing Taskmanager (restarting phase):
{noformat}
2024-02-26 10:10:12,147 DEBUG org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Ignore TaskManager pod that is already added: autoscaling-example-taskmanager-3-2
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Trigger heartbeat request.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.jobmaster.JobMaster                 [] - Received heartbeat request from 9b7e17b75812ab60ecf028e02368d0c2.
2024-02-26 10:10:13,799 DEBUG org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Received heartbeat from 251c25cf794e3c9396fc02306613507b.
2024-02-26 10:10:14,091 DEBUG org.apache.pekko.remote.transport.netty.NettyTransport       [] - Remote connection to [/10.244.0.120:55647] was disconnected because of [id: 0x4a61a791, /10.244.0.120:55647 :> /10.244.0.118:6123] DISCONNECTED
2024-02-26 10:10:14,091 DEBUG org.apache.pekko.remote.transport.ProtocolStateActor         [] - Association between local [tcp://flink@10.244.0.118:6123] and remote [tcp://flink@10.244.0.120:55647] was disassociated because the ProtocolStateActor failed: Unknown
2024-02-26 10:10:14,092 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - RECEIVED SIGNAL 15: SIGTERM. Shutting down as requested.
2024-02-26 10:10:14,094 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesApplicationClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2024-02-26 10:10:14,095 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
2024-02-26 10:10:14,095 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Shutting down rest endpoint.
2024-02-26 10:10:14,315 DEBUG org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.Watcher [] - Watcher closed
2024-02-26 10:10:14,511 DEBUG org.apache.pekko.actor.CoordinatedShutdown                   [] - Performing task [terminate-system] in CoordinatedShutdown phase [actor-system-terminate]
2024-02-26 10:10:14,595 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Shutting down remote daemon.
2024-02-26 10:10:14,596 INFO  org.apache.pekko.remote.RemoteActorRefProvider$RemotingTerminator [] - Remote daemon shut down; proceeding with flushing remote transports.{noformat}
Then the new JM comes back it doesn't find any checkpoints as the HA metadata was deleted (we couldn't see this in the logs of the shutting down JM):


{noformat}
2024-02-26 10:10:30,294 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Recovering checkpoints from KubernetesStateHandleStore{configMapName='autoscaling-example-5ddd0b1ba346d3bfd5ef53a63772e43c-config-map'}.2024-02-26 10:10:30,394 INFO  org.apache.flink.runtime.checkpoint.DefaultCompletedCheckpointStoreUtils [] - Found 0 checkpoints in KubernetesStateHandleStore{configMapName='autoscaling-example-5ddd0b1ba346d3bfd5ef53a63772e43c-config-map'}.{noformat}
Even the main method is re-run and the jobgraph is regenerated (which is expected given the HA metadata was removed incorrectly)"	FLINK	Resolved	2	1	6847	pull-request-available
13432464	FLIP-285: Refactoring code for multiple component leader election	"The current implementation of the multiple component leader election faces a number of issues. These issues mostly stem from an attempt to make the multiple leader election process work just the same way as the single component leader election.

An attempt at listing the issues follows:
* *Naming* MultipleComponentLeaderElectionService appears by name similar to the LeaderElectionService, but is in fact closer to the LeaderElectionDriver.
* *Similarity* The interfaces LeaderElectionService, LeaderElectionDriver and MultipleComponentLeaderElectionDriver are very similar to each other.
* *Cyclic dependency* DefaultMultipleComponentLeaderElectionService holds a reference to the ZooKeeperMultipleComponentLeaderElectionDriver (MultipleComponentLeaderElectionDriver), which in turn holds a reference to the DefaultMultipleComponentLeaderElectionService (LeaderLatchListener)
* *Unclear contract* With single component leader election drivers such as ZooKeeperLeaderElectionDriver a call to the LeaderElectionService#stop from JobMasterServiceLeadershipRunner#closeAsync implies giving up the leadership of the JobMaster. With the multiple component leader election this is no longer the case. The leadership is held until the HighAvailabilityServices shutdown. This logic may be difficult to understand from the perspective of one of the components (e.g., the Dispatcher)
* *Long call hierarchy* DefaultLeaderElectionService->MultipleComponentLeaderElectionDriverAdapter->MultipleComponentLeaderElectionService->ZooKeeperMultipleComponentLeaderElectionDriver
* *Long prefix* ""MultipleComponentLeaderElection"" is quite a long prefix but shared by many classes.
* *Adapter as primary implementation* All non-testing non-multiple-component leadership drivers are deprecated. The primary implementation of LeaderElectionDriver is the adapter MultipleComponentLeaderElectionDriverAdapter.
* *Possible redundancy* We currently have similar methods for the Dispatcher, ResourceManager, JobMaster and WebMonitorEndpoint. (E.g., for granting leadership.) As these methods are called at the same time due to the multiple component leader election, it may make sense to combine this logic into a single object."	FLINK	Resolved	3	4	6847	pull-request-available
13564080	Add docs generation to workflow	"The goal of this subtask is to mimick the optional docs generation from the Azure Pipelines workflow. This includes:
* Running the docs check in each PR CI if some artifacts in the ./docs folder were touched
* Do not run the docs build if it's not a PR
* Skip e2e tests in the Flink CI template if the PR is an docs-only change"	FLINK	In Progress	3	7	6847	github-actions, pull-request-available
13562470	Workflow: Add basic CI that will run with the default configuration	Runs the Flink CI template with the default configuration (Java 8) and can be enabled in each push to the branch.	FLINK	Resolved	3	7	6847	github-actions, pull-request-available
13495972	Stacktrace printing in DefaultExecutionGraphCacheTest is confusing maven test log output	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=42849&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8700]

 

 
{code:java}
java.util.concurrent.ExecutionException: org.apache.flink.runtime.messages.FlinkJobNotFoundException: Could not find Flink job (408c1ab89f41c2d4f99c870e8abde94d)
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCacheTest.testImmediateCacheInvalidationAfterFailure(DefaultExecutionGraphCacheTest.java:147)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
{code}
 

 "	FLINK	Resolved	4	1	6847	pull-request-available
13523205	"Too many CI failed due to ""Could not connect to azure.archive.ubuntu.com"""	"!image-2023-02-06-17-59-20-019.png!

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45762&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a"	FLINK	Resolved	2	1	6847	pull-request-available, test-stability
13289338	Maven central connection timeouts on Azure Pipelines	"Some test stages invoke maven again, where additional dependencies are downloaded, sometimes failing the build.

This ticket is about using the Google mirror wherever possible.

Examples of failing tests:
- https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5882&view=logs&j=636f54dd-dda5-5b4b-f495-2d92ec493b6c&t=6c30efdf-a92a-5da3-9a6a-004c8552b2df

A failure looks like this:
{code}
[ERROR] Failed to execute goal on project flink-hadoop-fs: Could not resolve dependencies for project org.apache.flink:flink-hadoop-fs:jar:1.11-SNAPSHOT: Could not transfer artifact org.apache.flink:flink-shaded-hadoop-2:jar:2.8.3-10.0 from/to central (https://repo.maven.apache.org/maven2): GET request of: org/apache/flink/flink-shaded-hadoop-2/2.8.3-10.0/flink-shaded-hadoop-2-2.8.3-10.0.jar from central failed: Connection reset -> [Help 1]
{code}"	FLINK	Closed	2	1	8669	pull-request-available
13343810	Add check to LicenseChecker for top level /LICENSE files in shaded jars	"During the release verification of the 1.12.0 release, we noticed several modules containing LICENSE files in the jar file, which are not Apache licenses.
This could mislead users that the JARs are licensed not according to the ASL, but something else."	FLINK	Closed	2	4	8669	pull-request-available
12941377	KafkaShortRetention08ITCase.testAutoOffsetReset failed on Travis	"The KafkaShortRetention08ITCase.testAutoOffsetReset test case failed on Travis [1].

[1] https://s3.amazonaws.com/archive.travis-ci.org/jobs/111166400/log.txt"	FLINK	Closed	2	1	8669	test-stability
12956187	YARNSessionCapacitySchedulerITCase.testNonexistingQueue unstable	I encountered a failed test on travis.	FLINK	Resolved	2	1	8669	test-stability
13344781	generate-stackbrew-library.sh in flink-docker doesn't properly prune the java11 tag	The output of {generate-stackbrew-library.sh} contains two {java11} tags.	FLINK	Closed	2	1	8669	pull-request-available
13323534	Forward JobStatus.INITIALIZING timestamp to ExecutionGraph	"This is a follow up to FLINK-16866.

Currently, in the ExecutionGraph, the timestamp for JobStatus.INITIALIZING is not set (defaulting to 0), leading to an inconsistent stateTimestamps array.

To resolve this ticket, one needs to forward the timestamp from the Dispatcher (where the initialization is started) to the ExecutionGraph. "	FLINK	Closed	3	4	8669	pull-request-available
13372167	Harden JobMasterStopWithSavepointITCase	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=16451&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0&l=3884


{code:java}
[ERROR] throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase)  Time elapsed: 0.154 s  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithoutRestartsHelper(JobMasterStopWithSavepointITCase.java:154)
	at org.apache.flink.runtime.jobmaster.JobMasterStopWithSavepointITCase.throwingExceptionOnCallbackWithNoRestartsShouldFailTheTerminate(JobMasterStopWithSavepointITCase.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)

{code}
"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13295963	'Walkthrough Table Java nightly end-to-end test' [FAIL] Test script contains errors	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=7009&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179&t=931b3127-d6ee-5f94-e204-48d51cd1c334


{noformat}
java.lang.AbstractMethodError: Method org/apache/flink/walkthrough/common/table/SpendReportTableSink.consumeDataSet(Lorg/apache/flink/api/java/DataSet;)Lorg/apache/flink/api/java/operators/DataSink; is abstract
	at org.apache.flink.walkthrough.common.table.SpendReportTableSink.consumeDataSet(SpendReportTableSink.java) ~[?:?]
	at org.apache.flink.table.api.internal.BatchTableEnvImpl.writeToSink(BatchTableEnvImpl.scala:133) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:732) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.table.api.internal.TableEnvImpl.insertInto(TableEnvImpl.scala:675) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.table.api.internal.TableImpl.insertInto(TableImpl.java:409) ~[flink-table_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.walkthrough.SpendReport.main(SpendReport.java:41) ~[?:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_242]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_242]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:321) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:205) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:142) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:662) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:210) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:893) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:966) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_242]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_242]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41) ~[flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:966) [flink-dist_2.12-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]

[FAIL] Test script contains errors.

{noformat}
"	FLINK	Closed	2	1	8669	test-stability
13291302	CheckPubSubEmulatorTest is flaky on Azure	Log: https://dev.azure.com/aljoschakrettek/Flink/_build/results?buildId=56&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=ce095137-3e3b-5f73-4b79-c42d3d5f8283&l=7842	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13312283	Test Flink on Azure-hosted VMs nightly	"There are some tests which happen a lot more frequently on the VMs provided by Azure (instead of the CI infrastructure hosted in Alibaba Cloud).

Since we have enough CI resources available (at night), we can add a run on the Azure machines to get more visibility into those failures, and to increase the stability of personal account CI runs."	FLINK	Closed	3	4	8669	pull-request-available
12720081	Webclient does not show System.out messages	"When submitting the KMeans Job using the web client, I'm getting the following exception:
```
An error occurred while invoking the program:

The program plan could not be fetched. The program silently swallowed the control flow exceptions.


eu.stratosphere.client.program.ProgramInvocationException: The program plan could not be fetched. The program silently swallowed the control flow exceptions.
	at eu.stratosphere.client.program.Client.getOptimizedPlan(Client.java:154)
	at eu.stratosphere.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:161)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)
	at org.eclipse.jetty.server.Server.handle(Server.java:352)
	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)
	at org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211)
	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:436)
	at java.lang.Thread.run(Thread.java:744)
```

The problem is that the main() method just returns (printing a message to `System.out`).
This is not very intuitive, since you have to look into the webclient.out log

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/900
Created by: [rmetzger|https://github.com/rmetzger]
Labels: 
Milestone: Release 0.5.1
Created at: Tue Jun 03 12:25:04 CEST 2014
State: open
"	FLINK	Resolved	3	1	8669	github-import
13354471	Reactive mode: Introduce execution mode configuration key and check for supported ClusterEntrypoint type	"According to the FLIP, introduce a ""scheduler-mode"" configuration key, and check in the ClusterEntrypoint if the chosen entry point type is supported by the selected execution mode."	FLINK	Closed	3	7	8669	pull-request-available
13356475	Add DeclarativeScheduler / Failing state	"This subtask of adding the declarative scheduler is about adding the Failing state to Flink, including tests.

Failing: An unrecoverable fault has occurred. The scheduler stops the ExecutionGraph by canceling it.
"	FLINK	Closed	3	7	8669	pull-request-available
13356470	Add DeclarativeScheduler / WaitingForResources state	"This subtask of adding the declarative scheduler is about adding the WaitingForResources state to Flink, including tests.

Waiting for resources: The required resources are declared. The scheduler waits until either the requirements are fulfilled or the set of resources has stabilised.
"	FLINK	Closed	3	7	8669	pull-request-available
13288241	Set up environment variables for the S3 tests in Azure	Some S3 tests on azure are skipped, because some environment variables have not been set	FLINK	Resolved	3	7	8669	pull-request-available
13381697	Move our Azure pipelines away from Ubuntu 16.04 by September	"Azure won't support Ubuntu 16.04 starting from October, hence we need to migrate to a newer ubuntu version.

We should do this at a time when the builds are relatively stable to be able to clearly identify issues relating to the version upgrade. Also, we shouldn't do this before a feature freeze ;) "	FLINK	Closed	2	1	8669	pull-request-available
12736067	Describe how to run the examples from the command line	"The examples page here: http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/java_api_examples.html does not describe how users can run the examples from the command line (or the web interface).

This is the thread where the user suggested this: http://flink.incubator.apache.org/docs/0.6-SNAPSHOT/run_example_quickstart.html#comment-1554345488"	FLINK	Resolved	4	4	8669	starter
13328411	YARNSessionFIFOITCase.checkForProhibitedLogContents found a log with prohibited string	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6661&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354]

{code}
2020-09-19T22:08:13.5364974Z [ERROR]   YARNSessionFIFOITCase.checkForProhibitedLogContents:83->YarnTestBase.ensureNoProhibitedStringInLogFiles:476 Found a file /__w/2/s/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-1_0/application_1600553154281_0001/container_1600553154281_0001_01_000002/taskmanager.log with a prohibited string (one of [Exception, Started SelectChannelConnector@0.0.0.0:8081]). Excerpts:
{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
12922000	KafkaITCase.testOffsetAutocommitTest	"https://travis-ci.org/apache/flink/jobs/96981797
{noformat}
Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 126.278 sec <<< FAILURE! - in org.apache.flink.streaming.connectors.kafka.KafkaITCase
testOffsetAutocommitTest(org.apache.flink.streaming.connectors.kafka.KafkaITCase)  Time elapsed: 12.735 sec  <<< FAILURE!
java.lang.AssertionError: Offset of o1=-915623761776 was not in range
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.flink.streaming.connectors.kafka.KafkaConsumerTestBase.runOffsetAutocommitTest(KafkaConsumerTestBase.java:333)
	at org.apache.flink.streaming.connectors.kafka.KafkaITCase.testOffsetAutocommitTest(KafkaITCase.java:56)

Results :

Failed tests: 
KafkaITCase.testOffsetAutocommitTest:56->KafkaConsumerTestBase.runOffsetAutocommitTest:333 Offset of o1=-915623761776 was not in range
{noformat}"	FLINK	Resolved	2	1	8669	test-stability
13307739	Test native K8s integration	"Test Flink's native K8s integration:

* session mode
* application mode
* custom Flink image
* custom configuration and log properties"	FLINK	Closed	2	7	8669	pull-request-available, release-testing
13468525	flink-parquet doesn't compile on M1 mac without rosetta	"Compiling Flink 1.16-SNAPSHOT fails on an M1 Mac (apple silicon) without the rosetta translation layer, because the automatically downloaded ""protoc-3.17.3-osx-aarch_64.exe"" file is actually just a copy of ""protoc-3.17.3-osx-x86_64.exe"". (as you can read here: https://github.com/os72/protoc-jar/issues/93)

This is the error:
{code}
[ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:test-compile (default) on project flink-parquet: An error occurred while invoking protoc. Error while executing process. Cannot run program ""/Users/rmetzger/Projects/flink/flink-formats/flink-parquet/target/protoc-plugins/protoc-3.17.3-osx-aarch_64.exe"": error=86, Bad CPU type in executable -> [Help 1]
{code}

"	FLINK	Closed	3	7	8669	pull-request-available
13375399	Document how to use the reactive mode on K8s	We should extend the existing standalone K8s documentation https://ci.apache.org/projects/flink/flink-docs-master/docs/deployment/resource-providers/standalone/kubernetes to contain an example for the reactive mode (including the resource definitions).	FLINK	Closed	2	4	8669	pull-request-available
12981817	CliFrontendYarnAddressConfigurationTest fails	"The {{CliFrontendYarnAddressConfigurationTest}} failed here:

https://s3.amazonaws.com/archive.travis-ci.org/jobs/139195089/log.txt

https://travis-ci.org/apache/flink/builds/139195083

Another failed run with logs is here: https://travis-ci.org/uce/flink/builds/139262754

"	FLINK	Resolved	3	1	8669	test-stability
13336881	Upgrade mesos to 1.7 or newer	"A user reported a dependency vulnerability which affects {{mesos}} [1]. We should upgrade {{mesos}} to {{1.7.0}} or newer.

[1] https://lists.apache.org/thread.html/r0dd7ff197b2e3bdd80a0326587ca3d0c22e10d1dba17c769d6da7d7a%40%3Cuser.flink.apache.org%3E"	FLINK	Closed	2	4	8669	pull-request-available
13333949	ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange times out	"Full logs:
https://dev.azure.com/sewen0794/19b23adf-d190-4fb4-ae6e-2e92b08923a3/_apis/build/builds/148/logs/115

Exception:
{code}
[ERROR] testJobExecutionOnClusterWithLeaderChange(org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase)  Time elapsed: 301.093 s  <<< ERROR!
java.util.concurrent.TimeoutException: Condition was not met in given timeout.
	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:132)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.getNextLeadingDispatcherGateway(ZooKeeperLeaderElectionITCase.java:140)
	at org.apache.flink.test.runtime.leaderelection.ZooKeeperLeaderElectionITCase.testJobExecutionOnClusterWithLeaderChange(ZooKeeperLeaderElectionITCase.java:122)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)

{code}"	FLINK	Closed	2	1	8669	test-stability
13342678	Rework standalone deployment documentation page	Similar to FLINK-20347 we need to update the standalone deployment documentation page. Additionally, we need to verify that everything we state on the documentation works.	FLINK	Closed	3	7	8669	pull-request-available
13282600	Set up nightly cron jobs on Azure Pipelines build	"FLINK-13978 introduced support for Azure Pipelines, however limited to building pull requests and pushes.

The scope of this issue is to add the cron jobs available in travis also to the Azure setup."	FLINK	Resolved	3	4	8669	pull-request-available
13243397	Add note to PubSub connector documentation about beta status	As part of the review of FLINK-9311, we decided to add a note to the documentation page that the connector is considered beta by the community.	FLINK	Resolved	3	4	8669	pull-request-available
12720067	Changes to be included in Release 0.5.1	"I'm currently preparing the 0.5 release.

This issue is for creating a list of all pull requests / commits we want to include in the 0.5.1 release.
I want that the 0.5.1 release contains only bugfixes. 

I will create a release-0.5.1 branch that I'll manually maintain by cherry picking commits from `master`.

Fixes included in 0.5.1:
- https://github.com/stratosphere/stratosphere/commit/73a9dcc92ae8911bfc7dd429fba78be6a63e3043 (from PR https://github.com/stratosphere/stratosphere/pull/876) 


Commits to be included into the 0.5.1 release:
- https://github.com/stratosphere/stratosphere/pull/884
- https://github.com/stratosphere/stratosphere/issues/887
- https://github.com/stratosphere/stratosphere/pull/850


---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/886
Created by: [rmetzger|https://github.com/rmetzger]
Labels: parent-for-major-feature, 
Assignee: [rmetzger|https://github.com/rmetzger]
Created at: Thu May 29 14:37:01 CEST 2014
State: open
"	FLINK	Resolved	3	4	8669	github-import
13384766	cron_snapshot_deployment_maven unstable on maven	"{{cron_snapshot_deployment_maven}}, the cron build on azure that deploys snapshot artifacts to maven central repository, has become unstable recently.

The failures fall into two categories.
- Maven failed to upload/download an artifact
- The stage overall takes too long time.

As far as I can see, the instability starts being observed since June 18th.

Observed instances:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19152&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19154&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19167&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19169&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19176&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19175&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=19177&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=e9844b5e-5aa3-546b-6c3e-5395c7c0cac7"	FLINK	Closed	2	4	8669	pull-request-available, test-stability
12846884	Instable test in flink-yarn-tests	"The test YARNSessionFIFOITCase fails from time to time on an irregular basis. For example see: https://travis-ci.org/apache/flink/jobs/72019690

{noformat}
Tests run: 12, Failures: 1, Errors: 0, Skipped: 2, Time elapsed: 205.163 sec <<< FAILURE! - in org.apache.flink.yarn.YARNSessionFIFOITCase
perJobYarnClusterWithParallelism(org.apache.flink.yarn.YARNSessionFIFOITCase)  Time elapsed: 60.651 sec  <<< FAILURE!
java.lang.AssertionError: During the timeout period of 60 seconds the expected string did not show up
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.flink.yarn.YarnTestBase.runWithArgs(YarnTestBase.java:478)
	at org.apache.flink.yarn.YARNSessionFIFOITCase.perJobYarnClusterWithParallelism(YARNSessionFIFOITCase.java:435)

Results :
Failed tests: 
  YARNSessionFIFOITCase.perJobYarnClusterWithParallelism:435->YarnTestBase.runWithArgs:478 During the timeout period of 60 seconds the expected string did not show up
{noformat}

Another error case is this (see https://travis-ci.org/mjsax/flink/jobs/77313444)
{noformat}
Tests run: 12, Failures: 3, Errors: 0, Skipped: 2, Time elapsed: 182.008 sec <<< FAILURE! - in org.apache.flink.yarn.YARNSessionFIFOITCase
testTaskManagerFailure(org.apache.flink.yarn.YARNSessionFIFOITCase)  Time elapsed: 27.356 sec  <<< FAILURE!
java.lang.AssertionError: Found a file /home/travis/build/mjsax/flink/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-0_0/application_1440595422559_0007/container_1440595422559_0007_01_000003/taskmanager.log with a prohibited string: [Exception, Started SelectChannelConnector@0.0.0.0:8081]
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:294)
	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:94)

testNonexistingQueue(org.apache.flink.yarn.YARNSessionFIFOITCase)  Time elapsed: 17.421 sec  <<< FAILURE!
java.lang.AssertionError: Found a file /home/travis/build/mjsax/flink/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-0_0/application_1440595422559_0007/container_1440595422559_0007_01_000003/taskmanager.log with a prohibited string: [Exception, Started SelectChannelConnector@0.0.0.0:8081]
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:294)
	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:94)

testJavaAPI(org.apache.flink.yarn.YARNSessionFIFOITCase)  Time elapsed: 11.984 sec  <<< FAILURE!
java.lang.AssertionError: Found a file /home/travis/build/mjsax/flink/flink-yarn-tests/target/flink-yarn-tests-fifo/flink-yarn-tests-fifo-logDir-nm-0_0/application_1440595422559_0007/container_1440595422559_0007_01_000003/taskmanager.log with a prohibited string: [Exception, Started SelectChannelConnector@0.0.0.0:8081]
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.yarn.YarnTestBase.ensureNoProhibitedStringInLogFiles(YarnTestBase.java:294)
	at org.apache.flink.yarn.YARNSessionFIFOITCase.checkForProhibitedLogContents(YARNSessionFIFOITCase.java:94)
{noformat}

Furthermore, this build failed too: https://travis-ci.org/apache/flink/jobs/77313450
(no error, but Travis terminated to due no progress for 300 seconds -> deadlock?)"	FLINK	Closed	2	1	8669	test-stability
13473435	No space left on device on Azure e2e_2_ci tests	"Many e2e tests failed due to no enough space. We previously cleaned up the space by cleaning up the flink-e2e directory, but at the moment this is not enough to solve the problem. 
"	FLINK	Closed	1	11500	8669	pull-request-available, test-stability
13337120	LeaderChangeClusterComponentsTest.testReelectionOfJobMaster is instable	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8214&view=logs&j=3b6ec2fd-a816-5e75-c775-06fb87cb6670&t=2aff8966-346f-518f-e6ce-de64002a5034

{code}
2020-10-23T21:07:32.6861747Z [ERROR] testReelectionOfJobMaster(org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest)  Time elapsed: 30.182 s  <<< FAILURE!
2020-10-23T21:07:32.6862546Z java.lang.AssertionError: Job failed.
2020-10-23T21:07:32.6865424Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.throwAssertionErrorOnFailedResult(JobResultUtils.java:54)
2020-10-23T21:07:32.6866512Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.assertSuccess(JobResultUtils.java:30)
2020-10-23T21:07:32.6867720Z 	at org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.testReelectionOfJobMaster(LeaderChangeClusterComponentsTest.java:152)
2020-10-23T21:07:32.6868707Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-23T21:07:32.6869428Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-23T21:07:32.6870293Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-23T21:07:32.6871062Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-23T21:07:32.6871954Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-10-23T21:07:32.6872726Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-10-23T21:07:32.6873503Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-10-23T21:07:32.6874393Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-10-23T21:07:32.6875218Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-10-23T21:07:32.6876001Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-10-23T21:07:32.6876816Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-10-23T21:07:32.6877475Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-10-23T21:07:32.6878216Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-10-23T21:07:32.6879061Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-10-23T21:07:32.6879819Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-10-23T21:07:32.6880502Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-10-23T21:07:32.6881215Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-10-23T21:07:32.6882109Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-10-23T21:07:32.6882850Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-10-23T21:07:32.6884171Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-10-23T21:07:32.6884969Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-10-23T21:07:32.6885641Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-10-23T21:07:32.6886201Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-10-23T21:07:32.6886841Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-10-23T21:07:32.6887378Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-10-23T21:07:32.6887913Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-10-23T21:07:32.6888478Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-10-23T21:07:32.6889109Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-10-23T21:07:32.6889625Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-10-23T21:07:32.6890110Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-10-23T21:07:32.6890607Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-10-23T21:07:32.6891237Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-10-23T21:07:32.6892166Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-10-23T21:07:32.6892827Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:217)
2020-10-23T21:07:32.6893382Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:210)
2020-10-23T21:07:32.6894048Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:204)
2020-10-23T21:07:32.6894667Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:526)
2020-10-23T21:07:32.6895205Z 	at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:240)
2020-10-23T21:07:32.6895872Z 	at org.apache.flink.runtime.jobmaster.DefaultExecutionDeploymentReconciler.reconcileExecutionDeployments(DefaultExecutionDeploymentReconciler.java:55)
2020-10-23T21:07:32.6896633Z 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.reportPayload(JobMaster.java:1234)
2020-10-23T21:07:32.6897239Z 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.reportPayload(JobMaster.java:1221)
2020-10-23T21:07:32.6897834Z 	at org.apache.flink.runtime.heartbeat.HeartbeatManagerImpl.receiveHeartbeat(HeartbeatManagerImpl.java:199)
2020-10-23T21:07:32.6898395Z 	at org.apache.flink.runtime.jobmaster.JobMaster.heartbeatFromTaskManager(JobMaster.java:672)
2020-10-23T21:07:32.6898846Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-23T21:07:32.6899289Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-23T21:07:32.6899816Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-23T21:07:32.6900257Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-23T21:07:32.6900723Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:281)
2020-10-23T21:07:32.6901265Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-10-23T21:07:32.6901938Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-10-23T21:07:32.6902497Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-10-23T21:07:32.6902977Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-10-23T21:07:32.6903678Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-10-23T21:07:32.6904110Z 	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
2020-10-23T21:07:32.6904545Z 	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
2020-10-23T21:07:32.6904974Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-10-23T21:07:32.6905435Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-10-23T21:07:32.6905893Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2020-10-23T21:07:32.6906325Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
2020-10-23T21:07:32.6906830Z 	at akka.actor.Actor.aroundReceive(Actor.scala:517)
2020-10-23T21:07:32.6907220Z 	at akka.actor.Actor.aroundReceive$(Actor.scala:515)
2020-10-23T21:07:32.6907618Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-10-23T21:07:32.6908050Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-10-23T21:07:32.6908458Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-10-23T21:07:32.6908831Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-10-23T21:07:32.6909213Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-10-23T21:07:32.6909554Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-10-23T21:07:32.6909959Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-10-23T21:07:32.6910436Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-10-23T21:07:32.6910887Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-10-23T21:07:32.6911360Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-10-23T21:07:32.6913020Z Caused by: org.apache.flink.util.FlinkException: Execution ff7439fa86b9f67e46b2b6715829af00_dccf9918c07aa47eb2b28a1de42a640f_3_0 is unexpectedly no longer running on task executor 3d5d979c-6898-4593-935e-f0914738d325.
2020-10-23T21:07:32.6913798Z 	at org.apache.flink.runtime.jobmaster.JobMaster$1.onMissingDeploymentsOf(JobMaster.java:244)
2020-10-23T21:07:32.6914167Z 	... 33 more
{code}"	FLINK	Closed	1	1	8669	pull-request-available, test-stability
13301179	Running HA per-job cluster (rocks, non-incremental) gets stuck killing a non-existing pid in Hadoop 3 build profile	"CI log: https://api.travis-ci.org/v3/job/678609505/log.txt

{code}
Waiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Starting standalonejob daemon on host travis-job-e606668f-b674-49c0-8590-e3508e22b99d.
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Killed TM @ 18864
kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]
Killed TM @ 


No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.
Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received

The build has been terminated
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
12719683	Add support for Tachyon File System to Flink	"Implementation of the Tachyon file system.

The code was tested using junit and the wordcount example on a tachyon file system running in local mode.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/512
Created by: [zentol|https://github.com/zentol]
Labels: 
Created at: Wed Feb 26 13:58:24 CET 2014
State: closed
"	FLINK	Resolved	4	2	8669	github-import
13287241	Add docker images based on Java 11	"Since 1.10.0 supports Java 11, we can add a version of the docker image based on Java 11

Feature [requested in our old issue tracker|https://github.com/docker-flink/docker-flink/issues/97] and moved here"	FLINK	Closed	3	2	8669	pull-request-available
13342842	Incorrect use of yarn-session.sh -id vs -yid in log statements.	"The Yarn per-job modes log about the recommended shutdown of yarn, which doesn't work.


See: https://github.com/apache/flink/pull/10964#issuecomment-734166399"	FLINK	Closed	2	1	8669	pull-request-available
13357707	Introduce stopping with savepoint state	The declarative scheduler is also affected by the problem described in FLINK-21030. We want to solve this problem by introducing a separate state when are taking a savepoint for stopping Flink.	FLINK	Closed	3	7	8669	pull-request-available
13327576	"LeaderChangeClusterComponentsTest.testReelectionOfJobMaster failed with ""NoResourceAvailableException: Could not allocate the required slot within slot request timeout"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6499&view=logs&j=6bfdaf55-0c08-5e3f-a2d2-2a0285fd41cf&t=fd9796c3-9ce8-5619-781c-42f873e126a6]

{code}
2020-09-14T21:11:02.8200203Z [ERROR] testReelectionOfJobMaster(org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest)  Time elapsed: 300.14 s  <<< FAILURE!
2020-09-14T21:11:02.8201761Z java.lang.AssertionError: Job failed.
2020-09-14T21:11:02.8202749Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.throwAssertionErrorOnFailedResult(JobResultUtils.java:54)
2020-09-14T21:11:02.8203794Z 	at org.apache.flink.runtime.jobmaster.utils.JobResultUtils.assertSuccess(JobResultUtils.java:30)
2020-09-14T21:11:02.8205177Z 	at org.apache.flink.runtime.leaderelection.LeaderChangeClusterComponentsTest.testReelectionOfJobMaster(LeaderChangeClusterComponentsTest.java:152)
2020-09-14T21:11:02.8206191Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-14T21:11:02.8206985Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-14T21:11:02.8207930Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-14T21:11:02.8208927Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-14T21:11:02.8209753Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-14T21:11:02.8210710Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-14T21:11:02.8211608Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-14T21:11:02.8214473Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-14T21:11:02.8215398Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-14T21:11:02.8216199Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-14T21:11:02.8216947Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-14T21:11:02.8217695Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-14T21:11:02.8218635Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-14T21:11:02.8219499Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-14T21:11:02.8220313Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-14T21:11:02.8221060Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-14T21:11:02.8222171Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-14T21:11:02.8222937Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-14T21:11:02.8223688Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-14T21:11:02.8225191Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-14T21:11:02.8226086Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-14T21:11:02.8226761Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-14T21:11:02.8227453Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-09-14T21:11:02.8228392Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-09-14T21:11:02.8229256Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-09-14T21:11:02.8235798Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-09-14T21:11:02.8237650Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-14T21:11:02.8239039Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-14T21:11:02.8239894Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-14T21:11:02.8240591Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-14T21:11:02.8241325Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
2020-09-14T21:11:02.8242225Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-09-14T21:11:02.8243358Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-09-14T21:11:02.8244425Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:215)
2020-09-14T21:11:02.8245291Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:208)
2020-09-14T21:11:02.8246150Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:202)
2020-09-14T21:11:02.8247006Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:523)
2020-09-14T21:11:02.8247960Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)
2020-09-14T21:11:02.8249102Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1722)
2020-09-14T21:11:02.8249971Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1283)
2020-09-14T21:11:02.8250675Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1251)
2020-09-14T21:11:02.8251369Z 	at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1082)
2020-09-14T21:11:02.8252104Z 	at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)
2020-09-14T21:11:02.8253060Z 	at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)
2020-09-14T21:11:02.8253956Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:458)
2020-09-14T21:11:02.8254967Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:445)
2020-09-14T21:11:02.8393562Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-09-14T21:11:02.8394920Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-09-14T21:11:02.8396122Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8397194Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8398150Z 	at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:169)
2020-09-14T21:11:02.8399234Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-09-14T21:11:02.8400048Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-09-14T21:11:02.8401048Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8402025Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8403171Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:731)
2020-09-14T21:11:02.8404708Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)
2020-09-14T21:11:02.8405751Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)
2020-09-14T21:11:02.8406633Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-09-14T21:11:02.8407378Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-09-14T21:11:02.8408120Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8408948Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8409748Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1168)
2020-09-14T21:11:02.8410511Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-09-14T21:11:02.8411543Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-09-14T21:11:02.8412553Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-14T21:11:02.8413340Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-09-14T21:11:02.8414204Z 	at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1072)
2020-09-14T21:11:02.8415364Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)
2020-09-14T21:11:02.8416128Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)
2020-09-14T21:11:02.8417172Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-09-14T21:11:02.8417995Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-09-14T21:11:02.8418997Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-09-14T21:11:02.8419692Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-09-14T21:11:02.8420336Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-09-14T21:11:02.8421055Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-09-14T21:11:02.8421655Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-09-14T21:11:02.8422336Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-14T21:11:02.8423049Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-09-14T21:11:02.8423681Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-09-14T21:11:02.8424505Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-09-14T21:11:02.8425209Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-09-14T21:11:02.8425760Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-09-14T21:11:02.8426376Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-09-14T21:11:02.8427252Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-09-14T21:11:02.8427931Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-09-14T21:11:02.8428684Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-09-14T21:11:02.8429375Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-09-14T21:11:02.8430118Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-09-14T21:11:02.8430853Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-09-14T21:11:02.8431971Z Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.
2020-09-14T21:11:02.8433179Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:464)
2020-09-14T21:11:02.8434082Z 	... 45 more
2020-09-14T21:11:02.8434809Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException
2020-09-14T21:11:02.8435611Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2020-09-14T21:11:02.8436379Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2020-09-14T21:11:02.8437159Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2020-09-14T21:11:02.8437976Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-09-14T21:11:02.8438658Z 	... 25 more
2020-09-14T21:11:02.8439085Z Caused by: java.util.concurrent.TimeoutException
2020-09-14T21:11:02.8439476Z 	... 23 more
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13287779	Execute all end to end tests on AZP	"Ensure that we execute all end to end tests on AZP:
- Make sure that all the e2e tests referenced in the splits are also referenced in the ""run nightly tests"" script
- make sure the java e2e tests are executed"	FLINK	Resolved	3	7	8669	pull-request-available
13076769	Elasticsearch 5 release artifacts not published to Maven central	Release artifacts for the Elasticsearch 5 connector is not published to the Maven Central. Elasticsearch 5 requires Java 8 at minimum, so for the release we need to build with Java 8 for this.	FLINK	Resolved	1	1	8669	flink-rel-1.3.1-blockers
13375357	AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond found unexpected files	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17285&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=9340

{code}
Apr 27 11:10:07 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Apr 27 11:10:24 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 17.177 s <<< FAILURE! - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
Apr 27 11:10:24 [ERROR] testStopWithSavepointFailOnFirstSavepointSucceedOnSecond(org.apache.flink.test.scheduling.AdaptiveSchedulerITCase)  Time elapsed: 0.305 s  <<< FAILURE!
Apr 27 11:10:24 java.lang.AssertionError: Found unexpected files: /tmp/junit3745203124457058148/savepoint/savepoint-8596b1-b3046c9bcf40
Apr 27 11:10:24 	at org.junit.Assert.fail(Assert.java:88)
Apr 27 11:10:24 	at org.apache.flink.test.scheduling.AdaptiveSchedulerITCase.testStopWithSavepointFailOnFirstSavepointSucceedOnSecond(AdaptiveSchedulerITCase.java:226)
Apr 27 11:10:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 27 11:10:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 27 11:10:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 27 11:10:24 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 27 11:10:24 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 27 11:10:24 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 27 11:10:24 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 27 11:10:24 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 27 11:10:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 11:10:24 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 27 11:10:24 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 27 11:10:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 27 11:10:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 27 11:10:24 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 27 11:10:24 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Apr 27 11:10:24 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 27 11:10:24 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Apr 27 11:10:24 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Apr 27 11:10:24 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Apr 27 11:10:24 
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13294106	End to end tests timeout on Azure	"Example: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6650&view=logs&j=08866332-78f7-59e4-4f7e-49a56faa3179 or https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6637&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5

{code}##[error]The job running on agent Azure Pipelines 6 ran longer than the maximum time of 200 minutes. For more information, see https://go.microsoft.com/fwlink/?linkid=2077134
{code}
and {code}##[error]The operation was canceled.{code}
"	FLINK	Closed	1	1	8669	pull-request-available
13284480	'Quickstarts Java nightly end-to-end test' is failed on travis	"{code:java}
==============================================================================
Running 'Quickstarts Java nightly end-to-end test'
==============================================================================
TEST_DATA_DIR: /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491
Flink dist directory: /home/travis/build/apache/flink/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
22:16:44.021 [INFO] Scanning for projects...
22:16:44.095 [INFO] ------------------------------------------------------------------------
22:16:44.095 [INFO] BUILD FAILURE
22:16:44.095 [INFO] ------------------------------------------------------------------------
22:16:44.098 [INFO] Total time: 0.095 s
22:16:44.099 [INFO] Finished at: 2020-02-10T22:16:44+00:00
22:16:44.143 [INFO] Final Memory: 5M/153M
22:16:44.143 [INFO] ------------------------------------------------------------------------
22:16:44.144 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -> [Help 1]
22:16:44.144 [ERROR] 
22:16:44.145 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:16:44.145 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:16:44.145 [ERROR] 
22:16:44.145 [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:16:44.145 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException
/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 57: cd: flink-quickstart-java: No such file or directory
cp: cannot create regular file '/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java': No such file or directory
sed: can't read /home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491/flink-quickstart-java/src/main/java/org/apache/flink/quickstart/Elasticsearch5SinkExample.java: No such file or directory
awk: fatal: cannot open file `pom.xml' for reading (No such file or directory)
sed: can't read pom.xml: No such file or directory
sed: can't read pom.xml: No such file or directory
22:16:45.312 [INFO] Scanning for projects...
22:16:45.386 [INFO] ------------------------------------------------------------------------
22:16:45.386 [INFO] BUILD FAILURE
22:16:45.386 [INFO] ------------------------------------------------------------------------
22:16:45.391 [INFO] Total time: 0.097 s
22:16:45.391 [INFO] Finished at: 2020-02-10T22:16:45+00:00
22:16:45.438 [INFO] Final Memory: 5M/153M
22:16:45.438 [INFO] ------------------------------------------------------------------------
22:16:45.440 [ERROR] The goal you specified requires a project to execute but there is no POM in this directory (/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/temp-test-directory-42718423491). Please verify you invoked Maven from the correct directory. -> [Help 1]
22:16:45.440 [ERROR] 
22:16:45.440 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:16:45.440 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:16:45.440 [ERROR] 
22:16:45.440 [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:16:45.440 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MissingProjectException
/home/travis/build/apache/flink/flink-end-to-end-tests/test-scripts/test_quickstarts.sh: line 73: cd: target: No such file or directory
java.io.FileNotFoundException: flink-quickstart-java-0.1.jar (No such file or directory)
	at java.util.zip.ZipFile.open(Native Method)
	at java.util.zip.ZipFile.<init>(ZipFile.java:225)
	at java.util.zip.ZipFile.<init>(ZipFile.java:155)
	at java.util.zip.ZipFile.<init>(ZipFile.java:126)
	at sun.tools.jar.Main.list(Main.java:1115)
	at sun.tools.jar.Main.run(Main.java:293)
	at sun.tools.jar.Main.main(Main.java:1288)
Success: There are no flink core classes are contained in the jar.
Failure: Since Elasticsearch5SinkExample.class and other user classes are not included in the jar. 
[FAIL] Test script contains errors.
{code}

Here are some instances:
- https://api.travis-ci.org/v3/job/648404584/log.txt
- https://api.travis-ci.org/v3/job/648404591/log.txt
- https://api.travis-ci.org/v3/job/648404598/log.txt
"	FLINK	Resolved	3	1	8669	pull-request-available
13357936	Add tests for StateWithExecutionGraph	"This ticket is about adding dedicated tests for the StateWithExecutionGraph class.

This is a follow up from https://github.com/apache/flink/pull/14879#discussion_r573707768

"	FLINK	Closed	3	7	8669	pull-request-available
12730082	Projects wiki page link in contribution page is broken	In http://flink.incubator.apache.org/how-to-contribute.html, there is a link to projects wiki page  (http://the/wiki/url). It is broken.	FLINK	Closed	4	1	8669	documentation
13318202	Migrate Jenkins jobs to ci-builds.apache.org	"Infra is [reworking the Jenkins setup|https://lists.apache.org/thread.html/re974eed417a1bc294694701d5c91b4bf92689fcf32a4c91f169be87d%40%3Cbuilds.apache.org%3E], so we have to migrate our jobs that do the snapshot deployments.

Alternatively, find other ways to do this (Azure?) to reduce number of used infrastructure services.

/cc [~rmetzger]"	FLINK	Closed	3	4	8669	pull-request-available
13342573	Add overview / reference architecture documentation page	To properly guide users, we should add some generic overview of the deployment concepts.	FLINK	Closed	3	7	8669	pull-request-available
13291090	ExecutionContextTest.testCatalogs gets stuck / Out of memory	"This error happened in my private azure account, where I pushed the current ""master"" branch without changes.
I'm reporting the issue here, because I want the Azure tests to pass as well on Azure provided machines. 

Build: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=150&view=logs&j=69332ead-8935-5abf-5b3d-e4280fb1ff4c&t=d65fc935-d659-5115-fb53-a769ad5d07ee

{code}
2020-03-11T10:02:52.7056391Z [INFO] -------------------------------------------------------
2020-03-11T10:02:52.7056712Z [INFO]  T E S T S
2020-03-11T10:02:52.7057189Z [INFO] -------------------------------------------------------
2020-03-11T10:02:53.5091320Z [INFO] Running org.apache.flink.table.client.cli.CliResultViewTest
2020-03-11T10:02:53.5160869Z [INFO] Running org.apache.flink.table.client.cli.CliUtilsTest
2020-03-11T10:02:53.7612077Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.242 s - in org.apache.flink.table.client.cli.CliUtilsTest
2020-03-11T10:02:53.7715919Z [INFO] Running org.apache.flink.table.client.cli.CliClientTest
2020-03-11T10:02:55.3789676Z [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.868 s - in org.apache.flink.table.client.cli.CliResultViewTest
2020-03-11T10:02:55.3802969Z [INFO] Running org.apache.flink.table.client.cli.CliTableauResultViewTest
2020-03-11T10:02:56.4855422Z [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.712 s - in org.apache.flink.table.client.cli.CliClientTest
2020-03-11T10:02:56.4868386Z [INFO] Running org.apache.flink.table.client.cli.SqlCommandParserTest
2020-03-11T10:02:56.4921425Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 s - in org.apache.flink.table.client.cli.SqlCommandParserTest
2020-03-11T10:02:56.4932198Z [INFO] Running org.apache.flink.table.client.gateway.local.DependencyTest
2020-03-11T10:02:56.7734481Z [INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.39 s - in org.apache.flink.table.client.cli.CliTableauResultViewTest
2020-03-11T10:02:56.7739931Z [INFO] Running org.apache.flink.table.client.gateway.local.ExecutionContextTest
2020-03-11T10:02:58.5211719Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.026 s - in org.apache.flink.table.client.gateway.local.DependencyTest
2020-03-11T10:02:58.5240380Z [INFO] Running org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest
2020-03-11T10:02:58.5688299Z [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.043 s - in org.apache.flink.table.client.gateway.local.result.MaterializedCollectStreamResultTest
2020-03-11T10:02:58.5717028Z [INFO] Running org.apache.flink.table.client.gateway.local.EnvironmentTest
2020-03-11T10:02:58.7056145Z [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.132 s - in org.apache.flink.table.client.gateway.local.EnvironmentTest
2020-03-11T10:06:03.0672319Z Exception in thread ""surefire-forkedjvm-ping-30s"" java.lang.OutOfMemoryError: Java heap space
2020-03-11T10:08:03.2462882Z ==============================================================================
2020-03-11T10:08:03.2480526Z Maven produced no output for 300 seconds.
2020-03-11T10:08:03.2501259Z ==============================================================================
2020-03-11T10:08:03.2519324Z ==============================================================================
2020-03-11T10:08:03.2519802Z The following Java processes are running (JPS)
2020-03-11T10:08:03.2520175Z ==============================================================================
2020-03-11T10:08:03.4586209Z 20288 Jps
2020-03-11T10:08:03.4835209Z 561 Launcher
2020-03-11T10:08:03.5034414Z 19924 surefirebooter4931382021512823063.jar
2020-03-11T10:08:03.7292516Z ==============================================================================
2020-03-11T10:08:03.7293009Z Printing stack trace of Java process 561
2020-03-11T10:08:03.7293428Z ==============================================================================
2020-03-11T10:08:04.1333517Z 2020-03-11 10:08:04
2020-03-11T10:08:04.1334211Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-03-11T10:08:04.1334461Z 
2020-03-11T10:08:04.1339512Z ""Attach Listener"" #257 daemon prio=9 os_prio=0 tid=0x00007fe6c0001000 nid=0x4f66 runnable [0x0000000000000000]
2020-03-11T10:08:04.1339992Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1340208Z 
2020-03-11T10:08:04.1347758Z ""Thread-94"" #255 daemon prio=5 os_prio=0 tid=0x00007fe6a8006800 nid=0x4de6 runnable [0x00007fe6b47f7000]
2020-03-11T10:08:04.1348235Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1353470Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-03-11T10:08:04.1354425Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-03-11T10:08:04.1359827Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-03-11T10:08:04.1360292Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-03-11T10:08:04.1366539Z 	- locked <0x00000000dd8612c8> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-03-11T10:08:04.1366999Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-03-11T10:08:04.1379726Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-03-11T10:08:04.1380146Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-03-11T10:08:04.1381057Z 	- locked <0x00000000dd866498> (a java.io.InputStreamReader)
2020-03-11T10:08:04.1381457Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-03-11T10:08:04.1381842Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-03-11T10:08:04.1382242Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-03-11T10:08:04.1391495Z 	- locked <0x00000000dd866498> (a java.io.InputStreamReader)
2020-03-11T10:08:04.1391915Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-03-11T10:08:04.1392409Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-03-11T10:08:04.1392741Z 
2020-03-11T10:08:04.1393280Z ""Thread-93"" #254 daemon prio=5 os_prio=0 tid=0x00007fe6a8005000 nid=0x4de1 runnable [0x00007fe6b48f8000]
2020-03-11T10:08:04.1393683Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1409673Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-03-11T10:08:04.1410071Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-03-11T10:08:04.1410524Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-03-11T10:08:04.1410964Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-03-11T10:08:04.1411759Z 	- locked <0x00000000dd85f208> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-03-11T10:08:04.1412211Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-03-11T10:08:04.1412763Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-03-11T10:08:04.1413334Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-03-11T10:08:04.1446935Z 	- locked <0x00000000dd863890> (a java.io.InputStreamReader)
2020-03-11T10:08:04.1447565Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-03-11T10:08:04.1448088Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-03-11T10:08:04.1448646Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-03-11T10:08:04.1452201Z 	- locked <0x00000000dd863890> (a java.io.InputStreamReader)
2020-03-11T10:08:04.1453134Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-03-11T10:08:04.1453869Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-03-11T10:08:04.1454254Z 
2020-03-11T10:08:04.1455042Z ""Thread-92"" #253 daemon prio=5 os_prio=0 tid=0x00007fe6a8003800 nid=0x4ddd waiting on condition [0x00007fe6b49f9000]
2020-03-11T10:08:04.1456089Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1456632Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1457235Z 	- parking to wait for  <0x00000000dc8d2468> (a java.util.concurrent.Semaphore$NonfairSync)
2020-03-11T10:08:04.1478145Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1478803Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
2020-03-11T10:08:04.1480227Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
2020-03-11T10:08:04.1481005Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
2020-03-11T10:08:04.1481586Z 	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
2020-03-11T10:08:04.1482244Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestProvidingInputStream.awaitNextTest(TestProvidingInputStream.java:181)
2020-03-11T10:08:04.1483544Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestProvidingInputStream.beforeNextCommand(TestProvidingInputStream.java:144)
2020-03-11T10:08:04.1484283Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.AbstractCommandStream.read(AbstractCommandStream.java:100)
2020-03-11T10:08:04.1484961Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.feed(StreamFeeder.java:123)
2020-03-11T10:08:04.1485602Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.run(StreamFeeder.java:60)
2020-03-11T10:08:04.1486221Z 
2020-03-11T10:08:04.1517830Z ""ThreadedStreamConsumer"" #247 daemon prio=5 os_prio=0 tid=0x00007fe6a8002000 nid=0x4dd0 waiting on condition [0x00007fe6b4cfc000]
2020-03-11T10:08:04.1518338Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1518651Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1519764Z 	- parking to wait for  <0x00000000dd504360> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1520507Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1521117Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-03-11T10:08:04.1521870Z 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
2020-03-11T10:08:04.1522644Z 	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:83)
2020-03-11T10:08:04.1523145Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1523363Z 
2020-03-11T10:08:04.1523989Z ""surefire-fork-starter"" #244 daemon prio=5 os_prio=0 tid=0x00007fe6f7d36000 nid=0x4dcd in Object.wait() [0x00007fe6b4dfd000]
2020-03-11T10:08:04.1525301Z    java.lang.Thread.State: WAITING (on object monitor)
2020-03-11T10:08:04.1525785Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:04.1526394Z 	- waiting on <0x00000000dd85cfe0> (a java.lang.UNIXProcess)
2020-03-11T10:08:04.1526737Z 	at java.lang.Object.wait(Object.java:502)
2020-03-11T10:08:04.1527107Z 	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
2020-03-11T10:08:04.1527659Z 	- locked <0x00000000dd85cfe0> (a java.lang.UNIXProcess)
2020-03-11T10:08:04.1528173Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.CommandLineUtils$1.call(CommandLineUtils.java:260)
2020-03-11T10:08:04.1528793Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:614)
2020-03-11T10:08:04.1552451Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-03-11T10:08:04.1553111Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)
2020-03-11T10:08:04.1553709Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)
2020-03-11T10:08:04.1560538Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-03-11T10:08:04.1561063Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-11T10:08:04.1567801Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1568265Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1568466Z 
2020-03-11T10:08:04.1578210Z ""surefire-fork-starter"" #243 daemon prio=5 os_prio=0 tid=0x00007fe6f7d35000 nid=0x4dcc waiting on condition [0x00007fe6b4afa000]
2020-03-11T10:08:04.1584015Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1584366Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1595055Z 	- parking to wait for  <0x00000000dc18c828> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1601421Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1602052Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-03-11T10:08:04.1607400Z 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
2020-03-11T10:08:04.1612969Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1613527Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1619272Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1619723Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1619930Z 
2020-03-11T10:08:04.1625276Z ""ping-timer-10s"" #241 daemon prio=5 os_prio=0 tid=0x00007fe6f7d34800 nid=0x4dcb waiting on condition [0x00007fe6d81c6000]
2020-03-11T10:08:04.1631517Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-03-11T10:08:04.1631865Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1637785Z 	- parking to wait for  <0x00000000dc18b368> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1644829Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-03-11T10:08:04.1645451Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-03-11T10:08:04.1650303Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-03-11T10:08:04.1655547Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-03-11T10:08:04.1662679Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1688069Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1688640Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1689085Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1689308Z 
2020-03-11T10:08:04.1690180Z ""timeout-check-timer"" #240 daemon prio=5 os_prio=0 tid=0x00007fe6f7246000 nid=0x4dca waiting on condition [0x00007fe6d8fdd000]
2020-03-11T10:08:04.1690679Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-03-11T10:08:04.1691012Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1691830Z 	- parking to wait for  <0x00000000dc18bc48> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1692368Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-03-11T10:08:04.1692960Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-03-11T10:08:04.1694011Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-03-11T10:08:04.1716005Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-03-11T10:08:04.1716818Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1717341Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1717883Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1718306Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1718532Z 
2020-03-11T10:08:04.1718878Z ""process reaper"" #29 daemon prio=10 os_prio=0 tid=0x00007fe6ac007800 nid=0x266 runnable [0x00007fe6d808c000]
2020-03-11T10:08:04.1719484Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1719988Z 	at java.lang.UNIXProcess.waitForProcessExit(Native Method)
2020-03-11T10:08:04.1720417Z 	at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)
2020-03-11T10:08:04.1720841Z 	at java.lang.UNIXProcess$$Lambda$10/601818328.run(Unknown Source)
2020-03-11T10:08:04.1721312Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-11T10:08:04.1753748Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1754389Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1754619Z 
2020-03-11T10:08:04.1755441Z ""resolver-5"" #13 daemon prio=5 os_prio=0 tid=0x00007fe6f4203000 nid=0x249 waiting on condition [0x00007fe6d9334000]
2020-03-11T10:08:04.1756077Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1756372Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1757043Z 	- parking to wait for  <0x0000000094e961d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1757880Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1758432Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-03-11T10:08:04.1759018Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-03-11T10:08:04.1759513Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1760028Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1760529Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1760956Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1761156Z 
2020-03-11T10:08:04.1761809Z ""resolver-4"" #12 daemon prio=5 os_prio=0 tid=0x00007fe6f4aea000 nid=0x248 waiting on condition [0x00007fe6d9435000]
2020-03-11T10:08:04.1762269Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1762567Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1763225Z 	- parking to wait for  <0x0000000094e961d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1763904Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1764491Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-03-11T10:08:04.1794465Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-03-11T10:08:04.1795040Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1795597Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1796138Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1796596Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1797124Z 
2020-03-11T10:08:04.1798139Z ""resolver-3"" #11 daemon prio=5 os_prio=0 tid=0x00007fe6f5358000 nid=0x247 waiting on condition [0x00007fe6d9536000]
2020-03-11T10:08:04.1798610Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1798974Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1799800Z 	- parking to wait for  <0x0000000094e961d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1800333Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1800901Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-03-11T10:08:04.1801470Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-03-11T10:08:04.1801975Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1802470Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1802996Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1803420Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1803619Z 
2020-03-11T10:08:04.1804211Z ""resolver-2"" #10 daemon prio=5 os_prio=0 tid=0x00007fe6f535c000 nid=0x246 waiting on condition [0x00007fe6d9637000]
2020-03-11T10:08:04.1804673Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1805346Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1806047Z 	- parking to wait for  <0x0000000094e961d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1834883Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1835766Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-03-11T10:08:04.1836366Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-03-11T10:08:04.1837039Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1837752Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1838292Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1838714Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1838948Z 
2020-03-11T10:08:04.1839732Z ""resolver-1"" #9 daemon prio=5 os_prio=0 tid=0x00007fe6f535e800 nid=0x245 waiting on condition [0x00007fe6d9738000]
2020-03-11T10:08:04.1840215Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1840521Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1849201Z 	- parking to wait for  <0x0000000094e961d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-03-11T10:08:04.1854647Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1860882Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-03-11T10:08:04.1861547Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-03-11T10:08:04.1866758Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-03-11T10:08:04.1873093Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-03-11T10:08:04.1873707Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:04.1879890Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:04.1880153Z 
2020-03-11T10:08:04.1880497Z ""Service Thread"" #7 daemon prio=9 os_prio=0 tid=0x00007fe6f40ba800 nid=0x243 runnable [0x0000000000000000]
2020-03-11T10:08:04.1886919Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1887148Z 
2020-03-11T10:08:04.1893442Z ""C1 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007fe6f40b7800 nid=0x242 waiting on condition [0x0000000000000000]
2020-03-11T10:08:04.1893938Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1894177Z 
2020-03-11T10:08:04.1915871Z ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007fe6f40b4800 nid=0x241 waiting on condition [0x0000000000000000]
2020-03-11T10:08:04.1916412Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1916615Z 
2020-03-11T10:08:04.1916992Z ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007fe6f40b3000 nid=0x240 runnable [0x0000000000000000]
2020-03-11T10:08:04.1917732Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:04.1917943Z 
2020-03-11T10:08:04.1918289Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fe6f4083000 nid=0x23f in Object.wait() [0x00007fe6daff6000]
2020-03-11T10:08:04.1918769Z    java.lang.Thread.State: WAITING (on object monitor)
2020-03-11T10:08:04.1919264Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:04.1920065Z 	- waiting on <0x0000000094e95c20> (a java.lang.ref.ReferenceQueue$Lock)
2020-03-11T10:08:04.1920539Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-03-11T10:08:04.1934009Z 	- locked <0x0000000094e95c20> (a java.lang.ref.ReferenceQueue$Lock)
2020-03-11T10:08:04.1934525Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-03-11T10:08:04.1940048Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-03-11T10:08:04.1940385Z 
2020-03-11T10:08:04.1940767Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fe6f407e800 nid=0x23e in Object.wait() [0x00007fe6db0f7000]
2020-03-11T10:08:04.1947488Z    java.lang.Thread.State: WAITING (on object monitor)
2020-03-11T10:08:04.1948254Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:04.1977375Z 	- waiting on <0x00000000941f7e38> (a java.lang.ref.Reference$Lock)
2020-03-11T10:08:04.1977837Z 	at java.lang.Object.wait(Object.java:502)
2020-03-11T10:08:04.1978245Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-03-11T10:08:04.1978904Z 	- locked <0x00000000941f7e38> (a java.lang.ref.Reference$Lock)
2020-03-11T10:08:04.1979495Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-03-11T10:08:04.1980007Z 
2020-03-11T10:08:04.1980342Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fe6f400b800 nid=0x23a waiting on condition [0x00007fe6fb210000]
2020-03-11T10:08:04.1980788Z    java.lang.Thread.State: WAITING (parking)
2020-03-11T10:08:04.1981092Z 	at sun.misc.Unsafe.park(Native Method)
2020-03-11T10:08:04.1981733Z 	- parking to wait for  <0x00000000dc8d2f28> (a java.util.concurrent.FutureTask)
2020-03-11T10:08:04.1982382Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-03-11T10:08:04.1983006Z 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
2020-03-11T10:08:04.1983457Z 	at java.util.concurrent.FutureTask.get(FutureTask.java:191)
2020-03-11T10:08:04.1983964Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:476)
2020-03-11T10:08:04.1984605Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)
2020-03-11T10:08:04.1996432Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
2020-03-11T10:08:04.1996998Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-03-11T10:08:04.2003702Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-03-11T10:08:04.2010348Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-03-11T10:08:04.2062711Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-03-11T10:08:04.2063387Z 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-03-11T10:08:04.2063955Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-03-11T10:08:04.2064506Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-03-11T10:08:04.2065049Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-03-11T10:08:04.2065645Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-03-11T10:08:04.2066303Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-03-11T10:08:04.2066972Z 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-03-11T10:08:04.2068959Z 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-03-11T10:08:04.2069479Z 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-03-11T10:08:04.2070161Z 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-03-11T10:08:04.2070787Z 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-03-11T10:08:04.2071521Z 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-03-11T10:08:04.2071957Z 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-03-11T10:08:04.2072343Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-11T10:08:04.2072805Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-11T10:08:04.2073502Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-11T10:08:04.2074174Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-11T10:08:04.2074635Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-03-11T10:08:04.2075135Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-03-11T10:08:04.2075657Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-03-11T10:08:04.2076174Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-03-11T10:08:04.2076453Z 
2020-03-11T10:08:04.2076707Z ""VM Thread"" os_prio=0 tid=0x00007fe6f4075000 nid=0x23d runnable 
2020-03-11T10:08:04.2077039Z 
2020-03-11T10:08:04.2077323Z ""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007fe6f4020800 nid=0x23b runnable 
2020-03-11T10:08:04.2077602Z 
2020-03-11T10:08:04.2077886Z ""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007fe6f4022000 nid=0x23c runnable 
2020-03-11T10:08:04.2078164Z 
2020-03-11T10:08:04.2078634Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007fe6f40bd000 nid=0x244 waiting on condition 
2020-03-11T10:08:04.2078921Z 
2020-03-11T10:08:04.2079289Z JNI global references: 238
2020-03-11T10:08:04.2079455Z 
2020-03-11T10:08:04.2113920Z ==============================================================================
2020-03-11T10:08:04.2114315Z Printing stack trace of Java process 19924
2020-03-11T10:08:04.2114864Z ==============================================================================
2020-03-11T10:08:10.7773213Z 2020-03-11 10:08:10
2020-03-11T10:08:10.7774650Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-03-11T10:08:10.7775281Z 
2020-03-11T10:08:10.7776038Z ""Attach Listener"" #80 daemon prio=9 os_prio=0 tid=0x00007f5828037800 nid=0x4f73 waiting on condition [0x0000000000000000]
2020-03-11T10:08:10.7776692Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7777056Z 
2020-03-11T10:08:10.7777826Z ""derby.rawStoreDaemon"" #79 daemon prio=5 os_prio=0 tid=0x00007f587a033000 nid=0x4f11 in Object.wait() [0x00007f5824dfd000]
2020-03-11T10:08:10.7778567Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-03-11T10:08:10.7779114Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:10.7779685Z 	at org.apache.derby.impl.services.daemon.BasicDaemon.rest(Unknown Source)
2020-03-11T10:08:10.7780613Z 	- locked <0x00000000fdc8e800> (a org.apache.derby.impl.services.daemon.BasicDaemon)
2020-03-11T10:08:10.7781345Z 	at org.apache.derby.impl.services.daemon.BasicDaemon.run(Unknown Source)
2020-03-11T10:08:10.7781943Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:10.7782332Z 
2020-03-11T10:08:10.7782937Z ""derby.rawStoreDaemon"" #62 daemon prio=5 os_prio=0 tid=0x00007f587a0ed800 nid=0x4eee in Object.wait() [0x00007f584a345000]
2020-03-11T10:08:10.7783656Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-03-11T10:08:10.7784212Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:10.7784780Z 	at org.apache.derby.impl.services.daemon.BasicDaemon.rest(Unknown Source)
2020-03-11T10:08:10.7785698Z 	- locked <0x00000000e3a7a390> (a org.apache.derby.impl.services.daemon.BasicDaemon)
2020-03-11T10:08:10.7787179Z 	at org.apache.derby.impl.services.daemon.BasicDaemon.run(Unknown Source)
2020-03-11T10:08:10.7787745Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:10.7788051Z 
2020-03-11T10:08:10.7788871Z ""org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner"" #50 daemon prio=5 os_prio=0 tid=0x00007f58787f7800 nid=0x4e73 in Object.wait() [0x00007f581d4f7000]
2020-03-11T10:08:10.7789604Z    java.lang.Thread.State: WAITING (on object monitor)
2020-03-11T10:08:10.7790236Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:10.7791091Z 	- waiting on <0x00000000d2850828> (a java.lang.ref.ReferenceQueue$Lock)
2020-03-11T10:08:10.7791685Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-03-11T10:08:10.7792608Z 	- locked <0x00000000d2850828> (a java.lang.ref.ReferenceQueue$Lock)
2020-03-11T10:08:10.7793180Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-03-11T10:08:10.7794240Z 	at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:3061)
2020-03-11T10:08:10.7794846Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:10.7795152Z 
2020-03-11T10:08:10.7795659Z ""derby.rawStoreDaemon"" #43 daemon prio=5 os_prio=0 tid=0x00007f587a0b9800 nid=0x4e58 in Object.wait() [0x00007f5848963000]
2020-03-11T10:08:10.7796268Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-03-11T10:08:10.7796728Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:10.7797230Z 	at org.apache.derby.impl.services.daemon.BasicDaemon.rest(Unknown Source)
2020-03-11T10:08:10.7798227Z 	- locked <0x00000000a93c8d28> (a org.apache.derby.impl.services.daemon.BasicDaemon)
2020-03-11T10:08:10.7798837Z 	at org.apache.derby.impl.services.daemon.BasicDaemon.run(Unknown Source)
2020-03-11T10:08:10.7799337Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:10.7799654Z 
2020-03-11T10:08:10.7800419Z ""Timer-0"" #42 daemon prio=5 os_prio=0 tid=0x00007f5879daa800 nid=0x4e55 in Object.wait() [0x00007f5824cfc000]
2020-03-11T10:08:10.7801045Z    java.lang.Thread.State: WAITING (on object monitor)
2020-03-11T10:08:10.7801496Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:10.7802144Z 	- waiting on <0x00000000a93c90a8> (a java.util.TaskQueue)
2020-03-11T10:08:10.7802663Z 	at java.lang.Object.wait(Object.java:502)
2020-03-11T10:08:10.7803143Z 	at java.util.TimerThread.mainLoop(Timer.java:526)
2020-03-11T10:08:10.7803811Z 	- locked <0x00000000a93c90a8> (a java.util.TaskQueue)
2020-03-11T10:08:10.7804326Z 	at java.util.TimerThread.run(Timer.java:505)
2020-03-11T10:08:10.7804653Z 
2020-03-11T10:08:10.7805430Z ""pool-2-thread-1"" #32 prio=5 os_prio=0 tid=0x00007f583800d800 nid=0x4e23 runnable [0x00007f5848a64000]
2020-03-11T10:08:10.7805995Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7806446Z 	at java.lang.Throwable.fillInStackTrace(Native Method)
2020-03-11T10:08:10.7807976Z 	at java.lang.Throwable.fillInStackTrace(Throwable.java:784)
2020-03-11T10:08:10.7808840Z 	- locked <0x00000000fff5b0f0> (a java.lang.Throwable)
2020-03-11T10:08:10.7809385Z 	at java.lang.Throwable.<init>(Throwable.java:251)
2020-03-11T10:08:10.7810258Z 	at org.mockito.internal.debugging.LocationImpl.<init>(LocationImpl.java:25)
2020-03-11T10:08:10.7811728Z 	at org.mockito.internal.debugging.LocationImpl.<init>(LocationImpl.java:21)
2020-03-11T10:08:10.7813539Z 	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49)
2020-03-11T10:08:10.7814435Z 	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptAbstract(MockMethodInterceptor.java:128)
2020-03-11T10:08:10.7816431Z 	at org.apache.flink.table.client.gateway.Executor$MockitoMock$1843877484.retrieveResultChanges(Unknown Source)
2020-03-11T10:08:10.7817235Z 	at org.apache.flink.table.client.cli.CliTableauResultView.printStreamResults(CliTableauResultView.java:205)
2020-03-11T10:08:10.7818038Z 	at org.apache.flink.table.client.cli.CliTableauResultView.lambda$displayStreamResults$0(CliTableauResultView.java:86)
2020-03-11T10:08:10.7834477Z 	at org.apache.flink.table.client.cli.CliTableauResultView$$Lambda$216/464634012.run(Unknown Source)
2020-03-11T10:08:10.7835069Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-03-11T10:08:10.7835546Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-03-11T10:08:10.7836050Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-03-11T10:08:10.7836596Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-03-11T10:08:10.7837066Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:10.7837279Z 
2020-03-11T10:08:10.7838127Z ""surefire-forkedjvm-command-thread"" #9 daemon prio=5 os_prio=0 tid=0x00007f58782c3800 nid=0x4dfc runnable [0x00007f584a650000]
2020-03-11T10:08:10.7838615Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7839168Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-03-11T10:08:10.7839574Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-03-11T10:08:10.7840020Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-03-11T10:08:10.7840502Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-03-11T10:08:10.7841181Z 	- locked <0x000000008002cee0> (a java.io.BufferedInputStream)
2020-03-11T10:08:10.7841616Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-03-11T10:08:10.7842134Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-03-11T10:08:10.7842861Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-03-11T10:08:10.7843340Z 	at java.lang.Thread.run(Thread.java:748)
2020-03-11T10:08:10.7843557Z 
2020-03-11T10:08:10.7843915Z ""Service Thread"" #8 daemon prio=9 os_prio=0 tid=0x00007f5878202800 nid=0x4df9 runnable [0x0000000000000000]
2020-03-11T10:08:10.7844375Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7844575Z 
2020-03-11T10:08:10.7844976Z ""C1 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f58781ff800 nid=0x4df8 waiting on condition [0x0000000000000000]
2020-03-11T10:08:10.7845443Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7845658Z 
2020-03-11T10:08:10.7846041Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f58781fd000 nid=0x4df5 waiting on condition [0x0000000000000000]
2020-03-11T10:08:10.7846549Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7846744Z 
2020-03-11T10:08:10.7847119Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f58781fb000 nid=0x4df4 runnable [0x0000000000000000]
2020-03-11T10:08:10.7847560Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7847774Z 
2020-03-11T10:08:10.7848197Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f58781f9800 nid=0x4df2 waiting on condition [0x0000000000000000]
2020-03-11T10:08:10.7848717Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7848913Z 
2020-03-11T10:08:10.7849291Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f58781c9800 nid=0x4def in Object.wait() [0x00007f584b335000]
2020-03-11T10:08:10.7849771Z    java.lang.Thread.State: WAITING (on object monitor)
2020-03-11T10:08:10.7850127Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:10.7850503Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-03-11T10:08:10.7851197Z 	- locked <0x0000000080034030> (a java.lang.ref.ReferenceQueue$Lock)
2020-03-11T10:08:10.7851652Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-03-11T10:08:10.7852120Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-03-11T10:08:10.7852397Z 
2020-03-11T10:08:10.7852791Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f58781c5000 nid=0x4dee in Object.wait() [0x00007f584b436000]
2020-03-11T10:08:10.7853306Z    java.lang.Thread.State: WAITING (on object monitor)
2020-03-11T10:08:10.7853652Z 	at java.lang.Object.wait(Native Method)
2020-03-11T10:08:10.7853984Z 	at java.lang.Object.wait(Object.java:502)
2020-03-11T10:08:10.7854386Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-03-11T10:08:10.7855025Z 	- locked <0x0000000080034020> (a java.lang.ref.Reference$Lock)
2020-03-11T10:08:10.7855460Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-03-11T10:08:10.7855759Z 
2020-03-11T10:08:10.7856081Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f587800b800 nid=0x4dd9 runnable [0x00007f587e67f000]
2020-03-11T10:08:10.7856498Z    java.lang.Thread.State: RUNNABLE
2020-03-11T10:08:10.7856818Z 	at java.io.FileDescriptor.sync(Native Method)
2020-03-11T10:08:10.7857199Z 	at org.apache.derby.impl.io.DirRandomAccessFile.sync(Unknown Source)
2020-03-11T10:08:10.7857654Z 	at org.apache.derby.impl.store.raw.data.RAFContainer.writeRAFHeader(Unknown Source)
2020-03-11T10:08:10.7858105Z 	at org.apache.derby.impl.store.raw.data.RAFContainer.clean(Unknown Source)
2020-03-11T10:08:10.7858911Z 	- locked <0x00000000fdff7a88> (a org.apache.derby.impl.store.raw.data.RAFContainer4)
2020-03-11T10:08:10.7859408Z 	at org.apache.derby.impl.services.cache.ConcurrentCache.cleanAndUnkeepEntry(Unknown Source)
2020-03-11T10:08:10.7859904Z 	at org.apache.derby.impl.services.cache.ConcurrentCache.cleanCache(Unknown Source)
2020-03-11T10:08:10.7860379Z 	at org.apache.derby.impl.services.cache.ConcurrentCache.cleanAll(Unknown Source)
2020-03-11T10:08:10.7860847Z 	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.checkpoint(Unknown Source)
2020-03-11T10:08:10.7861350Z 	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.createFinished(Unknown Source)
2020-03-11T10:08:10.7861901Z 	at org.apache.derby.impl.store.raw.RawStore.createFinished(Unknown Source)
2020-03-11T10:08:10.7862369Z 	at org.apache.derby.impl.store.access.RAMAccessManager.createFinished(Unknown Source)
2020-03-11T10:08:10.7862817Z 	at org.apache.derby.impl.db.BasicDatabase.createFinished(Unknown Source)
2020-03-11T10:08:10.7863247Z 	at org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)
2020-03-11T10:08:10.7863676Z 	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
2020-03-11T10:08:10.7864117Z 	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
2020-03-11T10:08:10.7864588Z 	at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)
2020-03-11T10:08:10.7865069Z 	at org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)
2020-03-11T10:08:10.7865582Z 	at org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)
2020-03-11T10:08:10.7866066Z 	at org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)
2020-03-11T10:08:10.7866491Z 	at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)
2020-03-11T10:08:10.7866924Z 	at org.apache.derby.impl.jdbc.EmbedConnection40.<init>(Unknown Source)
2020-03-11T10:08:10.7867340Z 	at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)
2020-03-11T10:08:10.7867766Z 	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)
2020-03-11T10:08:10.7868143Z 	at org.apache.derby.jdbc.Driver20.connect(Unknown Source)
2020-03-11T10:08:10.7868545Z 	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)
2020-03-11T10:08:10.7868986Z 	at java.sql.DriverManager.getConnection(DriverManager.java:664)
2020-03-11T10:08:10.7869435Z 	at java.sql.DriverManager.getConnection(DriverManager.java:208)
2020-03-11T10:08:10.7870096Z 	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:349)
2020-03-11T10:08:10.7870550Z 	at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
2020-03-11T10:08:10.7871020Z 	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
2020-03-11T10:08:10.7871744Z 	- locked <0x00000000fdd2d658> (a com.jolbox.bonecp.BoneCPDataSource)
2020-03-11T10:08:10.7872334Z 	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)
2020-03-11T10:08:10.7872989Z 	at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:297)
2020-03-11T10:08:10.7873477Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-03-11T10:08:10.7874012Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-03-11T10:08:10.7874639Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-03-11T10:08:10.7875216Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-03-11T10:08:10.7875810Z 	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)
2020-03-11T10:08:10.7876432Z 	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
2020-03-11T10:08:10.7877049Z 	at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)
2020-03-11T10:08:10.7877777Z 	at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:422)
2020-03-11T10:08:10.7878573Z 	- locked <0x00000000fa9fe878> (a org.datanucleus.PersistenceNucleusContextImpl)
2020-03-11T10:08:10.7879171Z 	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)
2020-03-11T10:08:10.7879963Z 	- locked <0x00000000fa9fe790> (a org.datanucleus.api.jdo.JDOPersistenceManagerFactory)
2020-03-11T10:08:10.7880597Z 	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)
2020-03-11T10:08:10.7881441Z 	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)
2020-03-11T10:08:10.7882353Z 	- locked <0x00000000a152a3b0> (a java.lang.Class for org.datanucleus.api.jdo.JDOPersistenceManagerFactory)
2020-03-11T10:08:10.7882851Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-11T10:08:10.7883320Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-11T10:08:10.7883888Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-11T10:08:10.7884369Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-11T10:08:10.7884779Z 	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
2020-03-11T10:08:10.7886262Z 	at java.security.AccessController.doPrivileged(Native Method)
2020-03-11T10:08:10.7886674Z 	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
2020-03-11T10:08:10.7887181Z 	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
2020-03-11T10:08:10.7887734Z 	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
2020-03-11T10:08:10.7888267Z 	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
2020-03-11T10:08:10.7888768Z 	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:519)
2020-03-11T10:08:10.7889622Z 	- locked <0x00000000a152d260> (a java.lang.Class for org.apache.hadoop.hive.metastore.ObjectStore)
2020-03-11T10:08:10.7890204Z 	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:548)
2020-03-11T10:08:10.7890777Z 	at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:403)
2020-03-11T10:08:10.7891340Z 	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:340)
2020-03-11T10:08:10.7891864Z 	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:301)
2020-03-11T10:08:10.7892332Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-11T10:08:10.7892800Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-11T10:08:10.7893362Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-11T10:08:10.7893861Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-11T10:08:10.7894323Z 	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:101)
2020-03-11T10:08:10.7894777Z 	at com.sun.proxy.$Proxy58.setConf(Unknown Source)
2020-03-11T10:08:10.7895241Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.setConf(HiveMetaStore.java:521)
2020-03-11T10:08:10.7895839Z 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:68)
2020-03-11T10:08:10.7896431Z 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)
2020-03-11T10:08:10.7897024Z 	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6891)
2020-03-11T10:08:10.7897642Z 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:164)
2020-03-11T10:08:10.7898148Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-03-11T10:08:10.7898683Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-03-11T10:08:10.7899453Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-03-11T10:08:10.7900009Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-03-11T10:08:10.7900544Z 	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1706)
2020-03-11T10:08:10.7901138Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)
2020-03-11T10:08:10.7901781Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)
2020-03-11T10:08:10.7902408Z 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:89)
2020-03-11T10:08:10.7903002Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-11T10:08:10.7903479Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-11T10:08:10.7904029Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-11T10:08:10.7904527Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-11T10:08:10.7905048Z 	at org.apache.flink.table.catalog.hive.client.HiveShimV230.getHiveMetastoreClient(HiveShimV230.java:50)
2020-03-11T10:08:10.7905775Z 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.createMetastoreClient(HiveMetastoreClientWrapper.java:240)
2020-03-11T10:08:10.7906529Z 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientWrapper.<init>(HiveMetastoreClientWrapper.java:71)
2020-03-11T10:08:10.7907226Z 	at org.apache.flink.table.catalog.hive.client.HiveMetastoreClientFactory.create(HiveMetastoreClientFactory.java:35)
2020-03-11T10:08:10.7907849Z 	at org.apache.flink.table.catalog.hive.HiveCatalog.open(HiveCatalog.java:194)
2020-03-11T10:08:10.7931586Z 	at org.apache.flink.table.client.gateway.local.DependencyTest$TestHiveCatalogFactory.createCatalog(DependencyTest.java:247)
2020-03-11T10:08:10.7932355Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.createCatalog(ExecutionContext.java:377)
2020-03-11T10:08:10.7933016Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$null$4(ExecutionContext.java:573)
2020-03-11T10:08:10.7933623Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext$$Lambda$337/745465688.accept(Unknown Source)
2020-03-11T10:08:10.7934114Z 	at java.util.HashMap.forEach(HashMap.java:1289)
2020-03-11T10:08:10.7934659Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.lambda$initializeCatalogs$5(ExecutionContext.java:572)
2020-03-11T10:08:10.7935312Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext$$Lambda$336/2009591182.run(Unknown Source)
2020-03-11T10:08:10.7935931Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:246)
2020-03-11T10:08:10.7936587Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeCatalogs(ExecutionContext.java:571)
2020-03-11T10:08:10.7937288Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.initializeTableEnvironment(ExecutionContext.java:520)
2020-03-11T10:08:10.7937951Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:165)
2020-03-11T10:08:10.7938578Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext.<init>(ExecutionContext.java:122)
2020-03-11T10:08:10.7939217Z 	at org.apache.flink.table.client.gateway.local.ExecutionContext$Builder.build(ExecutionContext.java:768)
2020-03-11T10:08:10.7939900Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createExecutionContext(ExecutionContextTest.java:299)
2020-03-11T10:08:10.7940655Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.createCatalogExecutionContext(ExecutionContextTest.java:335)
2020-03-11T10:08:10.7941383Z 	at org.apache.flink.table.client.gateway.local.ExecutionContextTest.testCatalogs(ExecutionContextTest.java:116)
2020-03-11T10:08:10.7941915Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-03-11T10:08:10.7942561Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-03-11T10:08:10.7943115Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-03-11T10:08:10.7943613Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-03-11T10:08:10.7944094Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-03-11T10:08:10.7944675Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-03-11T10:08:10.7945243Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-03-11T10:08:10.7945873Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-03-11T10:08:10.7946391Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-03-11T10:08:10.7946891Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-03-11T10:08:10.7947459Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-03-11T10:08:10.7947965Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-03-11T10:08:10.7948420Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-03-11T10:08:10.7948908Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-03-11T10:08:10.7949376Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-03-11T10:08:10.7950164Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-03-11T10:08:10.7950627Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-03-11T10:08:10.7951144Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-03-11T10:08:10.7951730Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-03-11T10:08:10.7952320Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-03-11T10:08:10.7952906Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-03-11T10:08:10.7953500Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-03-11T10:08:10.7954126Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-03-11T10:08:10.7993200Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-03-11T10:08:10.7993778Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-03-11T10:08:10.7994084Z 
2020-03-11T10:08:10.7994379Z ""VM Thread"" os_prio=0 tid=0x00007f58781bb800 nid=0x4dea runnable 
2020-03-11T10:08:10.7994647Z 
2020-03-11T10:08:10.7994985Z ""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007f5878020000 nid=0x4dda runnable 
2020-03-11T10:08:10.7995280Z 
2020-03-11T10:08:10.7995612Z ""Gang worker#1 (Parallel GC Threads)"" os_prio=0 tid=0x00007f5878022000 nid=0x4dde runnable 
2020-03-11T10:08:10.7995909Z 
2020-03-11T10:08:10.7996239Z ""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007f5878046000 nid=0x4de8 runnable 
2020-03-11T10:08:10.7996527Z 
2020-03-11T10:08:10.7996881Z ""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f5878047800 nid=0x4de9 runnable 
2020-03-11T10:08:10.7997191Z 
2020-03-11T10:08:10.7997508Z ""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007f5878028000 nid=0x4de5 runnable 
2020-03-11T10:08:10.7997819Z 
2020-03-11T10:08:10.7998134Z ""G1 Concurrent Refinement Thread#1"" os_prio=0 tid=0x00007f5878026000 nid=0x4de3 runnable 
2020-03-11T10:08:10.7998436Z 
2020-03-11T10:08:10.7998755Z ""G1 Concurrent Refinement Thread#2"" os_prio=0 tid=0x00007f5878024800 nid=0x4de0 runnable 
2020-03-11T10:08:10.7999059Z 
2020-03-11T10:08:10.7999380Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007f5878205800 nid=0x4dfa waiting on condition 
2020-03-11T10:08:10.7999674Z 
2020-03-11T10:08:10.7999892Z JNI global references: 1396
2020-03-11T10:08:10.8000118Z 
2020-03-11T10:08:10.8000721Z ==============================================================================
2020-03-11T10:08:10.8001098Z Printing stack trace of Java process 20301
2020-03-11T10:08:10.8001487Z ==============================================================================
2020-03-11T10:08:10.9228038Z 20301: No such process
2020-03-11T10:08:11.2856341Z MVN exited with EXIT CODE: 143.
{code}"	FLINK	Closed	3	1	8669	test-stability
13326005	"MetricsAvailabilityITCase.testReporter failed with ""Could not satisfy the predicate within the allowed time"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=6179&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a]

{code}
2020-09-03T23:33:18.3687261Z [ERROR] testReporter(org.apache.flink.metrics.tests.MetricsAvailabilityITCase)  Time elapsed: 15.217 s  <<< ERROR!
2020-09-03T23:33:18.3698260Z java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not satisfy the predicate within the allowed time.
2020-09-03T23:33:18.3698749Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-09-03T23:33:18.3699163Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-09-03T23:33:18.3699754Z 	at org.apache.flink.metrics.tests.MetricsAvailabilityITCase.fetchMetric(MetricsAvailabilityITCase.java:162)
2020-09-03T23:33:18.3700234Z 	at org.apache.flink.metrics.tests.MetricsAvailabilityITCase.checkJobManagerMetricAvailability(MetricsAvailabilityITCase.java:116)
2020-09-03T23:33:18.3700726Z 	at org.apache.flink.metrics.tests.MetricsAvailabilityITCase.testReporter(MetricsAvailabilityITCase.java:101)
2020-09-03T23:33:18.3701097Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-09-03T23:33:18.3701425Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-09-03T23:33:18.3701798Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-09-03T23:33:18.3702146Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-09-03T23:33:18.3702471Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-09-03T23:33:18.3702866Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-09-03T23:33:18.3703253Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-09-03T23:33:18.3703621Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-09-03T23:33:18.3703997Z 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-09-03T23:33:18.3704339Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2020-09-03T23:33:18.3704629Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-09-03T23:33:18.3704940Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-09-03T23:33:18.3705354Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-09-03T23:33:18.3705725Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-09-03T23:33:18.3706072Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-03T23:33:18.3706397Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-03T23:33:18.3706714Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-03T23:33:18.3707044Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-03T23:33:18.3707373Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-03T23:33:18.3707708Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-09-03T23:33:18.3708073Z 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
2020-09-03T23:33:18.3708410Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-03T23:33:18.3708691Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-09-03T23:33:18.3708976Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-09-03T23:33:18.3709273Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-09-03T23:33:18.3709579Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-09-03T23:33:18.3709910Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-09-03T23:33:18.3710242Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-09-03T23:33:18.3710554Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-09-03T23:33:18.3710875Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-09-03T23:33:18.3711203Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2020-09-03T23:33:18.3711585Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2020-09-03T23:33:18.3712015Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2020-09-03T23:33:18.3712428Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2020-09-03T23:33:18.3712814Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2020-09-03T23:33:18.3713251Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2020-09-03T23:33:18.3713675Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-09-03T23:33:18.3714085Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-09-03T23:33:18.3714469Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-09-03T23:33:18.3714833Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-09-03T23:33:18.3715243Z Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not satisfy the predicate within the allowed time.
2020-09-03T23:33:18.3715729Z 	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retrySuccessfulOperationWithDelay$12(FutureUtils.java:382)
2020-09-03T23:33:18.3716161Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-09-03T23:33:18.3716546Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-09-03T23:33:18.3716940Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-09-03T23:33:18.3717311Z 	at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:575)
2020-09-03T23:33:18.3717674Z 	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:943)
2020-09-03T23:33:18.3718062Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-09-03T23:33:18.3718431Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-09-03T23:33:18.3718749Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-09-03T23:33:18.3719191Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
2020-09-03T23:33:18.3719679Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
2020-09-03T23:33:18.3720093Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-09-03T23:33:18.3720472Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-09-03T23:33:18.3720782Z 	at java.lang.Thread.run(Thread.java:748)
{code}"	FLINK	Closed	3	1	8669	test-stability
13356476	Add DeclarativeScheduler / Finished state	"This subtask of adding the declarative scheduler is about adding the Finished state to Flink, including tests.

Finished: The job execution has been completed.
"	FLINK	Closed	3	7	8669	pull-request-available
12719785	Replace Avro serialization by Kryo 	"

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/610
Created by: [rmetzger|https://github.com/rmetzger]
Labels: java api, 
Milestone: Release 0.6 (unplanned)
Created at: Tue Mar 18 17:29:28 CET 2014
State: open
"	FLINK	Resolved	2	1	8669	github-import
13340526	"SQLClientKafkaITCase.testKafka failed with ""Did not get expected results before timeout."""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9558&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
2020-11-13T13:47:21.6002762Z Nov 13 13:47:21 [ERROR] testKafka[0: kafka-version:2.4.1 kafka-sql-version:universal](org.apache.flink.tests.util.kafka.SQLClientKafkaITCase) Time elapsed: 183.015 s <<< FAILURE! 2020-11-13T13:47:21.6003744Z Nov 13 13:47:21 java.lang.AssertionError: Did not get expected results before timeout. 2020-11-13T13:47:21.6004745Z Nov 13 13:47:21 at org.junit.Assert.fail(Assert.java:88) 2020-11-13T13:47:21.6005325Z Nov 13 13:47:21 at org.junit.Assert.assertTrue(Assert.java:41) 2020-11-13T13:47:21.6006007Z Nov 13 13:47:21 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.checkCsvResultFile(SQLClientKafkaITCase.java:226) 2020-11-13T13:47:21.6007091Z Nov 13 13:47:21 at org.apache.flink.tests.util.kafka.SQLClientKafkaITCase.testKafka(SQLClientKafkaITCase.java:166)
{code}"	FLINK	Closed	1	1	8669	test-stability
13337781	"YARN tests failed with ""java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8541&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=e7f339b2-a7c3-57d9-00af-3712d4b15354

{code}
2020-10-28T22:58:39.4927767Z 2020-10-28 22:57:33,866 main ERROR Could not create plugin of type class org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy for element DefaultRolloverStrategy: java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}"" java.lang.NumberFormatException: For input string: ""${env:MAX_LOG_FILE_NUMBER}""
2020-10-28T22:58:39.4929252Z 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
2020-10-28T22:58:39.4929823Z 	at java.lang.Integer.parseInt(Integer.java:569)
2020-10-28T22:58:39.4930327Z 	at java.lang.Integer.parseInt(Integer.java:615)
2020-10-28T22:58:39.4931047Z 	at org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy$Builder.build(DefaultRolloverStrategy.java:137)
2020-10-28T22:58:39.4931866Z 	at org.apache.logging.log4j.core.appender.rolling.DefaultRolloverStrategy$Builder.build(DefaultRolloverStrategy.java:90)
2020-10-28T22:58:39.4932720Z 	at org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:122)
2020-10-28T22:58:39.4933446Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:1002)
2020-10-28T22:58:39.4934275Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:942)
2020-10-28T22:58:39.4935029Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:934)
2020-10-28T22:58:39.4935837Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:934)
2020-10-28T22:58:39.4936605Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:552)
2020-10-28T22:58:39.4937573Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:241)
2020-10-28T22:58:39.4938429Z 	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:288)
2020-10-28T22:58:39.4939206Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:579)
2020-10-28T22:58:39.4939885Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-10-28T22:58:39.4940490Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-10-28T22:58:39.4941087Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-10-28T22:58:39.4941733Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-10-28T22:58:39.4942534Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-10-28T22:58:39.4943154Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-10-28T22:58:39.4943820Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-10-28T22:58:39.4944540Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-10-28T22:58:39.4945199Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-10-28T22:58:39.4945858Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-10-28T22:58:39.4946426Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-10-28T22:58:39.4946965Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:349)
2020-10-28T22:58:39.4947698Z 	at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.<clinit>(ClusterEntrypoint.java:108)
{code}"	FLINK	Closed	1	1	8669	pull-request-available, test-stability
13311390	Streaming File Sink s3 end-to-end test stalls	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3444&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2020-06-12T21:55:57.6277963Z Number of produced values 10870/60000
2020-06-12T21:57:10.5467073Z Number of produced values 22960/60000
2020-06-12T21:58:01.0025226Z Number of produced values 59650/60000
2020-06-12T21:58:52.5624619Z Number of produced values 60000/60000
2020-06-12T21:58:53.2407133Z Cancelling job 9412dcb358631ab461a3a1e851417b9e.
2020-06-12T21:58:54.0819168Z Cancelled job 9412dcb358631ab461a3a1e851417b9e.
2020-06-12T21:58:54.1097745Z Waiting for job (9412dcb358631ab461a3a1e851417b9e) to reach terminal state CANCELED ...
2020-06-13T00:00:35.0502923Z ##[error]The operation was canceled.
2020-06-13T00:00:35.0522780Z ##[section]Finishing: Run e2e tests
{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13568720	TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding failed	"https://github.com/XComp/flink/actions/runs/7927275243/job/21643615491#step:10:9880

{code}
Error: 07:48:06 07:48:06.643 [ERROR] Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.309 s <<< FAILURE! -- in org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest
Error: 07:48:06 07:48:06.646 [ERROR] org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding -- Time elapsed: 0.036 s <<< FAILURE!
Feb 16 07:48:06 Output was not correct.: array lengths differed, expected.length=8 actual.length=7; arrays first differed at element [6]; expected:<Watermark @ 5> but was:<Watermark @ 6>
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:78)
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:28)
Feb 16 07:48:06 	at org.junit.Assert.internalArrayEquals(Assert.java:534)
Feb 16 07:48:06 	at org.junit.Assert.assertArrayEquals(Assert.java:285)
Feb 16 07:48:06 	at org.apache.flink.streaming.util.TestHarnessUtil.assertOutputEquals(TestHarnessUtil.java:59)
Feb 16 07:48:06 	at org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTest.testWatermarkAndWatermarkStatusForwarding(TwoInputStreamTaskTest.java:248)
Feb 16 07:48:06 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 16 07:48:06 Caused by: java.lang.AssertionError: expected:<Watermark @ 5> but was:<Watermark @ 6>
Feb 16 07:48:06 	at org.junit.Assert.fail(Assert.java:89)
Feb 16 07:48:06 	at org.junit.Assert.failNotEquals(Assert.java:835)
Feb 16 07:48:06 	at org.junit.Assert.assertEquals(Assert.java:120)
Feb 16 07:48:06 	at org.junit.Assert.assertEquals(Assert.java:146)
Feb 16 07:48:06 	at org.junit.internal.ExactComparisonCriteria.assertElementsEqual(ExactComparisonCriteria.java:8)
Feb 16 07:48:06 	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:76)
Feb 16 07:48:06 	... 6 more
{code}

I couldn't reproduce it locally with 20000 runs."	FLINK	Open	2	1	8669	github-actions, test-stability
13211305	Unable to build docs in Docker image	"Running 
{code:java}
cd flink/docs/docker
./run.sh{code}
 

And then in the container
{code:java}
Welcome to Apache Flink docs
To build, execute
./build_docs.sh
To watch and regenerate automatically
./build_docs.sh -p
and access http://localhost:4000

bash-4.4$ ./build_docs.sh -p
Traceback (most recent call last):
2: from /usr/local/bin/bundle:23:in `<main>'
1: from /usr/share/rubygems/rubygems.rb:308:in `activate_bin_path'
/usr/share/rubygems/rubygems.rb:289:in `find_spec_for_exe': can't find gem bundler (>= 0.a) with executable bundle (Gem::GemNotFoundException){code}
I believe there's something wrong.

 "	FLINK	Closed	3	1	8669	pull-request-available
13235130	Describe new contribution process	"The community has decided to change the contribution process to seek consensus in Jira first.

Update the website to reflect this change."	FLINK	Resolved	3	4	8669	pull-request-available
13309727	YARN session logs about HADOOP_CONF_DIR even though HADOOP_CLASSPATH containing a config is set	"Flink prints 
{code}
Setting HADOOP_CONF_DIR=/etc/hadoop/conf because no HADOOP_CONF_DIR was set.
{code}

When running Flink on YARN with the HADOOP_CLASSPATH set. ""hadoop classpath"" also returns the configuration directory, so HADOOP_CONF_DIR is not needed anymore.

I suggest to revisit this log message / behavior as it is misleading"	FLINK	Closed	3	4	8669	pull-request-available, usability
13376787	Adaptive Scheduler: Can not cancel restarting job	"I have a job in state RESTARTING. When I now issue a cancel RPC call, I get the following exception:

Relevant trace:
{code}
Caused by: java.lang.IllegalStateException: Assuming running execution graph 
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
 at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.(StateWithExecutionGraph.java:94) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling.(Canceling.java:41) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:98) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:72) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1128) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCanceling(AdaptiveScheduler.java:802) 
 at org.apache.flink.runtime.scheduler.adaptive.Restarting.cancel(Restarting.java:74) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.cancel(AdaptiveScheduler.java:453) 
 at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:417) 
{code}

Full trace as reported in the UI:
{code}
org.apache.flink.runtime.rest.handler.RestHandlerException: Job cancellation failed: Cancellation failed. 
 at org.apache.flink.runtime.rest.handler.job.JobCancellationHandler.lambda$handleRequest$0(JobCancellationHandler.java:127) 
 at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) 
 at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) 
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) 
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079) 
 at akka.dispatch.OnComplete.internal(Future.scala:263) 
 at akka.dispatch.OnComplete.internal(Future.scala:261) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) 
 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) 
 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:573) 
 at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:23) 
 at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21) 
 at scala.concurrent.Future.$anonfun$andThen$1(Future.scala:532) 
 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29) 
 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) 
 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91) 
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) 
 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81) 
 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91) 
 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) 
 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44) 
 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 
 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) 
 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 
 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: org.apache.flink.util.FlinkException: Cancellation failed. 
 at org.apache.flink.runtime.jobmaster.JobMasterServiceLeadershipRunner.lambda$cancel$3(JobMasterServiceLeadershipRunner.java:197) 
 at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884) 
 at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) 
 at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) 
 at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) 
 at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) 
 at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) 
 at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1079) 
 at akka.dispatch.OnComplete.internal(Future.scala:263) 
 at akka.dispatch.OnComplete.internal(Future.scala:261) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) 
 at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) 
 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60) 
 at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) 
 at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:68) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:284) 
 at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:284) 
 at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:573) 
 at akka.actor.ActorRef.tell(ActorRef.scala:126) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:311) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) 
 at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) 
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) 
 at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) 
 at scala.PartialFunction.applyOrElse(PartialFunction.scala:123) 
 at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122) 
 at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172) 
 at akka.actor.Actor.aroundReceive(Actor.scala:517) 
 at akka.actor.Actor.aroundReceive$(Actor.scala:515) 
 at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) 
 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) 
 at akka.actor.ActorCell.invoke(ActorCell.scala:561) 
 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) 
 at akka.dispatch.Mailbox.run(Mailbox.scala:225) 
 at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ... 4 more Caused by: java.lang.IllegalStateException: Assuming running execution graph 
 at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193) 
 at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.(StateWithExecutionGraph.java:94) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling.(Canceling.java:41) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:98) 
 at org.apache.flink.runtime.scheduler.adaptive.Canceling$Factory.getState(Canceling.java:72) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.transitionToState(AdaptiveScheduler.java:1128) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.goToCanceling(AdaptiveScheduler.java:802) 
 at org.apache.flink.runtime.scheduler.adaptive.Restarting.cancel(Restarting.java:74) 
 at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.cancel(AdaptiveScheduler.java:453) 
 at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:417) 
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
 at java.lang.reflect.Method.invoke(Method.java:498) 
 at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) ... 23 more
{code}
(Sorry for the poor stack trace formatting, I coped the stack trace from the UI)

It seems that the Canceling state assumes we only transition into it from Executing (ExecutionGraph = RUNNING).

In my opinion a job should be cancellable at all times, for example when a job is stuck in a restart loop, cancelling is the only way out (unless retries are exhausted)."	FLINK	Closed	2	1	8669	pull-request-available
13293813	the java e2e profile isn't setting the hadoop switch on Azure	"Context: https://lists.apache.org/thread.html/r06e597b3dadfee00593989b1cfae0f2b83548f412c8fdca6d4bc3dbe%40%3Cdev.flink.apache.org%3E

{quote}- the azure setup doesn't appear to be equivalent yet since the java e2e profile isn't setting the hadoop switch (-Pe2e-hadoop), as a result of which SQLClientKafkaITCase isn't run
{quote}"	FLINK	Resolved	3	4	8669	pull-request-available
13289537	test_ha_per_job_cluster_datastream.sh gets stuck	"This was seen in https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5905&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=e7804547-1789-5225-2bcf-269eeaa37447 ... the relevant part of the logs is here:

{code}
2020-03-04T11:27:25.4819486Z ==============================================================================
2020-03-04T11:27:25.4820470Z Running 'Running HA per-job cluster (rocks, non-incremental) end-to-end test'
2020-03-04T11:27:25.4820922Z ==============================================================================
2020-03-04T11:27:25.4840177Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-25482960156
2020-03-04T11:27:25.6712478Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-04T11:27:25.6830402Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT
2020-03-04T11:27:26.2988914Z Starting zookeeper daemon on host fv-az655.
2020-03-04T11:27:26.3001237Z Running on HA mode: parallelism=4, backend=rocks, asyncSnapshots=true, and incremSnapshots=false.
2020-03-04T11:27:27.4206924Z Starting standalonejob daemon on host fv-az655.
2020-03-04T11:27:27.4217066Z Start 1 more task managers
2020-03-04T11:27:30.8412541Z Starting taskexecutor daemon on host fv-az655.
2020-03-04T11:27:38.1779980Z Job (00000000000000000000000000000000) is running.
2020-03-04T11:27:38.1781375Z Running JM watchdog @ 89778
2020-03-04T11:27:38.1781858Z Running TM watchdog @ 89779
2020-03-04T11:27:38.1783272Z Waiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...
2020-03-04T13:21:29.9076797Z ##[error]The operation was canceled.
2020-03-04T13:21:29.9094090Z ##[section]Finishing: Run e2e tests
{code}

The last three lines indicate that the test is waiting forever for a checkpoint to appear."	FLINK	Closed	1	1	8669	pull-request-available, test-stability
13304855	Remove flink-shaded-hadoop-2-parent and submodules	Since Flink does not use flink-shaded-hadoop-2 anymore, we want to remove it from flink-shaded.	FLINK	Closed	3	7	8669	pull-request-available
13232228	Bump required Maven version to 3.1.1 (from 3.0.3)	"See https://lists.apache.org/thread.html/57dec7c338eb95247b7a05ded371f4a78420a964045ea9557d501c3f@%3Cdev.flink.apache.org%3E 

The frontend-maven-plugin requires at least Maven 3.1.0.
I propose to bump the required Maven version to 3.1.1.
"	FLINK	Closed	3	4	8669	pull-request-available
13240859	YARNSessionCapacitySchedulerITCase failed due to non prohibited exception	"YARNSessionCapacitySchedulerITCase fails due to non prohibited exception.

[https://api.travis-ci.org/v3/job/548491542/log.txt]
{code:java}
2019-06-21 08:22:27,313 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (SUM(1), at main(WordCount.java:79) (2/2) (a1708bb0544633b4e57e8bb84a1a48f3) switched from RUNNING to FAILED.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeNoLongerUsedSlots(TaskExecutor.java:1475)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.syncSlotsWithSnapshotFromJobMaster(TaskExecutor.java:1436)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$3200(TaskExecutor.java:141)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobManagerHeartbeatListener.lambda$reportPayload$1(TaskExecutor.java:1691)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2019-06-21 08:22:27,333 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Flink Java Job at Fri Jun 21 08:22:16 UTC 2019 (8f8dced4fb89f8e5cb05d9286683ecaf) switched from state RUNNING to FAILING.
org.apache.flink.util.FlinkException: 0283de7d26d7fb08895955bfb75db496 is no longer allocated by job 8f8dced4fb89f8e5cb05d9286683ecaf.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.freeNoLongerUsedSlots(TaskExecutor.java:1475)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.syncSlotsWithSnapshotFromJobMaster(TaskExecutor.java:1436)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.access$3200(TaskExecutor.java:141)
	at org.apache.flink.runtime.taskexecutor.TaskExecutor$JobManagerHeartbeatListener.lambda$reportPayload$1(TaskExecutor.java:1691)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21){code}"	FLINK	Closed	2	1	8669	test-stability
13337178	Automatically run a basic NOTICE file check on CI 	"For every release, we are manually validating the NOTICE files, according to this wiki page: https://cwiki.apache.org/confluence/display/FLINK/Licensing

The most time-consuming task is ensuring that all modules that deploy a shaded dependency to maven central are properly documenting this dependency in their NOTICE file.

I would like to add a tool to Flink that checks if all shaded dependencies are at least mentioned in the NOTICE file.
We will still need to perform a manual checks, but the tool should catch the most common, severe and difficult to find problems."	FLINK	Closed	3	4	8669	pull-request-available
13338922	"HBaseDynamicTableFactoryTest.testTableSourceFactory failed with ""NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9020&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51]

{code}

2020-11-04T22:30:58.3273231Z testTableSourceFactory(org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest) Time elapsed: 0.894 sec <<< ERROR! 2020-11-04T22:30:58.3274285Z org.apache.flink.table.api.ValidationException: 2020-11-04T22:30:58.3274931Z Unable to create a source for reading table 'default.default.t1'. 2020-11-04T22:30:58.3275159Z 2020-11-04T22:30:58.3275364Z Table options are: 2020-11-04T22:30:58.3275523Z 2020-11-04T22:30:58.3275882Z 'connector'='hbase-1.4' 2020-11-04T22:30:58.3276283Z 'table-name'='testHBastTable' 2020-11-04T22:30:58.3276712Z 'zookeeper.quorum'='localhost:2181' 2020-11-04T22:30:58.3277158Z 'zookeeper.znode.parent'='/flink' 2020-11-04T22:30:58.3277553Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125) 2020-11-04T22:30:58.3278358Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.createTableSource(HBaseDynamicTableFactoryTest.java:332) 2020-11-04T22:30:58.3279046Z at org.apache.flink.connector.hbase1.HBaseDynamicTableFactoryTest.testTableSourceFactory(HBaseDynamicTableFactoryTest.java:104) 2020-11-04T22:30:58.3279623Z at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 2020-11-04T22:30:58.3280067Z at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 2020-11-04T22:30:58.3280703Z at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 2020-11-04T22:30:58.3281149Z at java.lang.reflect.Method.invoke(Method.java:498) 2020-11-04T22:30:58.3281615Z at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 2020-11-04T22:30:58.3282135Z at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 2020-11-04T22:30:58.3282637Z at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 2020-11-04T22:30:58.3283151Z at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 2020-11-04T22:30:58.3283749Z at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239) 2020-11-04T22:30:58.3284221Z at org.junit.rules.RunRules.evaluate(RunRules.java:20) 2020-11-04T22:30:58.3284624Z at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 2020-11-04T22:30:58.3285091Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 2020-11-04T22:30:58.3285590Z at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 2020-11-04T22:30:58.3286053Z at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 2020-11-04T22:30:58.3286487Z at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 2020-11-04T22:30:58.3286911Z at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 2020-11-04T22:30:58.3287348Z at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 2020-11-04T22:30:58.3287779Z at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 2020-11-04T22:30:58.3288262Z at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 2020-11-04T22:30:58.3288721Z at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367) 2020-11-04T22:30:58.3289254Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274) 2020-11-04T22:30:58.3289781Z at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 2020-11-04T22:30:58.3290312Z at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161) 2020-11-04T22:30:58.3291103Z at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290) 2020-11-04T22:30:58.3291649Z at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242) 2020-11-04T22:30:58.3292158Z at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121) 2020-11-04T22:30:58.3292703Z Caused by: java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V 2020-11-04T22:30:58.3293230Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357) 2020-11-04T22:30:58.3293876Z at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338) 2020-11-04T22:30:58.3294441Z at org.apache.flink.connector.hbase1.HBase1DynamicTableFactory.createDynamicTableSource(HBase1DynamicTableFactory.java:113) 2020-11-04T22:30:58.3295032Z at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122) 2020-11-04T22:30:58.3295401Z ... 28 more
{code}"	FLINK	Closed	3	1	8669	pull-request-available, test-stability
13310147	CheckPubSubEmulatorTest failed on azure	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2930&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=2930&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=1e2bbe5b-4657-50be-1f07-d84bfce5b1f5]

 

 
{code:java}
2020-06-08T12:45:15.9874996Z 82609 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest [] - Waiting a while to receive the m
 essage...
*2020-06-08T12:45:16.1955546Z 82816 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest [] - Timeout during shutdown
*2020-06-08T12:45:16.1956405Z java.util.concurrent.TimeoutException: Timed out waiting for InnerService [STOPPING] to reach a terminal state. Current state: ST*OPPING
...
2020-06-08T12:46:08.5914230Z 135213 [main] INFO org.apache.flink.streaming.connectors.gcp.pubsub.emulator.GCloudEmulatorManager [] -
 2020-06-08T12:46:08.6054783Z [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 54.754 s <<< FAILURE! - in org.apache.flink.streaming.con nectors.gcp.pubsub.CheckPubSubEmulatorTest
 2020-06-08T12:46:08.6062906Z [ERROR] testPull(org.apache.flink.streaming.connectors.gcp.pubsub.CheckPubSubEmulatorTest) Time elapsed: 52.123 s <<< FAILURE!
 2020-06-08T12:46:08.6063659Z java.lang.AssertionError: expected:<1> but was:<0>
{code}
 "	FLINK	Closed	2	1	8669	pull-request-available
12719627	Integrate runtime metrics / statistics	"The engine should collect job execution statistics (e.g., via accumulators) such as:
- total number of input / output records per operator
- histogram of input/output ratio of UDF calls
- histogram of number of input records per reduce / cogroup UDF call
- histogram of number of output records per UDF call
- histogram of time spend in UDF calls
- number of local and remote bytes read (not via accumulators)
- ...

These stats should be made available to the user after execution (via webfrontend). The purpose of this feature is to ease performance debugging of parallel jobs (e.g., to detect data skew).
It should be possible to deactivate (or activate) the gathering of these statistics.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/456
Created by: [fhueske|https://github.com/fhueske]
Labels: enhancement, runtime, user satisfaction, 
Created at: Tue Feb 04 20:32:49 CET 2014
State: open
"	FLINK	Closed	3	2	8669	github-import
13337443	e2e test failures are not causing the build to fail	"Example: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=8385&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529

{code}
Oct 27 13:37:05 Killing JM watchdog @ 11717
Oct 27 13:37:05 Killing TM watchdog @ 11801
Oct 27 13:37:05 [FAIL] Test script contains errors.
Oct 27 13:37:05 Checking of logs skipped.
Oct 27 13:37:05 
Oct 27 13:37:05 [FAIL] 'Running HA (file, sync) end-to-end test' failed after 15 minutes and 0 seconds! Test exited with exit code 1
Oct 27 13:37:05 
13:37:05 ##[group]Environment Information
Oct 27 13:37:06 Published e2e logs into debug logs artifact:
Oct 27 13:37:06 flink-vsts-client-fv-az678.log
Oct 27 13:37:06 flink-vsts-standalonesession-0-fv-az678.log
Oct 27 13:37:06 flink-vsts-standalonesession-0-fv-az678.out
Oct 27 13:37:06 flink-vsts-taskexecutor-0-fv-az678.log
Oct 27 13:37:06 flink-vsts-taskexecutor-0-fv-az678.out
Oct 27 13:37:06 flink-vsts-zookeeper-0-fv-az678.log
Oct 27 13:37:06 flink-vsts-zookeeper-0-fv-az678.out
Oct 27 13:37:06 Searching for .dump, .dumpstream and related files in '/home/vsts/work/1/s'
Oct 27 13:37:12 COMPRESSING build artifacts.
{code}

Despite the FAIL, the stage itself is green.
"	FLINK	Closed	1	1	8669	pull-request-available
13356474	Add DeclarativeScheduler / Canceling state	"This subtask of adding the declarative scheduler is about adding the Canceling state to Flink, including tests.

Canceling: The job has been canceled by the user. The scheduler stops the ExecutionGraph by canceling it.
"	FLINK	Closed	3	7	8669	pull-request-available
13353805	Document how to use the declarative scheduler and its limitations	We need to document how to use the declarative scheduler and what its limitations are. Ideally, we already create tickets for the limitations we want to fix in the foreseeable future so that we can link them.	FLINK	Closed	3	7	8669	pull-request-available
13335617	"Local recovery and sticky scheduling end-to-end test hangs with ""Expected to find info here."""	"The reason for all these e2e test hangs recently seems to be the Local recovery and sticky scheduling end-to-end test.

It is in a restart loop with this error:
{code}
020-10-15T13:01:42.4079891Z 2020-10-15 12:54:06,099 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Flat Map -> Sink: Unnamed (1/4) (78a56f7797be1d41b0b1b31a75bd90e1_20ba6b65f97481d5570070de90e4e791_0_1) switched from RUNNING to FAILED on org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot@65b70d8d.
2020-10-15T13:01:42.4080637Z java.lang.NullPointerException: Expected to find info here.
2020-10-15T13:01:42.4081365Z 	at org.apache.flink.util.Preconditions.checkNotNull(Preconditions.java:78) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4082067Z 	at org.apache.flink.streaming.tests.StickyAllocationAndLocalRecoveryTestJob$StateCreatingFlatMap.initializeState(StickyAllocationAndLocalRecoveryTestJob.java:343) ~[?:?]
2020-10-15T13:01:42.4083125Z 	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:185) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4103820Z 	at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:167) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4104926Z 	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4106020Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:107) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4107084Z 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:262) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4108295Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:400) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4109432Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:505) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4110458Z 	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4111428Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:501) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4112328Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:533) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4113167Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4113962Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547) ~[flink-dist_2.11-1.12-SNAPSHOT.jar:1.12-SNAPSHOT]
2020-10-15T13:01:42.4114434Z 	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_265]
{code}"	FLINK	Resolved	1	1	8669	pull-request-available, test-stability
13289286	"Remove ""FAQ"" section from Flink website"	"I propose to remove the [FAQ|https://flink.apache.org/faq.html] page from the Flink website for the following reasons:
- the information on there is not very up to date, nor helpful or extensive (its a small selection of what somebody a few years ago considered frequent questions)
- StackOverflow lists a different set of questions as most frequent: https://stackoverflow.com/questions/tagged/apache-flink?tab=Votes
- The page is only on position 39 in Google Analytics (in the last ~10 months)

I'm happy to hear opinions on this!"	FLINK	Resolved	3	4	8669	pull-request-available
13338856	Sanity check after bash e2e tests for no leftover processes	"As seen in FLINK-19974, if an e2e test is not cleaning up properly, other e2e tests might fail with difficult to diagnose issues.

I propose to check that no leftover processes (including docker containers) are running after each bash e2e test."	FLINK	Closed	2	2	8669	pull-request-available
13335977	PrometheusReporterEndToEndITCase crashes with exit code 143	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=7814&view=logs&j=68a897ab-3047-5660-245a-cce8f83859f6&t=16ca2cca-2f63-5cce-12d2-d519b930a729]

{code}
2020-10-18T23:46:04.9667443Z [ERROR] The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-18T23:46:04.9669237Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp
2020-10-18T23:46:04.9670440Z [ERROR] Error occurred in starting fork, check output in log
2020-10-18T23:46:04.9671283Z [ERROR] Process Exit Code: 143
2020-10-18T23:46:04.9671614Z [ERROR] Crashed tests:
2020-10-18T23:46:04.9672025Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
2020-10-18T23:46:04.9672649Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-10-18T23:46:04.9674834Z [ERROR] Command was /bin/sh -c cd /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target && /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=2 -XX:+UseG1GC -jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire/surefirebooter6797466627443523305.jar /home/vsts/work/1/s/flink-end-to-end-tests/flink-metrics-reporter-prometheus-test/target/surefire 2020-10-18T23-44-09_467-jvmRun2 surefire930806459376622178tmp surefire_41970585275084524978tmp
2020-10-18T23:46:04.9676153Z [ERROR] Error occurred in starting fork, check output in log
2020-10-18T23:46:04.9676556Z [ERROR] Process Exit Code: 143
2020-10-18T23:46:04.9676882Z [ERROR] Crashed tests:
2020-10-18T23:46:04.9677288Z [ERROR] org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
2020-10-18T23:46:04.9677827Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-10-18T23:46:04.9678408Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:282)
2020-10-18T23:46:04.9678965Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:245)
2020-10-18T23:46:04.9679575Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-10-18T23:46:04.9680983Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-10-18T23:46:04.9681749Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-10-18T23:46:04.9682246Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-10-18T23:46:04.9682728Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-10-18T23:46:04.9683179Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-10-18T23:46:04.9683609Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-10-18T23:46:04.9684102Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-10-18T23:46:04.9684639Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-10-18T23:46:04.9685180Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-10-18T23:46:04.9685711Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-10-18T23:46:04.9686145Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-10-18T23:46:04.9686516Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-10-18T23:46:04.9689517Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-10-18T23:46:04.9689917Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-10-18T23:46:04.9690262Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-10-18T23:46:04.9690606Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-10-18T23:46:04.9690994Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-10-18T23:46:04.9691435Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-10-18T23:46:04.9691856Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-10-18T23:46:04.9692450Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-10-18T23:46:04.9693419Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-10-18T23:46:04.9693885Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-10-18T23:46:04.9694334Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-10-18T23:46:04.9694939Z [ERROR] -> [Help 1]
{code}"	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13289460	ConnectedComponents iterations with high parallelism end-to-end test fails with OutOfMemoryError: Direct buffer memory	"Logs: https://dev.azure.com/georgeryan1322/Flink/_build/results?buildId=74&view=logs&j=1f3ed471-1849-5d3c-a34c-19792af4ad16&t=ce095137-3e3b-5f73-4b79-c42d3d5f8283

{code}
2020-03-04T08:03:46.0786078Z 2020-03-04 08:03:42,628 INFO  org.apache.flink.runtime.iterative.task.IterationIntermediateTask [] - starting iteration [1]:  Reduce (MIN(1), at main(HighParallelismIterationsTestProgram.java:61) (12/25)
2020-03-04T08:03:46.0787503Z 2020-03-04 08:03:42,875 ERROR org.apache.flink.runtime.io.network.netty.PartitionRequestQueue [] - Encountered error while consuming partitions
2020-03-04T08:03:46.0788060Z java.lang.OutOfMemoryError: Direct buffer memory
2020-03-04T08:03:46.0788460Z 	at java.nio.Bits.reserveMemory(Bits.java:175) ~[?:?]
2020-03-04T08:03:46.0788904Z 	at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118) ~[?:?]
2020-03-04T08:03:46.0789537Z 	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317) ~[?:?]
2020-03-04T08:03:46.0790381Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:772) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0791491Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:748) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0792483Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:245) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0793416Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:215) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0794359Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PoolArena.allocate(PoolArena.java:147) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0795385Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:342) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0796471Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0797575Z 	at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0798718Z 	at org.apache.flink.shaded.netty4.io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0799951Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0801172Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0802572Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:779) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0803719Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:424) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0804763Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0806007Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0807050Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0807612Z 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-03-04T08:03:46.0808499Z 2020-03-04 08:03:43,572 ERROR org.apache.flink.runtime.operators.BatchTask                 [] - Error in task code:  Reduce (MIN(1), at main(HighParallelismIterationsTestProgram.java:61) (5/25)
2020-03-04T08:03:46.0810179Z java.lang.Exception: The data preparation for task 'Reduce (MIN(1), at main(HighParallelismIterationsTestProgram.java:61)' , caused an error: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0811472Z 	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:480) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0813477Z 	at org.apache.flink.runtime.iterative.task.AbstractIterativeTask.run(AbstractIterativeTask.java:157) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0814813Z 	at org.apache.flink.runtime.iterative.task.IterationIntermediateTask.run(IterationIntermediateTask.java:107) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0816257Z 	at org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:369) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0817111Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0817911Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0818381Z 	at java.lang.Thread.run(Thread.java:834) [?:?]
2020-03-04T08:03:46.0819353Z Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0820498Z 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:650) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0821448Z 	at org.apache.flink.runtime.operators.BatchTask.getInput(BatchTask.java:1110) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0822376Z 	at org.apache.flink.runtime.operators.GroupReduceDriver.prepare(GroupReduceDriver.java:99) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0823248Z 	at org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:474) [flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0823661Z 	... 6 more
2020-03-04T08:03:46.0824426Z Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0825507Z 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:831) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0826579Z Caused by: org.apache.flink.runtime.io.network.netty.exception.LocalTransportException: readAddress(..) failed: Connection reset by peer (connection to '10.1.0.4/10.1.0.4:44453')
2020-03-04T08:03:46.0827970Z 	at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.exceptionCaught(CreditBasedPartitionRequestClientHandler.java:165) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0829232Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:297) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0830423Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0831611Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireExceptionCaught(AbstractChannelHandlerContext.java:268) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0832773Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.exceptionCaught(DefaultChannelPipeline.java:1388) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0834969Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:297) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0836413Z 	at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeExceptionCaught(AbstractChannelHandlerContext.java:276) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0838310Z 	at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireExceptionCaught(DefaultChannelPipeline.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0839629Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.handleReadException(AbstractEpollStreamChannel.java:730) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0841070Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:820) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0842211Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:424) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0843214Z 	at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0844284Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0845351Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]
2020-03-04T08:03:46.0845828Z 	... 1 more
2020-03-04T08:03:46.0846253Z Caused by: org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer
{code}"	FLINK	Resolved	3	1	8669	pull-request-available, test-stability
13328729	DispatcherResourceCleanupTest#testJobSubmissionUnderSameJobId is unstable on Azure Pipeline	Here is the log and stack: https://dev.azure.com/kevin-flink/3f520f11-5170-4153-99d0-2ade0d99b911/_apis/build/builds/88/logs/102	FLINK	Closed	2	1	8669	pull-request-available, test-stability
13295531	DistinctAggregateITCaseBase.testSingleDistinctAggOnMultiColumnsWithGroupingSets gets stuck	"CI: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=6912&view=logs&j=e25d5e7e-2a9c-5589-4940-0b638d75a414&t=294c2388-20e6-57a2-5721-91db544b1e69

Why I believe it is this test:

{code}
2020-04-01T09:46:12.3370955Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f22d800b800 nid=0xe5a waiting on condition [0x00007f22e1b8f000]
2020-04-01T09:46:12.3371365Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3371648Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3372211Z 	- parking to wait for  <0x0000000082d3b7b0> (a java.util.concurrent.CompletableFuture$Signaller)
2020-04-01T09:46:12.3372658Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3373126Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-04-01T09:46:12.3373611Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-04-01T09:46:12.3374068Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-04-01T09:46:12.3374626Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-04-01T09:46:12.3375108Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:653)
2020-04-01T09:46:12.3375650Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77)
2020-04-01T09:46:12.3376163Z 	at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:51)
2020-04-01T09:46:12.3376745Z 	at org.apache.flink.table.planner.utils.TestingTableEnvironment.execute(TableTestBase.scala:1054)
2020-04-01T09:46:12.3377250Z 	at org.apache.flink.table.api.TableUtils.collectToList(TableUtils.java:85)
2020-04-01T09:46:12.3377835Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:288)
2020-04-01T09:46:12.3378381Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:129)
2020-04-01T09:46:12.3378910Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:95)
2020-04-01T09:46:12.3379621Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.testSingleDistinctAggOnMultiColumnsWithGroupingSets(DistinctAggregateITCaseBase.scala:244)
2020-04-01T09:46:12.3380251Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-01T09:46:12.3380721Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-01T09:46:12.3381226Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-01T09:46:12.3381654Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-01T09:46:12.3382099Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-04-01T09:46:12.3382612Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-04-01T09:46:12.3383103Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-04-01T09:46:12.3383706Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-04-01T09:46:12.3384180Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-04-01T09:46:12.3384706Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-04-01T09:46:12.3385168Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-04-01T09:46:12.3385654Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-04-01T09:46:12.3386113Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-04-01T09:46:12.3386561Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-04-01T09:46:12.3386998Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-04-01T09:46:12.3387411Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-04-01T09:46:12.3387842Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-04-01T09:46:12.3388290Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3388724Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3389142Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-04-01T09:46:12.3389513Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-04-01T09:46:12.3389970Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-04-01T09:46:12.3390470Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-04-01T09:46:12.3391052Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-04-01T09:46:12.3391578Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-04-01T09:46:12.3392104Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-04-01T09:46:12.3392665Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-04-01T09:46:12.3393151Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-04-01T09:46:12.3393631Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}

Full stacktraces 
{code}
2020-04-01T09:40:58.9129390Z [INFO] Running org.apache.flink.table.planner.catalog.CatalogITCase
2020-04-01T09:40:58.9241845Z [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 s - in org.apache.flink.table.planner.catalog.CatalogITCase
2020-04-01T09:40:58.9251426Z [INFO] Running org.apache.flink.table.runtime.batch.sql.UnnestITCase
2020-04-01T09:41:00.1602556Z [INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.233 s - in org.apache.flink.table.runtime.batch.sql.UnnestITCase
2020-04-01T09:46:11.2452086Z ==============================================================================
2020-04-01T09:46:11.2454365Z Maven produced no output for 300 seconds.
2020-04-01T09:46:11.2455123Z ==============================================================================
2020-04-01T09:46:11.2472609Z ==============================================================================
2020-04-01T09:46:11.2473050Z The following Java processes are running (JPS)
2020-04-01T09:46:11.2473420Z ==============================================================================
2020-04-01T09:46:11.3898136Z 338 Launcher
2020-04-01T09:46:11.4010823Z 3669 surefirebooter2141596050142400008.jar
2020-04-01T09:46:11.4049294Z 27452 Jps
2020-04-01T09:46:11.5771473Z ==============================================================================
2020-04-01T09:46:11.5772055Z Printing stack trace of Java process 338
2020-04-01T09:46:11.5772468Z ==============================================================================
2020-04-01T09:46:11.9236096Z 2020-04-01 09:46:11
2020-04-01T09:46:11.9238468Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-04-01T09:46:11.9238896Z 
2020-04-01T09:46:11.9239328Z ""Attach Listener"" #81 daemon prio=9 os_prio=0 tid=0x00007f1ff0001000 nid=0x6bc8 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9239781Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9239963Z 
2020-04-01T09:46:11.9240526Z ""Thread-16"" #75 daemon prio=5 os_prio=0 tid=0x00007f1f8800d000 nid=0xe54 runnable [0x00007f2035211000]
2020-04-01T09:46:11.9241169Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9241695Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-04-01T09:46:11.9242092Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-04-01T09:46:11.9242635Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-04-01T09:46:11.9245960Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-04-01T09:46:11.9247627Z 	- locked <0x00000006890f9750> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-04-01T09:46:11.9248394Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-04-01T09:46:11.9249115Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-04-01T09:46:11.9249786Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-04-01T09:46:11.9250597Z 	- locked <0x00000006890fe920> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9251216Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-04-01T09:46:11.9251912Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-04-01T09:46:11.9252604Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-04-01T09:46:11.9253566Z 	- locked <0x00000006890fe920> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9254786Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-04-01T09:46:11.9255679Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-04-01T09:46:11.9256320Z 
2020-04-01T09:46:11.9257446Z ""Thread-15"" #74 daemon prio=5 os_prio=0 tid=0x00007f1f8800b000 nid=0xe53 runnable [0x00007f2035312000]
2020-04-01T09:46:11.9258152Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9258631Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-04-01T09:46:11.9258991Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-04-01T09:46:11.9259554Z 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
2020-04-01T09:46:11.9260014Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
2020-04-01T09:46:11.9260820Z 	- locked <0x00000006890f7690> (a java.lang.UNIXProcess$ProcessPipeInputStream)
2020-04-01T09:46:11.9261269Z 	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
2020-04-01T09:46:11.9261685Z 	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
2020-04-01T09:46:11.9262093Z 	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
2020-04-01T09:46:11.9262635Z 	- locked <0x00000006890fbd18> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9263303Z 	at java.io.InputStreamReader.read(InputStreamReader.java:184)
2020-04-01T09:46:11.9263720Z 	at java.io.BufferedReader.fill(BufferedReader.java:161)
2020-04-01T09:46:11.9264115Z 	at java.io.BufferedReader.readLine(BufferedReader.java:324)
2020-04-01T09:46:11.9264810Z 	- locked <0x00000006890fbd18> (a java.io.InputStreamReader)
2020-04-01T09:46:11.9265195Z 	at java.io.BufferedReader.readLine(BufferedReader.java:389)
2020-04-01T09:46:11.9265715Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:76)
2020-04-01T09:46:11.9266062Z 
2020-04-01T09:46:11.9266643Z ""Thread-14"" #73 daemon prio=5 os_prio=0 tid=0x00007f1f88002800 nid=0xe52 waiting on condition [0x00007f2035413000]
2020-04-01T09:46:11.9267184Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9267493Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9268051Z 	- parking to wait for  <0x0000000685926988> (a java.util.concurrent.Semaphore$NonfairSync)
2020-04-01T09:46:11.9268525Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9269082Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
2020-04-01T09:46:11.9269743Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
2020-04-01T09:46:11.9270420Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
2020-04-01T09:46:11.9271022Z 	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
2020-04-01T09:46:11.9271616Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestProvidingInputStream.awaitNextTest(TestProvidingInputStream.java:181)
2020-04-01T09:46:11.9272398Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.TestProvidingInputStream.beforeNextCommand(TestProvidingInputStream.java:144)
2020-04-01T09:46:11.9273124Z 	at org.apache.maven.plugin.surefire.booterclient.lazytestprovider.AbstractCommandStream.read(AbstractCommandStream.java:100)
2020-04-01T09:46:11.9273792Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.feed(StreamFeeder.java:123)
2020-04-01T09:46:11.9274414Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamFeeder.run(StreamFeeder.java:60)
2020-04-01T09:46:11.9274840Z 
2020-04-01T09:46:11.9275223Z ""ThreadedStreamConsumer"" #69 daemon prio=5 os_prio=0 tid=0x00007f1f88009800 nid=0xe4e waiting on condition [0x00007f2035110000]
2020-04-01T09:46:11.9275677Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9275990Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9276634Z 	- parking to wait for  <0x00000006890ccdc8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9277368Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9277914Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9278491Z 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
2020-04-01T09:46:11.9279078Z 	at org.apache.maven.plugin.surefire.booterclient.output.ThreadedStreamConsumer$Pumper.run(ThreadedStreamConsumer.java:83)
2020-04-01T09:46:11.9279565Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9279785Z 
2020-04-01T09:46:11.9280392Z ""surefire-fork-starter"" #68 daemon prio=5 os_prio=0 tid=0x00007f20a6caa000 nid=0xe4d waiting on condition [0x00007f2035a8b000]
2020-04-01T09:46:11.9280913Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9281210Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9281862Z 	- parking to wait for  <0x0000000682797140> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9282362Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9282919Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9283587Z 	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)
2020-04-01T09:46:11.9284066Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9284653Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9285154Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9285572Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9285771Z 
2020-04-01T09:46:11.9286385Z ""surefire-fork-starter"" #67 daemon prio=5 os_prio=0 tid=0x00007f20a6ca9000 nid=0xe4c in Object.wait() [0x00007f203598a000]
2020-04-01T09:46:11.9286863Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:11.9287279Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:11.9287782Z 	- waiting on <0x00000006890f5418> (a java.lang.UNIXProcess)
2020-04-01T09:46:11.9288125Z 	at java.lang.Object.wait(Object.java:502)
2020-04-01T09:46:11.9288496Z 	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)
2020-04-01T09:46:11.9289014Z 	- locked <0x00000006890f5418> (a java.lang.UNIXProcess)
2020-04-01T09:46:11.9289535Z 	at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.CommandLineUtils$1.call(CommandLineUtils.java:260)
2020-04-01T09:46:11.9290124Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:614)
2020-04-01T09:46:11.9290730Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-04-01T09:46:11.9291288Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:371)
2020-04-01T09:46:11.9291820Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:347)
2020-04-01T09:46:11.9292298Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-04-01T09:46:11.9292741Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:11.9293252Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9293669Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9293872Z 
2020-04-01T09:46:11.9294450Z ""ping-timer-10s"" #65 daemon prio=5 os_prio=0 tid=0x00007f20a6ca8000 nid=0xe4b waiting on condition [0x00007f2037030000]
2020-04-01T09:46:11.9295003Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:11.9295326Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9295956Z 	- parking to wait for  <0x00000006827917a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9296582Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:11.9297218Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:11.9297886Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:11.9298512Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:11.9299062Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9299563Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9300060Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9300479Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9300731Z 
2020-04-01T09:46:11.9301348Z ""timeout-check-timer"" #64 daemon prio=5 os_prio=0 tid=0x00007f20a5e72000 nid=0xe4a waiting on condition [0x00007f2035b8c000]
2020-04-01T09:46:11.9301853Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:11.9302343Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9303147Z 	- parking to wait for  <0x0000000682792088> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9303657Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:11.9304231Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:11.9305077Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:11.9305723Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:11.9306295Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9306789Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9307380Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9307787Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9308000Z 
2020-04-01T09:46:11.9308334Z ""process reaper"" #37 daemon prio=10 os_prio=0 tid=0x00007f1f84006000 nid=0x198 runnable [0x00007f2035687000]
2020-04-01T09:46:11.9308751Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9309068Z 	at java.lang.UNIXProcess.waitForProcessExit(Native Method)
2020-04-01T09:46:11.9309470Z 	at java.lang.UNIXProcess.lambda$initStreams$3(UNIXProcess.java:289)
2020-04-01T09:46:11.9309881Z 	at java.lang.UNIXProcess$$Lambda$10/1718188850.run(Unknown Source)
2020-04-01T09:46:11.9310329Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:11.9310899Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9311490Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9311689Z 
2020-04-01T09:46:11.9312309Z ""resolver-5"" #26 daemon prio=5 os_prio=0 tid=0x00007f20a5432800 nid=0x18c waiting on condition [0x00007f2037387000]
2020-04-01T09:46:11.9312754Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9313061Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9313694Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9314210Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9315023Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9315587Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9316082Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9316684Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9317294Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9317703Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9317924Z 
2020-04-01T09:46:11.9318519Z ""resolver-4"" #25 daemon prio=5 os_prio=0 tid=0x00007f20a5431800 nid=0x18b waiting on condition [0x00007f2037488000]
2020-04-01T09:46:11.9318977Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9319285Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9319911Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9320423Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9321026Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9321610Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9322095Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9322598Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9323206Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9323606Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9323823Z 
2020-04-01T09:46:11.9324397Z ""resolver-3"" #24 daemon prio=5 os_prio=0 tid=0x00007f20a5434800 nid=0x18a waiting on condition [0x00007f2037589000]
2020-04-01T09:46:11.9325012Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9325309Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9325964Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9326463Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9327113Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9327688Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9328173Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9328675Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9329170Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9329588Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9329793Z 
2020-04-01T09:46:11.9330375Z ""resolver-2"" #23 daemon prio=5 os_prio=0 tid=0x00007f20a546a800 nid=0x189 waiting on condition [0x00007f203768a000]
2020-04-01T09:46:11.9330878Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9331185Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9331816Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9332330Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9332892Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9333449Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9333944Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9334434Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9335115Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9335537Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9335736Z 
2020-04-01T09:46:11.9336320Z ""resolver-1"" #22 daemon prio=5 os_prio=0 tid=0x00007f20a542e000 nid=0x188 waiting on condition [0x00007f203778b000]
2020-04-01T09:46:11.9336876Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9337272Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9337910Z 	- parking to wait for  <0x00000003d746d190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:11.9338431Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9338974Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:11.9339548Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:11.9340050Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:11.9340539Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:11.9341118Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:11.9341528Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:11.9341739Z 
2020-04-01T09:46:11.9342073Z ""Service Thread"" #20 daemon prio=9 os_prio=0 tid=0x00007f20a4108000 nid=0x186 runnable [0x0000000000000000]
2020-04-01T09:46:11.9342585Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9342768Z 
2020-04-01T09:46:11.9343142Z ""C1 CompilerThread14"" #19 daemon prio=9 os_prio=0 tid=0x00007f20a4105000 nid=0x185 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9343574Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9343772Z 
2020-04-01T09:46:11.9344128Z ""C1 CompilerThread13"" #18 daemon prio=9 os_prio=0 tid=0x00007f20a4103000 nid=0x184 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9344721Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9344932Z 
2020-04-01T09:46:11.9345302Z ""C1 CompilerThread12"" #17 daemon prio=9 os_prio=0 tid=0x00007f20a4101000 nid=0x183 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9345739Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9345936Z 
2020-04-01T09:46:11.9346294Z ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007f20a40fe800 nid=0x182 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9346747Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9347017Z 
2020-04-01T09:46:11.9347375Z ""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007f20a40fd000 nid=0x181 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9347819Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9348001Z 
2020-04-01T09:46:11.9348367Z ""C2 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007f20a40fa800 nid=0x180 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9348806Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9348990Z 
2020-04-01T09:46:11.9349343Z ""C2 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007f20a40f9000 nid=0x17f waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9349781Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9349969Z 
2020-04-01T09:46:11.9350336Z ""C2 CompilerThread7"" #12 daemon prio=9 os_prio=0 tid=0x00007f20a40f7000 nid=0x17e waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9350833Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9351035Z 
2020-04-01T09:46:11.9351386Z ""C2 CompilerThread6"" #11 daemon prio=9 os_prio=0 tid=0x00007f20a40f5000 nid=0x17d waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9351826Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9352009Z 
2020-04-01T09:46:11.9352377Z ""C2 CompilerThread5"" #10 daemon prio=9 os_prio=0 tid=0x00007f20a40f2800 nid=0x17c waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9352800Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9352997Z 
2020-04-01T09:46:11.9353345Z ""C2 CompilerThread4"" #9 daemon prio=9 os_prio=0 tid=0x00007f20a40e8800 nid=0x17b waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9353778Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9354108Z 
2020-04-01T09:46:11.9354476Z ""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007f20a40e6800 nid=0x17a waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9355106Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9355312Z 
2020-04-01T09:46:11.9355661Z ""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007f20a40e4000 nid=0x179 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9356096Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9356278Z 
2020-04-01T09:46:11.9356640Z ""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f20a40e2800 nid=0x178 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9357142Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9357344Z 
2020-04-01T09:46:11.9357691Z ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f20a40df800 nid=0x177 waiting on condition [0x0000000000000000]
2020-04-01T09:46:11.9358126Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9358308Z 
2020-04-01T09:46:11.9358655Z ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f20a40de000 nid=0x176 runnable [0x0000000000000000]
2020-04-01T09:46:11.9359055Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:11.9359237Z 
2020-04-01T09:46:11.9359581Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f20a40ad800 nid=0x175 in Object.wait() [0x00007f2044d9d000]
2020-04-01T09:46:11.9360129Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:11.9360444Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:11.9361112Z 	- waiting on <0x00000003d9040d08> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:11.9361523Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-04-01T09:46:11.9362111Z 	- locked <0x00000003d9040d08> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:11.9362514Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-04-01T09:46:11.9362952Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-04-01T09:46:11.9363212Z 
2020-04-01T09:46:11.9363573Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f20a40a9000 nid=0x174 in Object.wait() [0x00007f2044e9e000]
2020-04-01T09:46:11.9364046Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:11.9364364Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:11.9365065Z 	- waiting on <0x00000003d9128018> (a java.lang.ref.Reference$Lock)
2020-04-01T09:46:11.9365418Z 	at java.lang.Object.wait(Object.java:502)
2020-04-01T09:46:11.9365803Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-04-01T09:46:11.9366361Z 	- locked <0x00000003d9128018> (a java.lang.ref.Reference$Lock)
2020-04-01T09:46:11.9366778Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-04-01T09:46:11.9367123Z 
2020-04-01T09:46:11.9367461Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f20a400b800 nid=0x15b waiting on condition [0x00007f20abbe2000]
2020-04-01T09:46:11.9367867Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:11.9368182Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:11.9368731Z 	- parking to wait for  <0x0000000685927188> (a java.util.concurrent.FutureTask)
2020-04-01T09:46:11.9369168Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:11.9369622Z 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
2020-04-01T09:46:11.9370035Z 	at java.util.concurrent.FutureTask.get(FutureTask.java:191)
2020-04-01T09:46:11.9370531Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:476)
2020-04-01T09:46:11.9371257Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkOnceMultiple(ForkStarter.java:382)
2020-04-01T09:46:11.9371828Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:297)
2020-04-01T09:46:11.9372357Z 	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-04-01T09:46:11.9372907Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-04-01T09:46:11.9373660Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-04-01T09:46:11.9374269Z 	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-04-01T09:46:11.9374944Z 	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-04-01T09:46:11.9375490Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-04-01T09:46:11.9375992Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-04-01T09:46:11.9376505Z 	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-04-01T09:46:11.9377126Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-04-01T09:46:11.9377748Z 	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-04-01T09:46:11.9378394Z 	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-04-01T09:46:11.9378990Z 	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-04-01T09:46:11.9379567Z 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-04-01T09:46:11.9379989Z 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-04-01T09:46:11.9380408Z 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-04-01T09:46:11.9380877Z 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-04-01T09:46:11.9381269Z 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-04-01T09:46:11.9381652Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-01T09:46:11.9382075Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-01T09:46:11.9382592Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-01T09:46:11.9383043Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-01T09:46:11.9383498Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-04-01T09:46:11.9384005Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-04-01T09:46:11.9384500Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-04-01T09:46:11.9385086Z 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-04-01T09:46:11.9385361Z 
2020-04-01T09:46:11.9385622Z ""VM Thread"" os_prio=0 tid=0x00007f20a409f800 nid=0x173 runnable 
2020-04-01T09:46:11.9385847Z 
2020-04-01T09:46:11.9386131Z ""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f20a4020800 nid=0x15c runnable 
2020-04-01T09:46:11.9386404Z 
2020-04-01T09:46:11.9386689Z ""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f20a4022000 nid=0x15d runnable 
2020-04-01T09:46:11.9387028Z 
2020-04-01T09:46:11.9387310Z ""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x00007f20a4024000 nid=0x15e runnable 
2020-04-01T09:46:11.9387578Z 
2020-04-01T09:46:11.9387857Z ""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x00007f20a4025800 nid=0x15f runnable 
2020-04-01T09:46:11.9388117Z 
2020-04-01T09:46:11.9388409Z ""GC task thread#4 (ParallelGC)"" os_prio=0 tid=0x00007f20a4027800 nid=0x160 runnable 
2020-04-01T09:46:11.9388664Z 
2020-04-01T09:46:11.9388959Z ""GC task thread#5 (ParallelGC)"" os_prio=0 tid=0x00007f20a4029000 nid=0x161 runnable 
2020-04-01T09:46:11.9389214Z 
2020-04-01T09:46:11.9389510Z ""GC task thread#6 (ParallelGC)"" os_prio=0 tid=0x00007f20a402b000 nid=0x162 runnable 
2020-04-01T09:46:11.9389765Z 
2020-04-01T09:46:11.9390056Z ""GC task thread#7 (ParallelGC)"" os_prio=0 tid=0x00007f20a402c800 nid=0x163 runnable 
2020-04-01T09:46:11.9390310Z 
2020-04-01T09:46:11.9390599Z ""GC task thread#8 (ParallelGC)"" os_prio=0 tid=0x00007f20a402e800 nid=0x164 runnable 
2020-04-01T09:46:11.9390996Z 
2020-04-01T09:46:11.9391280Z ""GC task thread#9 (ParallelGC)"" os_prio=0 tid=0x00007f20a4030000 nid=0x165 runnable 
2020-04-01T09:46:11.9391551Z 
2020-04-01T09:46:11.9391836Z ""GC task thread#10 (ParallelGC)"" os_prio=0 tid=0x00007f20a4032000 nid=0x166 runnable 
2020-04-01T09:46:11.9392115Z 
2020-04-01T09:46:11.9392400Z ""GC task thread#11 (ParallelGC)"" os_prio=0 tid=0x00007f20a4033800 nid=0x167 runnable 
2020-04-01T09:46:11.9392671Z 
2020-04-01T09:46:11.9392954Z ""GC task thread#12 (ParallelGC)"" os_prio=0 tid=0x00007f20a4035800 nid=0x168 runnable 
2020-04-01T09:46:11.9393211Z 
2020-04-01T09:46:11.9393507Z ""GC task thread#13 (ParallelGC)"" os_prio=0 tid=0x00007f20a4037000 nid=0x169 runnable 
2020-04-01T09:46:11.9393763Z 
2020-04-01T09:46:11.9394060Z ""GC task thread#14 (ParallelGC)"" os_prio=0 tid=0x00007f20a4039000 nid=0x16a runnable 
2020-04-01T09:46:11.9394314Z 
2020-04-01T09:46:11.9394680Z ""GC task thread#15 (ParallelGC)"" os_prio=0 tid=0x00007f20a403a800 nid=0x16b runnable 
2020-04-01T09:46:11.9394944Z 
2020-04-01T09:46:11.9395237Z ""GC task thread#16 (ParallelGC)"" os_prio=0 tid=0x00007f20a403c800 nid=0x16c runnable 
2020-04-01T09:46:11.9395491Z 
2020-04-01T09:46:11.9395772Z ""GC task thread#17 (ParallelGC)"" os_prio=0 tid=0x00007f20a403e000 nid=0x16d runnable 
2020-04-01T09:46:11.9396157Z 
2020-04-01T09:46:11.9396437Z ""GC task thread#18 (ParallelGC)"" os_prio=0 tid=0x00007f20a4040000 nid=0x16e runnable 
2020-04-01T09:46:11.9396710Z 
2020-04-01T09:46:11.9397053Z ""GC task thread#19 (ParallelGC)"" os_prio=0 tid=0x00007f20a4041800 nid=0x16f runnable 
2020-04-01T09:46:11.9397326Z 
2020-04-01T09:46:11.9397607Z ""GC task thread#20 (ParallelGC)"" os_prio=0 tid=0x00007f20a4043800 nid=0x170 runnable 
2020-04-01T09:46:11.9397861Z 
2020-04-01T09:46:11.9398157Z ""GC task thread#21 (ParallelGC)"" os_prio=0 tid=0x00007f20a4045000 nid=0x171 runnable 
2020-04-01T09:46:11.9398410Z 
2020-04-01T09:46:11.9398709Z ""GC task thread#22 (ParallelGC)"" os_prio=0 tid=0x00007f20a4047000 nid=0x172 runnable 
2020-04-01T09:46:11.9398963Z 
2020-04-01T09:46:11.9399282Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007f20a410a800 nid=0x187 waiting on condition 
2020-04-01T09:46:11.9399553Z 
2020-04-01T09:46:11.9399754Z JNI global references: 238
2020-04-01T09:46:11.9399923Z 
2020-04-01T09:46:11.9432174Z ==============================================================================
2020-04-01T09:46:11.9432645Z Printing stack trace of Java process 3669
2020-04-01T09:46:11.9433013Z ==============================================================================
2020-04-01T09:46:12.2695641Z 2020-04-01 09:46:12
2020-04-01T09:46:12.2696636Z Full thread dump OpenJDK 64-Bit Server VM (25.242-b08 mixed mode):
2020-04-01T09:46:12.2697002Z 
2020-04-01T09:46:12.2697502Z ""Attach Listener"" #43455 daemon prio=9 os_prio=0 tid=0x00007f21cc001000 nid=0x6bf9 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.2697943Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.2698159Z 
2020-04-01T09:46:12.2698944Z ""flink-akka.actor.default-dispatcher-209"" #43454 prio=5 os_prio=0 tid=0x00007f1ff0020800 nid=0x6b31 waiting on condition [0x00007f1f9f9fa000]
2020-04-01T09:46:12.2699565Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2700026Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2701023Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2701633Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-04-01T09:46:12.2702136Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-04-01T09:46:12.2702751Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2703581Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2703919Z 
2020-04-01T09:46:12.2704770Z ""CloseableReaperThread"" #43452 daemon prio=5 os_prio=0 tid=0x00007f1f5c027800 nid=0x6b26 in Object.wait() [0x00007f20984c5000]
2020-04-01T09:46:12.2705509Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:12.2706218Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.2706645Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-04-01T09:46:12.2707523Z 	- locked <0x0000000082d313d0> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:12.2708119Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-04-01T09:46:12.2708915Z 	at org.apache.flink.core.fs.SafetyNetCloseableRegistry$CloseableReaperThread.run(SafetyNetCloseableRegistry.java:193)
2020-04-01T09:46:12.2709469Z 
2020-04-01T09:46:12.2711509Z ""Source: Custom Source -> SourceConversion(table=[default_catalog.default_database.SmallTable3], fields=[a, b, c]) -> Expand(projects=[a, b, c, $e], projects=[{a, b, null AS c, 1 AS $e}, {a, null AS b, c, 2 AS $e}]) -> LocalHashAggregate(groupBy=[a, b, c, $e], select=[a, b, c, $e]) (1/1)"" #43451 prio=5 os_prio=0 tid=0x00007f1f7404d800 nid=0x6b25 runnable [0x00007f1f9e4ea000]
2020-04-01T09:46:12.2712603Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.2712955Z 	at java.util.Hashtable.putAll(Hashtable.java:524)
2020-04-01T09:46:12.2713550Z 	- eliminated <0x0000000090fd8ea8> (a java.util.Hashtable)
2020-04-01T09:46:12.2713927Z 	at java.util.Hashtable.<init>(Hashtable.java:226)
2020-04-01T09:46:12.2714659Z 	at javax.management.ObjectName.getKeyPropertyList(ObjectName.java:1624)
2020-04-01T09:46:12.2715190Z 	at com.sun.jmx.mbeanserver.Repository$ObjectNamePattern.matchKeys(Repository.java:179)
2020-04-01T09:46:12.2715703Z 	at com.sun.jmx.mbeanserver.Repository.addAllMatching(Repository.java:233)
2020-04-01T09:46:12.2716317Z 	- locked <0x000000008624e1d8> (a java.util.HashMap)
2020-04-01T09:46:12.2716776Z 	at com.sun.jmx.mbeanserver.Repository.query(Repository.java:569)
2020-04-01T09:46:12.2717334Z 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.queryNamesImpl(DefaultMBeanServerInterceptor.java:562)
2020-04-01T09:46:12.2717990Z 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.queryNames(DefaultMBeanServerInterceptor.java:554)
2020-04-01T09:46:12.2718565Z 	at com.sun.jmx.mbeanserver.JmxMBeanServer.queryNames(JmxMBeanServer.java:619)
2020-04-01T09:46:12.2719083Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
2020-04-01T09:46:12.2719610Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerConfigs(Server.java:301)
2020-04-01T09:46:12.2720344Z 	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:266)
2020-04-01T09:46:12.2721092Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
2020-04-01T09:46:12.2721990Z 	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
2020-04-01T09:46:12.2722859Z 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:590)
2020-04-01T09:46:12.2723618Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:651)
2020-04-01T09:46:12.2724380Z 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:668)
2020-04-01T09:46:12.2725053Z 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
2020-04-01T09:46:12.2725853Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)
2020-04-01T09:46:12.2726597Z 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)
2020-04-01T09:46:12.2727125Z 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)
2020-04-01T09:46:12.2727668Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:138)
2020-04-01T09:46:12.2728230Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:45)
2020-04-01T09:46:12.2728777Z 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:48)
2020-04-01T09:46:12.2729337Z 	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:30)
2020-04-01T09:46:12.2729809Z 	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:329)
2020-04-01T09:46:12.2730367Z 	at LocalHashAggregateWithKeys$74756.<clinit>(Unknown Source)
2020-04-01T09:46:12.2730826Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
2020-04-01T09:46:12.2731349Z 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
2020-04-01T09:46:12.2731961Z 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2020-04-01T09:46:12.2732491Z 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
2020-04-01T09:46:12.2733010Z 	at org.apache.flink.table.runtime.generated.GeneratedClass.newInstance(GeneratedClass.java:65)
2020-04-01T09:46:12.2733747Z 	at org.apache.flink.table.runtime.operators.CodeGenOperatorFactory.createStreamOperator(CodeGenOperatorFactory.java:40)
2020-04-01T09:46:12.2734679Z 	at org.apache.flink.streaming.api.operators.StreamOperatorFactoryUtil.createOperator(StreamOperatorFactoryUtil.java:61)
2020-04-01T09:46:12.2735353Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:473)
2020-04-01T09:46:12.2735961Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:396)
2020-04-01T09:46:12.2737025Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:462)
2020-04-01T09:46:12.2737639Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:396)
2020-04-01T09:46:12.2738260Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedOperator(OperatorChain.java:462)
2020-04-01T09:46:12.2738927Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createOutputCollector(OperatorChain.java:396)
2020-04-01T09:46:12.2739809Z 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:157)
2020-04-01T09:46:12.2740469Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:416)
2020-04-01T09:46:12.2741126Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:445)
2020-04-01T09:46:12.2741626Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:718)
2020-04-01T09:46:12.2742092Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:542)
2020-04-01T09:46:12.2742471Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2742682Z 
2020-04-01T09:46:12.2743481Z ""flink-akka.actor.default-dispatcher-208"" #43448 prio=5 os_prio=0 tid=0x00007f1f7404e000 nid=0x6b22 waiting on condition [0x00007f209a9ea000]
2020-04-01T09:46:12.2744087Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2744672Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2745366Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2745868Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2746350Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2746923Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2747224Z 
2020-04-01T09:46:12.2748157Z ""flink-akka.actor.default-dispatcher-207"" #43444 prio=5 os_prio=0 tid=0x00007f1fa801d000 nid=0x6b08 waiting on condition [0x00007f1f9e8e9000]
2020-04-01T09:46:12.2748896Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2749364Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2750203Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2751161Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2751894Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2752413Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2752711Z 
2020-04-01T09:46:12.2753884Z ""flink-akka.actor.default-dispatcher-205"" #43443 prio=5 os_prio=0 tid=0x00007f202801f000 nid=0x6aed waiting on condition [0x00007f21659e2000]
2020-04-01T09:46:12.2754745Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2755184Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2756104Z 	- parking to wait for  <0x0000000087e3a918> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2756818Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2757570Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2758316Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2758643Z 
2020-04-01T09:46:12.2759629Z ""flink-metrics-akka.remote.default-remote-dispatcher-17"" #43001 prio=1 os_prio=0 tid=0x00007f20f80b4000 nid=0x475e waiting on condition [0x00007f1f9dfe2000]
2020-04-01T09:46:12.2760444Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2761061Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2761967Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2762602Z 	at akka.dispatch.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
2020-04-01T09:46:12.2763451Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
2020-04-01T09:46:12.2764122Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2764984Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2765492Z 
2020-04-01T09:46:12.2766401Z ""mini-cluster-io-thread-32"" #42882 daemon prio=5 os_prio=0 tid=0x00007f1f74001000 nid=0x4115 waiting on condition [0x00007f1f9fcfd000]
2020-04-01T09:46:12.2767213Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2767529Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2768339Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2769112Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2770012Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2771007Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2771740Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2772393Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2773062Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2773697Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2774019Z 
2020-04-01T09:46:12.2775199Z ""mini-cluster-io-thread-31"" #42881 daemon prio=5 os_prio=0 tid=0x00007f1fa8024800 nid=0x4114 waiting on condition [0x00007f1f9fffe000]
2020-04-01T09:46:12.2775881Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2776205Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2777044Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2777585Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2778164Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2778760Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2779262Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2779793Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2780310Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2781010Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2781218Z 
2020-04-01T09:46:12.2781892Z ""mini-cluster-io-thread-30"" #42880 daemon prio=5 os_prio=0 tid=0x00007f1fa8023800 nid=0x4113 waiting on condition [0x00007f209a2e3000]
2020-04-01T09:46:12.2782889Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2783408Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2784459Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2785398Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2786266Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2787148Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2787919Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2788713Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2789431Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2790068Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2790561Z 
2020-04-01T09:46:12.2791727Z ""mini-cluster-io-thread-29"" #42879 daemon prio=5 os_prio=0 tid=0x00007f1fa8022800 nid=0x4112 waiting on condition [0x00007f20993d4000]
2020-04-01T09:46:12.2792457Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2792929Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2793849Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2794782Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2795651Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2796438Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2797268Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2798006Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2798739Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2799315Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2799618Z 
2020-04-01T09:46:12.2800765Z ""mini-cluster-io-thread-28"" #42878 daemon prio=5 os_prio=0 tid=0x00007f1fa8021800 nid=0x4111 waiting on condition [0x00007f2098acd000]
2020-04-01T09:46:12.2801643Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2801970Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2802976Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2803784Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2804713Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2805584Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2806466Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2807091Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2807849Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2808459Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2808790Z 
2020-04-01T09:46:12.2809748Z ""mini-cluster-io-thread-27"" #42877 daemon prio=5 os_prio=0 tid=0x00007f1fa8020800 nid=0x4110 waiting on condition [0x00007f20988cb000]
2020-04-01T09:46:12.2810462Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2811189Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2811854Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2812370Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2812916Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2813487Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2813983Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2814479Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2815081Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2815484Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2815698Z 
2020-04-01T09:46:12.2816324Z ""jobmanager-future-thread-32"" #42867 daemon prio=5 os_prio=0 tid=0x00007f1fa801c000 nid=0x40e1 waiting on condition [0x00007f2098ccf000]
2020-04-01T09:46:12.2816908Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2817215Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2817982Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2818493Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2819080Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2819733Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2820352Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2820975Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2821472Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2821982Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2822407Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2822608Z 
2020-04-01T09:46:12.2823253Z ""jobmanager-future-thread-31"" #42866 daemon prio=5 os_prio=0 tid=0x00007f1fa801b000 nid=0x40e0 waiting on condition [0x00007f20997d8000]
2020-04-01T09:46:12.2823729Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2824035Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2824739Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2825257Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2825797Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2826473Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2827138Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2827698Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2828199Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2828691Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2829107Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2829305Z 
2020-04-01T09:46:12.2829954Z ""jobmanager-future-thread-30"" #42865 daemon prio=5 os_prio=0 tid=0x00007f1fa8014000 nid=0x40df waiting on condition [0x00007f2164bd8000]
2020-04-01T09:46:12.2830425Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2830882Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2831537Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2832037Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2832599Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2833225Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2833856Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2834423Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2834994Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2835510Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2835918Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2836130Z 
2020-04-01T09:46:12.2836807Z ""jobmanager-future-thread-29"" #42861 daemon prio=5 os_prio=0 tid=0x00007f1f0006f800 nid=0x4059 waiting on condition [0x00007f2098dd0000]
2020-04-01T09:46:12.2837408Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2837713Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2838359Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2838883Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2839446Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2840104Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2840789Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2841359Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2841865Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2842362Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2842778Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2842979Z 
2020-04-01T09:46:12.2843617Z ""jobmanager-future-thread-28"" #42860 daemon prio=5 os_prio=0 tid=0x00007f1f00003000 nid=0x4058 waiting on condition [0x00007f20992d3000]
2020-04-01T09:46:12.2844103Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2844431Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2845135Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2845664Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2846240Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2846947Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2847581Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2848127Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2848635Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2849142Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2849544Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2849742Z 
2020-04-01T09:46:12.2850479Z ""jobmanager-future-thread-27"" #42857 daemon prio=5 os_prio=0 tid=0x00007f1f00011000 nid=0x4055 waiting on condition [0x00007f20994d5000]
2020-04-01T09:46:12.2851057Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2851366Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2852015Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2852521Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2853099Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2853752Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2854365Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2855002Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2855499Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2856013Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2856603Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2856805Z 
2020-04-01T09:46:12.2857441Z ""jobmanager-future-thread-26"" #42852 daemon prio=5 os_prio=0 tid=0x00007f1f3c023800 nid=0x4020 waiting on condition [0x00007f20998d9000]
2020-04-01T09:46:12.2857931Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2858242Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2858864Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2859378Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2859918Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2860578Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2861344Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2861897Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2862401Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2862894Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2863308Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2863508Z 
2020-04-01T09:46:12.2864139Z ""mini-cluster-io-thread-26"" #42851 daemon prio=5 os_prio=0 tid=0x00007f1f00014000 nid=0x401f waiting on condition [0x00007f2099adb000]
2020-04-01T09:46:12.2864681Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2864995Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2865623Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2866140Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2866753Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2867316Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2867815Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2868304Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2868810Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2869228Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2869426Z 
2020-04-01T09:46:12.2870149Z ""jobmanager-future-thread-25"" #42850 daemon prio=5 os_prio=0 tid=0x00007f1fa801a000 nid=0x401e waiting on condition [0x00007f2099bdc000]
2020-04-01T09:46:12.2870772Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2871105Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2871737Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2872262Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2872819Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2873475Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2874102Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2874720Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2875233Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2875728Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2876229Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2876444Z 
2020-04-01T09:46:12.2877129Z ""mini-cluster-io-thread-25"" #42849 daemon prio=5 os_prio=0 tid=0x00007f1fa8018800 nid=0x401d waiting on condition [0x00007f2099cdd000]
2020-04-01T09:46:12.2877608Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2877913Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2878548Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2879044Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2879597Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2880161Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2880738Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2881253Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2881751Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2882165Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2882364Z 
2020-04-01T09:46:12.2882997Z ""jobmanager-future-thread-24"" #42848 daemon prio=5 os_prio=0 tid=0x00007f1f3c022800 nid=0x401c waiting on condition [0x00007f2099edf000]
2020-04-01T09:46:12.2883471Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2883781Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2884397Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2885000Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2885555Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2886188Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2886881Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2887432Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2887933Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2888444Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2888847Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2889129Z 
2020-04-01T09:46:12.2889774Z ""mini-cluster-io-thread-24"" #42847 daemon prio=5 os_prio=0 tid=0x00007f1f3c022000 nid=0x401b waiting on condition [0x00007f209b0ef000]
2020-04-01T09:46:12.2890257Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2890553Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2891292Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2891791Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2892346Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2892904Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2893399Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2893901Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2894399Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2894895Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2895094Z 
2020-04-01T09:46:12.2895831Z ""mini-cluster-io-thread-23"" #42846 daemon prio=5 os_prio=0 tid=0x00007f1f3c00b000 nid=0x401a waiting on condition [0x00007f209b3f2000]
2020-04-01T09:46:12.2896301Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2896670Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2897298Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2897812Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2898362Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2898922Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2899421Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2899910Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2900423Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2900922Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2901121Z 
2020-04-01T09:46:12.2901730Z ""mini-cluster-io-thread-22"" #42845 daemon prio=5 os_prio=0 tid=0x00007f1f3c017800 nid=0x4018 waiting on condition [0x00007f209a3e4000]
2020-04-01T09:46:12.2902216Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2902627Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2903225Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2903717Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2904236Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2904866Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2905348Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2905821Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2906313Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2906752Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2906957Z 
2020-04-01T09:46:12.2907558Z ""mini-cluster-io-thread-21"" #42844 daemon prio=5 os_prio=0 tid=0x00007f1f3c019800 nid=0x4017 waiting on condition [0x00007f209a1e2000]
2020-04-01T09:46:12.2908027Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2908309Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2909015Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2909497Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2910033Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2910588Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2911103Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2911586Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2912061Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2912460Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2912652Z 
2020-04-01T09:46:12.2913270Z ""jobmanager-future-thread-23"" #42843 daemon prio=5 os_prio=0 tid=0x00007f1f3c002800 nid=0x4016 waiting on condition [0x00007f2099fe0000]
2020-04-01T09:46:12.2913745Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2914056Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2914734Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2915311Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2915866Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2916532Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2917328Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2917903Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2918383Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2918877Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2919262Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2919478Z 
2020-04-01T09:46:12.2920119Z ""jobmanager-future-thread-22"" #42839 daemon prio=5 os_prio=0 tid=0x00007f1ff000c000 nid=0x4011 waiting on condition [0x00007f209b5f4000]
2020-04-01T09:46:12.2920599Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2920979Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2921603Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2922099Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2922620Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2923249Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2923845Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2924402Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2924970Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2925447Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2925846Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2926039Z 
2020-04-01T09:46:12.2926708Z ""jobmanager-future-thread-21"" #42834 daemon prio=5 os_prio=0 tid=0x00007f1fa8017800 nid=0x3d6e waiting on condition [0x00007f209a5e6000]
2020-04-01T09:46:12.2927170Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2927465Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2928188Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2928686Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2929229Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2929838Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2930444Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2931045Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2931536Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2932021Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2932414Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2932605Z 
2020-04-01T09:46:12.2933222Z ""jobmanager-future-thread-20"" #42833 daemon prio=5 os_prio=0 tid=0x00007f1fa8016800 nid=0x3d6d waiting on condition [0x00007f2164ad7000]
2020-04-01T09:46:12.2933777Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2934062Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2934766Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2935254Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2935783Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2936405Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2937048Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2937597Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2938065Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2938557Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2938943Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2939150Z 
2020-04-01T09:46:12.2939752Z ""jobmanager-future-thread-19"" #42832 daemon prio=5 os_prio=0 tid=0x00007f1fa8015800 nid=0x3d6c waiting on condition [0x00007f209b9f8000]
2020-04-01T09:46:12.2940221Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2940522Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2941192Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2941691Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2942216Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2942833Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2943427Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2943968Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2944454Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2945009Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2945413Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2945604Z 
2020-04-01T09:46:12.2946270Z ""flink-metrics-akka.remote.default-remote-dispatcher-16"" #42829 prio=1 os_prio=0 tid=0x00007f1ee8023000 nid=0x3c9b waiting on condition [0x00007f21668ef000]
2020-04-01T09:46:12.2946902Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2947200Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2947788Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.2948263Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.2948705Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.2949155Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.2949447Z 
2020-04-01T09:46:12.2950037Z ""jobmanager-future-thread-18"" #42825 daemon prio=5 os_prio=0 tid=0x00007f1fa8013000 nid=0x3937 waiting on condition [0x00007f209b7f6000]
2020-04-01T09:46:12.2950511Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2950862Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2951476Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2951961Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2952503Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2953218Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2953811Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2954353Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2954904Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2955405Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2955812Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2956007Z 
2020-04-01T09:46:12.2956661Z ""jobmanager-future-thread-17"" #42823 daemon prio=5 os_prio=0 tid=0x00007f1fa8012000 nid=0x3935 waiting on condition [0x00007f209baf9000]
2020-04-01T09:46:12.2957146Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.2957462Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2958066Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2958574Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.2959109Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.2959745Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.2960359Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2960950Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2961434Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2961913Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2962311Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2962503Z 
2020-04-01T09:46:12.2963117Z ""jobmanager-future-thread-16"" #42822 daemon prio=5 os_prio=0 tid=0x00007f1fa8011800 nid=0x3934 waiting on condition [0x00007f209bbfa000]
2020-04-01T09:46:12.2963577Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2963877Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2964488Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2965038Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2965659Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2966261Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2966920Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2967459Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2967929Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2968418Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2968804Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2969015Z 
2020-04-01T09:46:12.2969630Z ""jobmanager-future-thread-15"" #42817 daemon prio=5 os_prio=0 tid=0x00007f1fa800f800 nid=0x3910 waiting on condition [0x00007f209bdfc000]
2020-04-01T09:46:12.2970105Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2970391Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2971069Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2971651Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2972172Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2972789Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2973381Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2973926Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2974413Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2974965Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2975371Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2975563Z 
2020-04-01T09:46:12.2976189Z ""mini-cluster-io-thread-20"" #42816 daemon prio=5 os_prio=0 tid=0x00007f1fa800e800 nid=0x390f waiting on condition [0x00007f209befd000]
2020-04-01T09:46:12.2976687Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2976971Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2977573Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2978072Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2978592Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2979143Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2979635Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2980110Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2980655Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2981151Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2981461Z 
2020-04-01T09:46:12.2982183Z ""jobmanager-future-thread-14"" #42815 daemon prio=5 os_prio=0 tid=0x00007f203c014800 nid=0x390e waiting on condition [0x00007f209bffe000]
2020-04-01T09:46:12.2982657Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2982938Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2983553Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2984053Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2984776Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2985407Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2986003Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2986602Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2987095Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2987572Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2987977Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2988171Z 
2020-04-01T09:46:12.2988801Z ""mini-cluster-io-thread-19"" #42814 daemon prio=5 os_prio=0 tid=0x00007f203c00c800 nid=0x390d waiting on condition [0x00007f21643d0000]
2020-04-01T09:46:12.2989260Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2989553Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2990150Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2990842Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2991379Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2991916Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.2992391Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2992864Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2993349Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.2993752Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.2993944Z 
2020-04-01T09:46:12.2994623Z ""jobmanager-future-thread-13"" #42813 daemon prio=5 os_prio=0 tid=0x00007f1f3c020800 nid=0x390c waiting on condition [0x00007f21644d1000]
2020-04-01T09:46:12.2995110Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.2995405Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.2996012Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.2996554Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.2997075Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.2997695Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.2998300Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.2998834Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.2999315Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.2999796Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3000193Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3000385Z 
2020-04-01T09:46:12.3001080Z ""mini-cluster-io-thread-18"" #42812 daemon prio=5 os_prio=0 tid=0x00007f1f3c01b000 nid=0x390b waiting on condition [0x00007f21646d3000]
2020-04-01T09:46:12.3001535Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3001834Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3002432Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3002930Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3003554Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3004092Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3004645Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3005118Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3005608Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3006004Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3006196Z 
2020-04-01T09:46:12.3006849Z ""mini-cluster-io-thread-17"" #42811 daemon prio=5 os_prio=0 tid=0x00007f1f3c005800 nid=0x390a waiting on condition [0x00007f21647d4000]
2020-04-01T09:46:12.3007322Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3007620Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3008226Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3008724Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3009322Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3009872Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3010352Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3010882Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3011373Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3011759Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3011967Z 
2020-04-01T09:46:12.3012567Z ""mini-cluster-io-thread-16"" #42810 daemon prio=5 os_prio=0 tid=0x00007f1fa800e000 nid=0x3909 waiting on condition [0x00007f21667ee000]
2020-04-01T09:46:12.3013043Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3013327Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3014234Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3015093Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3015622Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3016182Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3016710Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3017207Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3017694Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3018091Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3018287Z 
2020-04-01T09:46:12.3018941Z ""mini-cluster-io-thread-15"" #42809 daemon prio=5 os_prio=0 tid=0x00007f1fa8001000 nid=0x3908 waiting on condition [0x00007f2165be4000]
2020-04-01T09:46:12.3019423Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3019709Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3020329Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3020889Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3021424Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3021963Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3022441Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3023075Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3023555Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3023965Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3024157Z 
2020-04-01T09:46:12.3024961Z ""jobmanager-future-thread-12"" #42808 daemon prio=5 os_prio=0 tid=0x00007f1fa8004800 nid=0x3907 waiting on condition [0x00007f209b4f3000]
2020-04-01T09:46:12.3025430Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3025726Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3026343Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3026898Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3027437Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3028049Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3028657Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3029311Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3029806Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3030300Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3030771Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3030980Z 
2020-04-01T09:46:12.3031592Z ""jobmanager-future-thread-11"" #42804 daemon prio=5 os_prio=0 tid=0x00007f2004018800 nid=0x3903 waiting on condition [0x00007f2164edb000]
2020-04-01T09:46:12.3032062Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3032345Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3032963Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3033444Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3033981Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3034736Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3035393Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3035934Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3036404Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3036952Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3037359Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3037551Z 
2020-04-01T09:46:12.3038171Z ""jobmanager-future-thread-10"" #42797 daemon prio=5 os_prio=0 tid=0x00007f203c00a800 nid=0x38d0 waiting on condition [0x00007f2164cd9000]
2020-04-01T09:46:12.3038662Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3038971Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3039570Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3040072Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3040676Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3041307Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3041913Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3042558Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3043049Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3043537Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3043937Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3044130Z 
2020-04-01T09:46:12.3044961Z ""jobmanager-future-thread-9"" #42796 daemon prio=5 os_prio=0 tid=0x00007f203c004800 nid=0x38cf waiting on condition [0x00007f20989cc000]
2020-04-01T09:46:12.3045426Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3045723Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3046356Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3046900Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3047435Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3048040Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3048755Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3049295Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3049768Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3050258Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3050707Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3050916Z 
2020-04-01T09:46:12.3051522Z ""jobmanager-future-thread-8"" #42795 daemon prio=5 os_prio=0 tid=0x00007f203c002800 nid=0x38ce waiting on condition [0x00007f21665ec000]
2020-04-01T09:46:12.3052006Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3052302Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3052918Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3053423Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3053967Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3054740Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3055336Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3055882Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3056372Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3056908Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3057312Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3057504Z 
2020-04-01T09:46:12.3058135Z ""jobmanager-future-thread-7"" #42791 daemon prio=5 os_prio=0 tid=0x00007f1fe401e800 nid=0x387d waiting on condition [0x00007f2165ae3000]
2020-04-01T09:46:12.3058605Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3058914Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3059509Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3060010Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3060547Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3061327Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3061934Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3062468Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3062958Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3063434Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3063841Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3064033Z 
2020-04-01T09:46:12.3064805Z ""jobmanager-future-thread-6"" #42789 daemon prio=5 os_prio=0 tid=0x00007f1fe400d000 nid=0x387b waiting on condition [0x00007f21662eb000]
2020-04-01T09:46:12.3065301Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3065602Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3066229Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3066771Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3067416Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3068030Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3068636Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3069179Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3069654Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3070155Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3070546Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3070814Z 
2020-04-01T09:46:12.3071424Z ""jobmanager-future-thread-5"" #42787 daemon prio=5 os_prio=0 tid=0x00007f1fe400c000 nid=0x3879 waiting on condition [0x00007f21642cf000]
2020-04-01T09:46:12.3071898Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3072179Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3072787Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3073284Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3073803Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3074422Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3075202Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3075756Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3076240Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3076772Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3077175Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3077367Z 
2020-04-01T09:46:12.3078052Z ""flink-metrics-akka.remote.default-remote-dispatcher-15"" #42784 prio=1 os_prio=0 tid=0x00007f1ee8022800 nid=0x385b waiting on condition [0x00007f21885fd000]
2020-04-01T09:46:12.3078540Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3078838Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3079410Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3079983Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3080423Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3080939Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3081236Z 
2020-04-01T09:46:12.3081844Z ""Flink-MetricRegistry-thread-1"" #42780 daemon prio=5 os_prio=0 tid=0x00007f1ebc003000 nid=0x3854 waiting on condition [0x00007f2166ff2000]
2020-04-01T09:46:12.3082341Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3082633Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3083244Z 	- parking to wait for  <0x0000000087ec9df0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3083737Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3084292Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3085702Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3086296Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3087000Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3087472Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3087962Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3088363Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3088553Z 
2020-04-01T09:46:12.3089175Z ""jobmanager-future-thread-4"" #42779 daemon prio=5 os_prio=0 tid=0x00007f1fe400a800 nid=0x3853 waiting on condition [0x00007f21672f5000]
2020-04-01T09:46:12.3089645Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3089951Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3090547Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3091099Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3091627Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3092247Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
2020-04-01T09:46:12.3092849Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3093380Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3093870Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3094346Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3094836Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3095028Z 
2020-04-01T09:46:12.3095642Z ""mini-cluster-io-thread-14"" #42778 daemon prio=5 os_prio=0 tid=0x00007f1fbc007800 nid=0x3852 waiting on condition [0x00007f21678fb000]
2020-04-01T09:46:12.3096101Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3096407Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3097082Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3097563Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3098100Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3098636Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3099111Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3099686Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3100164Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3100570Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3100834Z 
2020-04-01T09:46:12.3101416Z ""pool-121-thread-1"" #42777 prio=5 os_prio=0 tid=0x00007f1fbc006800 nid=0x3851 waiting on condition [0x00007f2167dfe000]
2020-04-01T09:46:12.3101863Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3102175Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3102765Z 	- parking to wait for  <0x00000000880850b8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3103263Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3103816Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3104436Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3105136Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3105749Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3106238Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3106786Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3107176Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3107367Z 
2020-04-01T09:46:12.3107992Z ""jobmanager-future-thread-3"" #42776 daemon prio=5 os_prio=0 tid=0x00007f1fbc003000 nid=0x3850 waiting on condition [0x00007f21881fb000]
2020-04-01T09:46:12.3108478Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3108778Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3109391Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3109885Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3110444Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3111140Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3111747Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3112288Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3112760Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3113248Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3113641Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3113849Z 
2020-04-01T09:46:12.3114443Z ""mini-cluster-io-thread-13"" #42775 daemon prio=5 os_prio=0 tid=0x00007f1fbc001000 nid=0x384f waiting on condition [0x00007f21d0a85000]
2020-04-01T09:46:12.3115005Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3115302Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3115907Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3116399Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3116974Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3117525Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3117987Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3118563Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3119056Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3119445Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3119654Z 
2020-04-01T09:46:12.3120262Z ""jobmanager-future-thread-2"" #42774 daemon prio=5 os_prio=0 tid=0x00007f1fe400a000 nid=0x384e waiting on condition [0x00007f21677fa000]
2020-04-01T09:46:12.3120834Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3121129Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3121748Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3122237Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3122795Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3123427Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3124017Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3124741Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3125213Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3125709Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3126110Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3126304Z 
2020-04-01T09:46:12.3126967Z ""mini-cluster-io-thread-12"" #42773 daemon prio=5 os_prio=0 tid=0x00007f1fe4009000 nid=0x384d waiting on condition [0x00007f1ee598b000]
2020-04-01T09:46:12.3127438Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3127739Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3128338Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3128830Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3129354Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3129903Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3130381Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3130903Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3131394Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3131780Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3131983Z 
2020-04-01T09:46:12.3132579Z ""mini-cluster-io-thread-11"" #42772 daemon prio=5 os_prio=0 tid=0x00007f1fe4008800 nid=0x384c waiting on condition [0x00007f209a4e5000]
2020-04-01T09:46:12.3133044Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3133329Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3133943Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3134440Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3135031Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3135584Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3136046Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3136580Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3137144Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3137549Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3137740Z 
2020-04-01T09:46:12.3138363Z ""mini-cluster-io-thread-10"" #42771 daemon prio=5 os_prio=0 tid=0x00007f2084026800 nid=0x384b waiting on condition [0x00007f209b2f1000]
2020-04-01T09:46:12.3138835Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3139122Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3139741Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3140221Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3140811Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3141347Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3141830Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3142319Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3142793Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3143273Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3143467Z 
2020-04-01T09:46:12.3144077Z ""mini-cluster-io-thread-9"" #42770 daemon prio=5 os_prio=0 tid=0x00007f2084003000 nid=0x384a waiting on condition [0x00007f1f9d4db000]
2020-04-01T09:46:12.3144606Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3144910Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3145515Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3146008Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3146596Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3147137Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3147612Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3148086Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3148573Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3148977Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3149168Z 
2020-04-01T09:46:12.3149765Z ""jobmanager-future-thread-1"" #42769 daemon prio=5 os_prio=0 tid=0x00007f2084002000 nid=0x3849 waiting on condition [0x00007f209a0e1000]
2020-04-01T09:46:12.3150249Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3150556Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3151215Z 	- parking to wait for  <0x0000000087e42190> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3151726Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3152263Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3152897Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3153504Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3154033Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3154593Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3155073Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3155475Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3155750Z 
2020-04-01T09:46:12.3156370Z ""mini-cluster-io-thread-8"" #42768 daemon prio=5 os_prio=0 tid=0x00007f1fe4007800 nid=0x3848 waiting on condition [0x00007f1f9ecf5000]
2020-04-01T09:46:12.3156868Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3157173Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3157777Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3158270Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3158808Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3159343Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3159820Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3160289Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3160844Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3161243Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3161510Z 
2020-04-01T09:46:12.3162109Z ""mini-cluster-io-thread-7"" #42767 daemon prio=5 os_prio=0 tid=0x00007f1fe4007000 nid=0x3847 waiting on condition [0x00007f21d0dc7000]
2020-04-01T09:46:12.3162576Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3162875Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3163472Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3163966Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3164487Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3165111Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3165593Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3166062Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3166607Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3166994Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3167201Z 
2020-04-01T09:46:12.3167796Z ""mini-cluster-io-thread-6"" #42766 daemon prio=5 os_prio=0 tid=0x00007f1fe4006000 nid=0x3846 waiting on condition [0x00007f209abec000]
2020-04-01T09:46:12.3168262Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3168546Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3169162Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3169658Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3170184Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3170793Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3171261Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3171745Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3172221Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3172620Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3172812Z 
2020-04-01T09:46:12.3173417Z ""mini-cluster-io-thread-5"" #42765 daemon prio=5 os_prio=0 tid=0x00007f1e87fed000 nid=0x3845 waiting on condition [0x00007f1f9d8df000]
2020-04-01T09:46:12.3173879Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3174164Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3174954Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3175438Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3175976Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3176558Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3177039Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3177526Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3178004Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3178405Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3178596Z 
2020-04-01T09:46:12.3179198Z ""mini-cluster-io-thread-4"" #42764 daemon prio=5 os_prio=0 tid=0x00007f1fd4001800 nid=0x3844 waiting on condition [0x00007f209bcfb000]
2020-04-01T09:46:12.3179655Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3179956Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3180772Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3181267Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3181806Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3182343Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3182825Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3183295Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3183786Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3184188Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3184380Z 
2020-04-01T09:46:12.3185045Z ""mini-cluster-io-thread-3"" #42763 daemon prio=5 os_prio=0 tid=0x00007f1ebc002000 nid=0x3843 waiting on condition [0x00007f209b1f0000]
2020-04-01T09:46:12.3185518Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3185813Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3186416Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3186955Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3187476Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3188025Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3188505Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3188978Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3189473Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3189863Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3190071Z 
2020-04-01T09:46:12.3190724Z ""mini-cluster-io-thread-2"" #42762 daemon prio=5 os_prio=0 tid=0x00007f1e87fec800 nid=0x3842 waiting on condition [0x00007f1f9eff6000]
2020-04-01T09:46:12.3191193Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3191477Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3192095Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3192574Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3193108Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3193747Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3194210Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3194773Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3195259Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3195660Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3195852Z 
2020-04-01T09:46:12.3196509Z ""mini-cluster-io-thread-1"" #42761 daemon prio=5 os_prio=0 tid=0x00007f211ee98000 nid=0x3841 waiting on condition [0x00007f209aeed000]
2020-04-01T09:46:12.3196967Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3197267Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3197888Z 	- parking to wait for  <0x0000000087e58b10> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3198377Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3198912Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3199545Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3200024Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3200510Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3201053Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3201458Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3201651Z 
2020-04-01T09:46:12.3202276Z ""flink-rest-server-netty-boss-thread-1"" #42760 daemon prio=5 os_prio=0 tid=0x00007f211ee97000 nid=0x3840 runnable [0x00007f209b8f7000]
2020-04-01T09:46:12.3202718Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3203038Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3203404Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3203830Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3204244Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3204990Z 	- locked <0x0000000087e522c0> (a org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet)
2020-04-01T09:46:12.3205617Z 	- locked <0x0000000087e522b0> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3206142Z 	- locked <0x0000000087e522d8> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3206568Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3207099Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:62)
2020-04-01T09:46:12.3207732Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:806)
2020-04-01T09:46:12.3208284Z 	at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:454)
2020-04-01T09:46:12.3208882Z 	at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
2020-04-01T09:46:12.3209523Z 	at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
2020-04-01T09:46:12.3209960Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3210163Z 
2020-04-01T09:46:12.3210526Z ""IOManager reader thread #1"" #42755 daemon prio=5 os_prio=0 tid=0x00007f22d8354800 nid=0x383f waiting on condition [0x00007f20987ca000]
2020-04-01T09:46:12.3211056Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3211340Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3211962Z 	- parking to wait for  <0x0000000088072b30> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3212555Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3213073Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3213630Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3214137Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$ReaderThread.run(IOManagerAsync.java:354)
2020-04-01T09:46:12.3214472Z 
2020-04-01T09:46:12.3214930Z ""IOManager writer thread #1"" #42754 daemon prio=5 os_prio=0 tid=0x00007f22d8354000 nid=0x383e waiting on condition [0x00007f2165ce5000]
2020-04-01T09:46:12.3215400Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3215683Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3216321Z 	- parking to wait for  <0x0000000087e37990> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3216876Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3217395Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3218033Z 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
2020-04-01T09:46:12.3218542Z 	at org.apache.flink.runtime.io.disk.iomanager.IOManagerAsync$WriterThread.run(IOManagerAsync.java:460)
2020-04-01T09:46:12.3218877Z 
2020-04-01T09:46:12.3219429Z ""Timer-830"" #42752 daemon prio=5 os_prio=0 tid=0x00007f22d87eb800 nid=0x383d in Object.wait() [0x00007f20991d2000]
2020-04-01T09:46:12.3219888Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-04-01T09:46:12.3220201Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3220520Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-04-01T09:46:12.3221081Z 	- locked <0x0000000087e45ba8> (a java.util.TaskQueue)
2020-04-01T09:46:12.3221406Z 	at java.util.TimerThread.run(Timer.java:505)
2020-04-01T09:46:12.3221610Z 
2020-04-01T09:46:12.3222159Z ""Timer-829"" #42750 daemon prio=5 os_prio=0 tid=0x00007f22d87eb000 nid=0x383c in Object.wait() [0x00007f2098ed1000]
2020-04-01T09:46:12.3222618Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-04-01T09:46:12.3222935Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3223256Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-04-01T09:46:12.3223736Z 	- locked <0x0000000087e45b68> (a java.util.TaskQueue)
2020-04-01T09:46:12.3224072Z 	at java.util.TimerThread.run(Timer.java:505)
2020-04-01T09:46:12.3224270Z 
2020-04-01T09:46:12.3224710Z ""BLOB Server listener at 41242"" #42746 daemon prio=5 os_prio=0 tid=0x00007f22d94a6000 nid=0x383b runnable [0x00007f2165fea000]
2020-04-01T09:46:12.3225136Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3225457Z 	at java.net.PlainSocketImpl.socketAccept(Native Method)
2020-04-01T09:46:12.3225846Z 	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
2020-04-01T09:46:12.3226286Z 	at java.net.ServerSocket.implAccept(ServerSocket.java:560)
2020-04-01T09:46:12.3226731Z 	at java.net.ServerSocket.accept(ServerSocket.java:528)
2020-04-01T09:46:12.3227121Z 	at org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:254)
2020-04-01T09:46:12.3227387Z 
2020-04-01T09:46:12.3227940Z ""Timer-828"" #42747 daemon prio=5 os_prio=0 tid=0x00007f22d94a5800 nid=0x383a in Object.wait() [0x00007f1f9dbe0000]
2020-04-01T09:46:12.3228403Z    java.lang.Thread.State: TIMED_WAITING (on object monitor)
2020-04-01T09:46:12.3228718Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3229037Z 	at java.util.TimerThread.mainLoop(Timer.java:552)
2020-04-01T09:46:12.3229515Z 	- locked <0x0000000087e45b90> (a java.util.TaskQueue)
2020-04-01T09:46:12.3229851Z 	at java.util.TimerThread.run(Timer.java:505)
2020-04-01T09:46:12.3230049Z 
2020-04-01T09:46:12.3230397Z ""New I/O server boss #408"" #42745 prio=1 os_prio=0 tid=0x00007f1ee8020000 nid=0x3839 runnable [0x00007f1f9dce1000]
2020-04-01T09:46:12.3230941Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3231258Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3231639Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3232052Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3232480Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3233000Z 	- locked <0x0000000087e3b248> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3233523Z 	- locked <0x0000000087e3b238> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3234044Z 	- locked <0x0000000087e3b258> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3234417Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3234874Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
2020-04-01T09:46:12.3235360Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
2020-04-01T09:46:12.3235991Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3236648Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
2020-04-01T09:46:12.3237309Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3237907Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3238452Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3238949Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3239334Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3239540Z 
2020-04-01T09:46:12.3239864Z ""New I/O worker #407"" #42744 prio=1 os_prio=0 tid=0x00007f1ee801f800 nid=0x3838 runnable [0x00007f21653de000]
2020-04-01T09:46:12.3240269Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3240569Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3241002Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3241415Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3241842Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3242393Z 	- locked <0x0000000087e59310> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3242906Z 	- locked <0x0000000087e59300> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3243444Z 	- locked <0x0000000087e59320> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3243803Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3244293Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3244979Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3245638Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3246279Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3246914Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3247507Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3248128Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3248672Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3249165Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3249637Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3249846Z 
2020-04-01T09:46:12.3250170Z ""New I/O worker #406"" #42743 prio=1 os_prio=0 tid=0x00007f1ee800d000 nid=0x3837 runnable [0x00007f21882fc000]
2020-04-01T09:46:12.3250577Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3250932Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3251314Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3251720Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3252147Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3252701Z 	- locked <0x0000000087e335a0> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3253214Z 	- locked <0x0000000087e33590> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3253753Z 	- locked <0x0000000087e335b0> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3254118Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3254684Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3255318Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3256056Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3256744Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3257324Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3257909Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3258534Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3259084Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3259580Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3259968Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3260179Z 
2020-04-01T09:46:12.3260498Z ""New I/O boss #405"" #42742 prio=1 os_prio=0 tid=0x00007f1ee800c800 nid=0x3836 runnable [0x00007f21641ce000]
2020-04-01T09:46:12.3260957Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3261259Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3261637Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3262047Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3262465Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3263016Z 	- locked <0x000000008808f1d0> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3263537Z 	- locked <0x000000008808f1c0> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3264077Z 	- locked <0x000000008805a500> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3264434Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3265000Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3265638Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3266276Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3266942Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
2020-04-01T09:46:12.3267532Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3268250Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3268807Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3269287Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3269687Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3269879Z 
2020-04-01T09:46:12.3270213Z ""New I/O worker #404"" #42740 prio=1 os_prio=0 tid=0x00007f1ee8087000 nid=0x3835 runnable [0x00007f21666ed000]
2020-04-01T09:46:12.3270601Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3270991Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3271355Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3271775Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3272183Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3272739Z 	- locked <0x0000000087e38058> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3273273Z 	- locked <0x0000000087e38048> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3273794Z 	- locked <0x0000000087e38068> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3274240Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3274788Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3275419Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3276077Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3276748Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3277347Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3277921Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3278543Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3279106Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3279580Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3279982Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3280174Z 
2020-04-01T09:46:12.3280509Z ""New I/O worker #403"" #42739 prio=1 os_prio=0 tid=0x00007f1ee8001000 nid=0x3834 runnable [0x00007f2099dde000]
2020-04-01T09:46:12.3280959Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3281279Z 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2020-04-01T09:46:12.3281643Z 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
2020-04-01T09:46:12.3282066Z 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)
2020-04-01T09:46:12.3282474Z 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
2020-04-01T09:46:12.3283031Z 	- locked <0x0000000087e595d8> (a sun.nio.ch.Util$3)
2020-04-01T09:46:12.3283554Z 	- locked <0x0000000087e595c8> (a java.util.Collections$UnmodifiableSet)
2020-04-01T09:46:12.3284075Z 	- locked <0x0000000087e595e8> (a sun.nio.ch.EPollSelectorImpl)
2020-04-01T09:46:12.3284448Z 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
2020-04-01T09:46:12.3284995Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
2020-04-01T09:46:12.3285624Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
2020-04-01T09:46:12.3286276Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
2020-04-01T09:46:12.3287041Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
2020-04-01T09:46:12.3287641Z 	at org.apache.flink.shaded.akka.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2020-04-01T09:46:12.3288218Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2020-04-01T09:46:12.3288839Z 	at org.apache.flink.shaded.akka.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2020-04-01T09:46:12.3289393Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-04-01T09:46:12.3289871Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3290270Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3290462Z 
2020-04-01T09:46:12.3291185Z ""flink-metrics-akka.remote.default-remote-dispatcher-6"" #42737 prio=1 os_prio=0 tid=0x00007f20b8010000 nid=0x3833 waiting on condition [0x00007f21645d2000]
2020-04-01T09:46:12.3291681Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3291978Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3292631Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3293102Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3293544Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3293997Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3294283Z 
2020-04-01T09:46:12.3294998Z ""flink-metrics-akka.remote.default-remote-dispatcher-7"" #42738 prio=1 os_prio=0 tid=0x00007f2110002000 nid=0x3832 waiting on condition [0x00007f209a6e7000]
2020-04-01T09:46:12.3295508Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3295794Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3296388Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3296898Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3297341Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3297806Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3298082Z 
2020-04-01T09:46:12.3298726Z ""flink-metrics-akka.remote.default-remote-dispatcher-5"" #42736 prio=1 os_prio=0 tid=0x00007f1f28038000 nid=0x3831 waiting on condition [0x00007f209aaeb000]
2020-04-01T09:46:12.3299228Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3299523Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3300092Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3300567Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3301064Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3301534Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3301815Z 
2020-04-01T09:46:12.3302472Z ""flink-metrics-akka.remote.default-remote-dispatcher-4"" #42735 prio=1 os_prio=0 tid=0x00007f20b8063800 nid=0x3830 waiting on condition [0x00007f2098bce000]
2020-04-01T09:46:12.3302953Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3303247Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3303829Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3304286Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3304789Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3305240Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3305625Z 
2020-04-01T09:46:12.3306273Z ""flink-metrics-akka.remote.default-remote-dispatcher-3"" #42734 prio=1 os_prio=0 tid=0x00007f20b8065800 nid=0x382f waiting on condition [0x00007f209afee000]
2020-04-01T09:46:12.3306832Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3307115Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3307717Z 	- parking to wait for  <0x0000000087e36cc0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
2020-04-01T09:46:12.3308191Z 	at akka.dispatch.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
2020-04-01T09:46:12.3308615Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-04-01T09:46:12.3309079Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-04-01T09:46:12.3309354Z 
2020-04-01T09:46:12.3309908Z ""flink-metrics-2"" #42733 prio=1 os_prio=0 tid=0x00007f22d94ab000 nid=0x382e waiting on condition [0x00007f20999da000]
2020-04-01T09:46:12.3310351Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3310716Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3311319Z 	- parking to wait for  <0x0000000087e42ae0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3311910Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3312465Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3313015Z 	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
2020-04-01T09:46:12.3313495Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-04-01T09:46:12.3313965Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3314800Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3315214Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3315408Z 
2020-04-01T09:46:12.3315997Z ""flink-metrics-scheduler-1"" #42732 prio=5 os_prio=0 tid=0x00007f22d94aa800 nid=0x382d waiting on condition [0x00007f2164dda000]
2020-04-01T09:46:12.3316530Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-04-01T09:46:12.3316848Z 	at java.lang.Thread.sleep(Native Method)
2020-04-01T09:46:12.3317237Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-04-01T09:46:12.3317780Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-04-01T09:46:12.3318307Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-04-01T09:46:12.3318737Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3318928Z 
2020-04-01T09:46:12.3319473Z ""flink-scheduler-1"" #42727 prio=5 os_prio=0 tid=0x00007f1e8bffc800 nid=0x3828 sleeping[0x00007f209b6f5000]
2020-04-01T09:46:12.3319894Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2020-04-01T09:46:12.3320214Z 	at java.lang.Thread.sleep(Native Method)
2020-04-01T09:46:12.3320600Z 	at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:85)
2020-04-01T09:46:12.3321244Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:265)
2020-04-01T09:46:12.3321790Z 	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
2020-04-01T09:46:12.3322196Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3322405Z 
2020-04-01T09:46:12.3323040Z ""FlinkCompletableFutureDelayScheduler-thread-1"" #72 daemon prio=5 os_prio=0 tid=0x00007f2168030000 nid=0xf55 waiting on condition [0x00007f21656df000]
2020-04-01T09:46:12.3323542Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3323823Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3324436Z 	- parking to wait for  <0x0000000088bb0640> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3325133Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3325672Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
2020-04-01T09:46:12.3326303Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
2020-04-01T09:46:12.3326950Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3327498Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3327970Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3328466Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3328869Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3329061Z 
2020-04-01T09:46:12.3329406Z ""process reaper"" #24 daemon prio=10 os_prio=0 tid=0x00007f2178040000 nid=0xefd waiting on condition [0x00007f21d1e05000]
2020-04-01T09:46:12.3329864Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3330173Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3330883Z 	- parking to wait for  <0x00000000807233f8> (a java.util.concurrent.SynchronousQueue$TransferStack)
2020-04-01T09:46:12.3331363Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3331850Z 	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
2020-04-01T09:46:12.3332389Z 	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
2020-04-01T09:46:12.3332885Z 	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
2020-04-01T09:46:12.3333339Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)
2020-04-01T09:46:12.3333828Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3334314Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3334795Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3334991Z 
2020-04-01T09:46:12.3335611Z ""surefire-forkedjvm-ping-30s"" #23 daemon prio=5 os_prio=0 tid=0x00007f22d83cb800 nid=0xef7 waiting on condition [0x00007f21d26b6000]
2020-04-01T09:46:12.3336075Z    java.lang.Thread.State: TIMED_WAITING (parking)
2020-04-01T09:46:12.3336384Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3337034Z 	- parking to wait for  <0x0000000080733098> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2020-04-01T09:46:12.3337544Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2020-04-01T09:46:12.3338100Z 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
2020-04-01T09:46:12.3338721Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
2020-04-01T09:46:12.3339331Z 	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
2020-04-01T09:46:12.3339885Z 	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
2020-04-01T09:46:12.3340354Z 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
2020-04-01T09:46:12.3340908Z 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-04-01T09:46:12.3341315Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3341507Z 
2020-04-01T09:46:12.3342104Z ""surefire-forkedjvm-command-thread"" #22 daemon prio=5 os_prio=0 tid=0x00007f22d83b4000 nid=0xef4 runnable [0x00007f21d29c1000]
2020-04-01T09:46:12.3342529Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3342843Z 	at java.io.FileInputStream.readBytes(Native Method)
2020-04-01T09:46:12.3343284Z 	at java.io.FileInputStream.read(FileInputStream.java:255)
2020-04-01T09:46:12.3343697Z 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
2020-04-01T09:46:12.3344111Z 	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
2020-04-01T09:46:12.3344760Z 	- locked <0x0000000080725688> (a java.io.BufferedInputStream)
2020-04-01T09:46:12.3345149Z 	at java.io.DataInputStream.readInt(DataInputStream.java:387)
2020-04-01T09:46:12.3345606Z 	at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)
2020-04-01T09:46:12.3346149Z 	at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:391)
2020-04-01T09:46:12.3346607Z 	at java.lang.Thread.run(Thread.java:748)
2020-04-01T09:46:12.3346816Z 
2020-04-01T09:46:12.3347134Z ""Service Thread"" #21 daemon prio=9 os_prio=0 tid=0x00007f22d82cb800 nid=0xef0 runnable [0x0000000000000000]
2020-04-01T09:46:12.3347532Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3347708Z 
2020-04-01T09:46:12.3348080Z ""C1 CompilerThread14"" #20 daemon prio=9 os_prio=0 tid=0x00007f22d82c8000 nid=0xeed waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3348497Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3348692Z 
2020-04-01T09:46:12.3349115Z ""C1 CompilerThread13"" #19 daemon prio=9 os_prio=0 tid=0x00007f22d82c6000 nid=0xeeb waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3349548Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3349726Z 
2020-04-01T09:46:12.3350094Z ""C1 CompilerThread12"" #18 daemon prio=9 os_prio=0 tid=0x00007f22d82c4800 nid=0xeea waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3350507Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3350735Z 
2020-04-01T09:46:12.3351095Z ""C1 CompilerThread11"" #17 daemon prio=9 os_prio=0 tid=0x00007f22d82c2000 nid=0xee8 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3351529Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3351705Z 
2020-04-01T09:46:12.3352047Z ""C1 CompilerThread10"" #16 daemon prio=9 os_prio=0 tid=0x00007f22d82c0800 nid=0xee6 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3352482Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3352658Z 
2020-04-01T09:46:12.3353017Z ""C2 CompilerThread9"" #15 daemon prio=9 os_prio=0 tid=0x00007f22d82bd800 nid=0xee4 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3353449Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3353624Z 
2020-04-01T09:46:12.3353964Z ""C2 CompilerThread8"" #14 daemon prio=9 os_prio=0 tid=0x00007f22d82bb800 nid=0xee2 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3354391Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3354637Z 
2020-04-01T09:46:12.3354988Z ""C2 CompilerThread7"" #13 daemon prio=9 os_prio=0 tid=0x00007f22d82b9800 nid=0xee1 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3355398Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3355585Z 
2020-04-01T09:46:12.3355923Z ""C2 CompilerThread6"" #12 daemon prio=9 os_prio=0 tid=0x00007f22d82b7800 nid=0xedf waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3356350Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3356572Z 
2020-04-01T09:46:12.3356930Z ""C2 CompilerThread5"" #11 daemon prio=9 os_prio=0 tid=0x00007f22d82b5000 nid=0xedc waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3357340Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3357535Z 
2020-04-01T09:46:12.3357877Z ""C2 CompilerThread4"" #10 daemon prio=9 os_prio=0 tid=0x00007f22d82b3000 nid=0xedb waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3358303Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3358480Z 
2020-04-01T09:46:12.3358837Z ""C2 CompilerThread3"" #9 daemon prio=9 os_prio=0 tid=0x00007f22d82a9000 nid=0xed9 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3359243Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3359433Z 
2020-04-01T09:46:12.3359770Z ""C2 CompilerThread2"" #8 daemon prio=9 os_prio=0 tid=0x00007f22d82a6800 nid=0xed7 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3360272Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3360447Z 
2020-04-01T09:46:12.3360852Z ""C2 CompilerThread1"" #7 daemon prio=9 os_prio=0 tid=0x00007f22d82a4800 nid=0xed5 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3361262Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3361451Z 
2020-04-01T09:46:12.3361788Z ""C2 CompilerThread0"" #6 daemon prio=9 os_prio=0 tid=0x00007f22d82a2800 nid=0xed3 waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3362211Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3362387Z 
2020-04-01T09:46:12.3362720Z ""Signal Dispatcher"" #5 daemon prio=9 os_prio=0 tid=0x00007f22d82a0800 nid=0xed1 runnable [0x0000000000000000]
2020-04-01T09:46:12.3363107Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3363282Z 
2020-04-01T09:46:12.3363673Z ""Surrogate Locker Thread (Concurrent GC)"" #4 daemon prio=9 os_prio=0 tid=0x00007f22d829f000 nid=0xecf waiting on condition [0x0000000000000000]
2020-04-01T09:46:12.3364143Z    java.lang.Thread.State: RUNNABLE
2020-04-01T09:46:12.3364319Z 
2020-04-01T09:46:12.3364696Z ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f22d826e800 nid=0xecd in Object.wait() [0x00007f21ec3a1000]
2020-04-01T09:46:12.3365217Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:12.3365534Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3365868Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
2020-04-01T09:46:12.3366525Z 	- locked <0x00000000807456f8> (a java.lang.ref.ReferenceQueue$Lock)
2020-04-01T09:46:12.3366918Z 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
2020-04-01T09:46:12.3367340Z 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)
2020-04-01T09:46:12.3367587Z 
2020-04-01T09:46:12.3367941Z ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f22d826a000 nid=0xeca in Object.wait() [0x00007f21ec4a2000]
2020-04-01T09:46:12.3368384Z    java.lang.Thread.State: WAITING (on object monitor)
2020-04-01T09:46:12.3368708Z 	at java.lang.Object.wait(Native Method)
2020-04-01T09:46:12.3368991Z 	at java.lang.Object.wait(Object.java:502)
2020-04-01T09:46:12.3369360Z 	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
2020-04-01T09:46:12.3369927Z 	- locked <0x0000000080725b88> (a java.lang.ref.Reference$Lock)
2020-04-01T09:46:12.3370315Z 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
2020-04-01T09:46:12.3370586Z 
2020-04-01T09:46:12.3370955Z ""main"" #1 prio=5 os_prio=0 tid=0x00007f22d800b800 nid=0xe5a waiting on condition [0x00007f22e1b8f000]
2020-04-01T09:46:12.3371365Z    java.lang.Thread.State: WAITING (parking)
2020-04-01T09:46:12.3371648Z 	at sun.misc.Unsafe.park(Native Method)
2020-04-01T09:46:12.3372211Z 	- parking to wait for  <0x0000000082d3b7b0> (a java.util.concurrent.CompletableFuture$Signaller)
2020-04-01T09:46:12.3372658Z 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
2020-04-01T09:46:12.3373126Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
2020-04-01T09:46:12.3373611Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
2020-04-01T09:46:12.3374068Z 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
2020-04-01T09:46:12.3374626Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-04-01T09:46:12.3375108Z 	at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:653)
2020-04-01T09:46:12.3375650Z 	at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77)
2020-04-01T09:46:12.3376163Z 	at org.apache.flink.table.planner.delegation.ExecutorBase.execute(ExecutorBase.java:51)
2020-04-01T09:46:12.3376745Z 	at org.apache.flink.table.planner.utils.TestingTableEnvironment.execute(TableTestBase.scala:1054)
2020-04-01T09:46:12.3377250Z 	at org.apache.flink.table.api.TableUtils.collectToList(TableUtils.java:85)
2020-04-01T09:46:12.3377835Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.executeQuery(BatchTestBase.scala:288)
2020-04-01T09:46:12.3378381Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:129)
2020-04-01T09:46:12.3378910Z 	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:95)
2020-04-01T09:46:12.3379621Z 	at org.apache.flink.table.planner.runtime.batch.sql.agg.DistinctAggregateITCaseBase.testSingleDistinctAggOnMultiColumnsWithGroupingSets(DistinctAggregateITCaseBase.scala:244)
2020-04-01T09:46:12.3380251Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-04-01T09:46:12.3380721Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-04-01T09:46:12.3381226Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-04-01T09:46:12.3381654Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-04-01T09:46:12.3382099Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-04-01T09:46:12.3382612Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-04-01T09:46:12.3383103Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-04-01T09:46:12.3383706Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-04-01T09:46:12.3384180Z 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
2020-04-01T09:46:12.3384706Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-04-01T09:46:12.3385168Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-04-01T09:46:12.3385654Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-04-01T09:46:12.3386113Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-04-01T09:46:12.3386561Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-04-01T09:46:12.3386998Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-04-01T09:46:12.3387411Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-04-01T09:46:12.3387842Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-04-01T09:46:12.3388290Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3388724Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-04-01T09:46:12.3389142Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-04-01T09:46:12.3389513Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-04-01T09:46:12.3389970Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-04-01T09:46:12.3390470Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-04-01T09:46:12.3391052Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-04-01T09:46:12.3391578Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-04-01T09:46:12.3392104Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-04-01T09:46:12.3392665Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-04-01T09:46:12.3393151Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-04-01T09:46:12.3393631Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-04-01T09:46:12.3393897Z 
2020-04-01T09:46:12.3394154Z ""VM Thread"" os_prio=0 tid=0x00007f22d8260800 nid=0xec9 runnable 
2020-04-01T09:46:12.3394371Z 
2020-04-01T09:46:12.3394742Z ""Gang worker#0 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8021800 nid=0xe5c runnable 
2020-04-01T09:46:12.3395007Z 
2020-04-01T09:46:12.3395310Z ""Gang worker#1 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8023000 nid=0xe5f runnable 
2020-04-01T09:46:12.3395656Z 
2020-04-01T09:46:12.3395955Z ""Gang worker#2 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8025000 nid=0xe60 runnable 
2020-04-01T09:46:12.3396210Z 
2020-04-01T09:46:12.3396528Z ""Gang worker#3 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8026800 nid=0xe62 runnable 
2020-04-01T09:46:12.3396805Z 
2020-04-01T09:46:12.3397082Z ""Gang worker#4 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8028800 nid=0xe64 runnable 
2020-04-01T09:46:12.3397358Z 
2020-04-01T09:46:12.3397634Z ""Gang worker#5 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802a000 nid=0xe66 runnable 
2020-04-01T09:46:12.3397903Z 
2020-04-01T09:46:12.3398181Z ""Gang worker#6 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802c000 nid=0xe68 runnable 
2020-04-01T09:46:12.3398438Z 
2020-04-01T09:46:12.3398746Z ""Gang worker#7 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802d800 nid=0xe6a runnable 
2020-04-01T09:46:12.3398999Z 
2020-04-01T09:46:12.3399297Z ""Gang worker#8 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d802f800 nid=0xe6c runnable 
2020-04-01T09:46:12.3399554Z 
2020-04-01T09:46:12.3399851Z ""Gang worker#9 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8031000 nid=0xe6e runnable 
2020-04-01T09:46:12.3400104Z 
2020-04-01T09:46:12.3400404Z ""Gang worker#10 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8033000 nid=0xe70 runnable 
2020-04-01T09:46:12.3400802Z 
2020-04-01T09:46:12.3401086Z ""Gang worker#11 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8034800 nid=0xe72 runnable 
2020-04-01T09:46:12.3401369Z 
2020-04-01T09:46:12.3401647Z ""Gang worker#12 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8036800 nid=0xe74 runnable 
2020-04-01T09:46:12.3401921Z 
2020-04-01T09:46:12.3402203Z ""Gang worker#13 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8038000 nid=0xe76 runnable 
2020-04-01T09:46:12.3402486Z 
2020-04-01T09:46:12.3402768Z ""Gang worker#14 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803a000 nid=0xe78 runnable 
2020-04-01T09:46:12.3403037Z 
2020-04-01T09:46:12.3403321Z ""Gang worker#15 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803b800 nid=0xe7a runnable 
2020-04-01T09:46:12.3403582Z 
2020-04-01T09:46:12.3403881Z ""Gang worker#16 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803d800 nid=0xe7c runnable 
2020-04-01T09:46:12.3404139Z 
2020-04-01T09:46:12.3404445Z ""Gang worker#17 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d803f000 nid=0xe7e runnable 
2020-04-01T09:46:12.3404769Z 
2020-04-01T09:46:12.3405072Z ""Gang worker#18 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8041000 nid=0xe80 runnable 
2020-04-01T09:46:12.3405329Z 
2020-04-01T09:46:12.3405624Z ""Gang worker#19 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8042800 nid=0xe82 runnable 
2020-04-01T09:46:12.3405879Z 
2020-04-01T09:46:12.3406158Z ""Gang worker#20 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8044800 nid=0xe83 runnable 
2020-04-01T09:46:12.3406451Z 
2020-04-01T09:46:12.3406754Z ""Gang worker#21 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8046000 nid=0xe85 runnable 
2020-04-01T09:46:12.3407029Z 
2020-04-01T09:46:12.3407312Z ""Gang worker#22 (Parallel GC Threads)"" os_prio=0 tid=0x00007f22d8047800 nid=0xe87 runnable 
2020-04-01T09:46:12.3407584Z 
2020-04-01T09:46:12.3407862Z ""G1 Main Concurrent Mark GC Thread"" os_prio=0 tid=0x00007f22d8092000 nid=0xeba runnable 
2020-04-01T09:46:12.3408120Z 
2020-04-01T09:46:12.3408433Z ""Gang worker#0 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8093800 nid=0xebc runnable 
2020-04-01T09:46:12.3408705Z 
2020-04-01T09:46:12.3409011Z ""Gang worker#1 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8095800 nid=0xebe runnable 
2020-04-01T09:46:12.3409283Z 
2020-04-01T09:46:12.3409600Z ""Gang worker#2 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8097000 nid=0xec0 runnable 
2020-04-01T09:46:12.3409868Z 
2020-04-01T09:46:12.3410182Z ""Gang worker#3 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d8099000 nid=0xec2 runnable 
2020-04-01T09:46:12.3410449Z 
2020-04-01T09:46:12.3410804Z ""Gang worker#4 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d809a800 nid=0xec4 runnable 
2020-04-01T09:46:12.3411152Z 
2020-04-01T09:46:12.3411449Z ""Gang worker#5 (G1 Parallel Marking Threads)"" os_prio=0 tid=0x00007f22d809c800 nid=0xec6 runnable 
2020-04-01T09:46:12.3411733Z 
2020-04-01T09:46:12.3412015Z ""G1 Concurrent Refinement Thread#0"" os_prio=0 tid=0x00007f22d8074000 nid=0xeb8 runnable 
2020-04-01T09:46:12.3412291Z 
2020-04-01T09:46:12.3412571Z ""G1 Concurrent Refinement Thread#1"" os_prio=0 tid=0x00007f22d8072000 nid=0xeb6 runnable 
2020-04-01T09:46:12.3412849Z 
2020-04-01T09:46:12.3413131Z ""G1 Concurrent Refinement Thread#2"" os_prio=0 tid=0x00007f22d8070800 nid=0xeb4 runnable 
2020-04-01T09:46:12.3413386Z 
2020-04-01T09:46:12.3413685Z ""G1 Concurrent Refinement Thread#3"" os_prio=0 tid=0x00007f22d806e800 nid=0xeb2 runnable 
2020-04-01T09:46:12.3413937Z 
2020-04-01T09:46:12.3414234Z ""G1 Concurrent Refinement Thread#4"" os_prio=0 tid=0x00007f22d806d000 nid=0xeb0 runnable 
2020-04-01T09:46:12.3414486Z 
2020-04-01T09:46:12.3414842Z ""G1 Concurrent Refinement Thread#5"" os_prio=0 tid=0x00007f22d806b000 nid=0xeae runnable 
2020-04-01T09:46:12.3415102Z 
2020-04-01T09:46:12.3415404Z ""G1 Concurrent Refinement Thread#6"" os_prio=0 tid=0x00007f22d8069800 nid=0xeab runnable 
2020-04-01T09:46:12.3415656Z 
2020-04-01T09:46:12.3415953Z ""G1 Concurrent Refinement Thread#7"" os_prio=0 tid=0x00007f22d8067800 nid=0xeaa runnable 
2020-04-01T09:46:12.3416278Z 
2020-04-01T09:46:12.3416610Z ""G1 Concurrent Refinement Thread#8"" os_prio=0 tid=0x00007f22d8065800 nid=0xea8 runnable 
2020-04-01T09:46:12.3416884Z 
2020-04-01T09:46:12.3417161Z ""G1 Concurrent Refinement Thread#9"" os_prio=0 tid=0x00007f22d8064000 nid=0xea6 runnable 
2020-04-01T09:46:12.3417437Z 
2020-04-01T09:46:12.3417718Z ""G1 Concurrent Refinement Thread#10"" os_prio=0 tid=0x00007f22d8062000 nid=0xea4 runnable 
2020-04-01T09:46:12.3417993Z 
2020-04-01T09:46:12.3418274Z ""G1 Concurrent Refinement Thread#11"" os_prio=0 tid=0x00007f22d8060800 nid=0xea2 runnable 
2020-04-01T09:46:12.3418530Z 
2020-04-01T09:46:12.3418828Z ""G1 Concurrent Refinement Thread#12"" os_prio=0 tid=0x00007f22d805e800 nid=0xea0 runnable 
2020-04-01T09:46:12.3419088Z 
2020-04-01T09:46:12.3419380Z ""G1 Concurrent Refinement Thread#13"" os_prio=0 tid=0x00007f22d805d000 nid=0xe9e runnable 
2020-04-01T09:46:12.3419636Z 
2020-04-01T09:46:12.3419936Z ""G1 Concurrent Refinement Thread#14"" os_prio=0 tid=0x00007f22d805b000 nid=0xe9c runnable 
2020-04-01T09:46:12.3420190Z 
2020-04-01T09:46:12.3420484Z ""G1 Concurrent Refinement Thread#15"" os_prio=0 tid=0x00007f22d8059800 nid=0xe9a runnable 
2020-04-01T09:46:12.3420790Z 
2020-04-01T09:46:12.3421088Z ""G1 Concurrent Refinement Thread#16"" os_prio=0 tid=0x00007f22d8057800 nid=0xe98 runnable 
2020-04-01T09:46:12.3421343Z 
2020-04-01T09:46:12.3421622Z ""G1 Concurrent Refinement Thread#17"" os_prio=0 tid=0x00007f22d8056000 nid=0xe96 runnable 
2020-04-01T09:46:12.3421895Z 
2020-04-01T09:46:12.3422173Z ""G1 Concurrent Refinement Thread#18"" os_prio=0 tid=0x00007f22d8054000 nid=0xe94 runnable 
2020-04-01T09:46:12.3422442Z 
2020-04-01T09:46:12.3422722Z ""G1 Concurrent Refinement Thread#19"" os_prio=0 tid=0x00007f22d8052800 nid=0xe91 runnable 
2020-04-01T09:46:12.3422993Z 
2020-04-01T09:46:12.3423271Z ""G1 Concurrent Refinement Thread#20"" os_prio=0 tid=0x00007f22d8050800 nid=0xe90 runnable 
2020-04-01T09:46:12.3423525Z 
2020-04-01T09:46:12.3423817Z ""G1 Concurrent Refinement Thread#21"" os_prio=0 tid=0x00007f22d804e800 nid=0xe8e runnable 
2020-04-01T09:46:12.3424069Z 
2020-04-01T09:46:12.3424368Z ""G1 Concurrent Refinement Thread#22"" os_prio=0 tid=0x00007f22d804d000 nid=0xe8c runnable 
2020-04-01T09:46:12.3424683Z 
2020-04-01T09:46:12.3424980Z ""G1 Concurrent Refinement Thread#23"" os_prio=0 tid=0x00007f22d804b000 nid=0xe8a runnable 
2020-04-01T09:46:12.3425234Z 
2020-04-01T09:46:12.3425532Z ""VM Periodic Task Thread"" os_prio=0 tid=0x00007f22d82ce000 nid=0xef2 waiting on condition 
2020-04-01T09:46:12.3425792Z 
2020-04-01T09:46:12.3425974Z JNI global references: 2864
2020-04-01T09:46:12.3426156Z 
2020-04-01T09:46:12.3426414Z ==============================================================================
2020-04-01T09:46:12.3426865Z Printing stack trace of Java process 27499
2020-04-01T09:46:12.3427199Z ==============================================================================
2020-04-01T09:46:12.3894127Z 27499: No such process
2020-04-01T09:46:12.3968549Z ./tools/travis_watchdog.sh: line 243:   337 Terminated              $cmd
{code}"	FLINK	Closed	2	1	8669	test-stability
13314706	"flink-runtime lists ""org.uncommons.maths:uncommons-maths:1.2.2a"" as a bundled dependency, but it isn't"	"This is the relevant section in the NOTICE file

{code}
This project bundles the following dependencies under the Apache Software License 2.0. (http://www.apache.org/licenses/LICENSE-2.0.txt)

- com.typesafe.akka:akka-remote_2.11:2.5.21
- io.netty:netty:3.10.6.Final
- org.uncommons.maths:uncommons-maths:1.2.2a
{code}.

The uncommons-maths dependency is not declared anywhere, nor is it included in the binary file."	FLINK	Closed	2	1	8669	pull-request-available
13324992	Performance regression 2020-08-27 in globalWindow benchmark	"[http://codespeed.dak8s.net:8000/timeline/?ben=globalWindow&env=2]

[http://codespeed.dak8s.net:8000/timeline/#/?exe=1,3&ben=tumblingWindow&env=2&revs=200&equid=off&quarts=on&extr=on] 

The results started to decrease 2 days before decomissioning of an old jenkins node.

The other tests, however, were stable.

 

cc: [~pnowojski]"	FLINK	Closed	3	1	8742	pull-request-available
13429142	[Changelog] Incorrect MaterializationID passed to ChangelogStateBackendHandleImpl	"In ChangelogStateBackendHandleImpl constructor, materializationID and persistedSizeOfThisCheckpoint are mixed up.

 

cc: [~yunta]"	FLINK	Resolved	1	1	8742	pull-request-available
13570453	TVF Window Aggregations might get stuck	"RecordsWindowBuffer flushes buffered records in the following cases:
 * watermark
 * checkpoint barrier
 * buffer overflow

 

In two-phase aggregations, this creates the following problems:

1) Local aggregation: enters hard-backpressure because for flush, it outputs the data downstream and doesn't check network buffer availability

This already disrupts normal checkpointing and watermarks progression

 

2) Global aggregation: 

When the window is large enough and/or the watermark is lagging, lots of data is flushed to state backend (and the state is updated) in checkpoint SYNC phase.

 

All this eventually causes checkpoint timeouts (10 minutes in our env).

 

Example query
{code:java}
INSERT INTO `target_table` 

SELECT window_start, window_end, some, attributes, SUM(view_time) AS total_view_time, COUNT(*) AS num, LISTAGG(DISTINCT page_url) AS pages 

FROM TABLE(TUMBLE(TABLE source_table, DESCRIPTOR($rowtime), INTERVAL '1' HOUR)) 

GROUP BY window_start, window_end, some, attributes;{code}
In our setup, the issue can be reproduced deterministically.

 

As a quick fix, we might want to:
 # limit the amount of data buffered in Global Aggregation nodes
 # disable two-phase aggregations, i.e. Local Aggregations (we can try to limit buffing there two, but network buffer availability can not be easily checked from the operator)"	FLINK	In Progress	3	1	8742	pull-request-available
13378967	CheckpointStoreITCase.testRestartOnRecoveryFailure fails with RuntimeException	"Not sure if it is related to the adaptive scheduler: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18052&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=a0a633b8-47ef-5c5a-2806-3c13b9e48228

{code}
May 17 22:29:11 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.351 s <<< FAILURE! - in org.apache.flink.test.checkpointing.CheckpointStoreITCase
May 17 22:29:11 [ERROR] testRestartOnRecoveryFailure(org.apache.flink.test.checkpointing.CheckpointStoreITCase)  Time elapsed: 1.138 s  <<< ERROR!
May 17 22:29:11 org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
May 17 22:29:11 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
May 17 22:29:11 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
May 17 22:29:11 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:237)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
May 17 22:29:11 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
May 17 22:29:11 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1081)
May 17 22:29:11 	at akka.dispatch.OnComplete.internal(Future.scala:264)
May 17 22:29:11 	at akka.dispatch.OnComplete.internal(Future.scala:261)
May 17 22:29:11 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
May 17 22:29:11 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
May 17 22:29:11 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
May 17 22:29:11 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73)
May 17 22:29:11 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
May 17 22:29:11 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
May 17 22:29:11 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
May 17 22:29:11 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
{code}"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13333688	Implement distributed DSTL (DFS-based)	"Sub-tasks:
 # Implement scheduling and deployment (no rescaling)
 # Implement RPC between the tasks and DSTL (including load balancing)
 # Implement re-scaling
 # Implement filtering of records on replay to support up-scaling"	FLINK	Closed	3	2	8742	DSTL
13419872	Changlog materialization with incremental checkpoint cannot work well in local tests	"Currently, changelog materialization would call RocksDB state backend's snapshot method to generate {{IncrementalRemoteKeyedStateHandle}} as ChangelogStateBackendHandleImpl's materialized artifacts. And before next materialization, it will always report the same {{IncrementalRemoteKeyedStateHandle}} as before.

For local tests, TM would report the {{IncrementalRemoteKeyedStateHandle}} to JM via local {{LocalRpcInvocation}}. However, as {{LocalRpcInvocation}} would not de/serialize message, which leads once we register the {{IncrementalRemoteKeyedStateHandle}} on JM side, it will also add a {{sharedStateRegistry}} to the one located on TM side. For the 2nd checkpoint, TM would reported same {{IncrementalRemoteKeyedStateHandle}} with  {{sharedStateRegistry}} to JM. And it will then throw exception as it already contains a {{sharedStateRegistry}}:

IncrementalRemoteKeyedStateHandle
{code:java}
public void registerSharedStates(SharedStateRegistry stateRegistry, long checkpointID) {
       Preconditions.checkState(
                sharedStateRegistry != stateRegistry,
                ""The state handle has already registered its shared states to the given registry."");

}
{code}

This bug would go in distribution environment as {{IncrementalRemoteKeyedStateHandle}} would be serialized and {{sharedStateRegistry}} is tagged as {{transient}}."	FLINK	Resolved	3	1	8742	pull-request-available
13284800	ContinuousFileReaderOperatorBenchmark restarts indefinetly on failure	"CFRO was changed to be created using factory, but the benchmark wasn’t updated.

This resulted in error in CFRO super.dispose as it wasn’t properly initialized.

The error wasn’t reported properly in CFRO (flipped arguments to firstOrSuppressed)

The benchmark was running a job with a restarting strategy, so it repeated indefinitely."	FLINK	Closed	3	1	8742	pull-request-available
13407762	Prevent JM from discarding state on checkpoint abortion	"================================================================================
MOTIVATION
When a checkpoint is aborted, JM discards any state that was sent to it and wasn't used in other checkpoints. This forces incremental state backends to wait for confirmation from JM before re-using this state. For changelog backend this is even more critical.

One approach proposed was to make backends/TMs responsible for their state, until it's not shared with other TMs, i.e. until rescaling (private/shared state ownership track: FLINK-23342 and more).
However, that approach is quite invasive.

An alternative solution would be:
1. SharedStateRegistry remembers the latest checkpoint for each shared state (instead of usage count currently)
2. CompletedCheckpointStore notifies it about the lowest valid checkpoint (on subsumption)
3. SharedStateRegistry then discards any state associated with the lower (subsumed/aborted) checkpoints
So the aborted checkpoint can only be discarded after some subsequent successful checkpoint (which can mark state as used).

Mostly JM code is changed. IncrementalRemoteKeyedStateHandle.discard needs to be adjusted.
Backends don't re-upload state.

================================================================================
IMPLEMENTATION CONSIDERATIONS

On subsumption, JM needs to find all the unused state and discard it.
This can either be done by
1) simply traversing all entries; or by 
2) maintaining a set of entries per checkpoint (e.g. SortedMap<Long, Set<K>>). This allows to skip unnecessary traversal at the cost of higher memory usage

In both cases:
- each entry stores last checkpoint ID it was used in (long)
- key is hashed (even with plain traversal, map.entrySet.iterator.remove() computes hash internally)

Because of the need to maintain secondary sets, (2) isn't asymptotically better than (1), and is likely worse in practice and requires more memory (see discussion in the comments). So approach (1) seems reasonable.

================================================================================
CORNER CASES

The following cases shouldn't pose any difficulties:
1. Recovery, re-scaling, and state used by not all or by no tasks - we still register all states on recovery even after FLINK-22483/FLINK-24086
2. Cross-task state sharing - not an issue as long as everything is managed by JM
3. Dependencies between SharedStateRegistry and CompletedCheckpointStore - simple after FLINK-24086
4. Multiple concurrent checkpoints (below)

Consider the following case:
(nr. concurrent checkpoints > 1)
1. checkpoint 1 starts, TM reports that it uses file f1; checkpoint 1 gets aborted - f1 is now tracked
2. savepoint 2 starts, it *will* use f1
3. checkpoint 3 starts and finishes; it does NOT use file f1

When a checkpoint finishes, all pending checkpoints before it are aborted - but not savepoints.
Savepoints currently are NOT incremental. And in the future, incremental savepoints shouldn't share any artifacts with checkpionts.

The following should be kept in mind:
1. On job cancellation, state of aborted checkpoints should be cleaned up explicitly
2. Savepoints should be ignored and not change CheckpointStore.lowestCheckpointID
3. In case of JM failover, there might be more state left undeleted (see follow-up FLINK-24852 and/or comments)
4. To handle JM leadership change,  backends should NOT send PlaceholderStreamStateHandles unless the checkpoint was confirmed (see comments)
5. To handle TM failover after an incomplete checkpoint, JM should replace old state handle with a new one in case of collision (test that old state isn't included into a completed checkpoint) (see comments)

================================================================================
USER IMPACT

For the end users, this change might render as a delay in discarding state of aborted checkpoints and in slight increase of undeleted state in case of failures; which seems acceptable.

After updating backends to not re-upload state, checkpointing time should be reduced and less IO will be used (in cases when notifications are delayed or new JM is elected)."	FLINK	Resolved	3	7	8742	pull-request-available
13311386	Tests are crashing with exit code 239	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=3467&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=34f486e1-e1e4-5dd2-9c06-bfdd9b9c74a8]
Kafka011ProducerExactlyOnceITCase
 
{code:java}
2020-06-15T03:24:28.4677649Z [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
2020-06-15T03:24:28.4692049Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-connector-kafka-0.11_2.11: There are test failures.
2020-06-15T03:24:28.4692585Z [ERROR] 
2020-06-15T03:24:28.4693170Z [ERROR] Please refer to /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire-reports for the individual test results.
2020-06-15T03:24:28.4693928Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2020-06-15T03:24:28.4694423Z [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4696762Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4698486Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4699066Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4699458Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4699960Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4700849Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4703760Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4705501Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4706297Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4706592Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4706895Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4707386Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2020-06-15T03:24:28.4708053Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2020-06-15T03:24:28.4708908Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2020-06-15T03:24:28.4709720Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2020-06-15T03:24:28.4710497Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2020-06-15T03:24:28.4711448Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2020-06-15T03:24:28.4712395Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2020-06-15T03:24:28.4712997Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2020-06-15T03:24:28.4713524Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2020-06-15T03:24:28.4714079Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2020-06-15T03:24:28.4714560Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2020-06-15T03:24:28.4715096Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2020-06-15T03:24:28.4715672Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2020-06-15T03:24:28.4716445Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2020-06-15T03:24:28.4717024Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2020-06-15T03:24:28.4717478Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2020-06-15T03:24:28.4717939Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2020-06-15T03:24:28.4718378Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2020-06-15T03:24:28.4718852Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2020-06-15T03:24:28.4719230Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2020-06-15T03:24:28.4719676Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-06-15T03:24:28.4720309Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-06-15T03:24:28.4720882Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-06-15T03:24:28.4721339Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2020-06-15T03:24:28.4721888Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2020-06-15T03:24:28.4722658Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2020-06-15T03:24:28.4723430Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2020-06-15T03:24:28.4724062Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2020-06-15T03:24:28.4724657Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2020-06-15T03:24:28.4726770Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dlog4j.configurationFile=log4j2-test.properties -Dmvn.forkNumber=2 -XX:-UseGCOverheadLimit -jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire/surefirebooter617700788970993266.jar /__w/2/s/flink-connectors/flink-connector-kafka-0.11/target/surefire 2020-06-15T03-07-01_381-jvmRun2 surefire2676050245109796726tmp surefire_602825791089523551074tmp
2020-06-15T03:24:28.4728582Z [ERROR] Error occurred in starting fork, check output in log
2020-06-15T03:24:28.4729202Z [ERROR] Process Exit Code: 239
2020-06-15T03:24:28.4729612Z [ERROR] Crashed tests:
2020-06-15T03:24:28.4730247Z [ERROR] org.apache.flink.streaming.connectors.kafka.Kafka011ProducerExactlyOnceITCase
2020-06-15T03:24:28.4730781Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2020-06-15T03:24:28.4731292Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2020-06-15T03:24:28.4731829Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2020-06-15T03:24:28.4732353Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2020-06-15T03:24:28.4732792Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-06-15T03:24:28.4733235Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2020-06-15T03:24:28.4733718Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2020-06-15T03:24:28.4734170Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2020-06-15T03:24:28.4734682Z [ERROR] -> [Help 1]
2020-06-15T03:24:28.4734859Z [ERROR] 
2020-06-15T03:24:28.4735312Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2020-06-15T03:24:28.4735927Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2020-06-15T03:24:28.4736439Z [ERROR] 
2020-06-15T03:24:28.4736952Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2020-06-15T03:24:28.4737706Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2020-06-15T03:24:28.4738167Z [ERROR] 
2020-06-15T03:24:28.4738553Z [ERROR] After correcting the problems, you can resume the build with the command
2020-06-15T03:24:28.4739663Z [ERROR]   mvn <goals> -rf :flink-connector-kafka-0.11_2.11
2020-06-15T03:24:29.0980029Z MVN exited with EXIT CODE: 1.
{code}

This could be a CI environment issue...
When did it start?"	FLINK	Closed	1	1	8742	pull-request-available, test-stability
13567207	Wrong JobID in CheckpointStatsTracker	"The job id is generated randomly:
```
    public CheckpointStatsTracker(int numRememberedCheckpoints, MetricGroup metricGroup) {
        this(numRememberedCheckpoints, metricGroup, new JobID(), Integer.MAX_VALUE);
    }
```
This affects how it is logged (or reported elsewhere)."	FLINK	Closed	4	1	8742	pull-request-available
13371589	Improve support for JdbcXaSinkFunction	"JdbcXaSinkFunction uses Xa protocol/interface to implement exactly-once guarantee (within each subtask partition).

Xa is a protocol/interface designed for two-phase commit of distributed DBS (RMs).
XA guarantees that transactional updates are committed in all of the participating databases, or are fully rolled back out of all of the databases, reverting to the state prior to the start of the transaction.

Hence some of the dbs that support XA treats XA transaction as global trans, and some of them does not support multiple global trans (per connection) at a time, MYSQL for example (see FLINK-21743).

This ticket is a follow-up to address such limitations."	FLINK	Resolved	3	1	8742	pull-request-available
13307804	Erroneous check in FsCheckpointStateOutputStream#write(int)	When fixing FLINK-17820 a {{flush}} call was accidentally introduced on every single byte/int write to {{FsCheckpointStateOutputStream}}, which could significantly affect performance.	FLINK	Closed	1	1	8742	pull-request-available
13438883	PartiallyFinishedSourcesITCase.test hangs on azure	"
{code:java}
Apr 10 08:32:18 ""main"" #1 prio=5 os_prio=0 tid=0x00007f553400b800 nid=0x8345 waiting on condition [0x00007f553be60000]
Apr 10 08:32:18    java.lang.Thread.State: TIMED_WAITING (sleeping)
Apr 10 08:32:18 	at java.lang.Thread.sleep(Native Method)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
Apr 10 08:32:18 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
Apr 10 08:32:18 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
Apr 10 08:32:18 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
Apr 10 08:32:18 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 10 08:32:18 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 10 08:32:18 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 10 08:32:18 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 10 08:32:18 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Apr 10 08:32:18 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 10 08:32:18 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 10 08:32:18 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 10 08:32:18 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
Apr 10 08:32:18 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 10 08:32:18 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Apr 10 08:32:18 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Apr 10 08:32:18 	at org.junit.runners.Suite.runChild(Suite.java:128)
Apr 10 08:32:18 	at org.junit.runners.Suite.runChild(Suite.java:27)
Apr 10 08:32:18 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=34484&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=6757"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13429969	[Changelog] Thundering herd problem with materialization	"Quick note: CheckpointCleaner is not involved here.

When a checkpoint is subsumed, SharedStateRegistry schedules its unused shared state for async deletion. It uses common IO pool for this and adds a Runnable per state handle. ( see SharedStateRegistryImpl.scheduleAsyncDelete)

When a checkpoint is started, CheckpointCoordinator uses the same thread pool to initialize the location for it. (see CheckpointCoordinator.initializeCheckpoint)

The thread pool is of fixed size [jobmanager.io-pool.size|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#jobmanager-io-pool-size]; by default it's the number of CPU cores) and uses FIFO queue for tasks.

When there is a spike in state deletion, the next checkpoint is delayed waiting for an available IO thread.

Back-pressure seems reasonable here (similar to CheckpointCleaner); however, this shared state deletion could be spread across multiple subsequent checkpoints, not neccesarily the next one.

---- 

I believe the issue is an pre-existing one; but it particularly affects changelog state backend, because 1) such spikes are likely there; 2) workloads are latency sensitive.

In the tests, checkpoint duration grows from seconds to minutes immediately after the materialization."	FLINK	Resolved	3	1	8742	pull-request-available
13574402	"JobIDLoggingITCase fails because of ""checkpoint confirmation for unknown task"""	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=58640&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8735]
{code:java}
Mar 30 03:46:07 03:46:07.807 [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.147 s <<< FAILURE! -- in org.apache.flink.test.misc.JobIDLoggingITCase
Mar 30 03:46:07 03:46:07.807 [ERROR] org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(ClusterClient) -- Time elapsed: 2.301 s <<< FAILURE!
Mar 30 03:46:07 java.lang.AssertionError: 
Mar 30 03:46:07 [too many events without Job ID logged by org.apache.flink.runtime.taskexecutor.TaskExecutor] 
Mar 30 03:46:07 Expecting empty but was: [Logger=org.apache.flink.runtime.taskexecutor.TaskExecutor Level=DEBUG Message=TaskManager received a checkpoint confirmation for unknown task b45d406844d494592784a88e47d201e2_cbc357ccb763df2852fee8c4fc7d55f2_0_0.]
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.assertJobIDPresent(JobIDLoggingITCase.java:264)
Mar 30 03:46:07 	at org.apache.flink.test.misc.JobIDLoggingITCase.testJobIDLogging(JobIDLoggingITCase.java:149)
Mar 30 03:46:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 30 03:46:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Mar 30 03:46:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) {code}
[https://github.com/apache/flink/actions/runs/8502821551/job/23287730632#step:10:8131]

[https://github.com/apache/flink/actions/runs/8507870399/job/23300810619#step:10:8086]

 

 "	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13362519	Consider shaping newly introduced RuntimeContext.getJobId to return JobID with no Optional wrapper	"Currently, this newly introduced {{RuntimeContext.getJobId()}} returns {{Optional<JobID>}}. The only path where it returns no job id is {{RuntimeUDFContext}}(through {{CollectionExecutor}} through {{CollectionEnvironment}}).

But after {{DataSet}} dropped, there will be no paths to return no job id. Both FLINK-21581 and [my comment|https://github.com/apache/flink/pull/15053#issuecomment-789410967] raised this concern. But different with FLINK-21581, I think we could return an environment/executor/plan level unique job id in {{RuntimeUDFContext}} for this new api. This way there will be no breaking change after {{DataSet}} dropped. And more importantly, a careful chosen job id does not hurt callers of {{RuntimeUDFContext}} in my opinion.

cc  [~chesnay] [~roman_khachatryan] [~aljoscha] [~sewen] "	FLINK	Closed	1	4	8742	pull-request-available
13426956	[Changelog] IllegalArgumentException thrown from FsStateChangelogWriter.truncate	"{code}
java.lang.IllegalArgumentException
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.flink.changelog.fs.FsStateChangelogWriter.truncate(FsStateChangelogWriter.java:278)
	at org.apache.flink.state.changelog.ChangelogKeyedStateBackend.updateChangelogSnapshotState(ChangelogKeyedStateBackend.java:702)
	at org.apache.flink.state.changelog.PeriodicMaterializationManager.lambda$null$2(PeriodicMaterializationManager.java:163)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
	at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:750){code}
"	FLINK	Resolved	3	1	8742	pull-request-available
13411077	Release TaskManagerJobMetricGroup with the last slot rather than task	"// Beware that this ticket FLINK-24864 clones FLINK-23486.

 

FLINK-23486 adds metrics for Changelog Uploader.

Since that uploader is shared among tasks of the same job on a TM, the right level for metrics is TaskManagerJobMetricGroup (see [design doc|https://docs.google.com/document/d/1k5WkWIYzs3n3GYQC76H9BLGxvN3wuq7qUHJuBPR9YX0/edit?usp=sharing]).

 

However, the lifecycle of TaskManagerJobMetricGroup differs from that of StateChangelogStorage: the former is released on last task unregister; the latter - on last job slot release.

This causes problem that an old TMJMG can be used by Storage.

 

Releasing Storage on last task unregister has some disadvantages, including overhead, thread-safety and logical inconsistency.

Releasing TMJMG on last slot release doesn't have major disadvantages.

This ticket proposes the former change."	FLINK	Resolved	3	7	8742	pull-request-available
13567930	Add JobID to logging MDC	"Adding JobID to logging MDC allows to apply Structural Logging 
and analyze Flink logs more efficiently."	FLINK	Closed	3	4	8742	pull-request-available
13282353	StreamSourceOperatorWatermarksTest.testNoMaxWatermarkOnAsyncCancel fails on Travis	"https://api.travis-ci.org/v3/job/643480766/log.txt

{code}
08:06:17.382 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.812 s <<< FAILURE! - in org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest
08:06:17.382 [ERROR] testNoMaxWatermarkOnAsyncCancel(org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest)  Time elapsed: 0.235 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.flink.streaming.runtime.operators.StreamSourceOperatorWatermarksTest.testNoMaxWatermarkOnAsyncCancel(StreamSourceOperatorWatermarksTest.java:127)
{code}"	FLINK	Closed	1	1	8742	pull-request-available, test-stability
13285300	NullPointerException during ContinuousFileProcessingITCase	"[https://api.travis-ci.org/v3/job/650176066/log.txt] 

ContinuousFileReaderOperator was changed recently to be created using a factory. But some tests create it using constructor directly. This causes problems with deserialization.

 

Another issue is that tests don't fail but rather log the exception:
{code:java}
00:16:03.935 [INFO] Running org.apache.flink.hdfstests.ContinuousFileProcessingITCase Formatting using clusterid: testClusterID org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147) at org.apache.flink.runtime.minicluster.MiniCluster.executeJobBlocking(MiniCluster.java:648) at org.apache.flink.streaming.util.TestStreamEnvironment.execute(TestStreamEnvironment.java:77) at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1620) at org.apache.flink.hdfstests.ContinuousFileProcessingITCase$1.run(ContinuousFileProcessingITCase.java:148) Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:110) at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:76) at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192) at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:186) at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:180) at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:493) at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:383) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:279) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:194) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) at akka.actor.Actor$class.aroundReceive(Actor.scala:517) at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) at akka.actor.ActorCell.invoke(ActorCell.scala:561) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) at akka.dispatch.Mailbox.run(Mailbox.scala:225) at akka.dispatch.Mailbox.exec(Mailbox.scala:235) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: java.lang.NullPointerException at org.apache.flink.streaming.api.functions.source.ContinuousFileReaderOperator.open(ContinuousFileReaderOperator.java:263) at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:977) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:456) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:47) at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:451) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:463) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:717) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:541) at java.lang.Thread.run(Thread.java:748) Exception in thread ""Thread-83"" java.lang.AssertionError: Job execution failed. at org.junit.Assert.fail(Assert.java:88) at org.apache.flink.hdfstests.ContinuousFileProcessingITCase$1.run(ContinuousFileProcessingITCase.java:161)

  {code}"	FLINK	Closed	3	1	8742	pull-request-available
13526527	Make AdaptiveScheduler aware of the (local) state size	"FLINK-21450 makes the Adaptive Schulder aware of Local Recovery.

Each slot-group pair is assigned a score based on a keyGroupRange size.

That score isn't always optimlal - it could be improved by computing the score based on the actual state size on disk."	FLINK	Closed	3	4	8742	pull-request-available, stale-assigned
13353302	Remove PendingCheckpoint.statsCallback	During the offline discussion of FLINK-19462 with [~pnowojski] we decided to remove PendingCheckpoint.statsCallback. Instead of having it as a callback field, it can either be passed as an argument or even inlined into the caller (CheckpointCoordinator).	FLINK	Closed	4	4	8742	pull-request-available
13525410	PartiallyFinishedSourcesITCase hangs if a checkpoint fails	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b

This build ran into a timeout. Based on the stacktraces reported, it was either caused by [SnapshotMigrationTestBase.restoreAndExecute|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=13475]:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f23d800b800 nid=0x60cdd waiting on condition [0x00007f23e1c0d000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.test.checkpointing.utils.SnapshotMigrationTestBase.restoreAndExecute(SnapshotMigrationTestBase.java:382)
	at org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.testSnapshot(TypeSerializerSnapshotMigrationITCase.java:172)
	at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
[...]
{code}

or [PartiallyFinishedSourcesITCase.test|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10401]:
{code}
2023-02-20T07:13:05.6084711Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd35c00b800 nid=0x8c8a waiting on condition [0x00007fd363d0f000]
2023-02-20T07:13:05.6085149Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2023-02-20T07:13:05.6085487Z 	at java.lang.Thread.sleep(Native Method)
2023-02-20T07:13:05.6085925Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2023-02-20T07:13:05.6086512Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
2023-02-20T07:13:05.6087103Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
2023-02-20T07:13:05.6087730Z 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
2023-02-20T07:13:05.6088410Z 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
2023-02-20T07:13:05.6088957Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

Still, it sounds odd: Based on a code analysis it's quite unlikely that those two caused the issue. The former one has a 5 min timeout (see related code in [SnapshotMigrationTestBase:382|https://github.com/apache/flink/blob/release-1.15/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L382]). For the other one, we found it being not responsible in the past when some other concurrent test caused the issue (see FLINK-30261).

An investigation on where we lose the time for the timeout revealed that {{AdaptiveSchedulerITCase}} took 2980s to finish (see [build logs|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5265]).
{code}
2023-02-20T03:43:55.4546050Z Feb 20 03:43:55 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-02-20T03:43:58.0448506Z Feb 20 03:43:58 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
2023-02-20T04:33:38.6824634Z Feb 20 04:33:38 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,980.445 s - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13342494	"UnalignedCheckpointITCase.execute failed with ""Sequence number for checkpoint 20 is not known (it was likely been overwritten by a newer checkpoint 21)"""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=10059&view=logs&j=119bbba7-f5e3-5e08-e72d-09f1529665de&t=7dc1f5a9-54e1-502e-8b02-c7df69073cfc

{code}
2020-11-24T22:42:17.6704402Z [ERROR] execute[parallel pipeline with mixed channels, p = 20](org.apache.flink.test.checkpointing.UnalignedCheckpointITCase)  Time elapsed: 7.901 s  <<< ERROR!
2020-11-24T22:42:17.6706095Z org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2020-11-24T22:42:17.6707450Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:147)
2020-11-24T22:42:17.6708569Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$2(MiniClusterJobClient.java:119)
2020-11-24T22:42:17.6709626Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-11-24T22:42:17.6710452Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-11-24T22:42:17.6711271Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-24T22:42:17.6713170Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-24T22:42:17.6713974Z 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:229)
2020-11-24T22:42:17.6714517Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-11-24T22:42:17.6715372Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-11-24T22:42:17.6715871Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-11-24T22:42:17.6716514Z 	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
2020-11-24T22:42:17.6718475Z 	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:996)
2020-11-24T22:42:17.6719322Z 	at akka.dispatch.OnComplete.internal(Future.scala:264)
2020-11-24T22:42:17.6719887Z 	at akka.dispatch.OnComplete.internal(Future.scala:261)
2020-11-24T22:42:17.6720271Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191)
2020-11-24T22:42:17.6720645Z 	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188)
2020-11-24T22:42:17.6721114Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-24T22:42:17.6721585Z 	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:74)
2020-11-24T22:42:17.6722078Z 	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
2020-11-24T22:42:17.6722738Z 	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
2020-11-24T22:42:17.6723183Z 	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572)
2020-11-24T22:42:17.6723862Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:22)
2020-11-24T22:42:17.6724435Z 	at akka.pattern.PipeToSupport$PipeableFuture$$anonfun$pipeTo$1.applyOrElse(PipeToSupport.scala:21)
2020-11-24T22:42:17.6724914Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-11-24T22:42:17.6725323Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-11-24T22:42:17.6725866Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-24T22:42:17.6726313Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-11-24T22:42:17.6726829Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-11-24T22:42:17.6727376Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-24T22:42:17.6727891Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-24T22:42:17.6728400Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-11-24T22:42:17.6728855Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-11-24T22:42:17.6729390Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-11-24T22:42:17.6729853Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-11-24T22:42:17.6730345Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-24T22:42:17.6730781Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-24T22:42:17.6731212Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-24T22:42:17.6731657Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-11-24T22:42:17.6732230Z Caused by: org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=100)
2020-11-24T22:42:17.6733094Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)
2020-11-24T22:42:17.6733759Z 	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)
2020-11-24T22:42:17.6734418Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:224)
2020-11-24T22:42:17.6734964Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:217)
2020-11-24T22:42:17.6735683Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:208)
2020-11-24T22:42:17.6736231Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:533)
2020-11-24T22:42:17.6736756Z 	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:89)
2020-11-24T22:42:17.6737270Z 	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:419)
2020-11-24T22:42:17.6737684Z 	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
2020-11-24T22:42:17.6738114Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-24T22:42:17.6738542Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-24T22:42:17.6738960Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:286)
2020-11-24T22:42:17.6739537Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:201)
2020-11-24T22:42:17.6740059Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-11-24T22:42:17.6740560Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:154)
2020-11-24T22:42:17.6741004Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-11-24T22:42:17.6741489Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-11-24T22:42:17.6741882Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-11-24T22:42:17.6742302Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-11-24T22:42:17.6743029Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-11-24T22:42:17.6743601Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-24T22:42:17.6744206Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-11-24T22:42:17.6744642Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-11-24T22:42:17.6745047Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-11-24T22:42:17.6745450Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-11-24T22:42:17.6745807Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-11-24T22:42:17.6746184Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-11-24T22:42:17.6746540Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-11-24T22:42:17.6746880Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-11-24T22:42:17.6747244Z 	... 4 more
2020-11-24T22:42:17.6747712Z Caused by: java.lang.IllegalStateException: Sequence number for checkpoint 20 is not known (it was likely been overwritten by a newer checkpoint 21)
2020-11-24T22:42:17.6748244Z 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:220)
2020-11-24T22:42:17.6749007Z 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getInflightBuffersUnsafe(RemoteInputChannel.java:542)
2020-11-24T22:42:17.6750046Z 	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkpointStarted(RemoteInputChannel.java:514)
2020-11-24T22:42:17.6750982Z 	at org.apache.flink.runtime.io.network.partition.consumer.IndexedInputGate.checkpointStarted(IndexedInputGate.java:35)
2020-11-24T22:42:17.6751825Z 	at org.apache.flink.streaming.runtime.io.UnalignedController.preProcessFirstBarrier(UnalignedController.java:54)
2020-11-24T22:42:17.6752730Z 	at org.apache.flink.streaming.runtime.io.AlternatingController.preProcessFirstBarrier(AlternatingController.java:57)
2020-11-24T22:42:17.6753372Z 	at org.apache.flink.streaming.runtime.io.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:121)
2020-11-24T22:42:17.6754237Z 	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:174)
2020-11-24T22:42:17.6754995Z 	at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:151)
2020-11-24T22:42:17.6755929Z 	at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:157)
2020-11-24T22:42:17.6756494Z 	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:67)
2020-11-24T22:42:17.6757294Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:372)
2020-11-24T22:42:17.6757944Z 	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)
2020-11-24T22:42:17.6758714Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:575)
2020-11-24T22:42:17.6759583Z 	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:539)
2020-11-24T22:42:17.6760205Z 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:722)
2020-11-24T22:42:17.6760774Z 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:547)
2020-11-24T22:42:17.6761323Z 	at java.lang.Thread.run(Thread.java:748)
{code}"	FLINK	Resolved	1	1	8742	pull-request-available, test-stability
13340030	Race conditions in InputChannel.ChannelStatePersister	"In InputChannel.ChannelStatePersister, stopPersisting() and checkForBarrier() always update pendingCheckpointBarrierId, potentially overwriting newer id (or BARRIER_RECEIVED value) with an old one.


For stopPersisting(), consider a case:
 # Two consecutive UC barriers arrive at the same channel (1st being stale at some point)
 # In RemoteInputChannel.onBuffer, netty thread updates pendingCheckpointBarrierId to BARRIER_RECEIVED
 # Task thread processes the 1st barrier and triggers a checkpoint
Task thread processes the 2nd barrier and aborts 1st checkpoint, calling stopPersisting() from UC controller and setting pendingCheckpointBarrierId to CHECKPOINT_COMPLETED
 # Task thread starts 2nd checkpoint and calls startPersisting() setting pendingCheckpointBarrierId to 2
 # now new buffers have a chance to be included in the 2nd checkpoint (though they belong to the next one)

 

For pendingCheckpointBarrierId(), consider an input gate with two channels A and B and two barriers 1 and 2:
 # Channel A receives both barriers, channel B receives nothing yet
 # Task thread processes both barriers on A, eventually triggering 2nd checkpoint
 # Channel A state is now BARRIER_RECEIVED, channel B - pending (with id=2)
 # Channel B receives the 1st barrier and becomes BARRIER_RECEIVED
 # No buffers in B between barriers 1 and 2 will be included in the checkpoint 
 # Channel B receives the 2nd barrier which will eventually conclude the checkpoint

 

I see a solution in doing an action only if passed checkpointId >= pendingCheckpointId. For that, a separate field will be needed to hold the status (RECEIVED/COMPLETED/PENDING). The class isn't thread-safe so it shouldn't be a problem.
 "	FLINK	Resolved	2	1	8742	pull-request-available
13337630	Managed memory released check can block IterativeTask	"UnsafeMemoryBudget#reserveMemory, called on TempBarrier, needs time to wait on GC of all allocated/released managed memory at every iteration.

 

stack:

!image-2020-10-28-17-48-48-583.png!

new TempBarrier in BatchTask

!image-2020-10-28-17-48-28-395.png!

 

These will be very slow than before."	FLINK	Resolved	2	1	8742	pull-request-available
13279193	Implement exactly-once JDBC sink	"As per discussion in the dev mailing list, there are two options:
 # Write-ahead log
 # Two-phase commit (XA)

the latter being preferable.

 "	FLINK	Closed	3	2	8742	pull-request-available
13435159	ChangelogStorageMetricsTest.testAttemptsPerUpload failed	"[This build|https://dev.azure.com/mapohl/flink/_build/results?buildId=901&view=logs&j=f3dc9b18-b77a-55c1-591e-264c46fe44d1&t=2d3cd81e-1c37-5c31-0ee4-f5d5cdb9324d&l=24226] failed due a failure in {{ChangelogStorageMetricsTest.testAttemptsPerUpload}}:

{code}
Mar 22 12:23:09 [ERROR] Failures: 
Mar 22 12:23:09 [ERROR]   ChangelogStorageMetricsTest.testAttemptsPerUpload:208 
Mar 22 12:23:09 expected: 3L
Mar 22 12:23:09  but was: 0L
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13277326	Should wait for the end of the source thread during the Task cancellation	"     In the new mailBox model, SourceStreamTask starts a source thread to run user methods, and the current execution thread will block on mailbox.takeMail (). When a task cancels, the TaskCanceler thread will cancel the task and interrupt the execution thread. Therefore, the execution thread of SourceStreamTask will throw InterruptedException, then cancel the task again, and throw an exception.
{code:java}
//代码占位符
@Override
protected void performDefaultAction(ActionContext context) throws Exception {
   // Against the usual contract of this method, this implementation is not step-wise but blocking instead for
   // compatibility reasons with the current source interface (source functions run as a loop, not in steps).
   sourceThread.start();

   // We run an alternative mailbox loop that does not involve default actions and synchronizes around actions.
   try {
      runAlternativeMailboxLoop();
   } catch (Exception mailboxEx) {
      // We cancel the source function if some runtime exception escaped the mailbox.
      if (!isCanceled()) {
         cancelTask();
      }
      throw mailboxEx;
   }

   sourceThread.join();
   if (!isFinished) {
      sourceThread.checkThrowSourceExecutionException();
   }

   context.allActionsCompleted();
}
{code}
   When all tasks of this TaskExecutor are canceled, the blob file will be cleaned up. But the real source thread is not finished at this time, which will cause a ClassNotFoundException when loading a new class. In this case, the source thread may not be able to properly clean up and release resources (such as closing child threads, cleaning up local files, etc.). Therefore, I think we should mark this task canceled or finished after the execution of the source thread is completed."	FLINK	Resolved	3	1	8742	pull-request-available
13399487	PartiallyFinishedSourcesITCase fails due to 'Size of the state is larger than the maximum permitted memory-backed state.'	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23526&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5526

{code}
Sep 04 02:31:13 Caused by: java.io.IOException: Size of the state is larger than the maximum permitted memory-backed state. Size=6281708, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage.
Sep 04 02:31:13 	at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory.checkSize(MemCheckpointStreamFactory.java:63)
Sep 04 02:31:13 	at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory$MemoryCheckpointOutputStream.closeAndGetBytes(MemCheckpointStreamFactory.java:140)
Sep 04 02:31:13 	at org.apache.flink.runtime.state.memory.MemCheckpointStreamFactory$MemoryCheckpointOutputStream.closeAndGetHandle(MemCheckpointStreamFactory.java:120)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.finishWriteAndResult(ChannelStateCheckpointWriter.java:218)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.doComplete(ChannelStateCheckpointWriter.java:240)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.lambda$complete$5(ChannelStateCheckpointWriter.java:202)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.runWithChecks(ChannelStateCheckpointWriter.java:296)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.complete(ChannelStateCheckpointWriter.java:200)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.completeInput(ChannelStateCheckpointWriter.java:187)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.execute(ChannelStateWriteRequest.java:212)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:85)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
Sep 04 02:31:13 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
Sep 04 02:31:13 	... 1 more
{code}"	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13363076	Add extension points to heap backend to read/write incremental snapshots	"For each state, incremental backend can create either incremental or full snapshot.

States of type PQ or nested tables are always fully snapshotted.

The goal is allow future incremental backend to:
 # distinguish between full/inc snapshots on recovery
 # read incremental snapshot

This ticket is only about changing the existing classes (providing extension points).

 

Optional goal: compatibility between backends."	FLINK	Resolved	3	7	8742	pull-request-available
13357979	Implement incremental checkpointing and recovery using state changelog	"* Including state handles serialization (MetadataV2V3SerializerBase)
* Not including  materialization"	FLINK	Closed	3	7	8742	auto-unassigned, pull-request-available
13581200	Use common thread pools when transferring RocksDB state files	"Currently, each RocksDB state backend creates an executor backed by a thread pool.

This makes it difficult to control the total number of threads per TM because it might have at least one task per slot and theoretically, many state backends per task (because of chaining).

Additionally, using a common thread pool allows to indirectly control the load on the underlying DFS (e.g. the total number of requests to S3 from a TM)."	FLINK	Closed	3	4	8742	pull-request-available
13476881	Changelog 1st materialization delayed unneccesarily	"In PeriodicMaterializationManager.start(), the 1st materialization is scheduled with a delay: materialization_interval + random_offset 

Here, random_offset is added to avoid thundering herd problem.
The next materialization will be scheduled with a delay of only materialization_interval.

That means that the 1st materialization will have to compact up to 2 times more state changes than the subsequent ones. 

Which in turn can cause FLINK--26590 or other problems."	FLINK	Closed	3	1	8742	pull-request-available
13430115	ChangelogCompatibilityITCase.testRestore failed on azure	"{code:java}
Feb 22 19:09:43 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.815 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogCompatibilityITCase
Feb 22 19:09:43 [ERROR] ChangelogCompatibilityITCase.testRestore  Time elapsed: 1.309 s  <<< ERROR!
Feb 22 19:09:43 java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /tmp/junit2665494390926857042/junit2441494118455041375/accbd512d1546402f50f07832672cf2a/chk-1/._metadata.inprogress.7ebadfd7-88be-41ef-9889-f9cb5fa59113
Feb 22 19:09:43 	at java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:88)
Feb 22 19:09:43 	at java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:104)
Feb 22 19:09:43 	at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:499)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:486)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Feb 22 19:09:43 	at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.findAny(ReferencePipeline.java:536)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.hasMetadata(TestUtils.java:122)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.isCompletedCheckpoint(TestUtils.java:112)
Feb 22 19:09:43 	at java.nio.file.Files.lambda$find$2(Files.java:3691)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)
Feb 22 19:09:43 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Feb 22 19:09:43 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
Feb 22 19:09:43 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
Feb 22 19:09:43 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546)
Feb 22 19:09:43 	at java.util.stream.ReferencePipeline.max(ReferencePipeline.java:582)
Feb 22 19:09:43 	at org.apache.flink.test.util.TestUtils.getMostRecentCompletedCheckpointMaybe(TestUtils.java:105)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.waitForCheckpoint(ChangelogCompatibilityITCase.java:264)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.tryCheckpointAndStop(ChangelogCompatibilityITCase.java:145)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.runAndStoreIfAllowed(ChangelogCompatibilityITCase.java:109)
Feb 22 19:09:43 	at org.apache.flink.test.state.ChangelogCompatibilityITCase.testRestore(ChangelogCompatibilityITCase.java:103)
Feb 22 19:09:43 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 19:09:43 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 19:09:43 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 19:09:43 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 19:09:43 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Feb 22 19:09:43 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Feb 22 19:09:43 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Feb 22 19:09:43 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Feb 22 19:09:43 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=32057&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=12444"	FLINK	Closed	3	1	8742	pull-request-available, test-stability
13387896	Enable changelog backend in tests	FLINK-21448 adds the capability (test randomization), but it can't be turned on as there are some test failures: FLINK-23276, FLINK-23277, FLINK-23278 (should be enabled after those bugs fixed)..	FLINK	Closed	3	7	8742	pull-request-available
13323402	Optimize reading of channel state on recovery	"Curently, channel state is read not sequentially.

Inverting control would make it more efficient.

Current call chain: 
{code:java}
StreamTask.readRecoveredChannelState  
    ResultPartition.readRecoveredState - loop through subpartitions
        PipelinedSubpartition.readRecoveredState - loop while have data; bufferBuilder = parent.getBufferPool().requestBufferBuilderBlocking(subpartitionInfo.getSubPartitionIdx());
            ChannelStateReader.readOutputData   {code}
Proposed call chain:
{code:java}
StreamTask.readRecoveredChannelState
    ChannelStateReader.readOutputData loop through state handles ordererd by handle, offset
        request buffer in the same way: BufferBuilder bufferBuilder = resPart.getBufferPool().requestBufferBuilderBlocking(subpartitionInfo.getSubPartitionIdx());
        pass to resPart.getSubpartition(idx).add(BufferConsumer, boolean, boolean)
{code}
 "	FLINK	Resolved	3	7	8742	pull-request-available
13333678	Add StateChangelog interface and its in-memory implementation	"StateChangelog is a component proposed in FLIP-158 to store state changes for incremental checkpoints.

 "	FLINK	Closed	3	7	8742	pull-request-available
13337645	Incompatible semantics of channelIndex in UnionInputGate.resumeConsumption and its clients	"Given channelIndex only, UnionInputGate has to guess which wrapped input
gate this channel belongs to. For that, UnionInputGate expects channel
index with an inputGate offset. This contradicts with the contract of
other resumeConsumption() implementations.
UnionInputGate.resumeConsumption isn't used currently but is planned to
be used in FLINK-19856."	FLINK	Resolved	4	1	8742	pull-request-available
13357977	Send changes to the state changelog (still proxy everything)	"Subtasks:
 # -Changelog instantiation (including configuration)- (FLINK-21804)
 # Connecting Changelog with ProxyBackend
 # Connecting Proxy-State objects with Changelog
 # Serializing changes in ProxyState objects and sending changes to Changelog
 # no metadata logging (FLINK-22808)
 # Unit test coverage"	FLINK	Closed	3	7	8742	pull-request-available
13394871	Hide any configuration, API or docs	"As the feature will not make it to the upcoming 1.14 release,

hide the related config options like CheckpointingOptions.ENABLE_STATE_CHANGE_LOG, API  (e.g. StreamExecutionEnvironment.enableChangelogStateBackend).

Also check Python and Scala APIs and the documentation."	FLINK	Closed	2	7	8742	pull-request-available
13374533	testScheduleRunAsync fail	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17077&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=81f2da51-a161-54c7-5b84-6001fed26530&l=6833


{code:java}
Apr 22 22:56:40 [ERROR] testScheduleRunAsync(org.apache.flink.runtime.rpc.RpcEndpointTest)  Time elapsed: 0.404 s  <<< FAILURE!
Apr 22 22:56:40 java.lang.AssertionError
Apr 22 22:56:40 	at org.junit.Assert.fail(Assert.java:86)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:41)
Apr 22 22:56:40 	at org.junit.Assert.assertTrue(Assert.java:52)
Apr 22 22:56:40 	at org.apache.flink.runtime.rpc.RpcEndpointTest.testScheduleRunAsync(RpcEndpointTest.java:318)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 22 22:56:40 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 22 22:56:40 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Apr 22 22:56:40 	at java.lang.reflect.Method.invoke(Method.java:498)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
Apr 22 22:56:40 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Apr 22 22:56:40 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Apr 22 22:56:40 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Apr 22 22:56:40 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Apr 22 22:56:40 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Apr 22 22:56:40 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Apr 22 22:56:40 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Apr 22 22:56:40 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)

{code}
"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13399496	PartiallyFinishedSourcesITCase fails due to assertion error in DrainingValidator.validateOperatorLifecycle	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=23526&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=4639

{code}
Sep 03 23:17:11 [ERROR] Tests run: 6, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 19.233 s <<< FAILURE! - in org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase
Sep 03 23:17:11 [ERROR] test[simple graph SINGLE_SUBTASK, failover: true]  Time elapsed: 2.27 s  <<< FAILURE!
Sep 03 23:17:11 java.lang.AssertionError
Sep 03 23:17:11 	at org.junit.Assert.fail(Assert.java:87)
Sep 03 23:17:11 	at org.junit.Assert.assertTrue(Assert.java:42)
Sep 03 23:17:11 	at org.junit.Assert.assertFalse(Assert.java:65)
Sep 03 23:17:11 	at org.junit.Assert.assertFalse(Assert.java:75)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.DrainingValidator.validateOperatorLifecycle(DrainingValidator.java:56)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.TestOperatorLifecycleValidator.lambda$checkOperatorsLifecycle$1(TestOperatorLifecycleValidator.java:52)
Sep 03 23:17:11 	at java.util.HashMap.forEach(HashMap.java:1289)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.validation.TestOperatorLifecycleValidator.checkOperatorsLifecycle(TestOperatorLifecycleValidator.java:47)
Sep 03 23:17:11 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:94)
{code}"	FLINK	Resolved	1	1	8742	pull-request-available, test-stability
13368664	Failed to release input gate (IllegalReferenceCountException)	"When running UnalignedCheckpointITCase continuously, this error is periodically reported (though tests pass):
{code}
[INFO] Running org.apache.flink.test.checkpointing.UnalignedCheckpointITCase
34948 [Canceler for keyed (19/20)#1 (532afce89e3c90301e37b2f5e9a0bded).] ERROR org.apache.flink.runtime.taskmanager.Task [] - Failed to release input gate for task keyed (19/20)#1.
org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0, decrement: 1
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.toLiveRealRefCnt(ReferenceCountUpdater.java:74) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ReferenceCountUpdater.release(ReferenceCountUpdater.java:138) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:100) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.recycleBuffer(NetworkBuffer.java:156) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.releaseAllBuffers(BufferManager.java:234) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:267) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:555) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:119) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:978) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1559) [classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
38445 [Canceler for keyed (19/20)#2 (f94480c534db164b2cdf7f44682593d1).] ERROR org.apache.flink.runtime.taskmanager.Task [] - Failed to release input gate for task keyed (19/20)#2.
org.apache.flink.shaded.netty4.io.netty.util.IllegalReferenceCountException: refCnt: 0
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractByteBuf.ensureAccessible(AbstractByteBuf.java:1489) ~[flink-shaded-netty-4.1.49.Final-12.0.jar:?]
        at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.getMemorySegment(NetworkBuffer.java:139) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.BufferManager.releaseAllBuffers(BufferManager.java:232) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.releaseAllResources(RemoteInputChannel.java:267) ~[classes/:?]
        at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.close(SingleInputGate.java:555) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.close(InputGateWithMetrics.java:119) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.closeNetworkResources(Task.java:978) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1559) [classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code}"	FLINK	Resolved	3	1	8742	pull-request-available
13427881	[Changelog] Disallow recovery from non-changelog checkpoints	"Extracted from FLINK-25872. 

The issue is with the CLAIM mode:
> Because discarding an initial checkpoint will invalidate its ""private"" state which might be in use by future checkpoints.
> Normally, changelog backend wraps it and registers with tjhe SharedStateRegistry.
> But when recovering from non-changelog checkpoint, it is first added to the Checkpoint store, and wrapping in subsequent checkpoints doesn't help.

NO_CLAIM mode is not supported.
LEGACY could work.

But it's difficult to differentiate between the modes on TM, where backend type is reliably known (see the discussion below).

CANONICAL non-changelog savepoints must still be supported."	FLINK	Resolved	1	1	8742	pull-request-available
13428604	SavepointFormatITCase fails on azure	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=31474&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=13116]

{code}
[ERROR] org.apache.flink.test.checkpointing.SavepointFormatITCase.testTriggerSavepointAndResumeWithFileBasedCheckpointsAndRelocateBasePath(SavepointFormatType, StateBackendConfig)[2]  Time elapsed: 14.209 s  <<< ERROR!
java.util.concurrent.ExecutionException: java.io.IOException: Unknown implementation of StreamStateHa ndle: class org.apache.flink.runtime.state.PlaceholderStreamStateHandle
   at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
   at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
   at org.apache.flink.test.checkpointing.SavepointFormatITCase.submitJobAndTakeSavepoint(SavepointFormatITCase.java:328)
   at org.apache.flink.test.checkpointing.SavepointFormatITCase.testTriggerSavepointAndResumeWithFileBasedCheckpointsAndRelocateBasePath(SavepointFormatITCase.java:248)
   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   at java.lang.reflect.Method.invoke(Method.java:498)
   at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
   at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(Invo cationInterceptorChain.java:131)
   at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
   at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.ja va:140)
   at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestTemplateMethod(TimeoutExtensio n.java:92)
   at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMet hod$0(ExecutableInvoker.java:115)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105 )
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(Inv ocationInterceptorChain.java:106)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChai n.java:64)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationIntercep torChain.java:45)
   at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain .java:37)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
   at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
   at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMeth odTestDescriptor.java:214)
   at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.ja
{code}"	FLINK	Resolved	1	1	8742	pull-request-available
13390261	Don't log NoResourceAvailableException stacktrace on Execution state transition	"Logging NoResourceAvailableException stacktrace on Execution state transition 
results in [1G+ artifacts|https://artprodsu6weu.artifacts.visualstudio.com/A2d3c0ac8-fecf-45be-8407-6d87302181a9/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_apis/artifact/cGlwZWxpbmVhcnRpZmFjdDovL2FwYWNoZS1mbGluay9wcm9qZWN0SWQvOTg0NjM0OTYtMWFmMi00NjIwLThlYWItYTJlY2MxYTJlNmZlL2J1aWxkSWQvMjA0NTQvYXJ0aWZhY3ROYW1lL2xvZ3MtY3Jvbl9oYWRvb3AyNDEtdGVzdF9jcm9uX2hhZG9vcDI0MV9jb25uZWN0b3JzLTE2MjYyOTg3NjY1/content?format=zip] (for [this failure|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20454&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=14595])

while not providing any additional info:
{code:java}
01:25:15,527 [flink-akka.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: Custom Source -> Map -> Sink: Unnamed (1/4) (2a191a61b3a7d7fd416b6d181948b8bd) switched from SCHEDULED to FAILED on [unassigned resource].
java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimu*m required resources.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_282]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_282]
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_282]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_282]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge$PendingRequest.failRequest(DeclarativeSlotPoolBridge.java:545) ~[*flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.cancelPendingRequests(DeclarativeSlotPoolBridge.java:127) ~[flink*-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.failPendingRequests(DeclarativeSlotPoolBridge.java:355) ~[flink-r*untime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.slotpool.DeclarativeSlotPoolBridge.notifyNotEnoughResourcesAvailable(DeclarativeSlotPoolBridge.java:*344) ~[flink-runtime_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.notifyNotEnoughResourcesAvailable(JobMaster.java:816) ~[flink-runtime_2.11-1.14-SNAPSHOT.j*ar:1.14-SNAPSHOT]
        at sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_282]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_282]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:301) ~[flink-rpc-akka_2.11-1.14-SNAPSHOT.jar:1.14-*SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) ~[flink-rpc-akka_2.11-1.14-SNAPSHOT.jar:1.14-SNA*PSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77) ~[flink-rpc-akka_2.11-1.14-SNAPSHOT.j*ar:1.14-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-rpc-akka_2.11-1.14-SNAPSHOT.jar:1.14-SNAPSH*OT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
        at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
        at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not acquire the minimum required resources.
        ... 28 more
{code}
Could be changed to:
{code:java}
11453 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Custom Source -> Map -> Sink: Unnamed (1/4) (c75c66f40d4e458077e36a0c82154d00) switched from SCHEDULED to FAILED: Could not acquire the minimum required resources.
{code}
 "	FLINK	Closed	4	4	8742	pull-request-available
13360958	Tests time out on azure	"Both test_ci legacy_slot_management and test_ci finegrained_resource_management time out on azure

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13731&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9"	FLINK	Resolved	1	1	8742	pull-request-available, test-stability
13357345	SavepointITCase.testStopSavepointWithBoundedInputConcurrently is unstable	https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13036&view=logs&j=34f41360-6c0d-54d3-11a1-0292a2def1d9&t=2d56e022-1ace-542f-bf1a-b37dd63243f2	FLINK	Resolved	3	1	8742	pull-request-available
13362064	Add PublicEvolving to RuntimeContext.jobId	"The JobID added in FLINK-21570] is 1) a breaking change; 2) may be changed after DataSet is removed.

 

So PublicEvolging should be added.

 

cc: [~chesnay]"	FLINK	Resolved	3	4	8742	pull-request-available
13365091	Allow HeapBackend snapshotting customization	"Allow customization of snapshotting of HeapKeyedStateBackend
so that it can be snapshotted incrementally."	FLINK	In Progress	3	7	8742	auto-unassigned, pull-request-available, stale-assigned
13509855	TaskExecutorTest.testSharedResourcesLifecycle failed with TaskException	"This seems to be a follow-up of FLINK-30275. Same test but different test failure (2x in the same build):
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7479
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=43709&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7852
{code}
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
Dec 05 03:59:18 	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
Dec 05 03:59:18 	at java.util.Iterator.forEachRemaining(Iterator.java:116)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
Dec 05 03:59:18 	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
Dec 05 03:59:18 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Dec 05 03:59:18 Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskException: Cannot find task to stop for execution 096b33c46c225fd4af41a9484b64c7fe_010f83ce510d70707aaf04c441173b70_0_0.
Dec 05 03:59:18 	at org.apache.flink.runtime.taskexecutor.TaskExecutor.cancelTask(TaskExecutor.java:864)
Dec 05 03:59:18 	... 53 more
{code}"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13360271	Add local recovery support to adaptive scheduler	"local recovery means that, on a failure, we are able to re-use the state in a taskmanager, instead of loading it again from distributed storage (which means the scheduler needs to know where which state is located, and schedule tasks accordingly).

Adaptive Scheduler is currently not respecting the location of state, so failures require the re-loading of state from the distributed storage.

Adding this feature will allow us to enable the {{Local recovery and sticky scheduling end-to-end test}} for adaptive scheduler again."	FLINK	Resolved	3	4	8742	auto-deprioritized-major, auto-deprioritized-minor, auto-unassigned, pull-request-available
13368839	SavepointWindowReaderITCase.testApplyEvictorWindowStateReader	"The test case {{SavepointWindowReaderITCase.testApplyEvictorWindowStateReader}} failed on AZP with:

{code}

	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
	at org.apache.flink.state.api.utils.SavepointTestBase.takeSavepoint(SavepointTestBase.java:69)
	... 33 more
Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.triggerSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
	at com.sun.proxy.$Proxy32.triggerSavepoint(Unknown Source)
	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$triggerSavepoint$8(MiniCluster.java:716)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:751)
	at org.apache.flink.runtime.minicluster.MiniCluster.triggerSavepoint(MiniCluster.java:714)
	at org.apache.flink.client.program.MiniClusterClient.triggerSavepoint(MiniClusterClient.java:101)
	at org.apache.flink.state.api.utils.SavepointTestBase.triggerSavepoint(SavepointTestBase.java:93)
	at org.apache.flink.state.api.utils.SavepointTestBase.lambda$takeSavepoint$0(SavepointTestBase.java:68)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1646)
	at java.util.concurrent.CompletableFuture$AsyncRun.exec(CompletableFuture.java:1632)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_2#-390276455]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=15809&view=logs&j=b2f046ab-ae17-5406-acdc-240be7e870e4&t=93e5ae06-d194-513d-ba8d-150ef6da1d7c&l=9197"	FLINK	Resolved	1	1	8742	auto-deprioritized-critical, pull-request-available, test-stability
13353440	SemanticXidGeneratorTest.testXidsUniqueAmongGenerators test failed	"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=12245&view=logs&j=d44f43ce-542c-597d-bf94-b0718c71e5e8&t=03dca39c-73e8-5aaf-601d-328ae5c35f20]
{code:java}
2021-01-19T15:21:35.5665063Z [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.1 s <<< FAILURE! - in org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest
2021-01-19T15:21:35.5665936Z [ERROR] testXidsUniqueAmongGenerators(org.apache.flink.connector.jdbc.xa.SemanticXidGeneratorTest)  Time elapsed: 0.024 s  <<< FAILURE!
2021-01-19T15:21:35.5666770Z junit.framework.AssertionFailedError: expected:<10000> but was:<9999>
{code}"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13291142	Extract static classes from StreamTask	"StreamTask is currently 1400+ LOC.

We can cut it to 1100+ by simply extracting these static classes into separate files:
 * `CheckpointingOperation`
 * `AsyncCheckpointRunnable`"	FLINK	Closed	4	4	8742	pull-request-available
13479477	Fix logging in DefaultCompletedCheckpointStore	See [https://github.com/apache/flink/pull/16582#discussion_r949214456]	FLINK	Resolved	4	4	8742	pull-request-available
13348970	Update CompletedCheckpointStore.shutdown() signature	"# remove unused postCleanup argument
 # add javadoc for checkpointsCleaner"	FLINK	Resolved	4	4	8742	pull-request-available
13436941	Bump CopyOnWriteStateMap entry version before write	"CopyOnWriteStateMap copies the entry before returning it to the client
for update. This also updates its state and entry versions.

However, if the entry is NOT used by any snapshots, the versions
will stay the same despite that state is going to be updated.

With incremental checkpoints, this causes such updated version to
be ignored in the next snapshot."	FLINK	In Progress	3	7	8742	pull-request-available, stale-assigned
13582662	Release Testing Instructions: Verify FLINK-26050 Too many small sst files in rocksdb state backend when using time window created in ascending order	"Follow up the test for https://issues.apache.org/jira/browse/FLINK-26050

The problem occurs when using RocksDB and specific queries/jobs (please see the ticket for the detailed description).

To test the solution, run the following query with RocksDB as a state backend:

 
{code:java}
INSERT INTO top_5_highest_view_time
SELECT *
FROM   (
                SELECT   *,
                         ROW_NUMBER() OVER (PARTITION BY window_start, window_end ORDER BY view_time DESC) AS rownum
                FROM     (
                                  SELECT   window_start,
                                           window_end,
                                           product_id,
                                           SUM(view_time) AS view_time,
                                           COUNT(*)       AS cnt
                                  FROM     TABLE(TUMBLE(TABLE `shoe_clickstream`, DESCRIPTOR($rowtime), INTERVAL '10' MINUTES))
                                  GROUP BY window_start,
                                           window_end,
                                           product_id))
WHERE  rownum <= 5;{code}
 

With the feature disabled (default), the number of files in rocksdb working directory (as well as in the checkpoint) should grow indefinitely.

 

With feature enabled, the number of files should stays constant (as they should get merged with each other).

To enable the feature, set 
{code:java}
state.backend.rocksdb.manual-compaction.min-interval{code}
 set to 1 minute for example.

 

Please consult [https://github.com/apache/flink/blob/e7d7db3b6f87e53d9bace2a16cf95e5f7a79087a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/sstmerge/RocksDBManualCompactionOptions.java#L29] for other options if necessary."	FLINK	Closed	1	7	8742	release-testing
13369255	SubtaskCheckpointCoordinatorTest.testForceAlignedCheckpointResultingInPriorityEvents unstable	"https://dev.azure.com/rmetzger/Flink/_build/results?buildId=9021&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=ab910030-93db-52a7-74a3-34a0addb481b

{code}
2021-04-01T19:29:55.2392858Z [ERROR] Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.921 s <<< FAILURE! - in org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest
2021-04-01T19:29:55.2396751Z [ERROR] testForceAlignedCheckpointResultingInPriorityEvents(org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest)  Time elapsed: 0.02 s  <<< ERROR!
2021-04-01T19:29:55.2397415Z java.lang.RuntimeException: unable to send request to worker
2021-04-01T19:29:55.2397956Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:228)
2021-04-01T19:29:55.2398603Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.finishOutput(ChannelStateWriterImpl.java:183)
2021-04-01T19:29:55.2399310Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298)
2021-04-01T19:29:55.2400104Z 	at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.testForceAlignedCheckpointResultingInPriorityEvents(SubtaskCheckpointCoordinatorTest.java:215)
2021-04-01T19:29:55.2400746Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-04-01T19:29:55.2401202Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-04-01T19:29:55.2401746Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-04-01T19:29:55.2402237Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-04-01T19:29:55.2402722Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-04-01T19:29:55.2403270Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-04-01T19:29:55.2403818Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-04-01T19:29:55.2404354Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-04-01T19:29:55.2404854Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-04-01T19:29:55.2405359Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-04-01T19:29:55.2405896Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-04-01T19:29:55.2406393Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-04-01T19:29:55.2406855Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-04-01T19:29:55.2407331Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-04-01T19:29:55.2407806Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-04-01T19:29:55.2408279Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-04-01T19:29:55.2408907Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-04-01T19:29:55.2409403Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-04-01T19:29:55.2409954Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-04-01T19:29:55.2410524Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-04-01T19:29:55.2411318Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-04-01T19:29:55.2411880Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-04-01T19:29:55.2412467Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-04-01T19:29:55.2413006Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-04-01T19:29:55.2413519Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-04-01T19:29:55.2414082Z Caused by: java.lang.IllegalArgumentException: writer not found while processing request: writeOutput 0
2021-04-01T19:29:55.2414726Z 	at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.onWriterMissing(ChannelStateWriteRequest.java:223)
2021-04-01T19:29:55.2415463Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:80)
2021-04-01T19:29:55.2416331Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:59)
2021-04-01T19:29:55.2417066Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
2021-04-01T19:29:55.2417782Z 	at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
2021-04-01T19:29:55.2418329Z 	at java.lang.Thread.run(Thread.java:748)
2021-04-01T19:29:55.2418741Z 	Suppressed: java.lang.IllegalStateException: not running
2021-04-01T19:29:55.2419343Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.ensureRunning(ChannelStateWriteRequestExecutorImpl.java:152)
2021-04-01T19:29:55.2420249Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submitInternal(ChannelStateWriteRequestExecutorImpl.java:144)
2021-04-01T19:29:55.2421010Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submit(ChannelStateWriteRequestExecutorImpl.java:128)
2021-04-01T19:29:55.2421694Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:225)
2021-04-01T19:29:55.2422333Z 		at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.finishOutput(ChannelStateWriterImpl.java:183)
2021-04-01T19:29:55.2423029Z 		at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298)
2021-04-01T19:29:55.2423820Z 		at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorTest.testForceAlignedCheckpointResultingInPriorityEvents(SubtaskCheckpointCoordinatorTest.java:215)
2021-04-01T19:29:55.2425923Z 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-04-01T19:29:55.2426529Z 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-04-01T19:29:55.2427162Z 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-04-01T19:29:55.2427727Z 		at java.lang.reflect.Method.invoke(Method.java:498)
2021-04-01T19:29:55.2428283Z 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-04-01T19:29:55.2429008Z 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-04-01T19:29:55.2429611Z 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-04-01T19:29:55.2430357Z 		at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-04-01T19:29:55.2430921Z 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-04-01T19:29:55.2431809Z 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-04-01T19:29:55.2432459Z 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-04-01T19:29:55.2433013Z 		at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-04-01T19:29:55.2433513Z 		at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-04-01T19:29:55.2434043Z 		at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-04-01T19:29:55.2434568Z 		at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-04-01T19:29:55.2435093Z 		at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-04-01T19:29:55.2435609Z 		at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-04-01T19:29:55.2436161Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-04-01T19:29:55.2436772Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-04-01T19:29:55.2437399Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-04-01T19:29:55.2438127Z 		at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-04-01T19:29:55.2438881Z 		at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-04-01T19:29:55.2439509Z 		at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-04-01T19:29:55.2440088Z 		at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-04-01T19:29:55.2440642Z 		at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-04-01T19:29:55.2440899Z 
{code}"	FLINK	Resolved	2	1	8742	pull-request-available, test-stability
13472897	Remove separate error handling and adjust documentation for CLAIM mode + RocksDB native savepoint	"After FLINK-25872, checkpoint folder deletion is not performed as long as there is some state from that checkpoint used by other checkpoints.
Therefore, the following changes could be reverted/adjusted:
* FLINK-25745 e8bcbfd5a48fd8d3ca48ef7803867569214e0dbc Do not log exception
* FLINK-25745 c1f5c5320150402fc0cb4fbf3a31f9a27b1e4d9a Document incremental savepoints in CLAIM mode limitation

cc: [~Yanfei Lei], [~dwysakowicz]"	FLINK	Closed	3	4	8742	pull-request-available
13387894	Changelog backend doesn't apply TTL after recovery	"Upon recovery, changelog backend requests states to apply changes.
TTL config is not available at this moment, so states are created regardless of TTL config.
One solution is to serialize TTL config along with metadata (in changelog).

Note: values are already serialized as TTL values and serializers as TTL seralizers

{code}
Caused by: java.lang.ClassCastException: org.apache.flink.runtime.state.ttl.TtlValue cannot be cast to org.apache.flink.table.data.RowData
   at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:129)
   at org.apache.flink.table.runtime.operators.aggregate.GroupAggFunction.processElement(GroupAggFunction.java:43)
   at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
   at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:228)
   at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
   at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
   at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:428)
   at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:691)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:646)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:657)
   at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:630)
   at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
   at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
   at java.lang.Thread.run(Thread.java:748)
{code}

(doesn't affect test stability as changelog backend is currently disabled in tests)"	FLINK	Resolved	3	1	8742	pull-request-available
13393309	Cleanup unnecessary dependencies in dstl pom.xml	"- Clean up `flink-statebackend-changelog` denpendencies (move flink-statebackend-changelog depency to flink-test-utils) 
 - check whether some dependencies (i.e. flink-streaming-java, shaded guava, flink-test-utils-junit) are indeed necessary; comment if they are transitive
 - fix the scope of flink-runtime and flink-core - compile (not provided) and test for test-jar
 - Document how changelog randomization flags work."	FLINK	Closed	3	7	8742	pull-request-available
13272761	InternalTimerServiceImpl references restored state after use, taking up resources unnecessarily	"E.g. org.apache.flink.streaming.api.operators.InternalTimerServiceImpl#restoredTimersSnapshot:
 # written in restoreTimersForKeyGroup()

 # used in startTimerService()

 # and then never used again.

 "	FLINK	Resolved	4	4	8742	pull-request-available
13309485	Don't start channel state writing for savepoints (RPC)	ChannelStateWriter#start should be only called for unaligned checkpoint. While source triggering savepoint, SubtaskCheckpointCoordinator#initCheckpoint is introduced to judge the condition whether to start the internal writer or not. And this new method is also used in other places like CheckpointBarrierUnaligner.	FLINK	Closed	3	1	8742	pull-request-available
13521245	RocksDB Memory Management end-to-end test failed due to unexpected exception	"We see a test instability with {{RocksDB Memory Management end-to-end test}}. The test failed because an exception was detected in the logs:
{code}
2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...
2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.
2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...
2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:
[...]
2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...
2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.
2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 
2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

The only exception being reported in the Flink logs is due to a warning:
{code}
2023-01-25 02:47:38,242 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5117"	FLINK	Closed	2	1	8742	pull-request-available, test-stability
13340733	"FlinkKafkaProducerITCase.testRunOutOfProducersInThePool failed with ""CheckpointException: Could not complete snapshot 1 for operator MockTask (1/1)#0. Failure reason: Checkpoint was declined."""	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9596&view=logs&j=c5f0071e-1851-543e-9a45-9ac140befc32&t=1fb1a56f-e8b5-5a82-00a0-a2db7757b4f5

{code}
2020-11-15T22:54:53.4151222Z [ERROR] testRunOutOfProducersInThePool(org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase)  Time elapsed: 61.224 s  <<< ERROR!
2020-11-15T22:54:53.4152487Z org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 1 for operator MockTask (1/1)#0. Failure reason: Checkpoint was declined.
2020-11-15T22:54:53.4153577Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:226)
2020-11-15T22:54:53.4154747Z 	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:158)
2020-11-15T22:54:53.4155760Z 	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:343)
2020-11-15T22:54:53.4156772Z 	at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.snapshotWithLocalState(AbstractStreamOperatorTestHarness.java:638)
2020-11-15T22:54:53.4157865Z 	at org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.snapshot(AbstractStreamOperatorTestHarness.java:630)
2020-11-15T22:54:53.4159065Z 	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducerITCase.testRunOutOfProducersInThePool(FlinkKafkaProducerITCase.java:527)
{code}"	FLINK	Closed	2	1	8742	test-stability
13443935	Performance regression in checkpointSingleInput.UNALIGNED on 29.04.2022	http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200	FLINK	Closed	3	1	8742	pull-request-available
13374968	JdbcExactlyOnceSinkE2eTest.testInsert failed because of too many clients.	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17178&view=logs&j=ba53eb01-1462-56a3-8e98-0dd97fbcaab5&t=bfbc6239-57a0-5db0-63f3-41551b4f7d51&l=13514


{code:java}
Apr 25 23:05:31 [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 138.743 s <<< FAILURE! - in org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest
Apr 25 23:05:31 [ERROR] testInsert(org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest)  Time elapsed: 137.267 s  <<< ERROR!
Apr 25 23:05:31 org.postgresql.util.PSQLException: FATAL: sorry, too many clients already
Apr 25 23:05:31 	at org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:524)
Apr 25 23:05:31 	at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:145)
Apr 25 23:05:31 	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:196)
Apr 25 23:05:31 	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
Apr 25 23:05:31 	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:211)
Apr 25 23:05:31 	at org.postgresql.Driver.makeConnection(Driver.java:459)
Apr 25 23:05:31 	at org.postgresql.Driver.connect(Driver.java:261)
Apr 25 23:05:31 	at java.sql.DriverManager.getConnection(DriverManager.java:664)
Apr 25 23:05:31 	at java.sql.DriverManager.getConnection(DriverManager.java:247)
Apr 25 23:05:31 	at org.apache.flink.connector.jdbc.xa.JdbcXaFacadeTestHelper.getInsertedIds(JdbcXaFacadeTestHelper.java:81)
Apr 25 23:05:31 	at org.apache.flink.connector.jdbc.xa.JdbcExactlyOnceSinkE2eTest.testInsert(JdbcExactlyOnceSinkE2eTest.java:119)
Apr 25 23:05:31 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

{code}
"	FLINK	Resolved	3	1	8742	pull-request-available, test-stability
13395473	DeleteExecutor NPE	"Encountered a situation where I get an NPE from JDBCUpsertOutputFormat.

This occurs when jdbc disconnected and try to reconnect.

I need to write data to mysql in upsert way in sql, So it must group by unique key and the JdbcBatchingOutputFormat of Jdbc sink would use TableJdbcUpsertOutputFormat.

 

Jdbc would disconnected when The data interval exceeds the set connection time.I see that when jdbc reconnect , only JdbcBatchingOutputFormat#jdbcStatementExecutor(insert) would prepareStatements but TableJdbcUpsertOutputFormat#deleteExecutor would not prepareStatements so that come up NPE.

if in JdbcBatchingOutputFormat have a protected function to reset PrepareStatement and TableJdbcUpsertOutputFormat override this function to reset deleteExecutor, it would work well.

prepareStatements "	FLINK	Resolved	2	1	8742	pull-request-available
13427582	[Changelog] Non-deterministic recovery of PriorityQueue states	"Currently, InternalPriorityQueue.poll() is logged as a separate operation, without specifying the element that has been polled. On recovery, this recorded poll() is replayed.

However, this is not deterministic because the order of PQ elements with equal priorityis not specified. For example, TimerHeapInternalTimer only compares timestamps, which are often equal. This results in polling timers from queue in wrong order => dropping timers => and not firing timers.

 

ProcessingTimeWindowCheckpointingITCase.testAggregatingSlidingProcessingTimeWindow fails with materialization enabled and using heap state backend (both in-memory and fs-based implementations).

 

Proposed solution is to replace poll with remove operation (which is based on equality).
 
cc: [~masteryhx], [~ym], [~yunta]"	FLINK	Resolved	3	1	8742	pull-request-available
13306062	"""Resuming Savepoint"" e2e stalls indefinitely "	"CI; https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=1887&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=94459a52-42b6-5bfc-5d74-690b5d3c6de8

{code}
2020-05-19T21:05:52.9696236Z ==============================================================================
2020-05-19T21:05:52.9696860Z Running 'Resuming Savepoint (file, async, scale down) end-to-end test'
2020-05-19T21:05:52.9697243Z ==============================================================================
2020-05-19T21:05:52.9713094Z TEST_DATA_DIR: /home/vsts/work/1/s/flink-end-to-end-tests/test-scripts/temp-test-directory-52970362751
2020-05-19T21:05:53.1194478Z Flink dist directory: /home/vsts/work/1/s/flink-dist/target/flink-1.12-SNAPSHOT-bin/flink-1.12-SNAPSHOT
2020-05-19T21:05:53.2180375Z Starting cluster.
2020-05-19T21:05:53.9986167Z Starting standalonesession daemon on host fv-az558.
2020-05-19T21:05:55.5997224Z Starting taskexecutor daemon on host fv-az558.
2020-05-19T21:05:55.6223837Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:05:57.0552482Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:05:57.9446865Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:05:59.0098434Z Waiting for Dispatcher REST endpoint to come up...
2020-05-19T21:06:00.0569710Z Dispatcher REST endpoint is up.
2020-05-19T21:06:07.7099937Z Job (a92a74de8446a80403798bb4806b73f3) is running.
2020-05-19T21:06:07.7855906Z Waiting for job to process up to 200 records, current progress: 114 records ...
2020-05-19T21:06:55.5755111Z 
2020-05-19T21:06:55.5756550Z ------------------------------------------------------------
2020-05-19T21:06:55.5757225Z  The program finished with the following exception:
2020-05-19T21:06:55.5757566Z 
2020-05-19T21:06:55.5765453Z org.apache.flink.util.FlinkException: Could not stop with a savepoint job ""a92a74de8446a80403798bb4806b73f3"".
2020-05-19T21:06:55.5766873Z 	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:485)
2020-05-19T21:06:55.5767980Z 	at org.apache.flink.client.cli.CliFrontend.runClusterAction(CliFrontend.java:854)
2020-05-19T21:06:55.5769014Z 	at org.apache.flink.client.cli.CliFrontend.stop(CliFrontend.java:477)
2020-05-19T21:06:55.5770052Z 	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:921)
2020-05-19T21:06:55.5771107Z 	at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:982)
2020-05-19T21:06:55.5772223Z 	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:30)
2020-05-19T21:06:55.5773325Z 	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:982)
2020-05-19T21:06:55.5774871Z Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5777183Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-05-19T21:06:55.5778884Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2020-05-19T21:06:55.5779920Z 	at org.apache.flink.client.cli.CliFrontend.lambda$stop$5(CliFrontend.java:483)
2020-05-19T21:06:55.5781175Z 	... 6 more
2020-05-19T21:06:55.5782391Z Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5783885Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.lambda$stopWithSavepoint$9(SchedulerBase.java:890)
2020-05-19T21:06:55.5784992Z 	at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)
2020-05-19T21:06:55.5786492Z 	at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)
2020-05-19T21:06:55.5787601Z 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
2020-05-19T21:06:55.5788682Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)
2020-05-19T21:06:55.5790308Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)
2020-05-19T21:06:55.5791664Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
2020-05-19T21:06:55.5792767Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
2020-05-19T21:06:55.5793756Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2020-05-19T21:06:55.5794652Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2020-05-19T21:06:55.5795605Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2020-05-19T21:06:55.5796551Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2020-05-19T21:06:55.5797459Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2020-05-19T21:06:55.5798390Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-19T21:06:55.5799311Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2020-05-19T21:06:55.5800175Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2020-05-19T21:06:55.5801078Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2020-05-19T21:06:55.5802741Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2020-05-19T21:06:55.5803579Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2020-05-19T21:06:55.5804628Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2020-05-19T21:06:55.5805435Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2020-05-19T21:06:55.5806194Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2020-05-19T21:06:55.5807037Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-05-19T21:06:55.5808001Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-05-19T21:06:55.5808984Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-05-19T21:06:55.5809970Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2020-05-19T21:06:55.5811188Z Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5813260Z 	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
2020-05-19T21:06:55.5814556Z 	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
2020-05-19T21:06:55.5815578Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
2020-05-19T21:06:55.5816604Z 	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
2020-05-19T21:06:55.5817663Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-19T21:06:55.5822918Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-05-19T21:06:55.5824096Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.lambda$null$0(CheckpointCoordinator.java:464)
2020-05-19T21:06:55.5825220Z 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
2020-05-19T21:06:55.5826274Z 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
2020-05-19T21:06:55.5827334Z 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
2020-05-19T21:06:55.5828369Z 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
2020-05-19T21:06:55.5830735Z 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:493)
2020-05-19T21:06:55.5831962Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1565)
2020-05-19T21:06:55.5833475Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:1552)
2020-05-19T21:06:55.5834742Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoints(CheckpointCoordinator.java:1440)
2020-05-19T21:06:55.5836006Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoints(CheckpointCoordinator.java:1422)
2020-05-19T21:06:55.5837431Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingAndQueuedCheckpoints(CheckpointCoordinator.java:1660)
2020-05-19T21:06:55.5838737Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1410)
2020-05-19T21:06:55.5840060Z 	at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46)
2020-05-19T21:06:55.5841361Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifyJobStatusChange(ExecutionGraph.java:1668)
2020-05-19T21:06:55.5842509Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.transitionState(ExecutionGraph.java:1250)
2020-05-19T21:06:55.5843916Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.transitionState(ExecutionGraph.java:1228)
2020-05-19T21:06:55.5845083Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.transitionExecutionGraphState(SchedulerBase.java:432)
2020-05-19T21:06:55.5846293Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.addVerticesToRestartPending(DefaultScheduler.java:240)
2020-05-19T21:06:55.5847351Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.restartTasksWithDelay(DefaultScheduler.java:227)
2020-05-19T21:06:55.5847998Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeRestartTasks(DefaultScheduler.java:214)
2020-05-19T21:06:55.5848654Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:193)
2020-05-19T21:06:55.5849327Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)
2020-05-19T21:06:55.5850012Z 	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)
2020-05-19T21:06:55.5850701Z 	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)
2020-05-19T21:06:55.5851473Z 	at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)
2020-05-19T21:06:55.5852381Z 	at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1717)
2020-05-19T21:06:55.5853059Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1268)
2020-05-19T21:06:55.5853663Z 	at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1236)
2020-05-19T21:06:55.5854297Z 	at org.apache.flink.runtime.executiongraph.Execution.fail(Execution.java:954)
2020-05-19T21:06:55.5854938Z 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.signalPayloadRelease(SingleLogicalSlot.java:173)
2020-05-19T21:06:55.5855620Z 	at org.apache.flink.runtime.jobmaster.slotpool.SingleLogicalSlot.release(SingleLogicalSlot.java:165)
2020-05-19T21:06:55.5856296Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:732)
2020-05-19T21:06:55.5857025Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)
2020-05-19T21:06:55.5857747Z 	at org.apache.flink.runtime.jobmaster.slotpool.AllocatedSlot.releasePayload(AllocatedSlot.java:149)
2020-05-19T21:06:55.5858408Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseTaskManagerInternal(SlotPoolImpl.java:818)
2020-05-19T21:06:55.5859085Z 	at org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl.releaseTaskManager(SlotPoolImpl.java:777)
2020-05-19T21:06:55.5859806Z 	at org.apache.flink.runtime.jobmaster.JobMaster.disconnectTaskManager(JobMaster.java:435)
2020-05-19T21:06:55.5860469Z 	at org.apache.flink.runtime.jobmaster.JobMaster$TaskManagerHeartbeatListener.notifyHeartbeatTimeout(JobMaster.java:1193)
2020-05-19T21:06:55.5861152Z 	at org.apache.flink.runtime.heartbeat.HeartbeatMonitorImpl.run(HeartbeatMonitorImpl.java:109)
2020-05-19T21:06:55.5861751Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2020-05-19T21:06:55.5862340Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2020-05-19T21:06:55.5862732Z 	... 22 more
2020-05-19T21:06:55.5863134Z Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
2020-05-19T21:06:55.5863754Z 	at org.apache.flink.runtime.checkpoint.PendingCheckpoint.abort(PendingCheckpoint.java:492)
2020-05-19T21:06:55.5864204Z 	... 57 more
2020-05-19T21:06:55.5864528Z Waiting for job (a92a74de8446a80403798bb4806b73f3) to reach terminal state FINISHED ...
2020-05-20T00:30:52.9000401Z ##[error]The operation was canceled.
2020-05-20T00:30:52.9019065Z ##[section]Finishing: Run e2e tests
{code}"	FLINK	Closed	2	1	8742	pull-request-available, test-stability
13373051	Channel state iterator is accessed concurrently without proper synchronization	"ChannelStateWriter adds input/output data that is written by a dedicated thread.

The data is passed as CloseableIterator.

In some cases, iterator.close can be called from the task thread which can lead to double release of buffers."	FLINK	Closed	2	1	8742	pull-request-available
13254681	Integrate ContinuousFileReaderOperator with StreamTask mailbox execution model	"The {{ContinuousFileReaderOperator}} spawns an additional ({{reader}}) thread and coordinates actions with it via {{checkpointLock}}. The operator may block, waiting for a notification from the {{reader}} thread, potentially preventing progress in mailbox loop.

This is similar to {{AsyncWaitOperator}} situation described in FLINK-12958."	FLINK	Closed	3	7	8742	pull-request-available
13322731	Improve error message if checkpoint directory is not writable	"If the checkpoint directory from {{state.checkpoints.dir}} is not writable by the user that Flink is running with, checkpoints will be declined, but the real cause is not mentioned anywhere:

* the Web UI says: ""Cause: The job has failed"" (the Flink job is running though)
* the JM log says:
{code}
2020-08-14 12:13:18,820 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 2 (type=CHECKPOINT) @ 1597399998819 for job 2c567b14e8d0833404931ef47dfec266.
2020-08-14 12:13:18,921 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Decline checkpoint 2 by task 0d4fd75374ad16c8d963679e3c2171ec of job 2c567b14e8d0833404931ef47dfec266 at a184deea621e3923fbfcb1d899348448 @ Nico-PC.lan (dataPort=35531).
{code}
* the TM log says:
{code}
2020-08-14 12:13:14,102 INFO  org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl [] - Checkpoint 1 has been notified as aborted, would not trigger any checkpoint.
{code}

And that's it. It should have a real error message indicating that the checkpoint (sub)-directory could not be created."	FLINK	Closed	3	4	8742	pull-request-available, usability
13338219	Channel state (upstream) can be restored after emission of new elements (watermarks)	"In StreamTask.beforeInvoke:

1. operatorChain.initializeStateAndOpenOperators(createStreamTaskStateInitializer());

2. readRecoveredChannelState();

 But operatorChain.initializeStateAndOpenOperators can emit watermarks (or potentially some other stream elements).

 I've encountered this issue while adding an EndOfRecovery marker - in some runs of in OverWindowITCase.testRowTimeBoundedPartitionedRangeOver the marker was emitted after the watermark.

 

cc: [~zjwang], [~pnowojski]"	FLINK	Resolved	3	1	8742	pull-request-available
13248276	Instable KafkaProducerExactlyOnceITCase due to CheckpointFailureManager	"[~banmoy] and I met this instable test below:

[https://api.travis-ci.org/v3/job/565270958/log.txt]
 [https://api.travis-ci.com/v3/job/221237628/log.txt]

The root cause is task {{Source: Custom Source -> Map -> Sink: Unnamed (1/1)}} failed due to expected artificial test failure and then free task resource including closing the registry. However, the async checkpoint thread in {{SourceStreamTask}} would then failed and send decline checkpoint message to JM.
 The key logs is like:
{code:java}
03:36:46,639 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Custom Source -> Map -> Sink: Unnamed (1/1) (f45ff068d2c80da22c2a958739ec0c87) switched from RUNNING to FAILED.
java.lang.Exception: Artificial Test Failure
	at org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.map(FailingIdentityMapper.java:79)
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:637)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:612)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:592)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:727)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:705)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.run(IntegerSource.java:75)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:172)
03:36:46,637 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Decline checkpoint 12 by task f45ff068d2c80da22c2a958739ec0c87 of job d5b629623731c66f1bac89dec3e87b89 at 03cbfd77-0727-4366-83c4-9aa4923fc817 @ localhost (dataPort=-1).
03:36:46,640 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Discarding checkpoint 12 of job d5b629623731c66f1bac89dec3e87b89.
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 12 for operator Source: Custom Source -> Map -> Sink: Unnamed (1/1). Failure reason: Checkpoint was declined.
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:431)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1248)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1182)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:853)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:758)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:667)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.triggerCheckpoint(SourceStreamTask.java:147)
	at org.apache.flink.runtime.taskmanager.Task$1.run(Task.java:1138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Cannot register Closeable, registry is already closed. Closing argument.
	at org.apache.flink.util.AbstractCloseableRegistry.registerCloseable(AbstractCloseableRegistry.java:85)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:122)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable$AsyncSnapshotTask.<init>(AsyncSnapshotCallable.java:110)
	at org.apache.flink.runtime.state.AsyncSnapshotCallable.toAsyncSnapshotFutureTask(AsyncSnapshotCallable.java:104)
	at org.apache.flink.runtime.state.StateSnapshotContextSynchronousImpl.getKeyedStateStreamFuture(StateSnapshotContextSynchronousImpl.java:127)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:401)
	... 12 more
03:36:46,642 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Exactly once test (d5b629623731c66f1bac89dec3e87b89) switched from state RUNNING to FAILING.
java.lang.Exception: Artificial Test Failure
	at org.apache.flink.streaming.connectors.kafka.testutils.FailingIdentityMapper.map(FailingIdentityMapper.java:79)
	at org.apache.flink.streaming.api.operators.StreamMap.processElement(StreamMap.java:41)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:637)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:612)
	at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:592)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:727)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:705)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$NonTimestampContext.collect(StreamSourceContexts.java:104)
	at org.apache.flink.streaming.connectors.kafka.testutils.IntegerSource.run(IntegerSource.java:75)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:172)
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Try to restart or fail the job Exactly once test (d5b629623731c66f1bac89dec3e87b89) if no longer possible.
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Exactly once test (d5b629623731c66f1bac89dec3e87b89) switched from state FAILING to RESTARTING.
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Restarting the job Exactly once test (d5b629623731c66f1bac89dec3e87b89).
03:36:46,643 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Discarding the results produced by task execution f45ff068d2c80da22c2a958739ec0c87.
03:36:46,644 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job Exactly once test (d5b629623731c66f1bac89dec3e87b89) switched from state RESTARTING to FAILING.
org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.lambda$null$1(ExecutionGraph.java:586)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:397)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:190)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}
The failure of {{Source: Custom Source -> Map -> Sink: Unnamed}} would fail the job for the 1st time. However, due to receive declined checkpoint {{CheckpointFailureManager}} would also fail the job again for the 2nd time. Unfortunately, some tests within {{KafkaProducerExactlyOnceITCase}} only allow one restart attempt by {{FixedDelayRestartStrategy}}, that's why the IT case failed at last."	FLINK	Closed	2	1	8742	test-stability
13306524	Fix the race condition of aborting unaligned checkpoint	"On ChannelStateWriter side, the lifecycle of checkpoint should be as follows:

start -> in progress/abort -> stop.

The ChannelStateWriteResult is created during #start, and removed by #abort or #stop processes. There are some potential race conditions here:
 * #start is called while receiving the first barrier by netty thread and schedule to execute the checkpoint
 * The task thread might process cancel checkpoint and call #abort before performing the above respective checkpoint
 * The checkpoint can still be executed by task thread afterwards even thought the above abort happened before, because we can not remove the checkpoint action from mailbox during aborting.
 * While checkpoint executing, it will call `ChannelStateWriter#getWriteResult` then it would cause `IllegalStateException` because the respective result was already removed in advance during handling #abort method before.
 * Therefore it will cause unnecessary task failure during performing checkpoint

I guess we do not want to fail the task when one checkpoint is aborted by design. And the illegal state check during ChannelStateWriter#getWriteResult was mainly proposed for normal process validation I guess.

If we do not remove the ChannelStateWriteResult while handling #abort and rely on #stop to remove it, then it might probably exist another scenario that the checkpoint will never be performed after #start (we have another mechanism to exit the triggering checkpoint in advance if the abort is sent by CheckpointCoordinator), then the legacy ChannelStateWriteResult will be retained inside ChannelStateWriter long time.

Maybe the potential option to fix this issue is to let SubtaskCheckpointCoordinatorImpl handle the exception from ChannelStateWriter#getWriteResult properly to not fail the task in the aborted case."	FLINK	Resolved	1	1	8742	pull-request-available
13365984	Running HA per-job cluster (rocks, non-incremental) end-to-end test could not finished in 900 seconds	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14921&view=logs&j=91bf6583-3fb2-592f-e4d4-d79d79c3230a&t=03dbd840-5430-533d-d1a7-05d0ebe03873&l=7318

{code:java}
Waiting for text Completed checkpoint [1-9]* for job 00000000000000000000000000000000 to appear 2 of times in logs...
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Starting standalonejob daemon on host fv-az232-135.
grep: /home/vsts/work/1/s/flink-dist/target/flink-1.11-SNAPSHOT-bin/flink-1.11-SNAPSHOT/log/*standalonejob-2*.log: No such file or directory
Killed TM @ 15744
Killed TM @ 19625
Test (pid: 9232) did not finish after 900 seconds.

{code}
"	FLINK	Closed	4	11500	10066	auto-deprioritized-major, pull-request-available, test-stability
13138860	Allow RpcEndpoint#postStop to complete asynchronously	Every {{RpcEndpoint}} should have the possibility to trigger asynchronous clean up operations in its {{RpcEndpoint#postStop}} method. In order to do that the {{postStop}} method should return a {{CompletableFuture<Void>}} which is completed once all post stop actions have finished. The {{RpcEndpoint#terminationFuture}} will only be completed afterwards.	FLINK	Closed	3	7	10066	flip-6
13396341	ZooKeeperLeaderRetrievalConnectionHandlingTest.testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=22534&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=6647

{code}
Aug 20 07:54:25 [ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 14.652 s <<< FAILURE! - in org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest
Aug 20 07:54:25 [ERROR] testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection  Time elapsed: 2.244 s  <<< FAILURE!
Aug 20 07:54:25 java.lang.AssertionError: 
Aug 20 07:54:25 
Aug 20 07:54:25 Expected: is null
Aug 20 07:54:25      but: was <java.util.concurrent.CompletableFuture@2d6764b2[Completed normally]>
Aug 20 07:54:25 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Aug 20 07:54:25 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
Aug 20 07:54:25 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderRetrievalConnectionHandlingTest.testSuspendedConnectionDoesNotClearLeaderInformationIfClearanceOnLostConnection(ZooKeeperLeaderRetrievalConnectionHandlingTest.java:211)
Aug 20 07:54:25 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Aug 20 07:54:25 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Aug 20 07:54:25 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Aug 20 07:54:25 	at java.lang.reflect.Method.invoke(Method.java:498)
Aug 20 07:54:25 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Aug 20 07:54:25 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Aug 20 07:54:25 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Aug 20 07:54:25 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.evaluate(TestingFatalErrorHandlerResource.java:91)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$CloseableStatement.access$200(TestingFatalErrorHandlerResource.java:83)
Aug 20 07:54:25 	at org.apache.flink.runtime.util.TestingFatalErrorHandlerResource$1.evaluate(TestingFatalErrorHandlerResource.java:55)
Aug 20 07:54:25 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
{code}"	FLINK	Closed	3	1	10066	pull-request-available, test-stability
13097363	Fence Dispatcher	With the completion of FLINK-7078 and FLINK-7457, the {{Dispatcher}} should extend {{FencedRpcEndpoint}} in order to support automatic fencing of RPC messages.	FLINK	Closed	3	7	10066	flip-6
13148183	Jobs can be dropped in HA when job submission fails	Jobs can be dropped in HA mode if the job submission step fails. In such a case, we should fail fatally to let the {{Dispatcher}} restart and retry to recover all jobs.	FLINK	Closed	1	1	10066	flip-6
13097792	Port DashboardConfigHandler to new REST endpoint	In order to use the {{DashboardConfigHandler}} for the new {{RestServerEndpoint}} we have to let it implement the {{LegacyRestHandler}} interface. Moreover, we have to define the appropriate {{MessageHeaders}}.	FLINK	Closed	4	7	10066	flip-6
13186824	Remove legacy mode switch from parent pom	Remove the legacy mode switch from the parent {{pom.xml}}.	FLINK	Closed	3	7	10066	pull-request-available
13140173	Support rescaling of jobs which are not fully running	We should support the rescaling of jobs which are only partially running. Currently, this fails because rescaling requires to take a savepoint. We can solve the problem by falling back to the latest rescaling savepoint.	FLINK	Closed	3	4	10066	flip-6
13137660	Expose JobMaster#rescaleJob via the Dispatcher	In order to call the {{JobMaster#rescaleJob}} via Rest handlers, it has to be exposed via the {{Dispatcher}}.	FLINK	Closed	4	4	10066	flip-6
13275505	Jepsen tests are broken due to copying un-relocated flink-s3-fs-hadoop* into lib	The Jepsen tests are currently broken because we copy {{flink-s3-fs-hadoop*}} into {{lib}}. With FLINK-11956 we removed the relocations from these classes and hence they need to reside in {{plugins}}.	FLINK	Closed	1	1	10066	pull-request-available, test-stability
13195685	Update Jepsen tests to run with activated local recovery	With FLINK-9635 Flink now supports to run properly with local recovery activated. We should update the Jepsen tests to run with this feature in order to give it more test exposure.	FLINK	Closed	3	4	10066	pull-request-available
13200570	Define flink-sql-client uber-jar dependencies via artifactSet	"The module {{flink-sql-client}} defines the content of its uber jar via filtering files from the set of all dependencies. I think this is not ideal because it misses for example the {{NOTICE}} files from down stream dependencies. 

A solution could be to define an {{<artifactSet><includes><include></include></includes></artifactSet>}} and exclude files via the filter."	FLINK	Closed	3	4	10066	pull-request-available
13423563	Upgrade Flink dependency to 1.14.3	For the next Statefun 3.2.0 release we should bump our Flink dependency to {{1.14.3}}.	FLINK	Closed	3	4	10066	pull-request-available
12782794	Add support to read libSVM and SVMLight input files	"In order to train SVMs, the machine learning library should be able to read standard SVM input file formats. A widespread format is used by libSVM and SMVLight which has the following format:

<line> .=. <target> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>
<target> .=. +1 | -1 | 0 | <float> 
<feature> .=. <integer> | ""qid""
<value> .=. <float>
<info> .=. <string>

Details can be found [here|http://svmlight.joachims.org/] and [here|http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#/Q03:_Data_preparation]"	FLINK	Closed	3	2	10066	ML
13127829	Split command options from deployment options in CliFrontend	In order to better support different {{CustomCommandLines}} we should split the command and deployment option parsing in the {{CliFrontend}}.	FLINK	Closed	3	7	10066	flip-6
13139005	Make MetricRegistryImpl#shutdown non blocking 	In order to better shut down multiple components concurrently, we should make all shutdown operation non-blocking if possible. This also includes the {{MetricRegistryImpl}}.	FLINK	Closed	3	4	10066	flip-6
13110026	Make MetricFetcher work with RestfulGateway	In order to make the {{MetricFetcher}} work together with the new architecture, we have to remove it's dependence on the {{JobManagerGateway}}.	FLINK	Closed	3	7	10066	flip-6
13198336	Submitting a jobs without enough slots times out due to a unspecified timeout	"When submitting a job without enough slots being available the job will stay in a SCHEDULED/CREATED state. After some time (a few minutes) the job execution will fail with the following timeout exception:
{code}
2018-11-14 13:38:26,614 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPool          - Pending slot request [SlotRequestId{d9c0c94b6b81eae406f3d6cb6150fee4}] timed out.
2018-11-14 13:38:26,615 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN DataSource (at getDefaultTextLineDataSet(WordCountData.java:70) (org.apache.flink.api.java.io.CollectionInputFormat)) -> FlatMap (FlatMap at main(WordCount.java:76)) -> Combine (SUM(1), at main(WordCount.java:79) (1/$java.util.concurrent.TimeoutException
        at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:795)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}

That the job submission may time out is not documented, neither is which timeout is responsible in the first place nor how/whether this can be disabled."	FLINK	Resolved	3	4	10066	pull-request-available
12781666	Add polynomial base feature mapper to ML library	Add feature mapper which maps a vector into the polynomial feature space. This can be used as a preprocessing step prior to applying a {{Learner}} of Flink's ML library.	FLINK	Closed	3	4	10066	ML
13339642	UnalignedCheckpointCompatibilityITCase.test failed with AskTimeoutException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=9362&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=45cc9205-bdb7-5b54-63cd-89fdc0983323

{code}
2020-11-09T22:19:47.2714024Z [ERROR] test[type: SAVEPOINT, startAligned: true](org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase)  Time elapsed: 1.293 s  <<< ERROR!
2020-11-09T22:19:47.2715260Z java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.stopWithSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2020-11-09T22:19:47.2716743Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2020-11-09T22:19:47.2718213Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
2020-11-09T22:19:47.2719166Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.runAndTakeSavepoint(UnalignedCheckpointCompatibilityITCase.java:113)
2020-11-09T22:19:47.2720278Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.test(UnalignedCheckpointCompatibilityITCase.java:97)
2020-11-09T22:19:47.2721126Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2020-11-09T22:19:47.2721771Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2020-11-09T22:19:47.2722773Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2020-11-09T22:19:47.2723479Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2020-11-09T22:19:47.2724187Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2020-11-09T22:19:47.2725026Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2020-11-09T22:19:47.2725817Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2020-11-09T22:19:47.2726595Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2020-11-09T22:19:47.2727515Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2020-11-09T22:19:47.2728192Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2020-11-09T22:19:47.2744089Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2020-11-09T22:19:47.2744907Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2020-11-09T22:19:47.2745573Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2020-11-09T22:19:47.2746037Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-09T22:19:47.2746445Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-09T22:19:47.2746868Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-09T22:19:47.2747443Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-09T22:19:47.2747876Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-09T22:19:47.2748297Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-09T22:19:47.2748694Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2020-11-09T22:19:47.2749054Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2020-11-09T22:19:47.2749414Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2020-11-09T22:19:47.2749819Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2020-11-09T22:19:47.2750373Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2020-11-09T22:19:47.2750923Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2020-11-09T22:19:47.2751555Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2020-11-09T22:19:47.2752148Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2020-11-09T22:19:47.2752938Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2020-11-09T22:19:47.3085383Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2020-11-09T22:19:47.3086377Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2020-11-09T22:19:47.3087146Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2020-11-09T22:19:47.3088051Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2020-11-09T22:19:47.3088815Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2020-11-09T22:19:47.3089472Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2020-11-09T22:19:47.3090109Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2020-11-09T22:19:47.3091501Z Caused by: java.util.concurrent.TimeoutException: Invocation of public default java.util.concurrent.CompletableFuture org.apache.flink.runtime.webmonitor.RestfulGateway.stopWithSavepoint(org.apache.flink.api.common.JobID,java.lang.String,boolean,org.apache.flink.api.common.time.Time) timed out.
2020-11-09T22:19:47.3092730Z 	at com.sun.proxy.$Proxy33.stopWithSavepoint(Unknown Source)
2020-11-09T22:19:47.3093348Z 	at org.apache.flink.runtime.minicluster.MiniCluster.lambda$stopWithSavepoint$9(MiniCluster.java:599)
2020-11-09T22:19:47.3094052Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2020-11-09T22:19:47.3094682Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2020-11-09T22:19:47.3095359Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2020-11-09T22:19:47.3096070Z 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:621)
2020-11-09T22:19:47.3096926Z 	at org.apache.flink.runtime.minicluster.MiniCluster.stopWithSavepoint(MiniCluster.java:599)
2020-11-09T22:19:47.3097996Z 	at org.apache.flink.client.program.PerJobMiniClusterFactory$PerJobMiniClusterJobClient.stopWithSavepoint(PerJobMiniClusterFactory.java:169)
2020-11-09T22:19:47.3098985Z 	at org.apache.flink.test.checkpointing.UnalignedCheckpointCompatibilityITCase.runAndTakeSavepoint(UnalignedCheckpointCompatibilityITCase.java:112)
2020-11-09T22:19:47.3099640Z 	... 36 more
2020-11-09T22:19:47.3101706Z Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/rpc/dispatcher_18#-978053777]] after [10000 ms]. Message of type [org.apache.flink.runtime.rpc.messages.LocalFencedMessage]. A typical reason for `AskTimeoutException` is that the recipient actor didn't send a reply.
2020-11-09T22:19:47.3103053Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-11-09T22:19:47.3103640Z 	at akka.pattern.PromiseActorRef$$anonfun$2.apply(AskSupport.scala:635)
2020-11-09T22:19:47.3104114Z 	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:648)
2020-11-09T22:19:47.3104521Z 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205)
2020-11-09T22:19:47.3104994Z 	at akka.actor.LightArrayRevolverScheduler$TaskHolder.run(LightArrayRevolverScheduler.scala:337)
2020-11-09T22:19:47.3105546Z 	at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(LightArrayRevolverScheduler.scala:141)
2020-11-09T22:19:47.3106140Z 	at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(LightArrayRevolverScheduler.scala:140)
2020-11-09T22:19:47.3106646Z 	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
2020-11-09T22:19:47.3107072Z 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
2020-11-09T22:19:47.3107770Z 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2020-11-09T22:19:47.3108203Z 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2020-11-09T22:19:47.3108659Z 	at akka.actor.LightArrayRevolverScheduler.close(LightArrayRevolverScheduler.scala:139)
2020-11-09T22:19:47.3109107Z 	at akka.actor.ActorSystemImpl.stopScheduler(ActorSystem.scala:937)
2020-11-09T22:19:47.3109575Z 	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply$mcV$sp(ActorSystem.scala:872)
2020-11-09T22:19:47.3110253Z 	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply(ActorSystem.scala:872)
2020-11-09T22:19:47.3110724Z 	at akka.actor.ActorSystemImpl$$anonfun$liftedTree2$1$1.apply(ActorSystem.scala:872)
2020-11-09T22:19:47.3111176Z 	at akka.actor.ActorSystemImpl$$anon$3.run(ActorSystem.scala:892)
2020-11-09T22:19:47.3111666Z 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1068)
2020-11-09T22:19:47.3112220Z 	at akka.actor.ActorSystemImpl$TerminationCallbacks$$anonfun$addRec$1$1.applyOrElse(ActorSystem.scala:1068)
2020-11-09T22:19:47.3112970Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:436)
2020-11-09T22:19:47.3113403Z 	at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:435)
2020-11-09T22:19:47.3113915Z 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
2020-11-09T22:19:47.3114373Z 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)
2020-11-09T22:19:47.3114909Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91)
2020-11-09T22:19:47.3115439Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-09T22:19:47.3115961Z 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91)
2020-11-09T22:19:47.3116507Z 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
2020-11-09T22:19:47.3116953Z 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90)
2020-11-09T22:19:47.3117501Z 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
2020-11-09T22:19:47.3117989Z 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)
2020-11-09T22:19:47.3118536Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2020-11-09T22:19:47.3118985Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2020-11-09T22:19:47.3119440Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2020-11-09T22:19:47.3119872Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}"	FLINK	Closed	1	1	10066	test-stability
13179389	Add support for resuming from savepoints to StandaloneJobClusterEntrypoint	The {{StandaloneJobClusterEntrypoint}} should support to resume from a savepoint/checkpoint. I suggest to introduce an optional command line parameter for specifying the savepoint/checkpoint path.	FLINK	Closed	3	4	10066	pull-request-available
13307737	Test ZooKeeper 3.5 support	Setup a Flink cluster with ZooKeeper 3.5 and run some HA tests on it (killing processes, failing jobs).	FLINK	Closed	2	7	10066	release-testing
13359043	Rename DeclarativeScheduler to AdaptiveScheduler	DeclarativeScheduler does not seem to be an appropriate name for what it does: In particular looking at the difference to the DefaultScheduler, calling it AdaptiveScheduler (as in, adapts the parallelism of the ExecutionGraph during the job lifetime) seems more appropriate.	FLINK	Closed	2	7	10066	pull-request-available
12903083	Failing test: RandomSamplerTest.testReservoirSamplerWithReplacement	"Tests run: 17, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 19.133 sec <<< FAILURE! - in org.apache.flink.api.java.sampling.RandomSamplerTest
testReservoirSamplerWithReplacement(org.apache.flink.api.java.sampling.RandomSamplerTest)  Time elapsed: 2.534 sec  <<< FAILURE!
java.lang.AssertionError: KS test result with p value(0.110000), d value(0.103090)
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.flink.api.java.sampling.RandomSamplerTest.verifyKSTest(RandomSamplerTest.java:342)
	at org.apache.flink.api.java.sampling.RandomSamplerTest.verifyRandomSamplerWithSampleSize(RandomSamplerTest.java:330)
	at org.apache.flink.api.java.sampling.RandomSamplerTest.verifyReservoirSamplerWithReplacement(RandomSamplerTest.java:289)
	at org.apache.flink.api.java.sampling.RandomSamplerTest.testReservoirSamplerWithReplacement(RandomSamplerTest.java:192)

Results :

Failed tests: 
  RandomSamplerTest.testReservoirSamplerWithReplacement:192->verifyReservoirSamplerWithReplacement:289->verifyRandomSamplerWithSampleSize:330->verifyKSTest:342 KS test result with p value(0.110000), d value(0.103090)

Full log [here|https://travis-ci.org/apache/flink/jobs/84120131]."	FLINK	Closed	2	1	10066	test-stability
13251897	Bad Error Message when TaskManager is lost	"When a TaskManager is lost, the job reports as the failure cause
{code}
org.apache.flink.util.FlinkException: The assigned slot 6d0e469d55a2630871f43ad0f89c786c_0 was removed.
{code}

That is a pretty bad error message, as a user I don't know what that means. Sounds like it could simply refer to internal book keeping, maybe some rebalancing or so.
You need to know a lot about Flink to understand that this means actually ""TaskManager failure"".
"	FLINK	Resolved	1	1	10066	pull-request-available
13304432	LaunchCoordinatorTest fails	"Here is the [instance|https://dev.azure.com/arvidheise0209/arvidheise/_build/results?buildId=234&view=logs&j=764762df-f65b-572b-3d5c-65518c777be4&t=8d823410-c7c7-5a4d-68bb-fa7b08da17b9].

 
{noformat}
[ERROR] Tests run: 24, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 1.828 s <<< FAILURE! - in org.apache.flink.mesos.scheduler.LaunchCoordinatorTest
[ERROR] The LaunchCoordinator when in state GatheringOffers should handle StateTimeout which stays in GatheringOffers when task queue is non-empty(org.apache.flink.mesos.scheduler.LaunchCoordinatorTest)  Time elapsed: 0.021 s  <<< ERROR!
java.lang.IllegalStateException: cannot reserve actor name '$$u': terminating
	at akka.actor.dungeon.ChildrenContainer$TerminatingChildrenContainer.reserve(ChildrenContainer.scala:188)
	at akka.actor.dungeon.Children$class.reserveChild(Children.scala:135)
	at akka.actor.ActorCell.reserveChild(ActorCell.scala:429)
	at akka.testkit.TestActorRef.<init>(TestActorRef.scala:33)
	at akka.testkit.TestFSMRef.<init>(TestFSMRef.scala:40)
	at akka.testkit.TestFSMRef$.apply(TestFSMRef.scala:91)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$Context.<init>(LaunchCoordinatorTest.scala:254)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34$$anon$25.<init>(LaunchCoordinatorTest.scala:459)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34.apply(LaunchCoordinatorTest.scala:459)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest$$anonfun$1$$anonfun$apply$mcV$sp$8$$anonfun$apply$mcV$sp$15$$anonfun$apply$mcV$sp$34.apply(LaunchCoordinatorTest.scala:459)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078)
	at org.scalatest.TestSuite$class.withFixture(TestSuite.scala:196)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.withFixture(LaunchCoordinatorTest.scala:57)
	at org.scalatest.WordSpecLike$class.invokeWithFixture$1(WordSpecLike.scala:1075)
	at org.scalatest.WordSpecLike$$anonfun$runTest$1.apply(WordSpecLike.scala:1088)
	at org.scalatest.WordSpecLike$$anonfun$runTest$1.apply(WordSpecLike.scala:1088)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)
	at org.scalatest.WordSpecLike$class.runTest(WordSpecLike.scala:1088)
	at org.apache.flink.mesos.scheduler.LaunchCoordinatorTest.runTest(LaunchCoordinatorTest.scala:57)
	at org.scalatest.WordSpecLike$$anonfun$runTests$1.apply(WordSpecLike.scala:1147)
	at org.scalatest.WordSpecLike$$anonfun$runTests$1.apply(WordSpecLike.scala:1147)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418){noformat}"	FLINK	Resolved	3	1	10066	pull-request-available, test-stability
13058842	Create SerialMainThreadValidatorUtil to support TestingSerialRpcService	The {{MainThreadValidatorUtil}} does not play well together with the {{TestingSerialRpcService}} which executes all rpc call in the same thread. In order to still support main thread validation in the {{RpcEndpoint}} methods, I porpose to implement a {{SerialMainThreadValidatorUtil}} which not only sets the main thread if the current thread is {{null}}, but also if current thread is the entering thread. In order to make this properly work with releasing the main thread field, we have to count how often we have entered the main thread and only set the field to {{null}} iff the count equals 0.	FLINK	Closed	4	7	10066	flip-6
13388414	AkkaRpcActorTest#testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=20163&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=5360d54c-8d94-5d85-304e-a89267eb785a&l=6023

{code}
Jul 08 11:03:13 java.lang.AssertionError: 
Jul 08 11:03:13 
Jul 08 11:03:13 Expected: is <false>
Jul 08 11:03:13      but: was <true>
Jul 08 11:03:13 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:964)
Jul 08 11:03:13 	at org.junit.Assert.assertThat(Assert.java:930)
Jul 08 11:03:13 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor(AkkaRpcActorTest.java:375)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Jul 08 11:03:13 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Jul 08 11:03:13 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Jul 08 11:03:13 	at java.lang.reflect.Method.invoke(Method.java:498)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Jul 08 11:03:13 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Jul 08 11:03:13 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
Jul 08 11:03:13 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Jul 08 11:03:13 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
Jul 08 11:03:13 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
Jul 08 11:03:13 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
Jul 08 11:03:13 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
Jul 08 11:03:13 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Jul 08 11:03:13 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Jul 08 11:03:13 
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.107 s - in org.apache.flink.runtime.rpc.akka.TimeoutCallStackTest
Jul 08 11:03:13 [INFO] Running org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:13 [INFO] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.823 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcServiceTest
Jul 08 11:03:14 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 s - in org.apache.flink.runtime.rpc.akka.AkkaRpcActorOversizedResponseMessageTest
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [INFO] Results:
Jul 08 11:03:14 [INFO] 
Jul 08 11:03:14 [ERROR] Failures: 
Jul 08 11:03:14 [ERROR]   AkkaRpcActorTest.testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor:375 
Jul 08 11:03:14 Expected: is <false>
Jul 08 11:03:14      but: was <true>

{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13138496	Add CLI command for rescaling	The REST rescaling calls should be made accessible via the {{CliFrontend}}. In order to do that I propose to add a {{modify}} command to the {{CliFrontend}} to which we can pass a new parallelism.	FLINK	Closed	3	2	10066	flip-6
13127988	Make Cluster id typesafe	Currently, the cluster id is of type {{String}}. We should make the id typesafe to avoid mixups between different {{CustomCommandLines}} and {{ClusterDescriptors}}.	FLINK	Closed	3	7	10066	flip-6
13359408	Consolidate Scheduler/SlotPool factories	"We have run into the unfortunate situation where different schedulers effectively work against different slot pool implementations and interfaces.
This makes it quite difficult to understand which slot pool factories can work with which scheduler.

We may be able to alleviate these issues by wrapping the factories for both into a single factory."	FLINK	Closed	3	7	10066	pull-request-available
13075846	StateDescriptor cannot be shared by multiple subtasks	"The {{StateDescriptor}} contains the {{TypeSerializer}} which is used to serialize the state. The serializer instance won't be duplicated when it is accessed. Therefore, the {{StateDescriptor}} cannot be shared if the {{TypeSerializer}} is stateful as in the case of the {{KryoSerializer}}.

This problem can easily arise when a user defines a stateful operator which defines the {{StateDescriptor}} statically. The work around is to not define a static {{StateDescriptor}}. However, I would still make it a blocker, because it is extremely hard to debug for the user if things fail because the {{TypeSerializer}} is used concurrently.

The following operator produces the problem:

{code}
private static final class StatefulMapper extends RichMapFunction<Tuple2<Long,Long>, Tuple2<Long, Long>> implements CheckpointedFunction {
        private static final long serialVersionUID = -1175717056869107847L;
        private static final ValueStateDescriptor<PojoType> POJO_VALUE_STATE = new ValueStateDescriptor<PojoType>(""pojoType"", PojoType.class);

        private transient ValueState<PojoType> valueState;

        public StatefulMapper() {
            valueState = null;
        }

        @Override
        public Tuple2<Long, Long> map(Tuple2<Long, Long> tuple) throws Exception {
            PojoType pojoType = new PojoType(1, 1.0, ""1.0"", new NestedPojo(2, 2.0));
            valueState.update(pojoType);
            return tuple;
        }

        @Override
        public void snapshotState(FunctionSnapshotContext functionSnapshotContext) throws Exception {}

        @Override
        public void initializeState(FunctionInitializationContext functionInitializationContext) throws Exception {
            valueState = functionInitializationContext.getKeyedStateStore().getState(POJO_VALUE_STATE);
        }
    }
{code}"	FLINK	Closed	1	1	10066	flink-rel-1.3.1-blockers
13243852	Support batch slot requests	In order to support the execution of batch jobs with fewer slots than requested we need to introduce a special slot request notion which does not register an eager timeout. Moreover, this slot request should not react to failure signals from the {{ResourceManager}} and only time out if there is not available or allocated slot which can fulfill the requested {{ResourceProfile}}.	FLINK	Closed	3	7	10066	pull-request-available
13187294	Shaded Hadoop S3A end-to-end test failed on Travis	https://api.travis-ci.org/v3/job/432916761/log.txt	FLINK	Resolved	2	1	10066	pull-request-available, test-stability
13190053	Replace TaskManagerActions#notifyFinalState with #updateTaskExecutionState	We can simplify the {{TaskManagerActions}} interface by replacing calls to the {{notifyFinalState}} method by {{updateTaskExecutionState}}.	FLINK	Closed	4	4	10066	pull-request-available
13167649	DispatcherTest.testSubmittedJobGraphListener fails on Travis	"The {{DispatcherTest.testSubmittedJobGraphListener}} fails on Travis

https://api.travis-ci.org/v3/job/395218112/log.txt"	FLINK	Closed	2	1	10066	test-stability
13180081	Make ZooKeeperStateHandleStore#releaseAndTryRemove synchronous	"The {{ZooKeeperStateHandleStore#releaseAndTryRemove}} method executes parts of its logic synchronously (retrieving the state handle and unlocking the ZNode) and others asynchronously (removing the ZNode). Moreover, the method takes a parameter which is used to execute some logic in case of a successful removal. This was done in order to execute a potentially blocking state discard operation in a different thread.

I think this can be simplified by executing all logic in the same thread and running the callback after having called {{ZooKeeperStateHandleStore#releaseAndTryRemove}}. Moreover, if this operation needs to be not blocking one could use a different thread to call into this method. "	FLINK	Resolved	3	4	10066	pull-request-available
13209931	Port JobManagerHARecoveryTest to new code base	Check and port {{JobManagerHARecoveryTest}} if necessary to new code base.	FLINK	Closed	3	7	10066	pull-request-available
13143615	YARN application name for Flink (per-job) submissions claims it is using only 1 TaskManager	"If (with FLIP-6) a per-job YARN session is created without specifying the number of nodes, it will show up as ""Flink session with 1 TaskManagers"", e.g. this job:

{code}
./bin/flink run -m yarn-cluster -yjm 768 -ytm 3072 -ys 2 -p 20 -c org.apache.flink.streaming.examples.wordcount.WordCount ./examples/streaming/WordCount.jar --input /usr/share/doc/rsync-3.0.6/COPYING
{code}"	FLINK	Resolved	1	1	10066	flip-6, pull-request-available
13418853	Store blobs in <WORKING_DIR>/blobs	With FLINK-25402, we now have a working directory at our disposal. I suggest to store the blobs in this directory to make the recoverable in case of a process restart with the same working directory.	FLINK	Closed	3	7	10066	pull-request-available
13356541	Remove SlotPool.suspend	Since the completion of FLINK-11719 we no longer need to suspend the {{SlotPool}}. Hence, I suggest to remove this method.	FLINK	Closed	4	1	10066	pull-request-available
13138858	Change termination future type of RpcEndpoint to Void	In order to align the termination futures of {{RpcService}} and {{RpcEndpoint}} we should change the future value type of the {{RcpEnpoint#getTerminationFuture()}} to {{Void}}.	FLINK	Closed	4	7	10066	flip-6
13084364	Deploy Yarn cluster with job	In order to start a yarn per-job cluster, we have to start a Yarn application running Flink which includes the job to be executed. One way to do it is to upload the serialized form of the {{JobGraph}} as a file. The Yarn entry point can then read the {{JobGraph}} from this file and start the {{JobManager}}.	FLINK	Closed	3	7	10066	flip-6
13334673	Issue retrieving leader after zookeeper session reconnect	"We have noticed an issue with leaders being retrieved after reconnecting to zookeeper. The steps to reproduce this issue are to break the connection between a job manager that is not the leader and zookeeper. Wait for the session to be lost between the two. At this point, flink notifies for a loss of leader. After the loss of leader has occured, reconnect the job manager to zookeeper. At this point, the leader will still be the same as it was before, but when trying to access the rest API, you will see this
{code}
$ curl -s localhost:8999/jobs
{""errors"":[""Service temporarily unavailable due to an ongoing leader election. Please refresh.""]}
{code}
I have been using `stress -t 60 -m 2048` (which spins up 2048 threads continuously alloc and freeing 256MB, to swap out the job manager and cause the connection loss.

I have done some amount of digging on this. The ZooKeeperLeaderRetrievalService has this code block for handling state changes
{code}
	protected void handleStateChange(ConnectionState newState) {
		switch (newState) {
			case CONNECTED:
				LOG.debug(""Connected to ZooKeeper quorum. Leader retrieval can start."");
				break;
			case SUSPENDED:
				LOG.warn(""Connection to ZooKeeper suspended. Can no longer retrieve the leader from "" +
						""ZooKeeper."");
				synchronized (lock) {
					notifyLeaderLoss();
				}
				break;
			case RECONNECTED:
				LOG.info(""Connection to ZooKeeper was reconnected. Leader retrieval can be restarted."");
				break;
			case LOST:
				LOG.warn(""Connection to ZooKeeper lost. Can no longer retrieve the leader from "" +
						""ZooKeeper."");
				synchronized (lock) {
					notifyLeaderLoss();
				}
				break;
		}
	}
{code}
It calls notifyLeaderLoss() when the connection is lost, but it doesn't do anything when the connection is reconnected. It appears that curator's NodeCache will retrieve the value of the leader znode after reconnect, but it won't notify the listeners if the value is the same as before the connection loss. So, unless a leader election happens after a zookeeper connection loss, the job managers that are not the leader will never know that there is a leader.

The method that is called for NodeCache when a new value is retrieved
{code}
    private void setNewData(ChildData newData) throws InterruptedException
    {
        ChildData   previousData = data.getAndSet(newData);
        if ( !Objects.equal(previousData, newData) )
        {
            listeners.forEach(listener -> {
                try
                {
                    listener.nodeChanged();
                }
                catch ( Exception e )
                {
                    ThreadUtils.checkInterrupted(e);
                    log.error(""Calling listener"", e);
                }
            });

            if ( rebuildTestExchanger != null )
            {
                try
                {
                    rebuildTestExchanger.exchange(new Object());
                }
                catch ( InterruptedException e )
                {
                    Thread.currentThread().interrupt();
                }
            }
        }
    }
{code}
note the
{code}
        if ( !Objects.equal(previousData, newData) )
{code}
seems to be preventing the job managers from getting the leader after a zookeeper connection loss."	FLINK	Closed	2	1	10066	pull-request-available
13142963	ClusterClient.getJobStatus can throw FencingTokenException	"*Description*
Calling {{RestClusterClient.getJobStatus}} or {{MiniClusterClient.getJobStatus}} can result in a {{FencingTokenException}}. 

*Analysis*
{{Dispatcher.requestJobStatus}} first looks the {{JobManagerRunner}} up by job id. If a reference is found, {{requestJobStatus}} is called on the respective instance. If not, the {{ArchivedExecutionGraphStore}} is queried. However, between the lookup and the method call, the {{JobMaster}} of the respective job may have lost leadership already (job finished), and has set the fencing token to {{null}}.

*Stacktrace*

{noformat}
Caused by: org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token mismatch: Ignoring message LocalFencedMessage(null, LocalRpcInvocation(requestJobStatus(Time))) because the fencing token null did not match the expected fencing token b8423c75bc6838244b8c93c8bd4a4f51.
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleMessage(FencedAkkaRpcActor.java:73)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$onReceive$1(AkkaRpcActor.java:132)
	at akka.actor.ActorCell$$anonfun$become$1.applyOrElse(ActorCell.scala:544)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}

{noformat}
Caused by: org.apache.flink.runtime.rpc.exceptions.FencingTokenException: Fencing token not set: Ignoring message LocalFencedMessage(null, LocalRpcInvocation(requestJobStatus(Time))) because the fencing token is null.
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleMessage(FencedAkkaRpcActor.java:56)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$onReceive$1(AkkaRpcActor.java:132)
	at akka.actor.ActorCell$$anonfun$become$1.applyOrElse(ActorCell.scala:544)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}"	FLINK	Resolved	1	1	10066	flip-6, pull-request-available
13186054	Enable YARNITCase	The {{YARNITCase}} is ignored because when it was added it was not possible to terminate the Flink cluster. This has changed now and consequently, we should enable this test.	FLINK	Closed	4	4	10066	pull-request-available
13127694	Move YarnClient out of YarnClusterClient	Move the {{YarnClient}} from the {{YarnClusterClient}} to the {{AbstractYarnClusterDescriptor}} which will be responsible for the lifecycle management of the {{YarnClient}}. This change is a clean up task which will better structure the client code.	FLINK	Closed	3	7	10066	flip-6
13418855	Let BlobServer/BlobCache detect & delete corrupted blobs	In order to make the BlobServer and BlobCache work together with the local working directories, they have be able to detect corrupted blobs (e.g. due to process crashes when downloading blobs) and delete them. Therefore, I propose to let the BlobServer and BlobCache verify by recomputing their checksums which blobs are valid and to delete those that did not pass.	FLINK	Closed	3	7	10066	pull-request-available
13093089	Let RpcEndpoint directly implement RpcGateways	"Right now, a {{RpcGateway}} is only implicitly coupled to a {{RpcEndpoint}} by specifying it a s type argument and having the {{RpcCompletenessTest}} watching that the endpoint implements all methods of the gateway.

This makes it possible that the endpoint and the gateway can have slightly different interfaces. E.g. the gateway defines a method which returns a {{Future<Integer>}} whereas this method is implemented by the endpoint as a method which returns {{Integer}}. This underlines the fact that the endpoint method call is synchronous whereas the gateway issues an asynchronous call.

The downside is that it is very cumbersome to establish type hierarchies among {{RpcEndpoints}}. Thus, it is not easily possible to extend a specific {{RcpEndpoint}} to add testing RPCs, for example. Moreover, you rely on the {{RpcCompletenessTest}} to catch all not implemented methods instead of relying on the compiler.

I think in the long run, it would be better to let the {{RpcEndpoints}} directly implement the {{RpcGateways}} because the benefits of being able to extend {{RpcEndpoints}} outweigh the benefits of the current approach."	FLINK	Closed	3	7	10066	flip-6
13091934	ignored (manual) tests in YARNSessionFIFOITCase do not run anymore	"{{YARNSessionFIFOITCase#testfullAlloc}} is a test case that is ignored because of a too high resource consumption but if run manually, it fails with

{code}
Error while deploying YARN cluster: Couldn't deploy Yarn session cluster
java.lang.RuntimeException: Couldn't deploy Yarn session cluster
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:367)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:663)
	at org.apache.flink.yarn.YarnTestBase$Runner.run(YarnTestBase.java:680)
Caused by: java.lang.IllegalArgumentException: The configuration value 'containerized.heap-cutoff-min' is higher (600) than the requested amount of memory 256
	at org.apache.flink.yarn.Utils.calculateHeapSize(Utils.java:101)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.setupApplicationMasterContainer(AbstractYarnClusterDescriptor.java:1356)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.startAppMaster(AbstractYarnClusterDescriptor.java:840)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deployInternal(AbstractYarnClusterDescriptor.java:456)
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.deploySessionCluster(AbstractYarnClusterDescriptor.java:362)
	... 2 more
{code}

in current master and

{code}
Error while starting the YARN Client: The JobManager memory (256) is below the minimum required memory amount of 768 MB
java.lang.IllegalArgumentException: The JobManager memory (256) is below the minimum required memory amount of 768 MB
	at org.apache.flink.yarn.AbstractYarnClusterDescriptor.setJobManagerMemory(AbstractYarnClusterDescriptor.java:187)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.createDescriptor(FlinkYarnSessionCli.java:314)
	at org.apache.flink.yarn.cli.FlinkYarnSessionCli.run(FlinkYarnSessionCli.java:622)
	at org.apache.flink.yarn.YarnTestBase$Runner.run(YarnTestBase.java:645)
{code}

in Flink 1.3.2 RC2"	FLINK	Closed	4	11500	10066	pull-request-available, starter
13097584	Add termination future to ClusterEntrypoint	In order to wait for the termination of {{ClusterEntrypoint}}, e.g. when testing it, a termination future would be helpful.	FLINK	Closed	4	4	10066	flip-6
13265053	Don't write FLINK_PLUGINS_DIR ENV variable to Flink configuration	"With FLINK-12143 we introduced the plugin mechanism. As part of this feature, we now write the {{FLINK_PLUGINS_DIR}} environment variable to the Flink {{Configuration}} we use for the cluster components. This is problematic, because we also use this {{Configuration}} to start new processes (Yarn and Mesos {{TaskExecutors}}). If the {{Configuration}} contains a configured {{FLINK_PLUGINS_DIR}} which differs from the one used by the newly created process, then this leads to problems.

In order to solve this problem, I suggest to not write env variables which are intended for local usage within the started process into the {{Configuration}}. Instead we should directly read the environment variable at the required site similar to what we do with the env variable {{FLINK_LIB_DIR}}."	FLINK	Closed	3	1	10066	pull-request-available
13362328	TaskManager connected to invalid JobManager leading to TaskSubmissionException	"While testing reactive mode, I had to start my JobManager a few times to get the configuration right. While doing that, I had at least on TaskManager (TM6), which was first connected to the first JobManager (with a running job), and then to the second one.

On the second JobManager, I was able to execute my test job (on another TaskManager (TMx)), once TM6 reconnected, and reactive mode tried to utilize all available resources, I repeatedly ran into this issue:

{code}
2021-03-04 15:49:36,322 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Window(GlobalWindows(), DeltaTrigger, TimeEvictor, ComparableAggregator, PassThroughWindowFunction) -> Sink: Print to Std. Out (5/7) (ae8f39c8dd88148aff93c8f811fab22e) switched from DEPLOYING to FAILED on 192.168.2.173:64041-4f7521 @ macbook-pro-2.localdomain (dataPort=64044).
java.util.concurrent.CompletionException: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job bbe8634736b5b1d813dd322cfaaa08ea.
	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$0(AkkaInvocationHandler.java:234) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_252]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_252]
	at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:1064) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:263) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.OnComplete.internal(Future.scala:261) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:191) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.japi$CallbackBridge.apply(Future.scala:188) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:73) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:572) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:101) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:999) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:458) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
Caused by: org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: Could not submit task because there is no JobManager associated for the job bbe8634736b5b1d813dd322cfaaa08ea.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.lambda$submitTask$3(TaskExecutor.java:523) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.Optional.orElseThrow(Optional.java:290) ~[?:1.8.0_252]
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:514) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:305) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:212) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158) ~[flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	... 9 more
{code}

I will upload all logs to this ticket and post my initial analysis.
"	FLINK	Closed	2	1	10066	pull-request-available
13119890	Cannot submit jobs to YARN Session in FLIP-6 mode	"Cannot submit jobs to YARN Session in FLIP-6 mode because {{FlinkYarnSessionCli}} becomes the _active_ CLI (should be {{Flip6DefaultCLI}}).

*Steps to reproduce*
# Build Flink 1.5 {{101fef7397128b0aba23221481ab86f62b18118f}}
# {{bin/yarn-session.sh -flip6 -d -n 1 -s 1 -jm 1024 -tm 1024}}
# {{bin/flink run -flip6 ./examples/streaming/WordCount.jar}}
# Verify that the job will not run.
"	FLINK	Resolved	1	7	10066	flip-6
13348318	Correct error message when calling TaskExecutor.disconnectJobManagerConnection	When calling {{TaskExecutor.disconnectJobManagerConnection}} then we create an exception with a message stating that the leading JobManager lost the leadership. This is not always true and consequently I propose to update this error message.	FLINK	Closed	4	4	10066	pull-request-available
13362378	SimpleRecoveryITCaseBase.testRestartMultipleTimes fails on azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14115&view=logs&j=5c8e7682-d68f-54d1-16a2-a09310218a49&t=f508e270-48d6-5f1e-3138-42a17e0714f0

{code}
2021-03-04T11:39:35.8958609Z [ERROR] testRestartMultipleTimes(org.apache.flink.test.recovery.SimpleRecoveryExponentialDelayRestartStrategyITBase)  Time elapsed: 1.753 s  <<< FAILURE!
2021-03-04T11:39:35.8959196Z java.lang.AssertionError: expected:<55> but was:<143>
2021-03-04T11:39:35.8959667Z 	at org.junit.Assert.fail(Assert.java:88)
2021-03-04T11:39:35.8959989Z 	at org.junit.Assert.failNotEquals(Assert.java:834)
2021-03-04T11:39:35.8962924Z 	at org.junit.Assert.assertEquals(Assert.java:645)
2021-03-04T11:39:35.8964175Z 	at org.junit.Assert.assertEquals(Assert.java:631)
2021-03-04T11:39:35.8964980Z 	at org.apache.flink.test.recovery.SimpleRecoveryITCaseBase.testRestartMultipleTimes(SimpleRecoveryITCaseBase.java:165)
2021-03-04T11:39:35.8965750Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-03-04T11:39:35.8966503Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-03-04T11:39:35.8967221Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-03-04T11:39:35.8967902Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-03-04T11:39:35.8968526Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-03-04T11:39:35.8969842Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-03-04T11:39:35.8970492Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-03-04T11:39:35.8971209Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-03-04T11:39:35.8971878Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-03-04T11:39:35.8972603Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-03-04T11:39:35.8973268Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-03-04T11:39:35.8973874Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-03-04T11:39:35.8974416Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-03-04T11:39:35.8987184Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-03-04T11:39:35.8987899Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-03-04T11:39:35.8988531Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-03-04T11:39:35.8989129Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-03-04T11:39:35.8989801Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-03-04T11:39:35.8990307Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-03-04T11:39:35.8991100Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
2021-03-04T11:39:35.8991907Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
2021-03-04T11:39:35.8993251Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
2021-03-04T11:39:35.8993978Z 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
2021-03-04T11:39:35.8994705Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-03-04T11:39:35.8995465Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-03-04T11:39:35.8996117Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-03-04T11:39:35.8996759Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Closed	2	1	10066	pull-request-available, test-stability
13139093	Make ClusterEntrypoint shut down non-blocking	Make the {{ClusterEntrypoint}} shut down method non blocking. That way we don't have to use the common Fork-Join-Pool to shutDownAndTerminate the cluster entrypoint when the Dispatcher terminates.	FLINK	Closed	3	4	10066	flip-6
13346815	Let JobManagerRunner return JobManagerRunnerResult to better distinguish result cases	"In order to better distinguish the result cases of the {{JobManagerRunner}} I propose to introduce a {{JobManagerRunnerResult}} which can have the values

1) the job was finished successfully 
2) the job was not finished
3) the job initialization failed (this is required in order to handle job initialization failures properly)"	FLINK	Closed	3	4	10066	pull-request-available
13138205	Introduce JobMasterConfiguration	The {{JobMaster}} already contains some configuration settings which we pass as constructor arguments. In order to make it better maintainable, I suggest to add a {{JobMasterConfiguration}} object similar to the {{TaskManagerConfiguration}}. This object will contain all {{JobMaster}} specific configuration settings.	FLINK	Closed	3	4	10066	flip-6
13200067	LICENSE and NOTICE files are not correct	Flink's LICENSE and NOTICE files are not correct wrt http://www.apache.org/dev/licensing-howto.html. We need to update them before we can release 1.7.0.	FLINK	Closed	1	1	10066	pull-request-available
13104211	Add serializable AccessExecutionGraph implementation	"In order to decouple the REST endpoint from the {{JobMaster}} we should have an ""offline"" implementation of the {{AccessExecutionGraph}} which is serializable and can be set to remote peers."	FLINK	Closed	3	7	10066	flip-6
13014217	Cleanup throws exception clause in HighAvailabilityServices	The {{HighAvailabilityServices}} interfaces defines methods with throws exception clauses which are not really needed. We should remove them to correct the interface.	FLINK	Closed	4	4	10066	flip-6
13118571	Bind logical slots to their request id instead of the slot allocation id	Since allocated slots can be reused to fulfil multiple slot requests, we should bind the resulting logical slots to their slot request id instead of the allocation id of the underlying allocated slot.	FLINK	Closed	3	4	10066	flip-6
13142176	Change default REST port to 8081	In order to avoid confusion, we should set the default REST port to 8081.	FLINK	Closed	1	4	10066	flip-6
12996106	Implement basic RPC abstraction	"As part of refactoring of the cluster management, we can introduce a new RPC abstraction on top of our Akka-based distributed coordination.

It should address the following issues:

  - Add type safety to the sender and receiver of messages. We want proper types methods to be called, rather than haveing generic message types and pattern matching everywhere. This is similar to typed actors.

  - Make the message receivers testable without involving actors, i.e. the methods should be callable directly. When used with other component, the receiver will be wrapped in an actor that calls the methods based on received messages.

  - We want to keep the paradigm of single-threaded execution per ""actor""

There is some basic code layout in the following branch and commit:

https://github.com/apache/flink/tree/flip-6/flink-runtime/src/main/java/org/apache/flink/runtime/rpc"	FLINK	Resolved	3	2	10066	flip-6
13386994	RpcService should fail result futures if messages could not be sent	The {{RpcService}} should fail result futures if messages could not be sent. This would speed up the failure detection mechanism because it would not rely on the timeout. One way to achieve this could be to listen to the dead letters and then sending a {{Failure}} message back to the sender.	FLINK	Closed	3	4	10066	pull-request-available
13240208	Job keeps in FAILING state	"There is a topology of 3 operator, such as, source, parser, and persist. Occasionally, 5 subtasks of the source encounters exception and turns to failed, at the same time, one subtask of the parser runs into exception and turns to failed too. The jobmaster gets a message of the parser's failed. The jobmaster then try to cancel all the subtask, most of the subtasks of the three operator turns to canceled except the 5 subtasks of the source, because the state of the 5 ones is already FAILED before jobmaster try to cancel it. Then the jobmaster can not reach a final state but keeps in  Failing state meanwhile the subtask of the source kees in canceling state. 
 
The job run on a flink 1.7 cluster on yarn, and there is only one tm with 10 slots.
 
The attached files contains a jm log , tm log and the ui picture.
 
The exception timestamp is about 2019-06-16 13:42:28."	FLINK	Resolved	2	1	10066	pull-request-available
13133350	Remove LibraryCacheManager from JobMaster	The {{JobMaster}} does not need access to the {{LibraryCacheManager}} because it is already started with the user code class loader. We should, therefore, remove the unused components.	FLINK	Closed	4	4	10066	flip-6
13370079	Remove console logging for Kafka connector for AZP runs	For the Kafka connector we do log to the console. These logging statements clutter the AZP output considerably. I propose to remove this logic. Moreover, we still have some DEBUG logging for FLINK-16383 which has been fixed.	FLINK	Closed	4	4	10066	pull-request-available
13139888	Fail new worker requests immediately in Standalone mode	In standalone mode, we should fail requests for new workers immediately, because we know that we cannot start new workers. That way, we would not have to wait for the pending slot request to time out.	FLINK	Closed	3	4	10066	flip-6
13192847	Update test_batch_allround.sh e2e to new testing infrastructure	The {{test_batch_allround.sh}} still does clean up tasks which is done by the {{test-runner-common.sh}}. This is no longer needed and should be the responsibility of the latter.	FLINK	Resolved	4	4	10066	pull-request-available
13350670	Add how to list jobs in Yarn deployment documentation when HA enabled	"We suggest users to list the running jobs in a Flink cluster via the following commands.
{code:java}
List running job on the cluster
./bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY
{code}
However, it could not work when the HA is enabled. The root cause is that GenericCLI do not override the ""high-availability.cluster-id"" with specified application id. The correct command could look like following.
{code:java}
flink list --target yarn-per-job -Dyarn.application.id=$application_id -Dhigh-availability.cluster-id=$application_id
{code}
 

Find more information in the discussion ML[1].

 

[1]. https://lists.apache.org/thread.html/r0ed905699a182d7b9180ed52d765fcd40caa659ef9640d60aa2ad20e%40%3Cuser.flink.apache.org%3E"	FLINK	Closed	3	4	10066	pull-request-available, usability
13259516	Pass in ioExecutor into AbstractDispatcherResourceManagerComponentFactory	The {{ClusterEntrypoint}} already starts an {{ioExecutor}}. However, this executor is not being used by the {{DispatcherResourceManagerComponent}} because it is not given to the {{DispatcherResourceManagerComponentFactory}}. In order to prepare the component to use the {{ioExecutor}}, I suggest to pass the {{ioExecutor}} into the {{DispatcherResourceManagerComponentFactory}}.	FLINK	Closed	4	4	10066	pull-request-available
13360505	StateWithExecutionGraph.suspend fails with IllegalStateException	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13665&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699

For discoverability, the Azure output is:
{code}
2021-02-23T23:48:50.6572167Z [INFO] BUILD FAILURE
2021-02-23T23:48:50.6573151Z [INFO] ------------------------------------------------------------------------
2021-02-23T23:48:50.6573684Z [INFO] Total time: 01:59 min
2021-02-23T23:48:50.6574520Z [INFO] Finished at: 2021-02-23T23:48:50+00:00
2021-02-23T23:48:51.4672056Z [INFO] Final Memory: 183M/3491M
2021-02-23T23:48:51.4673656Z [INFO] ------------------------------------------------------------------------
2021-02-23T23:48:51.4674310Z [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
2021-02-23T23:48:51.4675176Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (integration-tests) on project flink-connector-files: There are test failures.
2021-02-23T23:48:51.4675685Z [ERROR] 
2021-02-23T23:48:51.4676248Z [ERROR] Please refer to /__w/2/s/flink-connectors/flink-connector-files/target/surefire-reports for the individual test results.
2021-02-23T23:48:51.4677378Z [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
2021-02-23T23:48:51.4677963Z [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2021-02-23T23:48:51.4679891Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-files/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /__w/2/s/flink-connectors/flink-connector-files/target/surefire/surefirebooter72596965996179907.jar /__w/2/s/flink-connectors/flink-connector-files/target/surefire 2021-02-23T23-47-06_882-jvmRun1 surefire3083231786399295313tmp surefire_104007394662374862924tmp
2021-02-23T23:48:51.4680846Z [ERROR] Error occurred in starting fork, check output in log
2021-02-23T23:48:51.4681156Z [ERROR] Process Exit Code: 239
2021-02-23T23:48:51.4681413Z [ERROR] Crashed tests:
2021-02-23T23:48:51.4681737Z [ERROR] org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase
2021-02-23T23:48:51.4682470Z [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2021-02-23T23:48:51.4684122Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-files/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /__w/2/s/flink-connectors/flink-connector-files/target/surefire/surefirebooter72596965996179907.jar /__w/2/s/flink-connectors/flink-connector-files/target/surefire 2021-02-23T23-47-06_882-jvmRun1 surefire3083231786399295313tmp surefire_104007394662374862924tmp
2021-02-23T23:48:51.4685319Z [ERROR] Error occurred in starting fork, check output in log
2021-02-23T23:48:51.4685796Z [ERROR] Process Exit Code: 239
2021-02-23T23:48:51.4686229Z [ERROR] Crashed tests:
2021-02-23T23:48:51.4686926Z [ERROR] org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase
2021-02-23T23:48:51.4687709Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
2021-02-23T23:48:51.4688603Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
2021-02-23T23:48:51.4689451Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
2021-02-23T23:48:51.4690207Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
2021-02-23T23:48:51.4691076Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
2021-02-23T23:48:51.4692029Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
2021-02-23T23:48:51.4693058Z [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
2021-02-23T23:48:51.4693946Z [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
2021-02-23T23:48:51.4694768Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
2021-02-23T23:48:51.4695612Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
2021-02-23T23:48:51.4696345Z [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
2021-02-23T23:48:51.4697299Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
2021-02-23T23:48:51.4698243Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
2021-02-23T23:48:51.4699215Z [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
2021-02-23T23:48:51.4700380Z [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
2021-02-23T23:48:51.4701116Z [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
2021-02-23T23:48:51.4701775Z [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
2021-02-23T23:48:51.4702545Z [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
2021-02-23T23:48:51.4703179Z [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
2021-02-23T23:48:51.4703787Z [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
2021-02-23T23:48:51.4704402Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-23T23:48:51.4705114Z [ERROR] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-23T23:48:51.4705918Z [ERROR] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-23T23:48:51.4706668Z [ERROR] at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-23T23:48:51.4707481Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
2021-02-23T23:48:51.4708256Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
2021-02-23T23:48:51.4709066Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
2021-02-23T23:48:51.4709880Z [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
2021-02-23T23:48:51.4710740Z [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
2021-02-23T23:48:51.4714026Z [ERROR] Command was /bin/sh -c cd /__w/2/s/flink-connectors/flink-connector-files/target && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /__w/2/s/flink-connectors/flink-connector-files/target/surefire/surefirebooter72596965996179907.jar /__w/2/s/flink-connectors/flink-connector-files/target/surefire 2021-02-23T23-47-06_882-jvmRun1 surefire3083231786399295313tmp surefire_104007394662374862924tmp
2021-02-23T23:48:51.4715805Z [ERROR] Error occurred in starting fork, check output in log
2021-02-23T23:48:51.4716291Z [ERROR] Process Exit Code: 239
2021-02-23T23:48:51.4716701Z [ERROR] Crashed tests:
2021-02-23T23:48:51.4717323Z [ERROR] org.apache.flink.connector.file.sink.writer.FileSinkMigrationITCase
2021-02-23T23:48:51.4718074Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
2021-02-23T23:48:51.4718917Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
2021-02-23T23:48:51.4719729Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
2021-02-23T23:48:51.4720466Z [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
2021-02-23T23:48:51.4721147Z [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-02-23T23:48:51.4721808Z [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2021-02-23T23:48:51.4722700Z [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2021-02-23T23:48:51.4723348Z [ERROR] at java.lang.Thread.run(Thread.java:748)
2021-02-23T23:48:51.4724269Z [ERROR] -> [Help 1]
2021-02-23T23:48:51.4724583Z [ERROR] 
2021-02-23T23:48:51.4725296Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2021-02-23T23:48:51.4726133Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2021-02-23T23:48:51.4726565Z [ERROR] 
2021-02-23T23:48:51.4727239Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2021-02-23T23:48:51.4728131Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2021-02-23T23:48:51.4728631Z [ERROR] 
2021-02-23T23:48:51.4729322Z [ERROR] After correcting the problems, you can resume the build with the command
2021-02-23T23:48:51.4730277Z [ERROR]   mvn <goals> -rf :flink-connector-files
2021-02-23T23:48:51.9715714Z Process exited with EXIT CODE: 1.
2021-02-23T23:48:51.9723840Z Trying to KILL watchdog (4117).
{code}

From the logs, the real issue seems to be
{code}
23:48:38,279 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.
23:48:38,266 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor         [] - The RpcEndpoint jobmanager_3 failed.
org.apache.flink.runtime.jobmaster.JobMasterException: Could not properly stop the JobMaster.
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:405) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) ~[scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
Caused by: java.lang.IllegalStateException
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177) ~[flink-core-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.suspend(StateWithExecutionGraph.java:139) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.suspend(AdaptiveScheduler.java:307) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopScheduling(JobMaster.java:945) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopJobExecution(JobMaster.java:907) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:399) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	... 20 more
23:48:38,284 [flink-akka.actor.default-dispatcher-3] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 0fbf949ce33fe9af6c47b0cf3ceac0a0 was not finished by JobManager.
23:48:38,306 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-io-1b0f58d4-884c-4cac-bc87-f9028fb573a5
23:48:38,306 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Shutting down the network environment and its components.
23:48:38,285 [flink-akka.actor.default-dispatcher-3] ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler      [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-3' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.util.FlinkException: Could not properly shut down the JobManagerRunner
	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
	at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.lambda$closeAsync$0(JobManagerRunnerImpl.java:224) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_282]
	at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$22(FutureUtils.java:1341) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_282]
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_282]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_282]
Caused by: org.apache.flink.util.FlinkException: Could not properly shut down the JobManagerRunner
	... 12 more
Caused by: org.apache.flink.runtime.jobmaster.JobMasterException: Could not properly stop the JobMaster.
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:405) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) ~[scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.11-2.5.21.jar:2.5.21]
Caused by: java.lang.IllegalStateException
	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:177) ~[flink-core-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.adaptive.StateWithExecutionGraph.suspend(StateWithExecutionGraph.java:139) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.suspend(AdaptiveScheduler.java:307) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopScheduling(JobMaster.java:945) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.stopJobExecution(JobMaster.java:907) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.jobmaster.JobMaster.onStop(JobMaster.java:399) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.RpcEndpoint.internalCallOnStop(RpcEndpoint.java:214) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor$StartedState.terminate(AkkaRpcActor.java:563) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleControlMessage(AkkaRpcActor.java:186) ~[flink-runtime_2.11-1.13-SNAPSHOT.jar:1.13-SNAPSHOT]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [scala-library-2.11.12.jar:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [akka-actor_2.11-2.5.21.jar:2.5.21]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) ~[scala-library-2.11.12.jar:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [scala-library-2.11.12.jar:?]
	at akka.actor.Actor$class.aroundReceive(Actor.scala:517) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.actor.ActorCell.invoke(ActorCell.scala:561) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.run(Mailbox.scala:225) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ~[akka-actor_2.11-2.5.21.jar:2.5.21]
23:48:38,307 [flink-akka.actor.default-dispatcher-4] INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-ac49e716-c423-4dcc-9e42-34303a9e200d
{code}
"	FLINK	Closed	2	7	10066	pull-request-available, test-stability
13294511	Losing leadership does not clear rpc connection in JobManagerLeaderListener	"When losing the leadership the {{JobManagerLeaderListener}} closes the current {{rpcConnection}} but does not clear the field. This can lead to a failure of {{JobManagerLeaderListener#reconnect}} if this method is called after the {{JobMaster}} has lost its leadership.

I propose to clear the field so that {{RegisteredRpcConnection#tryReconnect}} won't be called on a closed rpc connection."	FLINK	Closed	3	1	10066	pull-request-available
13137751	Pass TaskManagerServices to TaskExecutor	In order to maintain the sheer number of {{TaskExecutor}} services, we should pass them to the {{TaskExecutor}} via the {{TaskManagerServices}} instead of individually.	FLINK	Closed	4	4	10066	flip-6
13192190	EventTimeWindowCheckpointingITCase fails on travis	"https://travis-ci.org/apache/flink/jobs/442580164
{code:java}
Tests run: 42, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 385.268 sec <<< FAILURE! - in org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase
testTumblingTimeWindowWithKVStateMinMaxParallelism[statebackend type =MEM_ASYNC](org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase)  Time elapsed: 0.093 sec  <<< ERROR!
java.net.BindException: Could not start actor system on any port in port range 0
	at org.apache.flink.runtime.clusterframework.BootstrapTools.startActorSystem(BootstrapTools.java:193)
	at org.apache.flink.runtime.metrics.util.MetricUtils.startMetricsActorSystem(MetricUtils.java:126)
	at org.apache.flink.runtime.minicluster.MiniCluster.start(MiniCluster.java:260)
	at org.apache.flink.test.util.MiniClusterResource.startMiniCluster(MiniClusterResource.java:168)
	at org.apache.flink.test.util.MiniClusterResource.before(MiniClusterResource.java:96)
	at org.apache.flink.test.checkpointing.EventTimeWindowCheckpointingITCase.setupTestCluster(EventTimeWindowCheckpointingITCase.java:227)
{code}"	FLINK	Resolved	1	1	10066	pull-request-available
13259527	Introduce DispatcherRunner#getShutDownFuture	I suggest to extend the {{DispatcherRunner}} interface to return a shut down future. The idea of the shut down future is that it gets completed once the runner intends to be shut down by its owner.	FLINK	Closed	4	4	10066	pull-request-available
13266555	ExecutionGraphSchedulingTest.testSlotReleasingFailsSchedulingOperation deadlocks	"https://api.travis-ci.com/v3/job/253364947/log.txt

{noformat}
""main"" #1 prio=5 os_prio=0 tid=0x00007fac3c00b800 nid=0x956 waiting on condition [0x00007fac449f1000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000008eb74600> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1693)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)
	at org.apache.flink.runtime.executiongraph.ExecutionGraphSchedulingTest.testSlotReleasingFailsSchedulingOperation(ExecutionGraphSchedulingTest.java:501)
{noformat}"	FLINK	Closed	2	1	10066	test-stability
13118575	Fulfil slot requests with unused offered slots	The {{SlotPool}} adds unused offered slots to the list of available slots without checking whether another pending slot request could be fulfilled with this slot. This should be changed.	FLINK	Closed	3	4	10066	flip-6
13282138	Expose the new type inference for table functions	This will allow to use table functions with the new type inference. It requires different changes through the stack. Support for structured type is not included here. Which means that only a ROW type can be used in the first version.	FLINK	Closed	3	7	10269	pull-request-available
13359278	Implement Schema, ResolvedSchema, SchemaResolver	Introduces the main classes and utilities around schema mentioned in FLIP-164.	FLINK	Closed	3	7	10269	pull-request-available
13296431	Add ability interfaces for table source/sink	"This will add the ability interfaces mentioned in FLIP-95:
 * SupportsWatermarkPushDown
 * SupportsProjectionPushDown
 * + already existing interfaces"	FLINK	Closed	3	7	10269	pull-request-available
13370030	Allow casting between row and structured type	There are still some minor barriers that prevent using {{toDataStream}} to its full extent.	FLINK	Closed	3	7	10269	pull-request-available
13219956	Simplify OVER window Table API classes	The OVER windows Table API can be simplified without breaking the backwards compatibility. Especially for FLINK-11449 and FLINK-11448 it makes sense to simplify the logic here.	FLINK	Closed	3	4	10269	pull-request-available
13177082	Document usage of INSERT INTO in SQL Client	Document the usage of {{INSERT INTO}} statements in SQL.	FLINK	Resolved	3	7	10269	pull-request-available
13296425	Add core table source/sink interfaces	"This will add the most important interfaces for the new source/sink interfaces:
 * DynamicTableSource
 * ScanTableSource extends DynamicTableSource
 * LookupTableSource extends DynamicTableSource
 * DynamicTableSink

And some initial ability interfaces:
 * SupportsComputedColumnPushDown
 * SupportsFilterPushDown

All interfaces will have extended JavaDocs."	FLINK	Closed	3	7	10269	pull-request-available
13352203	Allow SQL expressions in Table API	"Both FLIP-136 and FLIP-129 assume that a table schema is able to store both a SQL expression and a Table API one. Therefore we should introduce an expression in Table API that allows to use SQL expressions. This is also useful for other use cases where people prefer SQL expressions in Table API.

An expression `callSql(String)` next to `call(udf)` can offer scalar operations such as:
{code}
table.select(callSql(""1+1""), 12, call(Udf.class, $(""f0"")))
{code}"	FLINK	Closed	3	7	10269	pull-request-available
13389854	Expose a consistent GlobalDataExchangeMode	"The Table API makes the {{GlobalDataExchangeMode}} configurable via {{table.exec.shuffle-mode}}.

In Table API batch mode the StreamGraph is configured with {{ALL_EDGES_BLOCKING}} and in DataStream API batch mode {{FORWARD_EDGES_PIPELINED}}.

I would vote for unifying the exchange mode of both APIs so that complex SQL pipelines behave identical in {{StreamTableEnvironment}} and {{TableEnvironment}}. Also the feedback a got so far would make {{ALL_EDGES_BLOCKING}} a safer option to run pipelines successfully with limited resources.

[~lzljs3620320]
{quote}
The previous history was like this:
- The default value is pipeline, and we find that many times due to insufficient resources, the deployment will hang. And the typical use of batch jobs is small resources running large parallelisms, because in batch jobs, the granularity of failover is related to the amount of data processed by a single task. The smaller the amount of data, the faster the fault tolerance. So most of the scenarios are run with small resources and large parallelisms, little by little slowly running.

- Later, we switched the default value to blocking. We found that the better blocking shuffle implementation would not slow down the running speed much. We tested tpc-ds and it took almost the same time.
{quote}

[~dwysakowicz]
{quote}
I don't see a problem with changing the default value for DataStream batch mode if you think ALL_EDGES_BLOCKING is the better default option.
{quote}

In any case, we should make this configurable for DataStream API users and make the specific Table API option obsolete."	FLINK	Closed	3	7	10269	pull-request-available
13165840	Allow to define an auto watermark interval in SQL Client	Currently it is not possible to define an auto watermark interval in a non-programmatic way for the SQL Client.	FLINK	Resolved	3	4	10269	pull-request-available
13180234	SQL Client table visualization mode does not update correctly	"The table visualization modes does not seem to update correctly.
When I run a query that groups and aggregates on a few (6) distinct keys, the client visualizes some keys multiple times. Also the aggregated values do not seem to be correct.
Due to the small number of keys, these get frequently updated."	FLINK	Resolved	3	1	10269	pull-request-available
13282141	Support the new type inference in Scala Table API scalar functions	"Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used ""inline"". We should support them as well."	FLINK	Closed	3	7	10269	auto-unassigned, pull-request-available
13195911	Interval join produces wrong result type in Scala API	"When stream is a Scala case class, the TypeInformation will fall back to GenericType in the process function which result in bad performance when union another DataStream.

In the union method of DataStream, the type is first checked for equality.

Here is an example:
{code:java}
object Test {

    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment

      val orderA: DataStream[Order] = env.fromCollection(Seq(
        Order(1L, ""beer"", 3),
         Order(1L, ""diaper"", 4),
         Order(3L, ""rubber"", 2)))

      val orderB: DataStream[Order] = env.fromCollection(Seq(
        new Order(2L, ""pen"", 3),
        new Order(2L, ""rubber"", 3),
        new Order(4L, ""beer"", 1)))

      val orderC: DataStream[Order] = orderA.keyBy(_.user)
        .intervalJoin(orderB.keyBy(_.user))
        .between(Time.seconds(0), Time.seconds(0))
        .process(new ProcessJoinFunction[Order, Order, Order] {
          override def processElement(left: Order, right: Order, ctx: ProcessJoinFunction[Order, Order, Order]#Context, out: Collector[Order]): Unit = {
            out.collect(left)
          }})

      println(""C: "" + orderC.dataType.toString)
      println(""B: "" + orderB.dataType.toString)

      orderC.union(orderB).print()

      env.execute()
    }

    case class Order(user: Long, product: String, amount: Int)
}{code}
Here is the Exception:
{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot union streams of different types: GenericType<com.manbuyun.awesome.flink.Test.Order> and com.manbuyun.awesome.flink.Test$Order(user: Long, product: String, amount: Integer)
 at org.apache.flink.streaming.api.datastream.DataStream.union(DataStream.java:219)
 at org.apache.flink.streaming.api.scala.DataStream.union(DataStream.scala:357)
 at com.manbuyun.awesome.flink.Test$.main(Test.scala:38)
 at com.manbuyun.awesome.flink.Test.main(Test.scala){code}
 "	FLINK	Closed	3	1	10269	pull-request-available
13416233	Harden type serialization in JSON plan	"1. Introduce two representations for LogicalType

Compact one (using asSerializableString):

{code}
// compact one
outputType: ""ROW<i INT, s VARCHAR(2147483647)>""

// full one for all kinds of logical types (time attributes, char(0), inline structured, etc.)
outputType: {
  ""root"" : ""ROW"",
  ""nullable"" : true,
  ""fields"" : [ {
    ""i"" : ""INT""
  }, {
    ""s"" : ""VARCHAR(2147483647)""
  }]
}
{code}

2. Drop support of legacy types and symbol classes which should not be part of the plan

3. Rework DataView support (shorten, remove concrete classes, support any external type in accumulators)

4. Implement a DataTypeJsonDeSerializer

5. Replace RelDataTypeJsonDeSerializer with LogicalTypeJsonDeSerializer"	FLINK	Closed	3	7	10269	pull-request-available
13350623	Add a converter from TypeInformation to DataType	"Implement a new TypeInfoDataTypeConverter that will no longer produce LegacyTypeInformationType.

As mentioned in the FLIP:
- All types from DataStream API should be supported by this converter.
- TupleTypeInfoBase will be translated into a proper RowType or StructuredType.
- BigDecimals will be converted to DECIMAL(38,18) by default.
- Composite types (tuples, POJOs, rows) will be flattened by default if they are used as top-level records (similar to the old behavior).
- The order of POJO field's is determined by the DataTypeExtractor and must not be defined manually anymore.
- GenericTypeInfo is converted to RawType immediately by considering the current configuration.
- A DataStream that originated from Table API will keep its DataType information due to ExternalTypeInfo implementing DataTypeQueryable."	FLINK	Closed	3	7	10269	pull-request-available
13413369	Table to DataStream conversion, wrong field order	"It seems that in some cases, the field reordering as describe in the relevant [part of the docs|https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/table/data_stream_api/#examples-for-todatastream] does not seem to work properly. Given the following DDL
{code:java}
create table if not exists masterdata
(
   facility text,
   shortcode text,
   sks text,
   sksnumber integer,
   rdspp text,
   manufacturer text,
   facilitytype text,
   controls text,
   serial integer,
   powerkw double precision,
   hubheight double precision,
   rotorheight integer,
   latitude double precision,
   longitude double precision,
   elevation double precision
); {code}
which should map to this POJO:
{code:java}
public static class MasterData {

   public String controls;

   public Double elevation;

   public String facility;

   public String facilityType;

   public Double hubHeight;

   public Double latitude;

   public Double longitude;

   public String manufacturer;

   public Double powerKw;

   public String rdsPp;

   public Long rotorHeight;

   public Long serial;

   public String shortcode;

   public String sks;

   public Long sksNumber;

} {code}
I register the database using JdbcCatalog like this:

 
{code:java}
JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl);   
tableEnv.registerCatalog(""cat"", catalog);   
tableEnv.useCatalog(""cat""); {code}
and if I try to create a table with either ""SELECT * FROM masterdata"" or via
{code:java}
tableEnv.from(""masterdata""); {code}
It will bail out with an exception similar to
{code:java}
Exception in thread ""main"" org.apache.flink.table.api.ValidationException: Column types of query result and sink for registered table 'cat.postgres.Unregistered_DataStream_Sink_1' do not match.
Cause: Incompatible types for sink column 'elevation' at position 1.Query schema: [facility: STRING, shortcode: STRING, sks: STRING, sksnumber: INT, rdspp: STRING, manufacturer: STRING, facilitytype: STRING, controls: STRING, serial: INT, powerkw: DOUBLE, hubheight: DOUBLE, rotorheight: INT, latitude: DOUBLE, longitude: DOUBLE, elevation: DOUBLE]
Sink schema:  [controls: STRING, elevation: DOUBLE, facility: STRING, facilityType: STRING, hubHeight: DOUBLE, latitude: DOUBLE, longitude: DOUBLE, manufacturer: STRING, powerKw: DOUBLE, rdsPp: STRING, rotorHeight: BIGINT, serial: BIGINT, shortcode: STRING, sks: STRING, sksNumber: BIGINT] {code}
If i explicitly set the order of the columns in the SELECT like this:
{code:java}
tableEnv.sqlQuery(""SELECT elevation,facility,latitude,longitude,manufacturer,serial from masterdata"");{code}
it works. In the debugger I can see that ""queryFields"" and ""sinkField"" in the call to DynamicSinkUtils.validateSchemaAndApplyImplicitCast
() are not aligned, i.e. the order of the fields in those two lists are not the same, hence the exception.

Here is the full code for reproducing:
{code:java}
import org.apache.flink.connector.jdbc.catalog.JdbcCatalog;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.junit.Test;

public class FieldReorderTest {

   @Test
   public void testFieldReordering() throws Exception {
      String name = ""cat"";
      String defaultDatabase = ""postgres"";
      String username = ""cat"";
      String password = ""1234"";
      String baseUrl = ""jdbc:postgresql://cat.postgres.database.azure.com:5432"";

      var env = StreamExecutionEnvironment.getExecutionEnvironment();
      StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);

      JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl);
      tableEnv.registerCatalog(""cat"", catalog);
      tableEnv.useCatalog(""cat"");

      var table = tableEnv.from(""masterdata"");
      table.printSchema();
      tableEnv.toDataStream(table, MasterData.class).print();
      env.execute();       

      // this works
      // tableEnv.sqlQuery(""SELECT    controls,elevation,facility,facilitytype,hubheight,latitude,longitude,"" +
      //       ""manufacturer,powerkw,rdspp,rotorheight,serial,shortcode,sks,sksnumber from masterdata"");     
}

   public static class MasterData {

      public String controls;

      public Double elevation;

      public String facility;

      public String facilityType;

      public Double hubHeight;

      public Double latitude;

      public Double longitude;

      public String manufacturer;

      public Double powerKw;

      public String rdsPp;

      public Long rotorHeight;

      public Long serial;

      public String shortcode;

      public String sks;

      public Long sksNumber;

   }

}
 {code}
 

 "	FLINK	Closed	4	4	10269	pull-request-available
13424657	Use compact DataType serialization for default classes instead of internal ones	It is more likely that default conversion classes spam the plan than internal classes. In most cases when internal classes are used, they usually also use logical type instead of data type. So it should be safer to skip default conversion classes. This also reduces the plan size for serializing `ResolvedSchema`.	FLINK	Closed	3	7	10269	pull-request-available
13365258	Update use new schema in Table, TableResult, TableOperation	In order to complete FLIP-164, we should update important API parts to return the new `ResolvedSchema` and deprecate `TableSchema`.	FLINK	Closed	3	7	10269	pull-request-available
13434519	Support the new type inference in Scala Table API aggregate functions	"Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used ""inline"". We should support them as well."	FLINK	Open	3	7	10269	pull-request-available, stale-assigned
13089438	Merge the flink-java8 project into flink-core	This issue removes the flink-java8 module and merges some tests into flink-core/flink-runtime. It ensures to have the possibility for passing explicit type information in DataStream API as a fallback. Since the tycho compiler approach was very hacky and seems not to work anymore, this commit also removes all references in the docs and quickstarts.	FLINK	Resolved	3	4	10269	pull-request-available
13353480	Add an example for complex UDFs	People will always ask for more complex UDF examples, so we should have a dedicated package with some examples of UDFs.	FLINK	Closed	3	7	10269	pull-request-available
13080698	Add E() supported in TableAPI	See FLINK-6960 for detail.	FLINK	Resolved	3	7	10269	starter
13379717	Simplify TableEnvironment.create()	"After dropping the legacy planner, in many cases there is not much to build in the `EnvironmentSettings` except for batch/streaming mode. Simplify the API with:
{code:java}
EnvironmentSettings.inStreamingMode()    // shortcut
EnvironmentSettings.inBatchMode()        // shortcut
{code}"	FLINK	Closed	3	7	10269	pull-request-available
13300804	Support the NULL type for function calls	According to FLIP-37, the null type is used to represent untyped NULL literals. Those are in particular useful when it comes to passing them to a function call. The new type inference of FLIP-65 is able to process call such as `f(NULL)`.	FLINK	Closed	3	7	10269	pull-request-available
13401276	Support STRUCTURED_TYPE for JSON_OBJECT / JSON_ARRAY	In FLINK-16203 we excluded this because we run into a limitation with fromValues when testing it. I will apply the patch that should implement this, but we need to find a way to make the test work.	FLINK	Closed	4	7	10269	pull-request-available
13296451	Data structure should cover all conversions declared in logical types	In order to ensure that we don't loose any type precision or conversion class information in sources and sinks, this issue will add a type integrity test for data structure converters. Also UDFs will benefit from this test.	FLINK	Closed	3	7	10269	pull-request-available
13293817	Support RAW types through the stack	RAW types have already been exposed through the API even though the planner does not support the long-term format. Currently, it based on {{TypeInformation}} instead of {{TypeSerializer}}. We should switch from {{TypeInformationRawType}} to entirely using {{RawType}}.	FLINK	Closed	3	7	10269	pull-request-available
13475673	Non-deterministic UID generation might cause issues during restore	"I want to use the savepoint mechanism to move existing jobs from one version of Flink to another, by:
 # Stopping a job with a savepoint
 # Creating a new job from the savepoint, on the new version.

In Flink 1.15.1, it fails, even when going from 1.15.1 to 1.15.1. I get this error, meaning that it could not map the state from the previous job to the new one because of one operator:
{quote}{{Failed to rollback to checkpoint/savepoint hdfs://hdfs-name:8020/flink-savepoints/savepoint-046708-238e921f5e78. Cannot map checkpoint/savepoint state for operator d14a399e92154660771a806b90515d4c to the new program, because the operator is not available in the new program.}}
{quote}
After investigation, the problematic operator corresponds to a {{ChangelogNormalize}} operator, that I do not explicitly create. It is generated because I use [{{tableEnv.fromChangelogStream(stream, schema, ChangelogMode.upsert())}}|https://nightlies.apache.org/flink/flink-docs-release-1.15/api/java/org/apache/flink/table/api/bridge/java/StreamTableEnvironment.html#fromChangelogStream-org.apache.flink.streaming.api.datastream.DataStream-org.apache.flink.table.api.Schema-org.apache.flink.table.connector.ChangelogMode-] (the upsert mode is important, other modes do not fail). The table created is passed to an SQL query using the SQL API, which generates something like:
{quote}{{ChangelogNormalize[8] -> Calc[9] -> TableToDataSteam -> [my_sql_transformation] -> [my_sink]}}
{quote}
In previous versions of Flink it seems this operator was always given the same uid so the state could match when starting from the savepoint. In Flink 1.15.1, I see that a different uid is generated every time. I could not find a reliable way to set that uid manually. The only way I found was by going backwards from the transformation:
{quote}{{dataStream.getTransformation().getInputs().get(0).getInputs().get(0).getInputs().get(0).setUid(""the_user_defined_id"");}}
{quote}"	FLINK	Closed	1	1	10269	pull-request-available
13287257	Avoid unnecessary casting in TypeInferenceOperandChecker	{{TypeInferenceOperandChecker}} inserts casts that are actually not necessary. It should have the same behavior as in {{ResolveCallByArgumentsRule}}.	FLINK	Resolved	3	7	10269	pull-request-available
13389391	Support object reuse disabled in OperatorChain	"Currently, object reuse must be enabled in order to use chained sources.

Tests such as `HiveDialectQueryITCase` will fail with an exception:
{code}
2021-07-12T14:47:55.8233741Z Jul 12 14:47:55 [ERROR] testQueries(org.apache.flink.connectors.hive.HiveDialectQueryITCase)  Time elapsed: 12.283 s  <<< ERROR!
2021-07-12T14:47:55.8234433Z Jul 12 14:47:55 java.lang.RuntimeException: Failed to fetch next result
2021-07-12T14:47:55.8235133Z Jul 12 14:47:55 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-07-12T14:47:55.8235958Z Jul 12 14:47:55 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-07-12T14:47:55.8236774Z Jul 12 14:47:55 	at org.apache.flink.table.api.internal.TableResultImpl$CloseableRowIteratorWrapper.hasNext(TableResultImpl.java:370)
....
2021-07-12T14:47:55.8313594Z Jul 12 14:47:55 Caused by: java.lang.UnsupportedOperationException: Currently chained sources are supported only with objectReuse enabled
2021-07-12T14:47:55.8314356Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedSourceOutput(OperatorChain.java:355)
2021-07-12T14:47:55.8315109Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.createChainedSources(OperatorChain.java:322)
2021-07-12T14:47:55.8315820Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.OperatorChain.<init>(OperatorChain.java:220)
2021-07-12T14:47:55.8316506Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeRestore(StreamTask.java:558)
2021-07-12T14:47:55.8317209Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:661)
2021-07-12T14:47:55.8317948Z Jul 12 14:47:55 	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:547)
2021-07-12T14:47:55.8318626Z Jul 12 14:47:55 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:759)
2021-07-12T14:47:55.8319205Z Jul 12 14:47:55 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
2021-07-12T14:47:55.8319725Z Jul 12 14:47:55 	at java.lang.Thread.run(Thread.java:748)
2021-07-12T14:47:55.8320122Z Jul 12 1
{code}

The fix should looks as follows:

This particular exception should be rather straightforward to fix. The reason it's not implemented is because the chaining sources feature was implemented in the minimal scope required by blink planner and is missing around ~50-100 lines of production code to work with the object reuse disabled.
In the {{OperatorChain#createChainedSourceOutput}} we need to something similar as is done in {{OperatorChain#wrapOperatorIntoOutput}} , so something like:
{code}
        if (containingTask.getExecutionConfig().isObjectReuseEnabled()) {
            return closer.register(new ChainingOutput(input, metricGroup, outputTag));
        } else {
            TypeSerializer<IN> inSerializer =
                    operatorConfig.getTypeSerializerIn1(userCodeClassloader);
            return closer.register(new CopyingChainingOutput(input, inSerializer, metricGroup, outputTag));
        }
{code}
the missing part to do that is to make {{CopyingChainingOutput}} work with an Input instead of an Operator."	FLINK	Closed	3	7	10269	pull-request-available
13321301	Add StreamStatementSet.attachAsDataStream	"StatementSet solves use cases for pure SQL & Table API pipelines. However, currently there is no way of creating StatementSet for a DataStream API job.

We propose the following API:
{code}
StreamTableEnvironment.createStatementSet(): StreamStatementSet // return a stream-specific set

StreamStatementSet extends StatementSet {
  /**
   * Attaches the optimized statement set to the DataStream pipeline.
   */
  attachAsDataStream(): Unit
}
{code}

An example could look like:
{code}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

tEnv
  .createStatementSet()
  .addInsert(tEnv.from(""FromTable""))
  .attachAsDataStream();

tEnv.from(""OtherTable"").toDataStream(...) // continue with further operations
{code}"	FLINK	Closed	3	7	10269	pull-request-available, usability
13339692	Add documentation for FLIP-107	"Add documentation for FLIP-107:

- Connector/format metadata in general
- Kafka key/value and metadata
- Debezium metadata"	FLINK	Closed	2	7	10269	pull-request-available
13274712	Map Flink's TypeInference to Calcite's interfaces	After a {{TypeInference}} is available (either through reflective extraction or manual definition), the information needs to be connected to Calcite's interfaces. In particular, {{SqlOperandTypeInference}}, {{SqlOperandTypeChecker}}, {{SqlReturnTypeInference}}.	FLINK	Closed	3	7	10269	pull-request-available
13194331	Make flink-formats Scala-free	{{flink-table}} is the only dependency that pulls in Scala for {{flink-json}}, {{flink-avro}}. We should aim to make {{flink-formats}} Scala-free using only a dependency to {{flink-table-common}}.	FLINK	Resolved	3	7	10269	pull-request-available
12965022	Add a TableSink for Elasticsearch	Add a TableSink that writes data to Elasticsearch	FLINK	Resolved	3	2	10269	pull-request-available
13541748	Invalid input strategy for many functions which allows BINARY strings	"""string"" in SQL terms covers both character strings and binary strings. The author of CONCAT might not have known this. In any case, the code gen instead of the validator fails when executing:

{code}
TableEnvironment t = TableEnvironment.create(EnvironmentSettings.inStreamingMode());
t.createTemporaryView(""t"", t.fromValues(lit(new byte[] {97})));
t.executeSql(""SELECT CONCAT(f0, '-magic') FROM t"").print();
{code}

As future work, we should also allow binary strings."	FLINK	Closed	3	1	10269	pull-request-available
13242958	Add a type parser utility	For both the SQL Client YAML files as well as future type annotations, we need a parser that can create logical types from strings. It is the reverse operation of {{org.apache.flink.table.types.logical.LogicalType#asSerializableString}}.	FLINK	Closed	3	7	10269	pull-request-available
13322483	Update documentation about user-defined aggregate functions	The documentation needs an update because all functions support the new type inference now.	FLINK	Closed	3	7	10269	pull-request-available
13423009	Update ReplicateRows to the new type system	ReplicateRows is an internal built-in function that still uses the old type system. It needs an update.	FLINK	Closed	3	7	10269	pull-request-available
13389376	Propagate Boundedness of SourceFunctionProvider to Transformation	SourceFunctionProvider does currently not propagate the boundedness for {{StreamGraphGenerator}}.	FLINK	Closed	3	7	10269	pull-request-available
13298471	Align Calcite's and Flink's SYMBOL types	"Flink's `SymbolType` stores the class of the symbol. Calcite does not. When converting from Flink's to Calcite's type we are loosing that information. We should either remove that from the Flink's type or add it to Calcite's.

I am in favor of removing it from the {{SymbolType}}. This type is a pseudo type which can be used only for literals. The symbol class is already part of the corresponding value, therefore it is a redundant information in the type itself.

Initial discussion: https://github.com/apache/flink/pull/11694#discussion_r408058640"	FLINK	Closed	3	7	10269	pull-request-available
13247585	ThreadLocalCache clashes for Blink planner	{{org.apache.flink.table.runtime.functions.ThreadLocalCache}} currently clashes.	FLINK	Closed	1	7	10269	pull-request-available
13326813	Add a new streaming SQL examples	"Add a new streaming SQL example. It should work entirely with SQL. 

It should show:

How to get started with a connector (CSV or DataGen?) and event-time.
How the DDL works.
How to work with Table Environment and SQL.
How a complex SQL query looks like. Windows, joins, maybe Top N.
How to use pure streaming operators vs. changelog producing operators"	FLINK	Closed	3	7	10269	pull-request-available
13422762	LogicalRelDataTypeConverterTest is flaky on Azure	LogicalRelDataTypeConverterTest seems to be flaky on Azure.	FLINK	Closed	1	7	10269	pull-request-available, test-stability
13328019	Allow to read metadata for Debezium format	Expose the metadata mentioned in FLIP-107 for Debezium format.	FLINK	Closed	3	7	10269	pull-request-available
13172884	Build SQL jars with every build	Currently, the shaded fat jars for SQL are only built in the {{-Prelease}} profile. However, end-to-end tests require those jars and should also be able to test them. E.g. existing {{META-INF}} entry and proper shading. We should build them with every release. If a build should happen quicker one can use the {{-Pfast}} profile.	FLINK	Resolved	3	4	10269	pull-request-available
13393687	Use pipeline name consistently across DataStream API and Table API	Currently, the pipeline name configured in {{StreamExecutionEnvironment}} is not always considered in a table environment. E.g. when using {{executeSql}}. In general, the job name code can be simplified by relying on the {{StreamGraphGenerator}}.	FLINK	Closed	3	7	10269	pull-request-available
13315610	Update UNNEST to new type system	Updates the built-in UNNEST function to the new type system.	FLINK	Closed	3	7	10269	pull-request-available
13384549	Consider ConfigOption fallback keys in FactoryUtil	We are currently not taking fallback keys into consideration when performing validation in FactoryUtil.	FLINK	Closed	3	7	10269	pull-request-available
13382130	Allow prefix syntax for ConfigOption.mapType	"The current factory design does not allow placeholder options in {{EncodingFormatFactory}} or {{DecodingFormatFactory}}.

The past has shown that placeholder options are used at a couple of locations.
See FLINK-22475 or {{KafkaOptions#PROPERTIES_PREFIX}}.

We should think about adding an additional functionality to {{ReadableConfig}} or a special {{ConfigOption}} type to finally solve this problem. This could also be useful for FLIP-129. And would solve the [current shortcomings for Confluent Avro registry|https://github.com/apache/flink/pull/15808#discussion_r645494282]."	FLINK	Closed	3	7	10269	pull-request-available
13017746	TraversableSerializer does not perform a deep copy of the elements it is traversing	"I had an issue where the state in my rolling window was incorrectly being maintained from window to window.  

*The initial state of my window looked like this:*
{code}
Map[Key, MutableValue] = {(""A"", Value(0)}, (""B"", Value(0)}
{code}

*Then in Window 0 I update the state so it looks like this at the close of the window:*
{code}
Map[Key, MutableValue] = {(""A"", Value(1)}, (""B"", Value(3)}
{code}

*Then at the start of Window 1 the state looks like it did at the end of Window 0:*
{code}
Map[Key, MutableValue] = {(""A"", Value(1)}, (""B"", Value(3)}
{code}
*when I expected it to look like the initial state:*
{code}
Map[Key, MutableValue] = {(""A"", Value(0)}, (""B"", Value(0)}
{code}

It looks like [TraversableSerializer|https://github.com/apache/flink/blob/master/flink-scala/src/main/scala/org/apache/flink/api/scala/typeutils/TraversableSerializer.scala#L65-L69] is doing a shallow copy of the elements in the traversable instead of a deep copy"	FLINK	Resolved	1	1	10269	serialization
13370321	Implement type inference for agg functions	Update avg, count, min, max, sum, sum0, stddevPop, stddevSamp, varPop, varSamp.	FLINK	Closed	3	7	10269	pull-request-available
13307343	Implement type inference for AS	Type information gets lost due to the legacy planner expressions. The user might experience unexpected exceptions.	FLINK	Closed	3	7	10269	pull-request-available
13306705	Use new type inference for SQL table and scalar functions	We urgently need to reduce the friction around different type systems and types of function. Therefore, we should update table/scalar functions in SQL to the new type inference.	FLINK	Closed	3	7	10269	pull-request-available
13143592	"Rowtime materialization causes ""mismatched type"" AssertionError"	"As raised in [this thread|https://lists.apache.org/thread.html/e2ea38aa7ae224d7481145334955d84243690e9aad10d58310bdb8e7@%3Cuser.flink.apache.org%3E], the query created by the following code will throw a calcite ""mismatch type"" ({{Timestamp(3)}} and {{TimeIndicator}}) exception.

{code:java}
String sql1 = ""select id, eventTs as t1, count(*) over (partition by id order by eventTs rows between 100 preceding and current row) as cnt1 from myTable1"";
String sql2 = ""select distinct id as r_id, eventTs as t2, count(*) over (partition by id order by eventTs rows between 50 preceding and current row) as cnt2 from myTable2"";

Table left = tableEnv.sqlQuery(sql1);
Table right = tableEnv.sqlQuery(sql2);
left.join(right).where(""id === r_id && t1 === t2"").select(""id, t1"").writeToSink(...)
{code}
The logical plan is as follows.
{code}
LogicalProject(id=[$0], t1=[$1])
  LogicalFilter(condition=[AND(=($0, $3), =($1, $4))])
    LogicalJoin(condition=[true], joinType=[inner])
      LogicalAggregate(group=[{0, 1, 2}])
        LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])])
          LogicalProject(id=[$0], eventTs=[$3])
            LogicalTableScan(table=[[_DataStreamTable_0]])
      LogicalAggregate(group=[{0, 1, 2}])
        LogicalWindow(window#0=[window(partition {0} order by [1] rows between $2 PRECEDING and CURRENT ROW aggs [COUNT()])])
          LogicalProject(id=[$0], eventTs=[$3])
            LogicalTableScan(table=[[_DataStreamTable_0]])
{code}
That is because the the rowtime field after an aggregation will be materialized while the {{RexInputRef}} type for the filter's operands ({{t1 === t2}}) is still {{TimeIndicator}}. We should make them unified.

"	FLINK	Resolved	3	1	10269	pull-request-available
13362096	Implement ResolvedExpression.asSerializableString for SQL	SQL expressions are the first ones (and maybe even the only ones in the future) that implement `ResolvedExpression.asSerializableString`.	FLINK	Closed	3	7	10269	pull-request-available
13321948	Update LookupTableSource to the new type system	"Scalar/Aggregate/Table functions have been updated. AsyncTableFunction and TableFunction for LookupTableSource are still using the old type system.

Example can be found here: FLINK-18889"	FLINK	Closed	3	7	10269	pull-request-available
13326811	Add a new basic Table API example	"Add a new basic Table API example that does not require a connector and can be used for simply play around with operators.

It should show:
- How to get started without any prior knowledge.
- How to work with Table Environment.
- How to work with Table
- How to print/collect.
- How to perform simple ETL and some operations."	FLINK	Closed	3	7	10269	pull-request-available
13381406	Drop usages of legacy planner in Kafka modules	Remove references to {{flink-table-planner}} in the Kafka modules.	FLINK	Closed	3	7	10269	pull-request-available
13308027	Implement type inference for CAST	CAST has high priority because it looses a lot of information about types.	FLINK	Closed	3	7	10269	pull-request-available
13377609	Drop usages of BatchTableEnvironment and old planner in Python	"This is a major cleanup of the Python module that drops support for BatchTableEnvironment and old planner.

Removes usages of:
 - DataSet
 - BatchTableEnvironment
 - Legacy planner
 - ExecutionEnvironment

 

Note: Batch processing is still possible via the unified \{{TableEnvironment}}."	FLINK	Closed	3	7	10269	pull-request-available
13179545	Dependency problems when executing SQL query in sql-client	"When tried to run query:
{code}
select count(distinct name) from (Values ('a'), ('b')) AS NameTable(name)
{code}
in {{sql-client.sh}} I got:
{code}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 43, Column 10: Unknown variable or type ""org.apache.commons.codec.binary.Base64""
{code}"	FLINK	Resolved	1	1	10269	pull-request-available
13397847	Propagate unique keys for fromChangelogStream	Similar to FLINK-23915, we are not propagating unique keys for {{fromChangelogStream}} because it is not written into statistics.	FLINK	Closed	1	1	10269	pull-request-available
13432422	Support the new type inference in Scala Table API table functions	"Currently, we cannot distinguish between old and new type inference for Scala Table API because those functions are not registered in a catalog but are used ""inline"". We should support them as well."	FLINK	Closed	3	7	10269	pull-request-available
13315834	Remove RowDataTypeInfo	FLINK-17000 introduced a TypeInformation class that should replace most of the type information in the Blink planner. We start with removing RowDataTypeInfo.	FLINK	Closed	3	7	10269	pull-request-available
13228824	Expose the new type system through the API	"Exposes the new type system through API methods.

Introduces new methods, adds converters for backwards-compatibility, and deprecates old methods.

Adds checks to types that are not supported by the legacy planner."	FLINK	Closed	3	7	10269	pull-request-available, stale-assigned
13346121	Update TableResult.collect()/TableResult.print() to the new type system	Currently, TableResult.collect()/TableResult.print() use old sink interfaces and old type system. Those methods are very important in the API and should be updated.	FLINK	Closed	3	7	10269	pull-request-available
13354577	Flink guava Dependence problem	"We set up a new Hadoop cluster, and we use the flink1.12.0 compiled by the previous release-1.12.0 branch.If I add hive jar to flink/lib/, it will report errors.

*Operating environment：*
     flink1.12.0 
     Hadoop 3.3.0
     hive 3.1.2

*Flink run official demo shell： /tmp/yjb/buildjar/flink1.12.0/bin/flink run -m yarn-cluster /usr/local/flink1.12.0/examples/streaming/WordCount.jar*

If I put one of the jar *flink-sql-connector-hive-3.1.2_2.11-1.12.0.jar* or *hive-exec-3.1.2.jar* in the Lib directory and execute the above shell, an error will be reported  java.lang.NoSuchMethodError : com.google.common . base.Preconditions.checkArgument (ZLjava/lang/String;Ljava/lang/Object;)V. *We can see that it's the dependency conflict of guava.*

*My cluster guava‘s version:*
 /usr/local/hadoop-3.3.0/share/hadoop/yarn/csi/lib/guava-20.0.jar
 /usr/local/hadoop-3.3.0/share/hadoop/common/lib/guava-27.0-jre.jar
 /usr/local/apache-hive-3.1.2-bin/lib/guava-20.0.jar
 /usr/local/apache-hive-3.1.2-bin/lib/jersey-guava-2.25.1.jar
 /usr/local/spark-3.0.1-bin-hadoop3.2/jars/guava-14.0.1.jar

*Can you give me some advice？*
 Thank you！"	FLINK	Closed	4	1	10269	auto-unassigned, pull-request-available
13244959	Allow switching planners in SQL Client	Once FLINK-13267 is resolved, we can also enable switching planners in the SQL Client via a execution property. Even though this is kind of a new feature, it had to be postponed after the feature-freeze as the relocation of FLINK-13267 would have created many merge conflicts otherwise.	FLINK	Closed	1	2	10269	pull-request-available
13434389	Metadata keys should not conflict with physical columns	"If you have an field called timestamp and in addition want to read the timestamp from the metadata:

{code}
CREATE TABLE animal_sightings_with_metadata (
  `timestamp` TIMESTAMP(3),
  `name` STRING,
  `country` STRING,
  `number` INT,
  `append_time` TIMESTAMP(3) METADATA FROM 'timestamp',
  `partition` BIGINT METADATA VIRTUAL,
  `offset` BIGINT METADATA VIRTUAL,
  `headers` MAP<STRING, BYTES> METADATA,
  `timestamp-type` STRING METADATA,
  `leader-epoch` INT METADATA,
  `topic` STRING METADATA
)
{code}

This gives:
{code}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.ValidationException: Field names must be unique. Found duplicates: [timestamp]
{code}"	FLINK	Closed	3	7	10269	pull-request-available
13399730	Disable single rowtime column check for collect/print	"As seen in FLINK-23751, the single rowtime column check can occur also during collecting and printing which is not important there as watermarks as not used.

The exception is also misleading as it references a {{DataStream}}:
{code:java}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.api.TableException: Found more than one rowtime field: [bidtime, window_time] in the query when insert into 'default_catalog.default_database.Unregistered_Collect_Sink_8'.
Please select the rowtime field that should be used as event-time timestamp for the DataStream by casting all other fields to TIMESTAMP.
{code}"	FLINK	Closed	3	1	10269	pull-request-available, stale-assigned
12833025	Add a SQL API	"From the mailing list:

Fabian: Flink's Table API is pretty close to what SQL provides. IMO, the best
approach would be to leverage that and build a SQL parser (maybe together
with a logical optimizer) on top of the Table API. Parser (and optimizer)
could be built using Apache Calcite which is providing exactly this.
Since the Table API is still a fairly new component and not very feature
rich, it might make sense to extend and strengthen it before putting
something major on top.

Ted: It would also be relatively simple (I think) to retarget drill to Flink if
Flink doesn't provide enough typing meta-data to do traditional SQL."	FLINK	Closed	3	2	10269	requires-design-doc
13424728	Introduce test infra for building FLIP-190 tests 	"The FLIP-190 requires to build a new test infra. For this test infra, we want to define test cases and data once, and then for each case we want to execute the following:

* Integration test that roughly does {{create plan -> execute job -> trigger savepoint -> stop job -> restore plan -> restore savepoint -> execute job -> stop and assert}}. Plan and savepoint should be commited to git, so running this tests when a plan and savepoint is available will not regenerate plan and savepoint.
* Change detection test to check that for the defined test cases, the plan hasn't been changed. Similar to the existing {{JsonPlanITCase}} tests.
* Completeness of tests/Coverage, that is count how many times ExecNodes (including versions) are used in the test cases. Fail if an ExecNode version is never covered.

Other requirements includes to ""version"" the test cases, that is for each test case we can retain different versions of the plan and savepoint, in order to make sure that after we introduce a new plan change, the old plan still continues to run"	FLINK	Closed	3	7	10269	pull-request-available
13391864	Use a layered configuration in Executor and Planner	"The configuration story in Flink is not very consistent at the moment. Ideally, we should have a layered approach where pipeline executor, DataStream API, and Table API store their configuration and add it to a global configuration on request. However, this is a big change that we would like to avoid at this point.

Instead, we partially follow this approach by adding:
- {{Executor.getConfiguration}} to access the config of lower layers
- {{Planner.getConfiguration}} to access a global configuration during planning

This is required to access properties stored in {{StreamExecutionEnvironment.configuration}}."	FLINK	Closed	3	7	10269	pull-request-available
13307998	Add a new test base for evaluating expressions	The current `ExpressionTestBase` has many shortcomings. It uses the old type system and does not easily allow for testing errors. We should introduce a new test base that makes defining tests easier and works with the new type system.	FLINK	Closed	3	7	10269	pull-request-available
13377631	Drop BatchTableSource/Sink HBaseTableSource/Sink and related classes	The BatchTableSource interface is not supported by the Blink planner. Therefore, we drop this class to unblock the removal of the legacy planner. An alternative HBase connector using the new interfaces is available.	FLINK	Closed	3	7	10269	pull-request-available
13427412	Properly declare internal UNNEST function	UNNEST is not declared as an internal function. We should declare it similar to REPLICATE ROWS.	FLINK	Closed	3	7	10269	pull-request-available
13182166	User-defined function with LITERAL paramters yields CompileException	"When using a user-defined scalar function only with literal parameters, a {{CompileException}} is thrown. For example

{code}
SELECT myFunc(CAST(40.750444 AS FLOAT), CAST(-73.993475 AS FLOAT))

public class MyFunc extends ScalarFunction {

	public int eval(float lon, float lat) {
		// do something
	}
}
{code}

results in 

{code}
[ERROR] Could not execute SQL statement. Reason:
org.codehaus.commons.compiler.CompileException: Line 5, Column 10: Cannot determine simple type name ""com""
{code}

The problem is probably caused by the expression reducer because it disappears if a regular attribute is added to a parameter expression."	FLINK	Closed	3	1	10269	pull-request-available
13247621	Rename Batch/StreamTableSourceFactory methods for avoiding name clashes	FLINK-11067 introduced {{org.apache.flink.table.factories.BatchTableSourceFactory#createTableSource}} and {{org.apache.flink.table.factories.StreamTableSourceFactory#createTableSource}}. Those methods should have more specific names in order to prevent name clashes when implementing both interfaces.	FLINK	Closed	3	1	10269	pull-request-available
13285755	testDynamicTableFunction fails	"https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_apis/build/builds/5186/logs/16

 
{code:java}
2020-02-14T14:46:56.8515984Z [ERROR] testDynamicTableFunction(org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase) Time elapsed: 3.452 s <<< FAILURE!
 2020-02-14T14:46:56.8517003Z java.lang.AssertionError:
 2020-02-14T14:46:56.8517232Z
 2020-02-14T14:46:56.8517485Z Expected: <[Test is a string, 42, null]>
 2020-02-14T14:46:56.8517739Z but: was <[42, Test is a string, null]>
 2020-02-14T14:46:56.8518067Z at org.apache.flink.table.planner.runtime.stream.sql.FunctionITCase.testDynamicTableFunction(FunctionITCase.java:611){code}
 

 

The change was to enable chaining of the ContinuousFileReaderOperator (https://github.com/apache/flink/pull/11097)."	FLINK	Closed	2	1	10269	pull-request-available
13472691	Add a simple Json (De) SerializationSchema	"Add a basic schema to read/write JSON.

This is so common that users shouldn't have to implement that themselves."	FLINK	Closed	3	4	11245	pull-request-available
13447177	Elasticsearch e2e jars bundle way more than they should	"The jars bundle flink-end-to-end-tests-common-elasticsearch and all of it's transitive dependencies, like junit or flink-rpc-core.

All of these are unnecessary for the test to work and really shouldn't be bundled."	FLINK	Closed	3	7	11245	pull-request-available
13295737	ES 5 sink should allow users to select netty transport client	"When assembling the settings for the {{PreBuiltTransportClient}} the {{Elasticsearch5ApiCallBridge}} first adds the user-provided client and then overrides http/transport types with netty 3.

This means  that users are forced to use netty 3, despite the connector being able to work with a more recent and secure version.

{code:java}
Settings settings = Settings.builder().put(clientConfig)
	.put(NetworkModule.HTTP_TYPE_KEY, Netty3Plugin.NETTY_HTTP_TRANSPORT_NAME)
	.put(NetworkModule.TRANSPORT_TYPE_KEY, Netty3Plugin.NETTY_TRANSPORT_NAME)
	.build();
{code}

We should invert the order in which the settings are added."	FLINK	Closed	3	4	11245	pull-request-available
13563714	TestingCheckpointIDCounter can easily lead to NPEs	The TestingCheckpointIDCounter builder doesn't define safe defaults for all builder parameters. Using it can easily lead to surprising null pointer exceptions in tests when code is being modified to call more methods.	FLINK	In Progress	3	11500	11245	pull-request-available
13255258	Fix shading of the licence information of netty	"The license filter isn't actually filtering anything. It should be META-INF/license/**.

The first filter seems to be outdated btw.

Multiple modules affected.

{code:xml}
<filter>
	<artifact>io.netty:netty</artifact>
	<excludes>
		<exclude>META-INF/maven/io.netty/**</exclude>
		<!-- Only some of these licenses actually apply to the JAR and have been manually
placed in this module's resources directory. -->
		<exclude>META-INF/license</exclude>
		<!-- Only parts of NOTICE file actually apply to the netty JAR and have been manually
copied into this modules's NOTICE file. -->
		<exclude>META-INF/NOTICE.txt</exclude>
	</excludes>
</filter>
{code}"	FLINK	Closed	3	1	11245	pull-request-available
13264463	Capture partition meta info on TaskExecutor	We need store some meta information (initially the IntermediateDataSetID, later on things libe createdAtDate, jobname, taskname etc.) in the TaskExecutor book-keeping so we can forward this later to the RM in case of promotions.	FLINK	Closed	3	7	11245	pull-request-available
13270770	Publish OpenAPI specification of REST Monitoring API	"Hello,

Flink provides a very helpful REST Monitoring API.

OpenAPI is convenient standard to generate clients in a variety of language for REST API documented according to their specification. In this case, clients would be helpful to automate management of Flink clusters.

Currently, there is no ""official"" OpenAPI specification of Flink REST Monitoring API. [Some|https://github.com/nextbreakpoint/flink-client] have written by users, but their consistency across Flink releases is uncertain.

I think it would be beneficial to have an OpenAPI specification provided and maintained by the Flink project.

 

Kind regards,

 "	FLINK	Closed	3	2	11245	pull-request-available
13416952	Return 503 Service Unavailable if endpoint is not ready yet	"If an endpoint (e.g., the RM) has not been started yet when receiving a request from the REST API, then an exception is thrown that is treated as an unhandled exception. we can handle this in a nicer way by returning 503 Service Unavailable.

{code:java}
Dec 10 13:12:17 	at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at akka.actor.ActorCell.invoke(ActorCell.scala:548) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_ae8961cc-571e-483d-b468-da652db90bb9.jar:1.15-SNAPSHOT]
Dec 10 13:12:17 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_292]
Dec 10 13:12:17 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_292]
Dec 10 13:12:17 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_292]
Dec 10 13:12:17 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_292]
Dec 10 13:12:17 2021-12-10 13:12:07,296 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Starting the resource manager.
Dec 10 13:12:17 2021-12-10 13:12:09,426 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 10.1.0.79:35275-d26a40 (akka.tcp://flink@10.1.0.79:35275/user/rpc/taskmanager_0) at ResourceManager
Dec 10 13:12:17 2021-12-10 13:12:09,496 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 10.1.0.79:35275-d26a40 (akka.tcp://flink@10.1.0.79:35275/user/rpc/taskmanager_0) at ResourceManager
Dec 10 13:12:17 2021-12-10 13:12:13,189 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 'Elasticsearch7.x end to end sink test example' (effb86be1cfd961e947988200a724de7).
Dec 10 13:12:17 2021-12-10 13:12:13,190 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 'Elasticsearch7.x end to end sink test example' (effb86be1cfd961e947988200a724de7).
Dec 10 13:12:17 2021-12-10 13:12:13,225 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .
Dec 10 13:12:17 2021-12-10 13:12:13,252 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job 'Elasticsearch7.x end to end sink test example' (effb86be1cfd961e947988200a724de7).
Dec 10 13:12:17 2021-12-10 13:12:13,324 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=2147483647, backoffTimeMS=1000) for Elasticsearch7.x end to end sink test example (effb86be1cfd961e947988200a724de7).
Dec 10 13:12:17 2021-12-10 13:12:13,428 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job Elasticsearch7.x end to end sink test example (effb86be1cfd961e947988200a724de7).
 {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=27953&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=3408"	FLINK	Closed	4	4	11245	pull-request-available, stale-assigned, test-stability
13540036	Add FailsOnJava17 annotation	Add an annotation for disabling specific tests on Java 17, similar to FailsOnJava11.	FLINK	Closed	3	7	11245	pull-request-available
13425731	Capture time that the job spends on deploying tasks	"FLINK-23976 added standardized metrics for capturing how much time we spend in each {{JobStatus}}. However, certain states in practice consist of several stages; for example the RUNNING state also includes the deployment of tasks.

To get a better picture on where time is spent I propose to add new metrics that capture the deployingTime based on the execution states. This will additionally get us closer to a proper uptime metric, which ideally will be runningTime - various stage time metrics.

A job is considered to be deploying,
* for batch jobs, if no task is running and at least one task is being deployed
* for streaming jobs, if at least one task is being deployed

The semantics are different for batch/streaming jobs because they differ in terms of how they make progress. For a streaming job all tasks need to be deployed for checkpointing to make work. For batch jobs any deployed task immediately starts progressing the job."	FLINK	Closed	3	2	11245	pull-request-available
13387265	Migrate Docker images to Debian Bullseye	"The AWS ECR image scanning reports some HIGH vulnerabilities on apache/flink:1.13.1-scala_2.12 docker image. In addition, all versions prior to this one have these issues.

The vulnerabilities are the following:
 # [CVE-2021-33574|https://security-tracker.debian.org/tracker/CVE-2021-33574]
 # [CVE-2019-25013 - for this one a patch was been released in glibc version 2.31-9|https://security-tracker.debian.org/tracker/CVE-2019-25013]

Our security policy do not allow us to deploy images having security vulnerabilities. Searching through the Internet I found that for the first problem, a patch containing the solution will be release this year.

Do you plan to release a new image containing the newer glibc version in order to solve those issues?

Also, I checked and the alpine based flink images do not have these vulnerabilities. Do you plan to release newer versions of flink based on alpine (latest one is flink:1.8.x)?"	FLINK	Closed	2	4	11245	docker, flink, glibc
13391855	Maven snapshot build does not use MAVEN_GLOBAL_OPTIONS	"On CI we have a collection of useful settings for maven that we generally use everywhere, but the deployment of maven snapshot artifacts currently doesn't.

This leads to some duplication and noisy logs."	FLINK	Closed	3	4	11245	pull-request-available
13353399	Consider removing automatic configuration fo number of slots from docker	"The {{docker-entrypoint.sh}} supports setting the number of task slots via the {{TASK_MANAGER_NUMBER_OF_TASK_SLOTS}} environment variable, which defaults to the number of cpu cores via {{$(grep -c ^processor /proc/cpuinfo)}}.

The environment variable itself is redundant nowadays since we introduced {{FLINK_PROPERTIES}}, and is no longer documented.

Defaulting to the number of CPU cores can be considered convenience, but it seems odd to have this specific to docker while the distribution defaults to {{1}}.
The bigger issue in my mind though is that this creates a configuration mismatch between the Job- and TaskManager processes; the ResourceManager specifically needs to know how many slots a worker has to make decisions about redundancy and allocating resources."	FLINK	Closed	3	4	11245	pull-request-available
13204239	FileUploadHandler stops working if the upload directory is removed	"A user has reported on the ML that the FileUploadHandler does not accept any files anymore if the upload directory was deleted after the cluster has been started.
A cursory glance at the code shows that it currently uses {{Files.createDirectory(...)}} to create a temporary directory for the current request to store uploaded files in.
Changing this to use {{Files.createDirectories(...)}} instead should prevent this from happening again."	FLINK	Closed	3	1	11245	pull-request-available
13193139	End-to-end test: Metrics accessible via REST API	Verify that Flink's metrics can be accessed via the REST API.	FLINK	Closed	3	7	11245	pull-request-available
13540232	CI may unintentionally use fallback akka loader	"We have a fallback akka loader for developer convenience in the IDE, that is on the classpath of most modules. Depending on the order of jars on the classpath it can happen that the fallback loader appears first, which we dont want because it slows down the build and creates noisy logs.

We can add a simple prioritization scheme to the rpc system loading to remedy that."	FLINK	Closed	3	11500	11245	pull-request-available
13221433	GenericTypeInfoTest fails on Java 9	"Output difference:
{code:java}
    pojos:java.util.List
    key:int
    sqlDate:java.sql.Date
    bigInt:java.math.BigInteger
        signum:int
        mag:[I
-       bitCount:int
-       bitLength:int
-       lowestSetBit:int
-       firstNonzeroIntNum:int
+       bitCountPlusOne:int
+       bitLengthPlusOne:int
+       lowestSetBitPlusTwo:int
+       firstNonzeroIntNumPlusTwo:int
    bigDecimalKeepItNull:java.math.BigDecimal
        intVal:java.math.BigInteger
            signum:int
            mag:[I
-           bitCount:int
-           bitLength:int
-           lowestSetBit:int
-           firstNonzeroIntNum:int
+           bitCountPlusOne:int
+           bitLengthPlusOne:int
+           lowestSetBitPlusTwo:int
+           firstNonzeroIntNumPlusTwo:int
        scale:int
    scalaBigInt:scala.math.BigInt
        bigInteger:java.math.BigInteger
            signum:int
            mag:[I
-           bitCount:int
-           bitLength:int
-           lowestSetBit:int
-           firstNonzeroIntNum:int
+           bitCountPlusOne:int
+           bitLengthPlusOne:int
+           lowestSetBitPlusTwo:int
+           firstNonzeroIntNumPlusTwo:int
    mixed:java.util.List
    makeMeGeneric:org.apache.flink.test.operators.util.CollectionDataSets$PojoWithDateAndEnum
        group:java.lang.String
+           value:[B
+           coder:byte
+           hash:int
+       date:java.util.Date
+       cat:org.apache.flink.test.operators.util.CollectionDataSets$Category (is enum)

     
{code}"	FLINK	Closed	3	7	11245	pull-request-available
13288299	Remove mocking from DatadogHttpClientTest	"The {{DatadogHttpClientTest}} uses some nasty powermock features, like suppressing methods and mocking static methods.

There are simple workarounds for these that we should employ instead."	FLINK	Closed	3	4	11245	pull-request-available
13348316	Blink runtime classes partially ignored by spotless	"The spotless configuration has an exclude for classes whose package contains ""generated"", to filter out generated classes.
However the only classes matching this exclusion are several files in flink-table-runtime-blink, which do not appear to be generated.

I suggest to remove the exclusion and apply the formatting to these classes as well."	FLINK	Closed	3	4	11245	pull-request-available
13534480	Drop HadoopRecoverableWriterOldHadoopWithNoTruncateSupportTest	This test explicitly checks behavior for Hadoop < 2.7, which we no longer support.	FLINK	Resolved	3	7	11245	pull-request-available
13484525	Jar upload spec should define a schema	"Install nodejs and run

{{$ npx --yes @openapitools/openapi-generator-cli generate -i [https://nightlies.apache.org/flink/flink-docs-release-1.15/generated/rest_v1_dispatcher.yml] -g typescript-axios -o .}}

 

Then it outputs error 

 

{{Caused by: java.lang.RuntimeException: Request body cannot be null. Possible cause: missing schema in body parameter (OAS v2): class RequestBody {}}
{{    description: null}}
{{    content: class Content {}}
{{        {application/x-java-archive=class MediaType {}}
{{            schema: null}}
{{            examples: null}}
{{            example: null}}
{{            encoding: null}}
{\{        }}}}
{\{    }}}
{{    required: true}}
{{}}}

 

This is because in the YAML:

{{}}{{ requestBody:}}

{{  content:}}

{{{}    application/x-java-archive: {{}}}}

 

 "	FLINK	Closed	3	4	11245	openapi, pull-request-available
13537341	Show unassigned/total TM resources in web ui	"It is important to know how many resources of a TM are currently _assigned_ to jobs.
This is different to what resources currently _used_, since you can have assigned 1gb memory to a job with it only using 10mb at this time."	FLINK	Closed	3	7	11245	pull-request-available
13183085	Provide separate thread-pool for REST endpoint	The REST endpoints currently share their thread-pools with the RPC system, which can cause the Dispatcher to become unresponsive if the REST parts are overloaded.	FLINK	Closed	3	4	11245	pull-request-available
13286228	Add baseline set of allowed unused dependencies	"We define several dependencies in our root pom, usually for logging (e.g., log4j), testing (e.g., junit) or build purposes (e.g., force-shading), to reduce noise in downstream modules.

These commonly show up as unused, because they may actually not be used (like junit in uber modules), are used indirectly (log4j implementation) or don't contain any classes (force-shading).

We should define a baseline for unused dependencies that we allow."	FLINK	Closed	3	7	11245	pull-request-available
13289337	PrometheusReporterEndToEndITCase fails with ClassNotFoundException	"Logs: https://dev.azure.com/rmetzger/Flink/_build/results?buildId=5883&view=logs&j=b1623ac9-0979-5b0d-2e5e-1377d695c991&t=e7804547-1789-5225-2bcf-269eeaa37447

{code}
[INFO] Running org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.005 s <<< FAILURE! - in org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase
[ERROR] testReporter(org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase)  Time elapsed: 0.005 s  <<< ERROR!
java.lang.NoClassDefFoundError: org/apache/flink/runtime/rest/messages/RequestBody
	at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.<init>(PrometheusReporterEndToEndITCase.java:119)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.runtime.rest.messages.RequestBody
	at org.apache.flink.metrics.prometheus.tests.PrometheusReporterEndToEndITCase.<init>(PrometheusReporterEndToEndITCase.java:119)

[INFO] 

{code}"	FLINK	Closed	1	1	11245	pull-request-available, test-stability
13439658	Remove custom surefire config from sql-parser[-hive]	These modules disable fork reuse and set the fork count to 1. After a quick test I see now reason why that is required, so we get rid of this special case.	FLINK	Closed	3	11500	11245	pull-request-available
13369727	Setup .asf.yaml in flink-web	"Infra is making some changes to the hosting of websites from git, in 2 months and flink-web  appears to require a small change.

We need to add a .asf.yaml file, with these contents:
{code}
publish:
 whoami: asf-site
{code}

https://lists.apache.org/thread.html/r8d023c0f5afefca7f6ce4e26d02404762bd6234fbe328011e1564249%40%3Cusers.infra.apache.org%3E"	FLINK	Closed	3	4	11245	pull-request-available
13365201	SQLClientKafkaITCase.testKafka fails to download Kafka	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=14665&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=ff888d9b-cd34-53cc-d90f-3e446d355529&l=25905

{code}
Mar 15 09:26:02 java.io.IOException: Process ([wget, -q, -P, /home/vsts/work/1/e2e_cache/downloads/1964243538, --timeout, 240, https://archive.apache.org/dist/kafka/2.4.1/kafka_2.11-2.4.1.tgz]) exceeded timeout (600000) or number of retries (3).
Mar 15 09:26:02 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:160)
Mar 15 09:26:02 	at org.apache.flink.tests.util.cache.AbstractDownloadCache.getOrDownload(AbstractDownloadCache.java:135)
Mar 15 09:26:02 	at org.apache.flink.tests.util.cache.PersistingDownloadCache.getOrDownload(PersistingDownloadCache.java:36)
Mar 15 09:26:02 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.setupKafkaDist(LocalStandaloneKafkaResource.java:109)
Mar 15 09:26:02 	at org.apache.flink.tests.util.kafka.LocalStandaloneKafkaResource.before(LocalStandaloneKafkaResource.java:102)
Mar 15 09:26:02 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:46)
Mar 15 09:26:02 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
Mar 15 09:26:02 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
Mar 15 09:26:02 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
Mar 15 09:26:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
Mar 15 09:26:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
Mar 15 09:26:02 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Mar 15 09:26:02 	at org.junit.runners.Suite.runChild(Suite.java:128)
Mar 15 09:26:02 	at org.junit.runners.Suite.runChild(Suite.java:27)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Mar 15 09:26:02 	at org.apache.flink.util.ExternalResource$1.evaluate(ExternalResource.java:48)
Mar 15 09:26:02 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Mar 15 09:26:02 	at org.junit.runners.Suite.runChild(Suite.java:128)
Mar 15 09:26:02 	at org.junit.runners.Suite.runChild(Suite.java:27)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
Mar 15 09:26:02 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
Mar 15 09:26:02 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
Mar 15 09:26:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
Mar 15 09:26:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
Mar 15 09:26:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
Mar 15 09:26:02 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
Mar 15 09:26:02 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
Mar 15 09:26:02 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
Mar 15 09:26:02 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
Mar 15 09:26:02 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
Mar 15 09:26:02 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Mar 15 09:26:02 Caused by: java.io.IOException: Process failed due to timeout.
Mar 15 09:26:02 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlocking(AutoClosableProcess.java:137)
Mar 15 09:26:02 	at org.apache.flink.tests.util.AutoClosableProcess$AutoClosableProcessBuilder.runBlockingWithRetry(AutoClosableProcess.java:150)
Mar 15 09:26:02 	... 46 more
{code}"	FLINK	Closed	3	7	11245	test-stability
13177653	EXECUTION_FAILOVER_STRATEGY docs are wrong	"According to the documentation this option controls {{""The maximum number of prior execution attempts kept in history.""}}, which obviously isn't correct.

PR: https://github.com/apache/flink/pull/6303"	FLINK	Closed	3	4	11245	pull-request-available
13204250	Remove UdfAnalyzer	"{noformat}
org.apache.flink.api.java.sca.CodeAnalyzerException: Exception occurred during code analysis.

	at org.apache.flink.api.java.sca.UdfAnalyzer.analyze(UdfAnalyzer.java:341)
	at org.apache.flink.api.java.sca.UdfAnalyzerTest.compareAnalyzerResultWithAnnotationsSingleInputWithKeys(UdfAnalyzerTest.java:1339)
	at org.apache.flink.api.java.sca.UdfAnalyzerTest.compareAnalyzerResultWithAnnotationsSingleInput(UdfAnalyzerTest.java:1322)
	at org.apache.flink.api.java.sca.UdfAnalyzerTest.testForwardWithArrayModification(UdfAnalyzerTest.java:695)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
Caused by: java.lang.IllegalArgumentException
	at org.apache.flink.shaded.asm5.org.objectweb.asm.ClassReader.<init>(Unknown Source)
	at org.apache.flink.shaded.asm5.org.objectweb.asm.ClassReader.<init>(Unknown Source)
	at org.apache.flink.shaded.asm5.org.objectweb.asm.ClassReader.<init>(Unknown Source)
	at org.apache.flink.api.java.sca.UdfAnalyzerUtils.findMethodNode(UdfAnalyzerUtils.java:131)
	at org.apache.flink.api.java.sca.UdfAnalyzerUtils.findMethodNode(UdfAnalyzerUtils.java:115)
	at org.apache.flink.api.java.sca.UdfAnalyzer.analyze(UdfAnalyzer.java:290)
	... 25 more
{noformat}
"	FLINK	Closed	3	7	11245	pull-request-available
13254416	Instructions for building flink-shaded against vendor repository don't work	"According to [documentation|https://ci.apache.org/projects/flink/flink-docs-release-1.9/flinkDev/building.html#custom--vendor-specific-versions], to build Flink against a vendor specific Hadoop version it is necessary to build flink-shaded against this version first : 
{code:bash}
mvn clean install -DskipTests -Pvendor-repos -Dhadoop.version=<hadoop_version>
{code}
vendor-repos profile has to be activated to include Hadoop vendors repositories.
 But Maven cannot find expected Hadoop dependencies and returns an error because vendor-repos profile isn't defined in flink-shaded.

Example using flink-shaded 8.0 and HDP 2.6.5 Hadoop version :
{code:bash}
mvn clean install -DskipTests -Pvendor-repos -Dhadoop.version=2.7.3.2.6.5.0-292
{code}
{code:bash}
[INFO] ---------------< org.apache.flink:flink-shaded-hadoop-2 >---------------
[INFO] Building flink-shaded-hadoop-2 2.7.3.2.6.5.0-292-8.0             [10/11]
[INFO] --------------------------------[ jar ]---------------------------------
[WARNING] The POM for org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-hdfs:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-client:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[WARNING] The POM for org.apache.hadoop:hadoop-yarn-common:jar:2.7.3.2.6.5.0-292 is missing, no dependency information available
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] flink-shaded 8.0 ................................... SUCCESS [  2.122 s]
[INFO] flink-shaded-force-shading 8.0 ..................... SUCCESS [  0.607 s]
[INFO] flink-shaded-asm-7 7.1-8.0 ......................... SUCCESS [  0.667 s]
[INFO] flink-shaded-guava-18 18.0-8.0 ..................... SUCCESS [  1.452 s]
[INFO] flink-shaded-netty-4 4.1.39.Final-8.0 .............. SUCCESS [  4.597 s]
[INFO] flink-shaded-netty-tcnative-dynamic 2.0.25.Final-8.0 SUCCESS [  0.620 s]
[INFO] flink-shaded-jackson-parent 2.9.8-8.0 .............. SUCCESS [  0.018 s]
[INFO] flink-shaded-jackson-2 2.9.8-8.0 ................... SUCCESS [  0.914 s]
[INFO] flink-shaded-jackson-module-jsonSchema-2 2.9.8-8.0 . SUCCESS [  0.627 s]
[INFO] flink-shaded-hadoop-2 2.7.3.2.6.5.0-292-8.0 ........ FAILURE [  0.047 s]
[INFO] flink-shaded-hadoop-2-uber 2.7.3.2.6.5.0-292-8.0 ... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  11.947 s
[INFO] Finished at: 2019-09-03T16:52:59+02:00
[INFO] ------------------------------------------------------------------------
[WARNING] The requested profile ""vendor-repos"" could not be activated because it does not exist.
[ERROR] Failed to execute goal on project flink-shaded-hadoop-2: Could not resolve dependencies for project org.apache.flink:flink-shaded-hadoop-2:jar:2.7.3.2.6.5.0-292-8.0: The following artifacts could not be resolved: org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-hdfs:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-yarn-client:jar:2.7.3.2.6.5.0-292, org.apache.hadoop:hadoop-yarn-common:jar:2.7.3.2.6.5.0-292: Failure to find org.apache.hadoop:hadoop-common:jar:2.7.3.2.6.5.0-292 in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :flink-shaded-hadoop-2
{code}
vendor-repos profile exists in Flink pom.xml file : [https://github.com/apache/flink/blob/3079d11913f153ec40c75afb5356fd3be1a1e550/pom.xml#L1037]"	FLINK	Closed	3	1	11245	pull-request-available
13248781	Avro Confluent Schema Registry nightly end-to-end test failed on Travis	"The {{Avro Confluent Schema Registry nightly end-to-end test}} failed on Travis with

{code}
[FAIL] 'Avro Confluent Schema Registry nightly end-to-end test' failed after 2 minutes and 11 seconds! Test exited with exit code 1

No taskexecutor daemon (pid: 29044) is running anymore on travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.
No standalonesession daemon to stop on host travis-job-b0823aec-c4ec-4d4b-8b59-e9f968de9501.
rm: cannot remove '/home/travis/build/apache/flink/flink-dist/target/flink-1.10-SNAPSHOT-bin/flink-1.10-SNAPSHOT/plugins': No such file or directory
{code}

https://api.travis-ci.org/v3/job/567273939/log.txt"	FLINK	Closed	1	1	11245	pull-request-available, test-stability
13502123	Use separate Prometheus CollectorRegistries	The PrometheusReporter uses the singleton collector registry, which means that defining multiple prometheus reporters with different metric filters will not behave as expected.	FLINK	Closed	3	4	11245	pull-request-available
13376438	LGPL-2.1 files in flink-python jars	"Looking at, for example, [https://repo1.maven.org/maven2/org/apache/flink/flink-python_2.11/1.13.0/] the jar file contains three LGPL-2.1 source files:
 * flink-python_2.11-1.13.0/META-INF/maven/org.jboss.modules/jboss-modules/pom.xml
 * flink-python_2.11-1.13.0/schema/module-1_1.xsd
 * flink-python_2.11-1.13.0/schema/module-1_0.xsd

There's nothing in the DEPENDENCIES or licenses directory on the topic. It looks like Netty brings in the LGPL-2.1 JBoss dependency (depending on the Apache-2.0 JBoss Marshalling, which depends on the LGPL-2.1 JBoss Modules).

The xsd files are also appear to be coming from JBoss Modules. Given Apache's position on LGPL-2.1 dependency inclusion in ASF projects, this seems like an issue."	FLINK	Closed	1	1	11245	pull-request-available
13537145	Consolidate mocking library usage	Use mockito instead of powermock wherever possible, with the goal of restricting powermock to specific modules, eventually dropping it entirely.	FLINK	Closed	3	7	11245	pull-request-available
12993244	TextInputFormatTest.testNestedFileRead fails on Windows OS	"Stack-trace i got when running the test on W10:

Running org.apache.flink.api.java.io.TextInputFormatTest
test failed with exception: null
java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertTrue(Assert.java:52)
        at org.apache.flink.api.java.io.TextInputFormatTest.testNestedFileRead(TextInputFormatTest.java:133)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)
"	FLINK	Closed	4	1	11245	test-stability
13376496	Japicmp fails on 1.12 branch	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=17529&view=logs&j=ed6509f5-1153-558c-557a-5ee0afbcdf24&t=241b1e5e-1a8e-5e6a-469a-a9b8cad87065

{code}
2021-05-03T21:26:13.4153009Z [ERROR] Failed to execute goal com.github.siom79.japicmp:japicmp-maven-plugin:0.11.0:cmp (default) on project flink-scala_2.12: Breaking the build because there is at least one incompatibility: org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.CoGroupDataSet$$anonfun$apply$2(org.apache.flink.api.scala.CoGroupDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$2:CLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.CoGroupDataSet$$anonfun$apply$3(org.apache.flink.api.scala.CoGroupDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$3:CLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.CoGroupDataSet$$anonfun$apply$4(org.apache.flink.api.scala.CoGroupDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$apply$4:CLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.apply(scala.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.CoGroupDataSet$$anonfun$buildGroupSortList$1(org.apache.flink.api.scala.CoGroupDataSet,org.apache.flink.api.common.typeinfo.TypeInformation,java.util.ArrayList):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1.result$1:FIELD_REMOVED,org.apache.flink.api.scala.CoGroupDataSet$$anonfun$buildGroupSortList$1:CLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.CrossDataSet$$anonfun$apply$1(org.apache.flink.api.scala.CrossDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$1:CLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.CrossDataSet$$anonfun$apply$2(org.apache.flink.api.scala.CrossDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.CrossDataSet$$anonfun$apply$2:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.DataSet$$anonfun$1(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$1:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.DataSet$$anonfun$write$1(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$1:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.DataSet$$anonfun$write$2(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$write$2:CLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.DataSet$$anonfun$writeAsCsv$1(org.apache.flink.api.scala.DataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.DataSet$$anonfun$writeAsCsv$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.ExecutionEnvironment$$anonfun$createInput$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$createInput$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.ExecutionEnvironment$$anonfun$fromCollection$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.ExecutionEnvironment$$anonfun$fromCollection$2(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromCollection$2:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.ExecutionEnvironment$$anonfun$fromElements$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$fromElements$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.ExecutionEnvironment$$anonfun$readFile$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.ExecutionEnvironment$$anonfun$readFile$2(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFile$2:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readFileOfPrimitives$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.ExecutionEnvironment$$anonfun$readTextFile$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFile$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.ExecutionEnvironment$$anonfun$readTextFileWithValue$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$readTextFileWithValue$1:CLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.apply(org.apache.flink.api.scala.DataSet,org.apache.flink.api.scala.DataSet):METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.apply(java.lang.Object,java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.ExecutionEnvironment$$anonfun$union$1(org.apache.flink.api.scala.ExecutionEnvironment):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function2[scala.Function2]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.ExecutionEnvironment$$anonfun$union$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet.org$apache$flink$api$scala$GroupedDataSet$$groupSortKeyPositions():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet.org$apache$flink$api$scala$GroupedDataSet$$groupSortOrders():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.GroupedDataSet$$anonfun$combineGroup$1(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.GroupedDataSet$$anonfun$combineGroup$2(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$combineGroup$2:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.apply(int):METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1(org.apache.flink.api.scala.GroupedDataSet,org.apache.flink.api.java.operators.SortedGrouping):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$maybeCreateSortedGrouping$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.GroupedDataSet$$anonfun$reduce$1(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.GroupedDataSet$$anonfun$reduce$2(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduce$2:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.GroupedDataSet$$anonfun$reduceGroup$1(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$1:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.GroupedDataSet$$anonfun$reduceGroup$2(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$2:CLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.GroupedDataSet$$anonfun$reduceGroup$3(org.apache.flink.api.scala.GroupedDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3:SUPERCLASS_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.GroupedDataSet$$anonfun$reduceGroup$3:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.JoinDataSet$$anonfun$apply$1(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$1:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.JoinDataSet$$anonfun$apply$2(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$2:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.JoinDataSet$$anonfun$apply$3(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$3:CLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.apply():METHOD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.JoinDataSet$$anonfun$apply$4(org.apache.flink.api.scala.JoinDataSet):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function0[scala.Function0]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4:SUPERCLASS_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.JoinDataSet$$anonfun$apply$4:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo.org$apache$flink$api$scala$typeutils$CaseClassTypeInfo$$fieldTypes:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.apply(java.lang.String):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.CaseClassTypeInfo$$anonfun$getFieldIndices$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFieldIndices$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.apply(org.apache.flink.api.common.typeinfo.TypeInformation):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.CaseClassTypeInfo$$anonfun$getFlatFields$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo,int,java.util.List,scala.runtime.IntRef):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getFlatFields$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.apply(scala.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.CaseClassTypeInfo$$anonfun$getGenericParameters$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getGenericParameters$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.apply(int):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.apply$mcVI$sp(int):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.CaseClassTypeInfo$$anonfun$getTypeAt$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo,scala.runtime.ObjectRef,java.lang.String,java.lang.Object):CONSTRUCTOR_REMOVED,scala.Function1$mcVI$sp[scala.Function1$mcVI$sp]:INTERFACE_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$getTypeAt$1:CLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.apply(scala.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.CaseClassTypeInfo$$anonfun$toString$1(org.apache.flink.api.scala.typeutils.CaseClassTypeInfo):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.typeutils.CaseClassTypeInfo$$anonfun$toString$1:CLASS_REMOVED,org.apache.flink.api.scala.UnfinishedJoinOperation.createJoinFunctionAssigner(org.apache.flink.api.common.operators.Keys,org.apache.flink.api.common.operators.Keys):METHOD_REMOVED,org.apache.flink.api.scala.utils.package.DataSetUtils(org.apache.flink.api.scala.DataSet,org.apache.flink.api.common.typeinfo.TypeInformation,scala.reflect.ClassTag):METHOD_REMOVED,org.apache.flink.api.scala.utils.package:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$.DataSetUtils(org.apache.flink.api.scala.DataSet,org.apache.flink.api.common.typeinfo.TypeInformation,scala.reflect.ClassTag):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.apply(org.apache.flink.api.java.tuple.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.package$DataSetUtils$$anonfun$countElementsPerPartition$1(org.apache.flink.api.scala.utils.package$DataSetUtils):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$countElementsPerPartition$1:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.apply(org.apache.flink.api.java.tuple.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.package$DataSetUtils$$anonfun$zipWithIndex$1(org.apache.flink.api.scala.utils.package$DataSetUtils):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithIndex$1:CLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.apply(org.apache.flink.api.java.tuple.Tuple2):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.apply(java.lang.Object):METHOD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.package$DataSetUtils$$anonfun$zipWithUniqueId$1(org.apache.flink.api.scala.utils.package$DataSetUtils):CONSTRUCTOR_REMOVED,scala.Serializable[scala.Serializable]:INTERFACE_REMOVED,scala.Function1[scala.Function1]:INTERFACE_REMOVED,java.io.Serializable[java.io.Serializable]:INTERFACE_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1:SUPERCLASS_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1.serialVersionUID:FIELD_REMOVED,org.apache.flink.api.scala.utils.package$DataSetUtils$$anonfun$zipWithUniqueId$1:CLASS_REMOVED -> [Help 1]
{code}"	FLINK	Closed	1	1	11245	pull-request-available
13304580	Migrate e2e tests to flink-docker	"Some end-to-end tests make use of flink-container/docker.
We need to migrate them to flink-docker."	FLINK	Closed	3	7	11245	pull-request-available
13328480	Add ResourceTracker	"Add a component that tracks the requirements for a job and missing/acquired resources.

Data is kept up to date via notifications about new/update resource requirements and acquired/lost resources."	FLINK	Closed	3	4	11245	pull-request-available
13411260	Distributed e2e tests across 2 profiles	It's that time of the year again :/	FLINK	Closed	3	11500	11245	pull-request-available
13385113	Add RpcSystem abstraction	"Flink-runtime code does not only interact with akka-reliant code through the RpcService interface, but also through various static helper methods.

A new RpcSystem interface shall be added that serves both as a factory for RpcService and a facade for these utilities.
Through this interface we will later load the AkkaRpcSystem as a plugin."	FLINK	Closed	3	7	11245	pull-request-available
13282354	HistoryServer archiving is done in Dispatcher main thread	{{Dispatcher#archiveExecutionGraph}} should call {{HistoryServerArchivist#archiveExecutionGraph}} asynchronously since the archiving may involve IO operations.	FLINK	Closed	2	1	11245	pull-request-available
13569476	Classloading deadlock between ExecNodeMetadataUtil and JsonSerdeUtil	"This is a fun one!

ExecNodeMetadataUtil and JsonSerdeUtil have a circular dependency in their static initialization, which can cause a classloading lockup when 2 threads are running the class initialization of each class at the same time because during class initialization they hold a lock.

https://ternarysearch.blogspot.com/2013/07/static-initialization-deadlock.html

JsonSerdeUtils#createFlinkTableJacksonModule calls into the ExecNodeMetadataUtil, while ExecNodeMetadataUtil#addToLookupMap calls into the JsonSerdeUtils.

{code}
 ""ForkJoinPool-3-worker-11"" #25 daemon prio=5 os_prio=0 cpu=219.87ms elapsed=995.99s tid=0x00007ff11c50e000 nid=0xf0fc in Object.wait()  [0x00007ff12a4f3000]
    java.lang.Thread.State: RUNNABLE
 	at o.a.f.t.p.plan.nodes.exec.serde.JsonSerdeUtil.createFlinkTableJacksonModule(JsonSerdeUtil.java:133)
 	at o.a.f.t.p.plan.nodes.exec.serde.JsonSerdeUtil.<clinit>(JsonSerdeUtil.java:111)

""ForkJoinPool-3-worker-7"" #23 daemon prio=5 os_prio=0 cpu=54.83ms elapsed=996.00s tid=0x00007ff11c50c000 nid=0xf0fb in Object.wait()  [0x00007ff12a5f4000]
   java.lang.Thread.State: RUNNABLE
	at o.a.f.t.p.plan.utils.ExecNodeMetadataUtil.addToLookupMap(ExecNodeMetadataUtil.java:235)
	at o.a.f.t.p.plan.utils.ExecNodeMetadataUtil.<clinit>(ExecNodeMetadataUtil.java:156)
{code}"	FLINK	Closed	3	1	11245	pull-request-available
13364975	Replace StreamTaskTestHarness#TestTaskMetricGroup	Reduce reliance on MetricGroup constructors by creating the metric groups through the usual API.	FLINK	Closed	3	7	11245	auto-unassigned, pull-request-available
13213306	Port ClientTest	Check and port {{ClientTest}} to new code base if necessary	FLINK	Closed	3	7	11245	pull-request-available
13477916	Azure tests failed with Temporary failure resolving 'archive.ubuntu.com'	"{code:java}
2022-08-22T02:30:50.8451433Z Err:1 http://archive.ubuntu.com/ubuntu xenial/main amd64 libio-pty-perl amd64 1:1.08-1.1build1
2022-08-22T02:30:50.8452217Z   Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8505305Z Err:2 http://archive.ubuntu.com/ubuntu xenial/main amd64 libipc-run-perl all 0.94-1
2022-08-22T02:30:50.8506106Z   Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8561081Z Err:3 http://archive.ubuntu.com/ubuntu xenial/universe amd64 moreutils amd64 0.57-1
2022-08-22T02:30:50.8561877Z   Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8640593Z E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libio-pty-perl/libio-pty-perl_1.08-1.1build1_amd64.deb  Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8641098Z 
2022-08-22T02:30:50.8641965Z E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/libi/libipc-run-perl/libipc-run-perl_0.94-1_all.deb  Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8642685Z 
2022-08-22T02:30:50.8643536Z E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/universe/m/moreutils/moreutils_0.57-1_amd64.deb  Temporary failure resolving 'archive.ubuntu.com'
2022-08-22T02:30:50.8643964Z 
2022-08-22T02:30:50.8644767Z E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=40233&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8
"	FLINK	Closed	1	11500	11245	test-stability
13142826	Accumulators not updated for running jobs	"The FLIP-6 {{TaskExecutor}} is never sending the current state of accumulators to the JobMaster. They are only updated if the job is finished.

The legacy TaskManager did this regularly as part of the heartbeat to the JobManager.

This is a regression and blocks the porting of some tests (like the {{SavepointMigrationTestBase}}) that makes use of accumulators to determine when the job shutdown condition is fulfilled."	FLINK	Resolved	1	1	11245	flip-6
13180902	Bump mockito to 2.0+	"Mockito only properly supports java 9 with version 2. We have to bump the dependency and fix various API incompatibilities.

Additionally we could investigate whether we still need powermock after bumping the dependency (which we'd also have to bump otherwise)."	FLINK	Closed	3	7	11245	pull-request-available
13448742	Savepoint status cannot be queried from standby jobmanager	"The savepoint status handler currently doesn't work on standby dispatchers because the OperationResult isn't serializable.

This wasn't caught by the recently added serialization safeguards, as those only covered the caller side (i.e., arguments passed to callee), but not the return value. "	FLINK	Closed	3	1	11245	pull-request-available
13382351	Leader retrieval fails with NoNodeException	"The NodeCache used by the LeaderElection-/-RetrievalDrivers ensures that parents to the observed node exists by regularly issuing mkdir calls. This operation can fail if concurrently the HA data is being cleaned up, which results in curator throwing an unhandled exception which crashes the TM.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=18700&view=logs&j=2c3cbe13-dee0-5837-cf47-3053da9a8a78&t=2c7d57b9-7341-5a87-c9af-2cf7cc1a37dc&l=4382"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13392686	'Resuming Externalized Checkpoint (hashmap, sync, no parallelism change) end-to-end test' fails on Azure	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=21129&view=logs&j=6caf31d6-847a-526e-9624-468e053467d6&t=1fdd9d50-31f7-5383-5578-49e27385b5f1&l=785
{code}
Caused by: org.apache.flink.runtime.client.JobSubmissionException: Failed to submit JobGraph.
	at org.apache.flink.client.program.rest.RestClusterClient.lambda$submitJob$9(RestClusterClient.java:405)
	at java.base/java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:986)
	at java.base/java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:970)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.util.concurrent.FutureUtils.lambda$retryOperationWithDelay$9(FutureUtils.java:373)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:610)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1085)
	at java.base/java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:478)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.runtime.rest.util.RestClientException: [File upload failed.]
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:486)
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:466)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1072)

{code}"	FLINK	Closed	1	1	11245	stale-blocker, test-stability
13269872	Unify packaging process	"Right now we have a Java 11 profile for customizing the build in various ways, from bumping plugins, setting compiler flags, disabling tests and even some dependencies.

The dependencies bit a problematic since it implies that not all jars created by the Java 8 build process can actually run on Java 11.
This creates an unnecessary burden for users who want to use Java 11 (reminder: the only supported LTS version) as they'd have to compile Flink themselves for Java 11.

Releasing a second set of jars/binaries is out of the question due to the confusion and overhead this would cause, so instead we should ensure that the Java 8 jars are usable on Java 11.

My proposal is to use a multi-release jar to package additional dependencies that are only loaded on Java 11.
I have verified that this approach works conceptually, whether it works with the dependency we have to deal with (jaxb-api) remains to be seen."	FLINK	Closed	3	7	11245	pull-request-available
13296168	flink-runtime tests are crashing the JVM on Java11 because of PowerMock	"Nightly travis run: https://travis-ci.org/github/apache/flink/jobs/670686286?utm_medium=notification&utm_source=slack

{code}
22:11:49.063 [INFO] Reactor Summary:
22:11:49.063 [INFO] 
22:11:49.068 [INFO] flink-annotations .................................. SUCCESS [  4.733 s]
22:11:49.069 [INFO] flink-metrics ...................................... SUCCESS [  0.250 s]
22:11:49.069 [INFO] flink-metrics-core ................................. SUCCESS [  3.012 s]
22:11:49.069 [INFO] flink-core ......................................... SUCCESS [01:34 min]
22:11:49.069 [INFO] flink-java ......................................... SUCCESS [ 30.494 s]
22:11:49.074 [INFO] flink-runtime ...................................... FAILURE [25:01 min]
22:11:49.074 [INFO] flink-scala ........................................ SKIPPED
22:11:49.074 [INFO] flink-optimizer .................................... SKIPPED
22:11:49.074 [INFO] flink-clients ...................................... SKIPPED
22:11:49.074 [INFO] flink-streaming-java ............................... SKIPPED
22:11:49.074 [INFO] flink-test-utils ................................... SKIPPED
22:11:49.074 [INFO] flink-runtime-web .................................. SKIPPED
22:11:49.074 [INFO] flink-statebackend-rocksdb ......................... SKIPPED
22:11:49.074 [INFO] flink-streaming-scala .............................. SKIPPED
22:11:49.074 [INFO] flink-scala-shell .................................. SKIPPED
22:11:49.074 [INFO] ------------------------------------------------------------------------
22:11:49.074 [INFO] BUILD FAILURE
22:11:49.074 [INFO] ------------------------------------------------------------------------
22:11:49.074 [INFO] Total time: 27:20 min
22:11:49.077 [INFO] Finished at: 2020-04-03T22:11:49+00:00
22:11:49.355 [INFO] Final Memory: 97M/330M
22:11:49.355 [INFO] ------------------------------------------------------------------------
22:11:49.361 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.22.1:test (default-test) on project flink-runtime_2.11: There are test failures.
22:11:49.362 [ERROR] 
22:11:49.362 [ERROR] Please refer to /home/travis/build/apache/flink/flink-runtime/target/surefire-reports for the individual test results.
22:11:49.362 [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.
22:11:49.362 [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.362 [ERROR] Error occurred in starting fork, check output in log
22:11:49.362 [ERROR] Process Exit Code: 239
22:11:49.362 [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.362 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.362 [ERROR] Error occurred in starting fork, check output in log
22:11:49.362 [ERROR] Process Exit Code: 239
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.awaitResultsDone(ForkStarter.java:510)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.runSuitesForkPerTestSet(ForkStarter.java:457)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:298)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:246)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:1183)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:1011)
22:11:49.362 [ERROR] at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:857)
22:11:49.362 [ERROR] at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
22:11:49.362 [ERROR] at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
22:11:49.363 [ERROR] at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
22:11:49.363 [ERROR] at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
22:11:49.363 [ERROR] at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
22:11:49.363 [ERROR] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
22:11:49.363 [ERROR] at java.base/java.lang.reflect.Method.invoke(Method.java:566)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
22:11:49.363 [ERROR] at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
22:11:49.363 [ERROR] Caused by: org.apache.maven.surefire.booter.SurefireBooterForkException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
22:11:49.363 [ERROR] Command was /bin/sh -c cd /home/travis/build/apache/flink/flink-runtime && /usr/local/lib/jvm/openjdk11/bin/java -Xms256m -Xmx2048m -Dmvn.forkNumber=1 -XX:+UseG1GC -jar /home/travis/build/apache/flink/flink-runtime/target/surefire/surefirebooter5965056229397858556.jar /home/travis/build/apache/flink/flink-runtime/target/surefire 2020-04-03T21-44-37_853-jvmRun1 surefire4393983892864834687tmp surefire_47412704678292479303842tmp
22:11:49.363 [ERROR] Error occurred in starting fork, check output in log
22:11:49.363 [ERROR] Process Exit Code: 239
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:669)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$600(ForkStarter.java:115)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:444)
22:11:49.363 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$2.call(ForkStarter.java:420)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
22:11:49.363 [ERROR] at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
22:11:49.363 [ERROR] at java.base/java.lang.Thread.run(Thread.java:834)
22:11:49.363 [ERROR] -> [Help 1]
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
22:11:49.363 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] For more information about the errors and possible solutions, please read the following articles:
22:11:49.363 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
22:11:49.363 [ERROR] 
22:11:49.363 [ERROR] After correcting the problems, you can resume the build with the command
22:11:49.364 [ERROR]   mvn <goals> -rf :flink-runtime_2.11
{code}"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13481315	Publish TestUtils#getResource	The elasticsearch e2e tests need some utils from flink-end-to-end-test-common, that we should move to the flink test utils.	FLINK	Closed	1	11500	11245	pull-request-available
13416494	Allow Kubernetes E2E tests to run on Java 11	Our Kubernetes E2E tests currently on run on Java 8. We should adjust common_docker.sh to build a Java 11 image if we're in the appropriate profile.	FLINK	Closed	3	7	11245	pull-request-available
13487278	Remove japicmp dependency bumps	"Way back when we worked on Java 11 support we bumped several dependencies from japicmp.
These are no longer required for the latest version that we're using."	FLINK	Closed	3	11500	11245	pull-request-available
13457887	Remove workaround around avro sql jar	"Because of FLINK-17417 flink-python contains a workaround that manually assembles a sort-of avro sql jar.
Rely on the sql-avro jar instead and remove the workaround."	FLINK	Closed	3	11500	11245	pull-request-available
13181587	KafkaTableSourceSinkFactoryBase#getFlinkKafkaPartitioner does not compile with Java 9	"The method compilation fails on java 9. The problem is that the {{CONNECTOR_SINK_PARTITIONER_VALUE_CUSTOM}} branch returns an {{Optional<Class<? extends FlinkKafkaPartitioner>>}}, but the method requires a {{Optional<FlinkKafkaPartitioner<Row>>}}.

{code}
private Optional<FlinkKafkaPartitioner<Row>> getFlinkKafkaPartitioner(DescriptorProperties descriptorProperties) {
	return descriptorProperties
		.getOptionalString(CONNECTOR_SINK_PARTITIONER)
		.flatMap((String partitionerString) -> {
			switch (partitionerString) {
				case CONNECTOR_SINK_PARTITIONER_VALUE_FIXED:
					return Optional.of(new FlinkFixedPartitioner<>());
				case CONNECTOR_SINK_PARTITIONER_VALUE_ROUND_ROBIN:
					return Optional.empty();
				case CONNECTOR_SINK_PARTITIONER_VALUE_CUSTOM:
					final Class<? extends FlinkKafkaPartitioner> partitionerClass =
						descriptorProperties.getClass(CONNECTOR_SINK_PARTITIONER_CLASS, FlinkKafkaPartitioner.class);
					return Optional.of(InstantiationUtil.instantiate(partitionerClass));
				default:
					throw new TableException(""Unsupported sink partitioner. Validator should have checked that."");
			}
		});
}
{code}"	FLINK	Closed	3	7	11245	pull-request-available
13248316	CassandraConnectorITCase fails on Java 11	"The \{{CassandraConnectorITCase}} fails on Java 11 with an error. We may have to disable the test since cassandra only supports Java 11 in 4.x which isn't released yet.

{code}
java.lang.ExceptionInInitializerError
        at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.startCassandra(CassandraConnectorITCase.java:186)
Caused by: java.lang.StringIndexOutOfBoundsException: begin 0, end -1, length 5
        at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.startCassandra(CassandraConnectorITCase.java:186)

[ERROR] org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase  Time elapsed: 0.016 s  <<< ERROR!
java.lang.NullPointerException
        at org.apache.flink.streaming.connectors.cassandra.CassandraConnectorITCase.closeCassandra(CassandraConnectorITCase.java:230)
{code}"	FLINK	Closed	3	7	11245	pull-request-available
13388551	Bump flink-shaded-guava to 30.1.1	A guava version bump is long overdue.	FLINK	Closed	3	4	11245	pull-request-available
13288040	LocalStandaloneFlinkResource should use Dist#startFlinkCluster	"The {{LocalStandaloneFlinkResource}} currently manually calls the jobmanager.sh/taskmanager.sh scripts.

Since the documentation instructs users to call the start-cluster script we should do this too.

This also entails exposing a method on the distribution for modifying the {{slaves}} file."	FLINK	Closed	3	4	11245	pull-request-available
13503600	Remove dependency promotion from kinesis connector	"The shade-plugin in the kinesis connector is configured to promote transitive dependencies.
This adds a special case in our shading setup, breaks the dependency tree structure (since _all_ transitive dependencies are moved to the top) and it makes the sql-kinesis packaging overly complicated.

Get rid of the dependency promotion and explicitly depend on anything that we pull in transitively but don't shade in the kinesis connector itself."	FLINK	Closed	3	11500	11245	pull-request-available
13334465	JMXReporterFactoryTest fails on Azure	"The following build failed

[https://dev.azure.com/khachatryanroman/810e80cc-0656-4d3c-9d8c-186764456a01/_apis/build/builds/289/logs/106]

{code}
 [ERROR] testPortRangeArgument(org.apache.flink.metrics.jmx.JMXReporterFactoryTest)  Time elapsed: 0.02 s  <<< FAILURE!
 java.lang.AssertionError:
 
 Expected: (a value equal to or greater than <9000> and a value less than or equal to <9010>)
      but: a value less than or equal to <9010> <9040> was greater than <9010>
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:956)
    at org.junit.Assert.assertThat(Assert.java:923)
    at org.apache.flink.metrics.jmx.JMXReporterFactoryTest.testPortRangeArgument(JMXReporterFactoryTest.java:46)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)

{code}

 

I see the following problems in the code:

- tests in JMXReporterFactoryTest assumes JMXService.jmxServer wasn't started or was stopped

- JMXService.jmxServer is not volatile

 cc: [~chesnay], [~rongr]"	FLINK	Closed	1	1	11245	test-stability
13267096	Add total number of partitions to ResultPartitionDeploymentDescriptor	"The ResourceManager requires some way to tell whether it has received all partitions for a given dataset; to ensure that a) no incomplete datasets are shows to the user [as completed] and b) to initiate a timeout-based cleanup of other partitions.

Conversely, since the RM only has access to what the TaskExecutor sends, this information must also be accessible to the TE.

A good candidate for placing this seems to be the {{ResultPartitionDeploymentDescriptor}} or {{PartitionDescriptor}}, as these already contain the DataSetID."	FLINK	Closed	3	7	11245	pull-request-available
13410263	"""Post-job: Cache Maven local repo"" failed on Azure"	"{code:java}
2021-11-05T13:49:20.5298458Z Resolved to: maven|Linux|kL7EJ8TeMrJ0VZs51DUWRqheXcKK2cN2spGtx9IbVxQ=
2021-11-05T13:49:21.0445785Z ApplicationInsightsTelemetrySender will correlate events with X-TFS-Session b5cbe6a0-61e7-4345-81b6-efaed3924cbe
2021-11-05T13:49:21.0700758Z Getting a pipeline cache artifact with one of the following fingerprints:
2021-11-05T13:49:21.0702157Z Fingerprint: `maven|Linux|kL7EJ8TeMrJ0VZs51DUWRqheXcKK2cN2spGtx9IbVxQ=`
2021-11-05T13:49:21.3648278Z There is a cache miss.
2021-11-05T13:50:26.4782603Z tar: c9692460fbd54c808fca7be315d83578_archive.tar: Wrote only 2048 of 10240 bytes
2021-11-05T13:50:26.4784975Z tar: Error is not recoverable: exiting now
2021-11-05T13:50:27.0397318Z ApplicationInsightsTelemetrySender correlated 1 events with X-TFS-Session b5cbe6a0-61e7-4345-81b6-efaed3924cbe
2021-11-05T13:50:27.0531774Z ##[error]Process returned non-zero exit code: 2
2021-11-05T13:50:27.0666804Z ##[section]Finishing: Cache Maven local repo
{code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26018&view=logs&j=c88eea3b-64a0-564d-0031-9fdcd7b8abee&t=96bd9872-da2e-43b4-b013-1295f1c23a41&l=220]"	FLINK	Reopened	2	1	11245	stale-assigned, test-stability
13483666	Add a uid(hash) remapping function	Expose functionality for modifying the uid[hash] of a state.	FLINK	Closed	3	7	11245	pull-request-available
13413571	Disable shading of test jars by default	"The AZP build fails with a license check:

{code}
21:26:40,233 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-sql-parquet_2.12/1.15-SNAPSHOT/flink-sql-parquet_2.12-1.15-20211123.212027-1-tests.jar
21:26:40,738 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - The notice file in /tmp/flink-validation-deployment/org/apache/flink/flink-connector-cassandra_2.12/1.15-SNAPSHOT/flink-connector-cassandra_2.12-1.15-20211123.211736-1-tests.jar does not contain the expected entries.
21:26:40,739 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-connector-cassandra_2.12/1.15-SNAPSHOT/flink-connector-cassandra_2.12-1.15-20211123.211736-1-tests.jar
21:26:41,673 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - The notice file in /tmp/flink-validation-deployment/org/apache/flink/flink-kubernetes/1.15-SNAPSHOT/flink-kubernetes-1.15-20211123.212114-1-tests.jar does not contain the expected entries.
21:26:41,675 ERROR org.apache.flink.tools.ci.licensecheck.JarFileChecker        [] - Missing META-INF/LICENSE in /tmp/flink-validation-deployment/org/apache/flink/flink-kubernetes/1.15-SNAPSHOT/flink-kubernetes-1.15-20211123.212114-1-tests.jar
21:28:00,582 WARN  org.apache.flink.tools.ci.licensecheck.LicenseChecker        [] - Found a total of 5 severe license issues
==============================================================================
License Check failed. See previous output for details.
==============================================================================
##[error]Bash exited with code '1'.
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&view=logs&j=946871de-358d-5815-3994-8175615bc253&t=e0240c62-4570-5d1c-51af-dd63d2093da1&l=30668
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=a7382ec4-87d2-5a9d-7c53-a2f93e317458&l=31863
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=26967&view=logs&j=6e8542d7-de38-5a33-4aca-458d6c87066d&t=dffc2faa-5b48-5b4e-0797-dec1b1f74872&l=31863"	FLINK	Closed	1	11500	11245	pull-request-available
13399939	Jdbc connector uses postgres testcontainers in compile scope	testcontainer dependencies should only ever be used in the test scope. It has been fixed at least on 1.14 branch (I have not checked on master)	FLINK	Closed	1	1	11245	pull-request-available
13281165	Support Java 17 (LTS)	Long-term issue for preparing Flink for Java 17.	FLINK	Closed	3	2	11245	auto-deprioritized-major, pull-request-available, stale-assigned
13399927	Allow idempotent savepoint triggering	"As a user of Flink, I want to be able to trigger a savepoint from an external system in a way that I can detect if I have requested this savepoint already.

By passing a custom ID to the savepoint request, I can check (in case of an error of the original request, or the external system) if the request has been made already.
"	FLINK	Closed	3	4	11245	pull-request-available
13439447	Redistributed modules across CI profiles	With the recent improvements around testing times it is time to redistribute the modules again to achieve a more even distribution.	FLINK	Closed	3	11500	11245	pull-request-available
13229829	JDBCFullTest fails on Java 9	"The test is checking for an exception message which contains the class names of Strings and such, which vary between java versions.

{code}
13:44:31.313 [ERROR]   JDBCFullTest.testEnrichedClassCastException 
Expected: (an instance of java.lang.ClassCastException and exception with message a string containing ""java.lang.String cannot be cast to java.lang.Double, field index: 3, field value: 11.11."")
     but: exception with message a string containing ""java.lang.String cannot be cast to java.lang.Double, field index: 3, field value: 11.11."" message was ""java.base/java.lang.String cannot be cast to java.base/java.lang.Double, field index: 3, field value: 11.11.""
Stacktrace was: java.lang.ClassCastException: java.base/java.lang.String cannot be cast to java.base/java.lang.Double, field index: 3, field value: 11.11.
{code}"	FLINK	Closed	3	7	11245	pull-request-available
13400293	Only support reporter factories for instantiation	"Metric reporters can currently be instantiated in one of 2 ways:
a) the reporter class is loaded via reflection
b) the reporter factory is loaded via reflection/ServiceLoader (aka, plugins)

All reporters provided by Flink use the factory approach, and it is preferable because it supports plugins. The plugin approach also has been available 1.11, and I think it's fair to remove the old approach by now."	FLINK	Closed	3	11500	11245	pull-request-available
13286486	Remove testing logic from FlinkDistribution	"Currently, the {{FlinkDistribution}} is responsible for locating and copying the distribution for usage in tests, and contains hooks for backing up log files.
 This makes the class a bit inflexible to use, as for example a {{FlinkResource}} could not create 2 distributions in separate places.

I suggest to move these responsibilities into the {{FlinkResource}} implementations, and ""demote"" the distribution to a simple wrapper, only providing programmatic access for mutating/interacting with the distribution."	FLINK	Closed	3	4	11245	pull-request-available
13207653	Extend Download page to list optional components	"Extend the download page of the Flink Website to support listing additional optional jars to be added to a Flink distribution.

These optional jars may include (among others):
* flink-shaded-hadoop2 (see FLINK-11266)
* SQL format/connector jars
* metric reporters

Overall this will allow us to slim down flink-dist and make it more convenient for users to download these jars."	FLINK	Closed	3	4	11245	pull-request-available
13437450	Cleanup surefire configuration	We have a few redundant surefire configurations in some connector modules, and overall a lot of duplication and stuff defined on the argLine which could be systemEnvironmentVariables (which are easier to extend in sub-modules).	FLINK	Closed	3	11500	11245	pull-request-available
13235328	Refactor YARN tests to not do assertion in @After methods	The YARN are executing assertions in the {{@After}} methods, which leads to output that it unnecessarily obfuscated. For example, it is not said explicitly which test has failed.	FLINK	Closed	3	4	11245	pull-request-available
13421762	Document which URI based config options works with local / general filesystem	"From S3 documentation [1]:

{code}
Note that these examples are not exhaustive and you can use S3 in other places as well, including your high availability setup or the EmbeddedRocksDBStateBackend; everywhere that Flink expects a FileSystem URI.
{code}

This is not correct, because there are config options such as `web.upload.dir`, that don't work with the filesystem abstraction and therefore can be only used with the local (java) filesystem. We should document these options with what type of filesystem they expect and get rid of the misleading note in s3 documentation.

Example: https://lists.apache.org/thread/5sqz77nsr5mkzv3r1w114v2x52vyk80c

[1] https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/deployment/filesystems/s3/#amazon-s3"	FLINK	Closed	3	11500	11245	pull-request-available
13226133	TaskManagerProcessFailureBatchRecoveryITCase fails due to removed Slot	"https://travis-ci.org/apache/flink/jobs/515636826

{code}
org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: 4f32093d9c7554c3de832d20f0a06eb5)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:268)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:483)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:471)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:446)
	at org.apache.flink.client.RemoteExecutor.executePlanWithJars(RemoteExecutor.java:210)
	at org.apache.flink.client.RemoteExecutor.executePlan(RemoteExecutor.java:187)
	at org.apache.flink.api.java.RemoteEnvironment.execute(RemoteEnvironment.java:173)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:817)
	at org.apache.flink.api.java.DataSet.collect(DataSet.java:413)
	at org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:115)
	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest$1.run(AbstractTaskManagerProcessFailureRecoveryTest.java:143)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:265)
	... 10 more
Caused by: org.apache.flink.util.FlinkException: The assigned slot 786ec47f893240315bb01291aab680ec_1 was removed.
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlot(SlotManager.java:893)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlots(SlotManager.java:863)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.internalUnregisterTaskManager(SlotManager.java:1058)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.unregisterTaskManager(SlotManager.java:385)
	at org.apache.flink.runtime.resourcemanager.ResourceManager.closeTaskManagerConnection(ResourceManager.java:847)
	at org.apache.flink.runtime.resourcemanager.ResourceManager$TaskManagerHeartbeatListener$1.run(ResourceManager.java:1161)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:392)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:185)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:147)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
org.apache.flink.client.program.ProgramInvocationException: Job failed. (JobID: 4f32093d9c7554c3de832d20f0a06eb5)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:268)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:483)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:471)
	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:446)
	at org.apache.flink.client.RemoteExecutor.executePlanWithJars(RemoteExecutor.java:210)
	at org.apache.flink.client.RemoteExecutor.executePlan(RemoteExecutor.java:187)
	at org.apache.flink.api.java.RemoteEnvironment.execute(RemoteEnvironment.java:173)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:817)
	at org.apache.flink.api.java.DataSet.collect(DataSet.java:413)
	at org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase.testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:115)
	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest$1.run(AbstractTaskManagerProcessFailureRecoveryTest.java:143)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:146)
	at org.apache.flink.client.program.rest.RestClusterClient.submitJob(RestClusterClient.java:265)
	... 10 more
Caused by: org.apache.flink.util.FlinkException: The assigned slot 786ec47f893240315bb01291aab680ec_1 was removed.
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlot(SlotManager.java:893)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.removeSlots(SlotManager.java:863)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.internalUnregisterTaskManager(SlotManager.java:1058)
	at org.apache.flink.runtime.resourcemanager.slotmanager.SlotManager.unregisterTaskManager(SlotManager.java:385)
	at org.apache.flink.runtime.resourcemanager.ResourceManager.closeTaskManagerConnection(ResourceManager.java:847)
	at org.apache.flink.runtime.resourcemanager.ResourceManager$TaskManagerHeartbeatListener$1.run(ResourceManager.java:1161)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:392)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:185)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:147)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)
	at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
	at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
	at akka.actor.ActorCell.invoke(ActorCell.scala:495)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
	at akka.dispatch.Mailbox.run(Mailbox.scala:224)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
java.lang.AssertionError: The program encountered a ProgramInvocationException : Job failed. (JobID: 4f32093d9c7554c3de832d20f0a06eb5)
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:190)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{code}"	FLINK	Closed	2	1	11245	pull-request-available, test-stability
13360876	JobLeaderIdService completes leader future despite no leader being elected	"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=13722&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=9c1ddabe-d186-5a2c-5fcc-f3cafb3ec699
{code:java}
2021-02-24T22:47:55.4844360Z java.lang.RuntimeException: Failed to fetch next result
2021-02-24T22:47:55.4847421Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:109)
2021-02-24T22:47:55.4848395Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
2021-02-24T22:47:55.4849262Z 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSource(FileSourceTextLinesITCase.java:148)
2021-02-24T22:47:55.4850030Z 	at org.apache.flink.connector.file.src.FileSourceTextLinesITCase.testBoundedTextFileSourceWithJobManagerFailover(FileSourceTextLinesITCase.java:108)
2021-02-24T22:47:55.4850780Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2021-02-24T22:47:55.4851322Z 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2021-02-24T22:47:55.4858977Z 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2021-02-24T22:47:55.4860737Z 	at java.lang.reflect.Method.invoke(Method.java:498)
2021-02-24T22:47:55.4861855Z 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
2021-02-24T22:47:55.4862873Z 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
2021-02-24T22:47:55.4863598Z 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
2021-02-24T22:47:55.4864289Z 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
2021-02-24T22:47:55.4864937Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-02-24T22:47:55.4865570Z 	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
2021-02-24T22:47:55.4866152Z 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
2021-02-24T22:47:55.4866670Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-02-24T22:47:55.4867172Z 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
2021-02-24T22:47:55.4867765Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
2021-02-24T22:47:55.4868588Z 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
2021-02-24T22:47:55.4869683Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-24T22:47:55.4886595Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-24T22:47:55.4887656Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-24T22:47:55.4888451Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-24T22:47:55.4889199Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-24T22:47:55.4889845Z 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
2021-02-24T22:47:55.4890447Z 	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
2021-02-24T22:47:55.4891037Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-24T22:47:55.4891604Z 	at org.junit.runners.Suite.runChild(Suite.java:128)
2021-02-24T22:47:55.4892235Z 	at org.junit.runners.Suite.runChild(Suite.java:27)
2021-02-24T22:47:55.4892959Z 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
2021-02-24T22:47:55.4893573Z 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
2021-02-24T22:47:55.4894216Z 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
2021-02-24T22:47:55.4894824Z 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
2021-02-24T22:47:55.4895425Z 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
2021-02-24T22:47:55.4896027Z 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
2021-02-24T22:47:55.4896638Z 	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
2021-02-24T22:47:55.4897378Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
2021-02-24T22:47:55.4898342Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
2021-02-24T22:47:55.4899204Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
2021-02-24T22:47:55.4899965Z 	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
2021-02-24T22:47:55.4900709Z 	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:158)
2021-02-24T22:47:55.4901503Z 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
2021-02-24T22:47:55.4902315Z 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
2021-02-24T22:47:55.4903260Z 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
2021-02-24T22:47:55.4903941Z 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
2021-02-24T22:47:55.4904586Z Caused by: java.io.IOException: Failed to fetch job execution result
2021-02-24T22:47:55.4905378Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:169)
2021-02-24T22:47:55.4906286Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:118)
2021-02-24T22:47:55.4907230Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
2021-02-24T22:47:55.4907998Z 	... 44 more
2021-02-24T22:47:55.4908618Z Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-02-24T22:47:55.4909533Z 	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)
2021-02-24T22:47:55.4910233Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2021-02-24T22:47:55.4911035Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:167)
2021-02-24T22:47:55.4911650Z 	... 46 more
2021-02-24T22:47:55.4912141Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.
2021-02-24T22:47:55.4912942Z 	at org.apache.flink.runtime.jobmaster.JobResult.toJobExecutionResult(JobResult.java:144)
2021-02-24T22:47:55.4913955Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.lambda$getJobExecutionResult$3(MiniClusterJobClient.java:137)
2021-02-24T22:47:55.4914902Z 	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:616)
2021-02-24T22:47:55.4915610Z 	at java.util.concurrent.CompletableFuture.uniApplyStage(CompletableFuture.java:628)
2021-02-24T22:47:55.4916316Z 	at java.util.concurrent.CompletableFuture.thenApply(CompletableFuture.java:1996)
2021-02-24T22:47:55.4917123Z 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobExecutionResult(MiniClusterJobClient.java:134)
2021-02-24T22:47:55.4918172Z 	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.getAccumulatorResults(CollectResultFetcher.java:166)
2021-02-24T22:47:55.4918901Z 	... 46 more
2021-02-24T22:47:55.4919463Z Caused by: org.apache.flink.runtime.client.JobExecutionException: Not enough resources available for scheduling.
2021-02-24T22:47:55.4920378Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$determineParallelismAndAssignResources$18(AdaptiveScheduler.java:585)
2021-02-24T22:47:55.4921197Z 	at java.util.Optional.orElseThrow(Optional.java:290)
2021-02-24T22:47:55.4921999Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.determineParallelismAndAssignResources(AdaptiveScheduler.java:582)
2021-02-24T22:47:55.4923217Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.createExecutionGraphWithAvailableResources(AdaptiveScheduler.java:598)
2021-02-24T22:47:55.4924249Z 	at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.createExecutionGraphWithAvailableResources(WaitingForResources.java:105)
2021-02-24T22:47:55.4925252Z 	at org.apache.flink.runtime.scheduler.adaptive.WaitingForResources.resourceTimeout(WaitingForResources.java:99)
2021-02-24T22:47:55.4926126Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.runIfState(AdaptiveScheduler.java:898)
2021-02-24T22:47:55.4926986Z 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler.lambda$runIfState$20(AdaptiveScheduler.java:913)
2021-02-24T22:47:55.4927768Z 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
2021-02-24T22:47:55.4928488Z 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2021-02-24T22:47:55.4929180Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:440)
2021-02-24T22:47:55.4929946Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:208)
2021-02-24T22:47:55.4930725Z 	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:77)
2021-02-24T22:47:55.4931493Z 	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:158)
2021-02-24T22:47:55.4932187Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)
2021-02-24T22:47:55.4932944Z 	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)
2021-02-24T22:47:55.4933558Z 	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
2021-02-24T22:47:55.4934198Z 	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)
2021-02-24T22:47:55.4934856Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)
2021-02-24T22:47:55.4935498Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-24T22:47:55.4936136Z 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
2021-02-24T22:47:55.4936744Z 	at akka.actor.Actor$class.aroundReceive(Actor.scala:517)
2021-02-24T22:47:55.4937321Z 	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)
2021-02-24T22:47:55.4938023Z 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)
2021-02-24T22:47:55.4938569Z 	at akka.actor.ActorCell.invoke(ActorCell.scala:561)
2021-02-24T22:47:55.4939154Z 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)
2021-02-24T22:47:55.4939701Z 	at akka.dispatch.Mailbox.run(Mailbox.scala:225)
2021-02-24T22:47:55.4940205Z 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235)
2021-02-24T22:47:55.4940958Z 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
2021-02-24T22:47:55.4941788Z 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
2021-02-24T22:47:55.4942569Z 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
2021-02-24T22:47:55.4943274Z 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
2021-02-24T22:47:55.4943685Z 
{code}"	FLINK	Closed	3	1	11245	pull-request-available, test-stability
