id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
13220810	Chrome and Firefox browsers are crashing while opening Ambari UI	Ambari UI is consuming almost 1.4 GB of browser memory.	AMBARI	Resolved	3	1	1581	pull-request-available
13205714	Ambari Admin doesn't redirect user to login page if auth session becomes invalidated	When auth session becomes invalid, nothing happens (instead of redirection to login page)	AMBARI	Resolved	3	1	1581	pull-request-available
13153314	Unable to update credentials of a remotely registered cluster	"*STR*
# Register a remote cluster with Ambari server
# Go to Admin - Remote Cluster page and select the cluster
# Hit 'Update Credentials' and supply new/existing admin credentials of remote cluster
# Hit Update

Result: Credentials are not updated and UI shows an error"	AMBARI	Resolved	2	1	1581	pull-request-available
13155109	HDFS metrics page: list of namespaces is misaligned	After enabling NameNode federation, the section with namespace-scoped widgets on HDFS metric page appears. Namespaces list is misaligned relatively to select control.	AMBARI	Resolved	2	1	1581	pull-request-available
13181626	Nifi Registry install fails	"Facing issue installing Nifi Registry on HDP_HDF cluster. The create keytab step in Ambari is failing during installation. Below exception is seen in ambari logs.
 
{code}
2018-08-21 13:11:03,401 ERROR [Server Action Executor Worker 1305] CreatePrincipalsServerAction:309 - Failed to create principal,  - Failed to create new principal - no principal specified
org.apache.ambari.server.serveraction.kerberos.KerberosOperationException: Failed to create new principal - no principal specified
        at org.apache.ambari.server.serveraction.kerberos.MITKerberosOperationHandler.createPrincipal(MITKerberosOperationHandler.java:159)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.createPrincipal(CreatePrincipalsServerAction.java:268)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.processIdentity(CreatePrincipalsServerAction.java:157)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:460)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.execute(CreatePrincipalsServerAction.java:92)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:748)
2018-08-21 13:11:03,401  INFO [Server Action Executor Worker 1305] KerberosServerAction:481 - Processing identities completed.
2018-08-21 13:11:04,191 ERROR [ambari-action-scheduler] ActionScheduler:482 - Operation completely failed, aborting request id: 117
 {code}

The Ambari UI should not display any properties from Kerberos identity blocks that indicate they are referencing another Kerberos identity. There are 2 ways we know this:

- The new/preferred way: the identity block has a non-empty/non-null ""reference"" attribute
- The old (backwards compatible way): the identity block has a ""name"" attribute the starts with a '/'."	AMBARI	Resolved	2	1	1581	pull-request-available
13150260	NameNode HA: QuickLinks section issues	When NameNode HA is enabled, the QuickLink section shows the hostname of each NameNode as a hyperlink.  This is confusing since these are not meant to be clickable; they should be displayed as labels, not hyperlinks.	AMBARI	Resolved	2	1	1581	pull-request-available
13243232	[HA] RESOURCEMANAGER is not starting after adding and removing journal nodes	Adding and removing the Journal nodes, Resource manager is not starting	AMBARI	Resolved	2	1	1581	pull-request-available
13184340	Horizontal scroll bar on assign slaves and clients page is not convenient for deploy with numerous hosts	It is not convenient to configure slaves and clients for deploy with numerous hosts because horizontal scroll bar is placed in the bottom of hosts list.	AMBARI	Resolved	3	1	1581	pull-request-available
13167191	Ranger and KMS tab still present in Customize ServicesPage even after going back and deselecting them from Choose ServicesPage	"- Navigate to Customize ServicePage of UI install wizard by selecting all services from ChooseServicesPage. 
- Populate credentials tab
- Switch to Databases tab, Noticed that Ranger and Ranger KMS have multiple properties to be filled in
- Now go back to Choose Services page and deselect Ranger and Ranger KMS
- Proceed through wizard to reach Customize ServicesPage
- Ranger and KMS sub tabs are still present with errors at Databases tab. 
(None of the ranger/KMS credential properties are present in Credential tabs)

- Now even if we go back and choose Ranger and KMS - update all necessary properties for Ranger - Test Connection is hung with error"	AMBARI	Resolved	1	1	1581	pull-request-available
13166361	Ranger server password checks are not performed during Cluster Install Wizard	"While installing a new cluster, if the Ranger-specific passwords do not meet the (non-obvious) requirements, failures will occur during the install Ranger task. Once this happens, there is no way for the user to go back and fix the issue. So the issue needs to be caught sooner.

STR
# Create new Ambari 2.7.0 cluster
# Include Ranger
# Set simple passwords when prompted - for example: hadoop
# Proceed to install
# See failure"	AMBARI	Resolved	1	1	1581	pull-request-available
13158499	Alerts icon is absent in service page if no alerts present	Grayed-out bell should be displayed like in the top nav.	AMBARI	Resolved	1	1	1581	pull-request-available
13156082	Group HDFS components into the components section of the ambari service summary page	The HDFS Service Page in the new Ambari UI page displays useful information about the service components. This information would be easier to digest if all the HDFS components were grouped together at the top of the page (in the components section) as they are for the rest of the services.	AMBARI	Resolved	2	1	1581	pull-request-available
13162061	Selection information disappears when a filter is applied	"As part of AMBARI-23911, selection information was added back to Ambari. But when a filter is applied, the selected hosts section is removed and refreshed. In previous versions, the selection information persisted even when a filter was applied.
Another issue faced while automating is that there is no unique xpath to the selection information. It would be great if a class/data-qa attribute is added to the anchor tag.
"	AMBARI	Resolved	1	1	1581	pull-request-available
13160055	Remove dependency on marked.js 0.3.2 in Ambari Web	"Remove dependency on marked.js 0.3.2 in Ambari Web due to security concerns. See 
* https://nvd.nist.gov/vuln/detail/CVE-2015-1370
* https://nvd.nist.gov/vuln/detail/CVE-2017-1000427

{noformat}
[root@host ~]# ambari-server --version
2.7.0.0-519
{noformat}

{noformat}
[root@host ~]# find /usr/lib -name marked.js
/usr/lib/ambari-server/web/api-docs/lib/marked.js
{noformat}

Recommendation is to remove the dependency or upgrade to version 0.3.2-1 or the latest version, if possible. 
"	AMBARI	Resolved	1	1	1581	pull-request-available
13158569	NN Federation wizard: Restart Required Services should not restart JN and ZKFC	"STR:
Add namespace from UI. In the Restart All Services operation RM fails to start

{code}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 54, in wrapper
 return function(*args, **kwargs)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 244, in wait_for_dfs_directory_created
 raise Fail(""DFS directory '"" + dir_path + ""' does not exist !"")
Fail: DFS directory '/ats/done/' does not exist !
{code}

The above exception was the cause of the following exception:

{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 261, in <module>
 Resourcemanager().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 142, in start
 self.wait_for_dfs_directories_created(params.entity_groupfs_store_dir, params.entity_groupfs_active_dir)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 211, in wait_for_dfs_directories_created
 self.wait_for_dfs_directory_created(dir_path, ignored_dfs_dirs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 62, in wrapper
 return function(*args, **kwargs)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 244, in wait_for_dfs_directory_created
 raise Fail(""DFS directory '"" + dir_path + ""' does not exist !"")
resource_management.core.exceptions.Fail: DFS directory '/ats/done/' does not exist !
{code}

But after closing the wizard and starting all the services it started.

Also restarting history server failed on another host

{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 134, in <module>
 HistoryServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 95, in start
 skip=params.sysprep_skip_copy_tarballs_hdfs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 502, in copy_to_hdfs
 replace_existing_files=replace_existing_files,
 File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
 self.env.run()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
 self.run_action(resource, action)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
 provider_action()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 627, in action_create_on_execute
 self.action_delayed(""create"")
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 624, in action_delayed
 self.get_hdfs_resource_executor().action_delayed(action_name, self)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 333, in action_delayed
 self.action_delayed_for_nameservice(nameservice, action_name, main_resource)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 359, in action_delayed_for_nameservice
 self._create_resource()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 375, in _create_resource
 self._create_file(self.main_resource.resource.target, source=self.main_resource.resource.source, mode=self.mode)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 490, in _create_file
 self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 178, in run_command
 return self._run_command(*args, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 251, in _run_command
 raise WebHDFSCallException(err_msg, result_dict)
resource_management.libraries.providers.hdfs_resource.WebHDFSCallException: Execution of 'curl -sS -L -w '%\{http_code}' -X PUT --data-binary @/var/lib/ambari-agent/tmp/mapreduce-native-tarball-staging/mapreduce-native.tar.gz -H 'Content-Type: application/octet-stream' --negotiate -u : -k 'https://<Host>:50470/webhdfs/v1/hdp/apps/3.0.0.0-1309/mapreduce/mapreduce.tar.gz?op=CREATE&overwrite=True&permission=444'' returned status_code=403. 
{
 ""RemoteException"": {
 ""exception"": ""IOException"", 
 ""javaClassName"": ""java.io.IOException"", 
 ""message"": ""Failed to find datanode, suggest to check cluster health. excludeDatanodes=null""
 }
}
{code}

 "	AMBARI	Resolved	3	1	1581	pull-request-available
13164246	HDFS Metrics shows all blocks as 'Corrupt Replicas'	"*STR*
# Deployed a cluster with HDP-3.0 and Ambari-2.7 (or upgrade from HDP-2.6 to HDP-3.0)
# Go to HDFS Summary page
# Observe the count of blocks under 'Corrupt Replica'

*Result*
All blocks show as corrupt"	AMBARI	Resolved	1	1	1581	pull-request-available
13181905	Update styles for pre-upgrade modals	"- Move 'Preparing the Upgrade' text from the corresponding modal to its header
- Upgrade Options popup: add some vertical padding between peragraphs
- Upgrade Options popup: center the contents of upgrade type blocks and add more padding to them
- Make pre-upgrade checks popup wider
- Pre-upgrade checks popup: display warnings as regular text instead of well blocks"	AMBARI	Resolved	3	3	1581	pull-request-available
13165544	Ambari UI isn't loading in IE	After submitting login credentials, UI is stuck on loading data.	AMBARI	Resolved	1	1	1581	pull-request-available
13166702	"Components Filter in ""Manage ConfigGroup at Customize Services page"" doesn't list hosts correctly"	"- At Assign Slaves and Client page chose to have clients on all nodes
- At Customize Services Page create a config group by navigating to ManageConfigGroups
- create a new group
- While adding hosts use filter COMPONENTS to find hosts with particular component
- It doesn't list any host for HDFS client. It should actually list host based on what user selected in Assign slaves page"	AMBARI	Resolved	1	1	1581	pull-request-available
13146549	Ambari Upgrade from 2.6.0.0 - 2.7.0.0 - Dashboard is unable to load and spinner keeps spinning	"After Ambari Upgrade from 2.6.0.0 to 2.7.0.0 , the Ambari UI Dashboard is unable to load. Spinner keeps spinning,
Seems like Wizard-Data API calls are returning null and not being handled properly on the UI. Ideally, the dashboard should be shown with widgets with No data (if AMS is down)
Additional Information:
Adding few more observed issues. These might be related to the originally raised issue.
# Quick Links are not shown for services. Spinner keeps spinning.
# Component information is not shown for some services. e.g. Oozie.
# For HDFS, YARN -> DataNodes, NodeManagers keep showing Loading and never loads."	AMBARI	Resolved	1	1	1581	pull-request-available
13136235	Log Search UI: implement filter by username for access logs	User should be able to filter access logs by username	AMBARI	Resolved	3	3	1581	pull-request-available
13159388	unable to differentiate b/w new added service and existing service	While adding new service, both existing components and new components on Assign Masters step are displayed in green.	AMBARI	Resolved	1	1	1581	pull-request-available
13165320	Adding alerts to alert groups not working	"STR:
1. Navigate to Alerts> Manage Alert groups >  Create alert group e.g Test_group
2. Add alerts to the alert groups and click Save button
3. Navigate to Alerts> Manage Alert groups > Select Test_group and the alerts selected should be listed in the alert group, but only empty entry is available."	AMBARI	Resolved	1	1	1581	pull-request-available
13163243	Duplicate data is showing up when enabling NN Federation	We need to make a few changes to the HDFS Summary screen when NN Federation is enabled.  We're seeing the HDFS fs usage data duplicated in each namespace, when it should be separated out in a new section.	AMBARI	Resolved	1	1	1581	pull-request-available
13150652	Fixes for NameNode widgets on dashboard	"- Make widgets sortable
- Fix Edit Delete features
- Make widgets able to be added
- Fix pie chart colors"	AMBARI	Resolved	2	1	1581	pull-request-available
13156117	When Spark2 is selected in left pane, Spark also gets selected along with it	"When Spark2 is selected in services in left pane, Spark gets selected along with it.
The pointer should only point to Spark2 (which is the selected service)."	AMBARI	Resolved	2	1	1581	pull-request-available
13225573	Unable to move Hive metastore from one node to another 	"While moving Hive Metastore from one node to another, it is failing at test DB connection step in configure Component page.Seeing following error in ambari-agent logs:

{noformat}19-03-22 11:15:38,527 - DB connection check started.
2019-03-22 11:15:38,527 - There was an unknown error while checking database connectivity: Configuration parameter 'db_name' was not found in configurations dictionary!
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/check_host.py"", line 145, in actionexecute
    db_connection_check_structured_output = self.execute_db_connection_check(config, tmp_dir)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/check_host.py"", line 281, in execute_db_connection_check
    if db_name == DB_MYSQL:
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'db_name' was not found in configurations dictionary!
2019-03-22 11:15:38,528 - Host checks completed.
2019-03-22 11:15:38,529 - Check db_connection_check was unsuccessful. Exit code: 1. Message: Configuration parameter 'db_name' was not found in configurations dictionary!

Command failed after 1 tries
{noformat}

Reproduce steps:
# Goto Hive Service.
# Click on Actions tab and select Move Hive Metastore.
# Fill appropriate values in Move Wizard.
# Before going to configure component page from Review page, please run following ambari-server command on ambari server host:
{{ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar}}
# Click next on review page, it will fail at test DB connection step in Configure component page."	AMBARI	Resolved	1	1	1581	pull-request-available
13157865	Regenerate keytabs on single host should be an experimental feature	Regenerating keytabs on songle host should be available with experimental flag checked only ({{regenerateKeytabsOnSingleHost}})	AMBARI	Resolved	2	1	1581	pull-request-available
13153486	Header missing in Step2 Select hosts page in NN Federation Wizard	Header missing in Step2 Select hosts page in NN Federation Wizard. All other pages have proper headers	AMBARI	Resolved	3	1	1581	pull-request-available
13156604	Namespace names are converted to uppercase after selecting from dropdown	Selected namespace names are shown in uppercase where as they are actually created in lowercase and in dropdown it's in lowercase	AMBARI	Resolved	2	1	1581	pull-request-available
13210428	Cover errors utils with unit tests	"Cover the following files:
- {{utils/errors/assertions.js}}
- {{utils/errors/definitions.js}}"	AMBARI	Resolved	3	3	1581	pull-request-available
13174918	Fixes for modal with config validations and dependent properties	"Dependent configs table:
- Checkboxes should be located on the left side
- Properties names hyperlinks are broken; remove the links and add the tooltip with property description on hover instead
- Make modal wider
- Move modal closer to the top

Config validations results table:
- Limit the width and wrap the contents of 'Current Value' column
- Make modal wider"	AMBARI	Resolved	2	1	1581	pull-request-available
13169155	Add Service Wizard: Next Button is not enabled while adding Ranger after fixing an erroneous property	"Following are the steps performed
- On an already deployed cluster (without Ranger) setup ldap
- navigate to Add Service wizard to add ranger
- Use Authentication method as LDAP
- At customize services property populate all required fields
- Set one of the password to not meet the requirement (say set it as rangeradmin)
- Click Next. There will be a warning popup which says properties did not meet the requirements and user has to change it before proceeding
- Close the popup and find the property which has to be changed (It would be better if user is navigated to the property upon clicking on the warning popup itself)
- Fix this property, notice that there are no other properties which needs attention
- Still Next button is not enabled"	AMBARI	Resolved	1	1	1581	pull-request-available
13153708	Additional realm config change not working	"1) go to https://<host_name>/#/main/admin/kerberos
2) click edit
3) add Additional Realms config in ambari kerberos config tab
4) click save
5) Click OK from the popup which ask for the restart of the components
In network tab we got this error:
Request URL:https://<host_name>/api/v1/clusters/<cluster_name>/artifacts/kerberos_descriptor
Request Method:PUT
Status Code:404 Not Found
{ ""status"" : 404, ""message"" : ""org.apache.ambari.server.controller.spi.NoSuchResourceException: The requested resource doesn't exist: Artifact not found, Artifacts/cluster_name=<cluster_name> AND Artifacts/artifact_name=kerberos_descriptor"" }"	AMBARI	Resolved	2	1	1581	pull-request-available
13160014	Redo the Manage Journalnodes wizard in the context of federation changes	"STR: Deploy a cluster with multiple namespaces via Blueprint
Add JournalNode using the wizard
In the format JN step {code}sudo su cstm-hdfs -l -c 'hdfs namenode -initializeSharedEdits'{code} fails with 

{code}
Re-format filesystem in QJM to [<ip0>:8485, <ip1>:8485, <ip2>:8485] ? (Y or N) y
18/05/09 18:43:02 ERROR namenode.NameNode: Could not initialize shared edits dir
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Could not format one or more JournalNodes. 1 exceptions thrown:
<ip0>:8485: Directory /hadoop/hdfs/journal/ns1 is in an inconsistent state: Can't format the storage directory because the current directory is not empty.
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.checkEmptyCurrent(Storage.java:600)
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:683)
	at org.apache.hadoop.hdfs.qjournal.server.JNStorage.format(JNStorage.java:210)
	at org.apache.hadoop.hdfs.qjournal.server.Journal.format(Journal.java:235)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.format(JournalNodeRpcServer.java:181)
	at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.format(QJournalProtocolServerSideTranslatorPB.java:148)
	at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:27399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:286)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.format(QuorumJournalManager.java:228)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:426)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:1262)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1619)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
18/05/09 18:43:02 INFO util.ExitUtil: Exiting with status 1: ExitException
18/05/09 18:43:02 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at <host0>/<ip0>
************************************************************/
{code}

Solution
Reorganize wizard steps:
- Assign Journalnodes
- Save Namespace
- Add/Remove Journalnodes (no start JN)
- Copy Journalnode directories
- Start Journalnodes
- Start All Services"	AMBARI	Resolved	1	1	1581	pull-request-available
13164230	Alerts label isn't clickable at the service page after turning on mm for service	"Alerts label isn't clickable at the service page after turning on mm for service

STR:
# Go to the service page
# Enable mm for service
# Try to click alerts(bell) icon for the current service

Expected:
Will open alerts window

Actual
Nothing happens"	AMBARI	Resolved	1	1	1581	pull-request-available
13154004	Missing tooltip/indicator for unsupported services in Versions page when HDP-3.0 VDF is registered	"*STR*
# Deploy HDP-2.6.1.0 cluster with Ambari-2.6.x
# Upgrade Ambari to 2.7.0.0
# Register HDP-3.0 VDF
# Go to Version page and observe the small icon against services that are unsupported in 3.0 like Falcon, Flume, Spark, Mahout, Slider
# Hover mouse over them - it does not indicate any message

*Expected Result*
It would be good to say something like: This service is unsupported in the current version of the stack"	AMBARI	Resolved	2	1	1581	pull-request-available
13153412	Move NameNode wizard is stuck on Review step after enabling federation	"- Infinite spinner is displayed
- 'Next' button is disabled
- JS error is thrown: {{app.js:31501 Uncaught TypeError: Cannot read property 'indexOf' of undefined}}"	AMBARI	Resolved	2	1	1581	pull-request-available
13209921	"Ambari is not respecting host component maintenance mode when performing ""Restart All Required"" at the cluster level"	"* Put HSI in maintenance mode.
* Changed auth to local mapping in core-site.
* A bunch of services got a restart indicator.
* Triggering ""Restart All Required"" at the cluster level schedules HSI to be restarted.  HSI restart fails so the entire operation fails.  This is cumbersome because now the user has to trigger ""restart affected"" for individual services."	AMBARI	Open	3	1	1581	pull-request-available
13171459	Fix ambari-admin UI unit tests	"As of now UTs are failing:
{noformat}
#Editablelist directive Editing Updates permissions after save FAILED

#Editablelist directive Editing Show dialog window if user trying to leave page without save FAILED

#Editablelist directive Editing Saves current user in editing window if user click ""save"" FAILED

Executed 61 of 61 (3 FAILED) (0.387 secs / 0.378 secs)
{noformat}"	AMBARI	Resolved	2	1	1581	pull-request-available
13167435	Cannot distinguish components on Host Details page due to shortened display name (Ambari should show full component name on mouse over)	"Yarn has 2 version of timeline service .
1) TIMELINE SERVICE V1.5
2) TIMELINE SERVICE V2.0 READER
When both of these services are installed on one host, go to Component page . Component Page only shows first few chars ""Timeline Service... "".
Ambari UI shows ""Timeline Service.."" for both Timeline service 1.5 and 2.0 . Thus, user can not identify which is Timeline service 1.5 or 2.0
ambari should show the full component name when user brings mouse pointer on the component name.
"	AMBARI	Resolved	3	1	1581	pull-request-available
13151701	NameNode namespaces aren't displayed after HDFS page refresh	"*STR*
# Enable NameNode federation
# Go to HDFS section (summary or any other tab)
# Refresh page

*Expected result*
Namespaces data is displayed (sections on summary page, items of Service actions dropdown)

*Actual result*
- No namespaces data is displayed
- Service actions dropdown is empty
- JS error thrown: {{app.js:68529 Uncaught TypeError: Cannot read property 'contains' of undefined}}"	AMBARI	Resolved	2	1	1581	pull-request-available
13185183	Issues with tooltip containing custom time range for charts	"- The tooltip content is not formatted
- Tooltip isn't displayed on some pages"	AMBARI	Resolved	3	1	1581	pull-request-available
13159470	NN Federation wizard is stuck on step 3	NameNode Federation wizard is stuck on Review step. Infinite spinner is displayed instead of config properties to be changed, and also JS error is thrown: {{app.js:5361 Uncaught TypeError: Cannot read property 'properties' of undefined}}	AMBARI	Resolved	1	1	1581	pull-request-available
13214585	Implement additional error reporting for ambari-web unit tests	If ambari-web unit tests fail because of JS error thrown in test files outside the callbacks passed to {{it}}/{{before}}/{{beforeEach}}/{{after}}/{{afterEach}} methods, the execution is just stopped without reporting the cause. This might be confusing because failure isn't reported if none of the previous tests failed.	AMBARI	Resolved	3	3	1581	pull-request-available
13154811	UI Styling Is Incorrect On Upgrade Repositories Page	"Some of the following areas of the UI look a bit off WRT styling. 

- Upgrade repositories
-- Button size
-- Repository placement / size

- Upgrade History
- General styling and alignment"	AMBARI	Resolved	2	1	1581	pull-request-available
13239030	Cluster Information Page URL is broken	"Cluster Information Page URL is broken 
Before install HDP, login to ambari. The first page you will be navigated to is Cluster Information Page. The URL here is not what is expected

Expected: views/ADMIN_VIEW/2.7.4.0/INSTANCE/#/clusterInformation
Actual: views/ADMIN_VIEW/2.7.4.0/INSTANCE/#!/clusterInformation#%2F

"	AMBARI	Resolved	1	1	1581	pull-request-available
13158263	Unable to add Hive Metastore from Host detail Page	"In ambari-2.7.0.0-472, 

Adding Hive Metastore from Host detail page leads to the modal window being stuck and javascript errors:

Javascript errors: 
{code}
Uncaught TypeError: Cannot read property 'tag' of undefined
    at Class.loadHiveConfigs (app.js:24588)
    at Class.opt.success (app.js:181624)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{code}
"	AMBARI	Resolved	1	1	1581	pull-request-available
13211337	UI unit tests are sometimes failing	"Error is thrown:
{noformat}
Ambari Web Unit tests ""after each"" hook:
Uncaught Error: assertion failed: You must pass at least an object and event name to Ember.addListener
{noformat}"	AMBARI	Resolved	2	1	1581	pull-request-available
13138476	Quick links should be grouped by namespace	Currently quick links are grouped by host. These groups in their part should be grouped by namespace (if any).	AMBARI	Resolved	3	3	1581	pull-request-available
13160671	Selection information not present in Hosts page filtering	In the new Ambari web UI, when a host is selected by selecting the checkbox, the number of items selected is not shown. This was shown in previous releases.	AMBARI	Resolved	1	1	1581	pull-request-available
13161521	Reassign Master Wizard issues	"- On step 1, namespace ids are displayed next to hostname selects. When user selects hostname from other namespace, the displayed id isn't changed
- After completing the last step, page URL remains {{#/main/service/reassign/step6}}, with JS error thrown: {{app.js:212117 Uncaught TypeError: Cannot read property 'modal' of undefined}}. After page refresh user is broutght to the final step of wizard again, containing no relevant data"	AMBARI	Resolved	1	1	1581	pull-request-available
13152342	Ambari shows invalid passwords as plaintext	While installing an HDF cluster, if you provide a password which is not valid for NiFi toolkit (eg. it is less than 12 characters) the password you entered is displayed in the error message as plaintext.	AMBARI	Resolved	2	1	1581	pull-request-available
13209455	Duplicate title on YARN summary page	'Components' sub-title is displayed twice	AMBARI	Resolved	3	1	1581	pull-request-available
13136277	Log Search UI: move Capture button to top menu	"- Move 'Capture' button from filters panel to top menu
- Buttons order: Undo, Redo, History, Filter, Capture, Refresh"	AMBARI	Resolved	3	3	1581	pull-request-available
13234567	Ambari UI evaluates Javascript embedded in user input when adding hosts, adding remote clusters, and renaming the cluster	"Ambari's UI evaluates Javascript blocks embedded in user input when adding hosts, adding remote clusters, and renaming the cluster.

The script evaluation appears to occur before the data is submitted and saved to the Ambari database (if save at all).  Therefore, no XSS vulnerability needs to be reported since the scope of the threat is only to the interactive user at the instance the data is evaluated.

*Add remote cluster steps to reproduce:*
# Log into ambari and navigate to admin > Manage Ambari> Cluster Management>  Remote Cluster > Register Remote Cluster
# Enter malicious script in Ambari Cluster URL textbox and click on save. The output of XSS is reflected. 

*Add hosts steps to reproduce:*
# Log into ambari and navigate to Hosts> Actions>  Add New Hosts
# Enter malicious script in Target Hosts textbox and click on save. The output of XSS is reflected

*Edit cluster name steps to reproduce:*
# Log into ambari and navigate to admin > Manage Ambari> Cluster Management>  Cluster Information
# Enter malicious script in Cluster Name textbox. The output of XSS is reflected"	AMBARI	Resolved	3	1	1581	pull-request-available
13155760	Adding and deleting widgets of NameNode section on dashboard isn't persisted	"*STR*
# Enable NameNode federation
# Add or remove some NameNode widgets on dashboard
# Go to other page and retirn to dashboard, or stay on dashboard and refresh the page

*Expected result*
NameNode widgets are displayed/hidden according to the adding/removing actions described above

*Actual result*
Default set of NameNode widgets is displayed"	AMBARI	Resolved	2	1	1581	pull-request-available
13213376	Cover mappers files with unit tests	"Cover following files:
- {{ambari-web/app/mappers/alert_notification_mapper.js}}
- {{ambari-web/app/mappers/cluster_mapper.js}}
- {{ambari-web/app/mappers/stack_version_mapper.js}}
- {{ambari-web/app/mappers/widget_mapper.js}}
"	AMBARI	Resolved	3	3	1581	pull-request-available
13228123	Hive service check is failing after moving Hive Metastore from node to another using system tests.	hive-interactive-site/hive.metastore.uris is not updated after move of Hive Metastore via wizard	AMBARI	Resolved	1	1	1581	pull-request-available
13158476	Identical sets of capacity scheduler properties are displayed as unequal ones in configs comparison view	While comparing different YARN config versions, capacity scheduler values are shown as different ones. The only displayed difference is the order of properties in those sets, though even the order is the same for both versions.	AMBARI	Resolved	1	1	1581	pull-request-available
13158713	"Save button is inactive for changed filtered property after configs comparing	"	" # Go to HIVE configs.
 # Filter configs by hive.metastore.client.socket.timeout.
 # Compare current version with some previous.
 # Close comparing panel.
 # Change filtered property.

Result: ""Save"" button is inactive. Was reproduced not for any service/property."	AMBARI	Resolved	3	1	1581	pull-request-available
13162053	Next button enabled when invalid values entered	"STR:
Enter invalid value on Configs page for any property."	AMBARI	Resolved	1	1	1581	pull-request-available
13179065	Overlapping text in Recommendations in Configurations page while UI installer	Overlapping text in Centralized Configurations page: [^config-validation-overlap.png]	AMBARI	Resolved	1	1	1581	pull-request-available
13154441	No widgets displayed on dashboard after adding third NameNode namespace	After enabling NameNode federation and adding two additional namespaces, dashboards has no widgets displayed, with infinite spinner instead of them. Also, JS error is thrown: {{Uncaught TypeError: Cannot read property '1' of undefined}}	AMBARI	Resolved	2	1	1581	pull-request-available
13145156	Incorrect property for NN namespace value is used	Currently {{ClusterId}} property is used on UI to identify the namespaces. Values from {{hdfs-site}} configs should be used instead.	AMBARI	Resolved	3	1	1581	pull-request-available
13164668	Save button is disabled after adding the custom property	"Save button is disabled after adding the custom property

STR:
# Navigate to Ranger page
# Go to advanced tab of config page
# Add custom property

Expected:
Save button is enabled

Actual:
Save button is disabled"	AMBARI	Resolved	2	1	1581	pull-request-available
13133133	Log Search UI: implement log level filter	User should be able to choose the log levels visible to log feeder.	AMBARI	Resolved	3	3	1581	pull-request-available
13213890	Remove unused models from ambari-web	"Remove the following models:
- {{App.TargetCluster}}
- {{App.RootService}}
- {{App.RootServiceComponents}}
- {{App.Rack}}
- {{App.Authentication}}
- {{App.BackgroundOperation}}
- {{App.BackgroundOperationEvent}}
- {{App.Form}}
- {{App.FormField}}
- {{App.CreateUserForm}}
- {{App.ServiceAudit}}"	AMBARI	Resolved	3	3	1581	pull-request-available
13163806	Remove HDFS Disk Usage widget from host summary page	There are NameNode widgets on host summary page which are specific for the namespace containing this host. Since HDFS DIsk Usage metrics is an aggregate from all the namespaces, the corresponding widget on host page is confusing and should be removed from there.	AMBARI	Resolved	1	1	1581	pull-request-available
13153051	NN Federation Lower priority changes	"- The add metric in the HDFS Metrics needs more padding around it
- Create metric - because of menu depth the name of the metric is hard to see, we should wrap them and do anything to make sure the metric names are easily readable
- In the move master wizard we need to add the namespace before the word ""NameNode"", like ""ns1 NameNode"""	AMBARI	Resolved	2	1	1581	pull-request-available
13165592	Reordering of dashboard widgets doesn't work after enabling NN federation	"*STR*
# Enable NameNode federation
# Go to dashboard
# Try to reorder some widgets (common or namespace-specific ones)

*Expected result*
New widgets order is persisted

*Actual result*
- JS error is thrown: {{app.js:237946 Uncaught TypeError: Cannot read property 'getAttribute' of undefined}}
- New order isn't persisted"	AMBARI	Resolved	1	1	1581	pull-request-available
13166540	Add notification to the Alert Groups not working	"Add notification to the Alert Groups not working.

STR:
1. Navigate to alerts page
2. Open Manage Alert Groups
3. Select an Alert Group
4. Add Notification by clicking Add and selecting a 'pre created' Alert Notification
5. Click Save and close the popup, the alert group should be saved with selected notification type, but only an empty String is being saved.

E.g API being invoked:

{code:java}
PUT https://<host>:<port>/api/v1/clusters/<cluster>/alert_groups/4

{""AlertGroup"":{""name"":""AMBARI_INFRA_SOLR"",""definitions"":[14],""targets"":[4]}}: 
{code}

Notes:
1. Observed that the auto fill for the notification is case sensitive, which is not ideal.
2. Also observed trying to add another notification after performing the steps in STR, a 500 server error is thrown."	AMBARI	Resolved	1	1	1581	pull-request-available
13141595	Additional fixes for service summary page to support NameNode Federation	"- Add mapping of namespace id property from API
- Display NameNodes as active and standby ones
- Remove tabs to select certain namespace
- Add namespace-scoped HDFS checkpoint check and warning message before stopping and restarting NameNodes
- Too long namespace id should be cropped with adding ellipsis and beung displayed fully in tooltip"	AMBARI	Resolved	3	3	1581	pull-request-available
13228838	upgrade moment.js to v2.22.2	The application is running a vulnerable version of Moment.js.	AMBARI	Resolved	3	1	1581	pull-request-available
13148948	Issues for no NN federation case appearing after federation support implementation	"- HDFS links dropdown on dashboard isn't displayed with JS error thrown: {{Uncaught TypeError: Cannot read property 'hosts' of undefined}}
- 'N/A' / 'Not Running' is displayed for NameNode Uptime instead of actual value
- Incorrect values in Service metrics section of service summary page
- Order of dashboard widgets is not restored
- Incorrect value of NameNode RPC on host summary page"	AMBARI	Resolved	3	1	1581	pull-request-available
13170579	Ambari shows success when HBase Decommission/Recommission operations fail	If HBase Decommission/Recommission operation fails with non zero exit code, that component still gets decommissioned/recommissioned. We need to handle this failure and not transition the state to Decommissioned/Recommissioned when the respective operation fails	AMBARI	Resolved	3	1	1581	pull-request-available
13208603	Remove Flume Live widget from Ambari, alongside the Flume service during upgrade to HDP3. 	"During the ambari-managed upgrade from HDP 2.6 -> HDP 3.0, the flume service is removed (as it is no longer supported in the new stack). 

The ""Flume Live"" widget is still displayed on the Ambari home page.  

Let's remove this widget as well, when the flume service is removed. "	AMBARI	Resolved	3	3	1581	pull-request-available
13147985	Streamline Application manager's UI link is not visible in Ambari-2.7.0.0	SAM's UI link is not available on side bar using Ambari-2.7.0.0. Independently, you can open SAM's UI at port 7777 (default) just fine.	AMBARI	Resolved	3	1	1581	pull-request-available
13155678	Kafka Quick Links Shows all Views links	"The Kafka Service Summary screen shows all Ambari View instances under the ""Views"" Quick Link."	AMBARI	Resolved	2	1	1581	pull-request-available
13155413	Need better xpaths for quicklinks when multiple namespaces are present	Need better xpaths for quicklinks when multiple namespaces are present. It would be better if quicklinks are grouped by namespaces	AMBARI	Resolved	2	1	1581	pull-request-available
13159451	No relationship between generic parameter and method argument	"Call to a generic collection method contains an argument with an incompatible class from that of the collection's parameter (i.e., the type of the argument is neither a supertype nor a subtype of the corresponding generic type argument). Therefore, it is unlikely that the collection contains any objects that are equal to the method argument used here. Most likely, the wrong value is being passed to the method.

* {{String}} is incompatible with expected argument type {{Long}} in {{onHostRemoved(String)}}
* {{Long}} is incompatible with expected argument type {{AlertDefinitionEntity}} in {{AlertGroupsUpdateListener.onAlertDefinitionDeleted(AlertDefinitionDeleteEvent)}}
* {{String}} is incompatible with expected argument type {{Long}} in {{HostConfigMappingDAO.removeByClusterAndHostName(long, String)}}"	AMBARI	Resolved	3	1	1699	pull-request-available
13199537	Apply user-defined configuration for Add Service request	Continuing AMBARI-24917, apply any configuration specified in the request to override the stack defaults.	AMBARI	Resolved	2	3	1699	pull-request-available
13205748	Refactor AddServiceInfo to use a builder	"{{AddServiceInfo}} constructor could be improved by the Builder pattern.

https://github.com/apache/ambari/blob/1431ab44887c2ff7f9e94f8fcb42e24e3ce33800/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java#L45-L83"	AMBARI	Resolved	3	4	1699	pull-request-available
13167364	Provide a way to disable topology validation in cluster creation request	"The topology validation that takes place during blueprint creation request can be disabled by passing {{validate_topology=false}} as query param.  Validation also has a side-effect of adding any auto-deployable components/dependencies to the topology.

For mpack-based deployment most of the validation is moved to the cluster creation request, since mpacks may not be available prior to that.  We need to provide a way to disable validation in this request, too.  Using the same {{validate_topology=false}} flag may be the best for this purpose."	AMBARI	Resolved	3	3	1699	pull-request-available
13202791	Process Kerberos descriptor for Add Service request	The goal of this task is to process the Kerberos descriptor provided in the Add Service request, and merge it with the cluster's existing one.  Only settings for newly added services should be accepted.	AMBARI	Resolved	3	3	1699	pull-request-available
13203121	Allow unattended mpack install with purge	"The question that pops up if {{purge}} is specified prevents installing mpacks without user input.  The default answer is ""no"", which gets automatically selected in {{silent}} mode, causing the installation to be cancelled.

{noformat:title=ambari-server install-mpack --purge --silent --verbose --mpack=...}
...
CAUTION: You have specified the --purge option with --purge-list=['stack-definitions', 'mpacks']. This will replace all existing stack definitions, management packs currently installed.
Are you absolutely sure you want to perform the purge [yes/no]? (no)
ERROR: Exiting with exit code 1.
REASON: Management pack installation cancelled by user
{noformat}

The default answer should be ""no"" for regular mode, but ""yes"" for silent mode, to make unattended install possible."	AMBARI	Resolved	3	4	1699	pull-request-available
13162971	Host components API call doesn't return all host components	"Some host components are missing from the list response, but can be queried individually:

{noformat}
$ curl ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components"" | jq -r '.items[].href'
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/1
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/2
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/3
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/4
$ curl --head ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/5""
HTTP/1.1 200 OK
$ curl --head ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/6""
HTTP/1.1 200 OK
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13192257	Allow skipping Python unit tests	{{ambari-server}} Python tests can be skipped by passing {{-DskipPythonTests}} to Maven.  The goal of this task is to allow the same for {{ambari-agent}}.	AMBARI	Resolved	3	4	1699	pull-request-available
13067658	Create idempotent Ambari DB Schema SQL script for AzureDB	The schema file should be idempotent so that we can retry the script in case of exception.	AMBARI	Resolved	2	3	1699	pull-request-available
13004321	All classes recompiled due to Maven bug, even if none changed	"maven-compiler-plugin version 3.0 has a [bug|https://issues.apache.org/jira/browse/MCOMPILER-187] that causes all classes to be recompiled even if no classes have changed.

{noformat}
[INFO] --- maven-compiler-plugin:3.0:compile (default-compile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1720 source files ...
...
[INFO] --- maven-compiler-plugin:3.0:testCompile (default-testCompile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 760 source files ...
{noformat}

version 3.1+ has [another, related bug|https://issues.apache.org/jira/browse/MCOMPILER-209] that causes all classes to be compiled even if only one has changed, although it seems to correctly detect the ""no change"" case.

{noformat}
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1720 source files ...
{noformat}"	AMBARI	Resolved	4	1	1699	pull-request-available
13155460	Agent uses compressed topology upon retry	"HiveServer 2 fails to start due to:

{noformat}
2018-04-26 07:54:04,971 - call['/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server 0:2181,2:2181,4:2181 ls /hiveserver2 | grep '\[serverUri=''] {}
2018-04-26 07:54:05,693 - call returned (1, 'Exception in thread ""main"" org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
{noformat}

ZooKeeper connection string contains host indexes instead of hostnames.  This only happens upon command retry, initial run uses hostnames."	AMBARI	Resolved	2	1	1699	pull-request-available
13139436	ServiceInfo: credential_store_supported attempts to overwrite maintenance_state	"Try to update {{credential_store_supported}} property of a service:

{noformat}
$ curl -X PUT -d @- ""http://$AMBARI_SERVER:8080/api/v1/clusters/TEST/services/HDFS"" <<EOF
{ ""ServiceInfo"": { ""credential_store_supported"": ""true"" } }
EOF
HTTP/1.1 400 Bad Request
...
  ""message"" : ""java.lang.IllegalArgumentException: No enum constant org.apache.ambari.server.state.MaintenanceState.true""
{noformat}

Expected response:

{{IllegalArgumentException: Invalid arguments, cannot update credential_store_supported as it is set only via service definition.}}

The response code is the same as expected due to a coincidence.

The problem is setting the wrong property:

{noformat}
 414     o = properties.get(SERVICE_CREDENTIAL_STORE_SUPPORTED_PROPERTY_ID);
 415     if (null != o) {
 416       svcRequest.setMaintenanceState(o.toString());
 417     }
{noformat}"	AMBARI	Resolved	4	1	1699	pull-request-available
13201811	APT/DPKG existence check broken for packages with long names	"AMBARI-24632 addressed package existence check for system packages.  However, it may still not work correctly for packages with longer names, depending on the environment:

{noformat}
output-2.txt:2018-12-01 19:33:54,576 - Installing package unzip ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install unzip')
output-2.txt:2018-12-01 19:34:02,873 - Installing package hdp-select ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install hdp-select')
output-2.txt:2018-12-01 19:34:05,443 - Installing package zookeeper-3-0-1-0-187 ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187')
output-2.txt:2018-12-01 19:34:08,832 - Installing package zookeeper-3-0-1-0-187-server ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187-server')
output-3.txt:2018-12-01 19:34:14,450 - Installing package zookeeper-3-0-1-0-187 ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187')
output-3.txt:2018-12-01 19:34:16,253 - Installing package zookeeper-3-0-1-0-187-server ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187-server')
{noformat}

This was reproduced with non-root agent user."	AMBARI	Resolved	4	1	1699	pull-request-available
13026442	When disabling Kerberos, rm should be used when deleting the Ambari Server keytab file(s)	"When disabling Kerberos, {{rm}} should be used when deleting the Ambari Server keytab file(s) rather than the Java {{java.io.File#delete}} method. This is to allow for the file to be removed properly when Ambari is not executed as _root_.  

Currently the Ambari server keytab files may not be removed due to permission issues. 
"	AMBARI	Resolved	3	1	1699	kerberos, sudo
13155355	Adding new namespace fails at Reconfigure Services for HDFS 2	"Add New HDFS Namespace wizard fails at Reconfigure Services step when trying to add new namespace ""ns2"" (existing namespace is ""TEST"") in a HDP 2.6 cluster.

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_client.py"", line 78, in <module>
    HdfsClient().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_client.py"", line 35, in install
    import params
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py"", line 330, in <module>
    if hostname.lower() in nn_host.lower() or public_hostname.lower() in nn_host.lower():
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'dfs.namenode.rpc-address.TEST,ns2.nn1' was not found in configurations dictionary!
{noformat}"	AMBARI	Resolved	2	1	1699	pull-request-available
13198530	"Handle complex ""Add Service"" request in ServiceResourceProvider"	"Related to AMBARI-24881, change {{ServiceResourceProvider}} to handle the complex ""Add Service"" request.  So far only stub logic is needed, details of the service creation will be filled later."	AMBARI	Resolved	3	3	1699	pull-request-available
13097579	test_kms_server timing issue	"Python unit test in {{test_kms_server}} occasionally fails due to timing.  Expected and actual output use two separate calls to get current time, which may be different if happens to be executed around the turn of a second.

{noformat}
FAIL: test_start_secured (test_kms_server.TestRangerKMS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/stacks/2.5/RANGER_KMS/test_kms_server.py"", line 522, in test_start_secured
    mode = 0644
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 330, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'owner': 'kms', 'content': '<ranger>\n<enabled>2017-08-25 11:02:46</enabled>\n< [truncated]... != {'content': '<ranger>\n<enabled>2017-08-25 11:02:45</enabled>\n</ranger>', 'owne [truncated]...
- {'content': '<ranger>\n<enabled>2017-08-25 11:02:46</enabled>\n</ranger>',
?                                                   ^

+ {'content': '<ranger>\n<enabled>2017-08-25 11:02:45</enabled>\n</ranger>',
?                                                   ^

-  'group': 'kms',
+  'group': u'kms',
?           +

   'mode': 420,
-  'owner': 'kms'}
+  'owner': u'kms'}
?           +

{noformat}

Affected code: {{test_start_default}} and {{test_start_secured}} in {{ambari-server/src/test/python/stacks/2.5/RANGER_KMS/test_kms_server.py}}."	AMBARI	Resolved	4	1	1699	pull-request-available
13169091	Blueprint deployment with custom service name	Blueprint deployment with custom service name fails, because some parts of Ambari mix and match service name (eg. zk1) and service type (eg. ZOOKEEPER).	AMBARI	Resolved	3	1	1699	pull-request-available
13076196	Code cleanup	Clean up Ambari Server source code warnings.	AMBARI	Resolved	3	15	1699	pull-request-available
13181544	Allow skipping Hive Metastore schema creation for sysprepped cluster	Similar to AMBARI-24540, Hive Metastore DB schema may be manually pre-created to save time during initial service start. However, {{schematool}} could still take quite some time to confirm that the schema exists. The goal of this change is to allow users who pre-create Hive Metastore DB schema to make Ambari skip managing the DB (create or check existence).	AMBARI	Resolved	3	4	1699	pull-request-available
13184530	Allow skipping package operations for LZO on sysprepped hosts	LZO packages may be pre-installed in sysprepped environments, but Ambari still manages the repo and checks for existence of the packages, which takes time.  The goal of this change is to allow users who pre-install packages to skip package manager operations for LZO packages, too.	AMBARI	Resolved	3	4	1699	pull-request-available
13231418	Hive Server Interactive process alert triggered in HA setup	"With 2 HSI instances deployed in HA setup the _HiveServer2 Interactive Process_ alert is triggered for the inactive one:

{noformat}
Connecting to jdbc:hive2://...:10501/;transportMode=http;httpPath=cliservice
19/05/02 09:13:11 [main]: WARN jdbc.HiveConnection: Failed to connect to ...:10501
Error: Could not open client transport with JDBC Uri: jdbc:hive2://...:10501/;transportMode=http;httpPath=cliservice: Cannot open sessions on an inactive HS2 instance; use service discovery to connect (state=08S01,code=0)
Cannot run commands specified using -e. No current connection
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13106881	Package install fails on Debian7 with 'EMPTY_FILE' global variable not defined	"*STR*
# Deployed cluster with Ambari version: 2.5.1.0-159 and HDP version: 2.5.6.0-40
# Upgrade Ambari to 2.6.0.0-173
# Register HDP Version 2.6.3.0-151 and try to install the packages

*Result*
Package install fails with below error
{code}
2017-10-03 05:27:09,454 - Will install packages for repository version 2.6.3.0-151
2017-10-03 05:27:09,455 - Repository['HDP-2.6-repo-51'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2017-10-03 05:27:09,460 - File['/tmp/tmpPriIC1'] {'content': 'deb http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151 HDP main'}
2017-10-03 05:27:09,461 - Writing File['/tmp/tmpPriIC1'] because contents don't match
2017-10-03 05:27:09,461 - File['/tmp/tmpWBU9KY'] {'content': StaticFile('/etc/apt/sources.list.d/ambari-hdp-51.list')}
2017-10-03 05:27:09,462 - Writing File['/tmp/tmpWBU9KY'] because contents don't match
2017-10-03 05:27:09,462 - File['/etc/apt/sources.list.d/ambari-hdp-51.list'] {'content': StaticFile('/tmp/tmpPriIC1')}
2017-10-03 05:27:09,463 - Writing File['/etc/apt/sources.list.d/ambari-hdp-51.list'] because contents don't match
2017-10-03 05:27:09,463 - checked_call[['apt-get', 'update', '-qq', '-o', 'Dir::Etc::sourcelist=sources.list.d/ambari-hdp-51.list', '-o', 'Dir::Etc::sourceparts=-', '-o', 'APT::Get::List-Cleanup=0']] {'sudo': True, 'quiet': False}
2017-10-03 05:27:10,166 - checked_call returned (0, '')
2017-10-03 05:27:10,169 - Repository['HDP-UTILS-1.1.0.21-repo-51'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian7', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2017-10-03 05:27:10,176 - File['/tmp/tmpnjTvFL'] {'content': 'deb http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151 HDP main\ndeb http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian7 HDP-UTILS main'}
2017-10-03 05:27:10,176 - Writing File['/tmp/tmpnjTvFL'] because contents don't match
2017-10-03 05:27:10,177 - File['/tmp/tmprvUQz1'] {'content': StaticFile('/etc/apt/sources.list.d/ambari-hdp-51.list')}
2017-10-03 05:27:10,178 - Writing File['/tmp/tmprvUQz1'] because contents don't match
2017-10-03 05:27:10,179 - File['/etc/apt/sources.list.d/ambari-hdp-51.list'] {'content': StaticFile('/tmp/tmpnjTvFL')}
2017-10-03 05:27:10,180 - Writing File['/etc/apt/sources.list.d/ambari-hdp-51.list'] because contents don't match
2017-10-03 05:27:10,181 - checked_call[['apt-get', 'update', '-qq', '-o', 'Dir::Etc::sourcelist=sources.list.d/ambari-hdp-51.list', '-o', 'Dir::Etc::sourceparts=-', '-o', 'APT::Get::List-Cleanup=0']] {'sudo': True, 'quiet': False}
2017-10-03 05:27:11,641 - checked_call returned (0, '')
2017-10-03 05:27:11,642 - call[('ambari-python-wrap', '/usr/bin/hdp-select', 'versions')] {}
2017-10-03 05:27:11,681 - call returned (0, '2.5.6.0-40')
2017-10-03 05:27:11,683 - Package['hdp-select'] {'retry_on_repo_unavailability': True, 'retry_count': 5, 'use_repos': ['HDP-2.6-repo-51', 'HDP-UTILS-1.1.0.21-repo-51'], 'action': ['upgrade']}
2017-10-03 05:27:11,683 - Temporal sources directory was created: /tmp/tmpN_6yuF-ambari-apt-sources-d
2017-10-03 05:27:11,683 - Package Manager failed to install packages. Error: global name 'EMPTY_FILE' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 335, in install_packages
    retry_count=agent_stack_retry_count)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 57, in action_upgrade
    self.upgrade_package(package_name, self.resource.use_repos, self.resource.skip_repos)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 74, in wrapper
    return function_to_decorate(self, name, *args[2:])
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 385, in upgrade_package
    return self.install_package(name, use_repos, skip_repos, is_upgrade)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 74, in wrapper
    return function_to_decorate(self, name, *args[2:])
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 352, in install_package
    cmd = cmd + ['-o', 'Dir::Etc::SourceList=%s' % EMPTY_FILE]
NameError: global name 'EMPTY_FILE' is not defined
{code}"	AMBARI	Resolved	1	1	1699	express_upgrade
13186544	Workaround for non-atomic directory creation	{{before-*}} hooks create a few directories.  If parallel agent execution is enabled tasks may need to be retried, because directory creation is not atomic (see AMBARI-24670).  This causes delays during cluster deployment.  While the underlying problem is being fixed, the goal of this task is to provide a workaround by creating the directory during sysprep phase.	AMBARI	Resolved	3	3	1699	pull-request-available
13157173	Corrupt mapreduce/tez tar.gz may be uploaded to HDFS if parallel execution is enabled	"If parallel_execution on Ambari Agent is enabled, two components (History Server and Hive Server) may create and upload the MapReduce and Tez archives concurrently.  This could result in a corrupt uploaded file, preventing Hive Server from starting.

{noformat:title=output-32.txt}
2018-05-04 07:24:40,421 - Creating a new Tez tarball at /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz
...
2018-05-04 07:24:49,751 - Creating new file /hdp/apps/2.6.4.5-2/tez/tez.tar.gz in DFS
2018-05-04 07:24:49,753 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '""'""'%{http_code}'""'""' -X PUT --data-binary @/var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz -H '""'""'Content-Type: application/octet-stream'""'""' '""'""'http://localhost:50070/webhdfs/v1/hdp/apps/2.6.4.5-2/tez/tez.tar.gz?op=CREATE&user.name=hdfs&overwrite=True&permission=444'""'""' 1>/tmp/tmpoEINHo 2>/tmp/tmpkjlNxP''] {'logoutput': None, 'quiet': False}
2018-05-04 07:24:51,629 - call returned (0, '')
{noformat}

{noformat:title=output-30.txt}
2018-05-04 07:24:46,818 - Creating a new Tez tarball at /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz
...
2018-05-04 07:24:54,117 - DFS file /hdp/apps/2.6.4.5-2/tez/tez.tar.gz is identical to /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz, skipping the copying
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13188144	Implement support for Minimal Blueprint Export	"Implement a new mode for Blueprint exports, a ""minimal"" mode in which only modified configuration is included in the exported Blueprint. The original ""full"" Blueprint export should still be possible, in order to maintain backwards compatibility."	AMBARI	Resolved	3	4	1699	pull-request-available
13131695	Blueprints do not handle some failures properly	"Failures in the cluster configuration task and topology host tasks during blueprint cluster deployment or upscaling are not visible via request status. The logical request stays PENDING even after Ambari Server gave up retrying. Both the fact that it no longer makes progress and any reason of the failure can be seen only in {{ambari-server.log}}.

Some ways to reproduce (all via blueprints):
 * Create cluster with ZooKeeper and HDFS, but omit the NAMENODE component
 * Create a secure cluster with wrong Kerberos credentials
 * Create a secure cluster without storing Kerberos credentials, restart Ambari Server, add a new node"	AMBARI	Resolved	2	1	1699	blueprints, pull-request-available
13193860	TrimmingStrategy implementations should be singletons	{{BlueprintConfigurationProcessor}} trims properties during deployment using various implementations of {{TrimmingStrategy}}.  A new strategy object is created for each property.  This is unnecessary, since all current implementations are stateless.	AMBARI	Resolved	4	4	1699	pull-request-available
13186644	Upgrade checkstyle version to 8.9	"Currently we use checkstyle 6.19 which is very old.

We should upgrade to 8.9 release."	AMBARI	Resolved	3	3	1699	pull-request-available
13191468	Wrong settings in exported blueprint	"Cluster blueprint export is broken wrt. the {{settings}} section.

# Only one component per service is included.
# Default setting value assumed to be {{false}}, ignoring component-level stack definition

STR:

# Disable ""Auto Start"" for Metrics Collector, for which it is enabled by default per stack definition
# Enable ""Auto Start"" for both DataNode and NameNode
# Export blueprint

Result: 

{noformat:title=http://$AMBARI_SERVER:8080/api/v1/clusters/TEST?format=blueprint}
  ""settings"" : [
    {
      ""recovery_settings"" : [
        {
          ""recovery_enabled"" : ""true""
        }
      ]
    },
    {
      ""service_settings"" : [
        {
          ""recovery_enabled"" : ""true"",
          ""name"" : ""HDFS""
        }
      ]
    },
    {
      ""component_settings"" : [
        {
          ""recovery_enabled"" : ""true"",
          ""name"" : ""NAMENODE""
        }
      ]
    }
{noformat}

Problem: creating a cluster using the exported blueprint results in different settings than the original cluster.  ""Auto Start"" would be disabled for DataNode and enabled for Metrics Collector per stack defaults."	AMBARI	Resolved	3	1	1699	pull-request-available
13130968	Cannot scale cluster if Ambari Server restarted since blueprint cluster creation	"STR:
# Create cluster using blueprint
# Restart Ambari Server
# Install Ambari Agent on new host and register with server
# Add the new host via API request
# Start Ambari Agent on the new host

Result: Ambari Server accepts the scale request, but does not proceed to install/start components on the new host

The problem is that AMBARI-22012 fixed a timing/ordering issue by introducing an executor, which is started on ""cluster configured"" event during blueprint cluster creation. If Ambari Server is restarted afterwards, the executor will stay stopped, hence tasks for scale requests are never executed."	AMBARI	Resolved	1	1	1699	blueprints, pull-request-available
13181152	Allow skipping Oozie DB schema creation for sysprepped cluster	Oozie DB schema may be manually pre-created to save time during initial service start.  However, {{ooziedb.sh}} could still take quite some time to confirm that the schema exists.  The goal of this change is to allow users who pre-create Oozie DB schema to make Ambari skip managing the DB (create or check existence).	AMBARI	Resolved	3	4	1699	pull-request-available
13202787	Support component-level provision_action in Add Service request	AMBARI-24988 added support for request-level {{provision_action}} to the Add Service request.  The goal of this task is to add support at the host component-level, similar to blueprints.	AMBARI	Resolved	3	4	1699	pull-request-available
13146045	Maven cleanup	Clean up some of the duplications in {{pom.xml}}.	AMBARI	Resolved	3	3	1699	pull-request-available
13202505	Disallow changing Kerberos-related configs in Add Service request	Ambari should not allow a user to set configurations in the {{kerberos-env}} and {{krb5-conf}} configuration types within the complex Add Service request.  Generally, setting these configurations on an existing Kerberized cluster could be problematic, since existing services already depend upon the existing configuration to interact with the KDC.	AMBARI	Resolved	3	3	1699	pull-request-available
13204025	Handle blueprint/VDF stack version mismatch	If a repository version is explicitly specified in the cluster creation request (using {{repository_version_id}} or {{repository_version}}), then Ambari should reject the cluster creation request if the blueprint and the VDF stacks are different (eg. HDP 2.6 vs HDP 3.0).  If it allowed deployment, the cluster would run into various errors due to the mismatch (eg. UI glitches, attempting to install the wrong package, etc.)	AMBARI	Resolved	3	4	1699	pull-request-available
13155076	dfs_ha_initial_* properties should be removed after installation	{{dfs_ha_initial_namenode_active}} and {{dfs_ha_initial_namenode_standby}} properties are not removed from {{hadoop-env}} config after installing a cluster with NameNode HA using blueprints.	AMBARI	Resolved	3	1	1699	pull-request-available
13203295	Duplicate kerberos_descriptor name reported as HTTP 500	"Duplicate {{kerberos_descriptor}} name is reported as HTTP 500 Server Error.  It should result in HTTP 409 Conflict.

{noformat}
$ curl -X POST -d @metrics_descriptor.json ""http://${AMBARI_SERVER}:8080/api/v1/kerberos_descriptors/metrics_descriptor""
HTTP/1.1 201 Created
$ curl -X POST -d @metrics_descriptor.json ""http://${AMBARI_SERVER}:8080/api/v1/kerberos_descriptors/metrics_descriptor""
HTTP/1.1 500 Server Error
...
  Detail: Key (kerberos_descriptor_name)=(metrics_descriptor) already exists.
{noformat}"	AMBARI	Resolved	4	1	1699	pull-request-available
13194905	Make Ambaripreupload.py more configurable	When testing {{Ambaripreupload.py}} locally, one needs to work around some hardcoded values (JDBC driver, DFS type). The goal of this change is to make the script more configurable.	AMBARI	Resolved	3	4	1699	pull-request-available
13194086	Set path encoding for GCS	Add {{fs.gs.path.encoding}} to {{core-site}} with default value of {{uri-path}}.	AMBARI	Resolved	3	3	1699	pull-request-available
13158510	TimelineMetricsFilterTest failure if dir name contains @	"[PullRequest Builder|https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/] is occasionally running into failure in the following 2 test cases:

{noformat}
testHybridFilter(org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest)  Time elapsed: 0.725 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testHybridFilter(TimelineMetricsFilterTest.java:221)

testMetricWhitelisting(org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest)  Time elapsed: 0.035 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testMetricWhitelisting(TimelineMetricsFilterTest.java:78)
{noformat}

Looking at the logs, it only happens if two builds are running on the same Jenkins node concurrently (workspace dir has suffix {{@2}}), but timing doesn't matter, the two concurrent builds can be at completely different stages.  The problem is caused by not finding the whitelist file due to URL escaping ({{@}} is converted to {{%40}}):

{noformat}
FileNotFoundException: Ambari-Github-PullRequest-Builder%402/ambari-metrics/ambari-metrics-timelineservice/target/test-classes/test_data/metric_whitelist.dat (No such file or directory)
{noformat}

https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2225/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2220/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2214/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2195/"	AMBARI	Resolved	3	1	1699	pull-request-available
13228253	Blueprint processor should support multiple ZooKeeper nodes for livy.server.recovery.state-store.url	"Livy can store its state in ZooKeeper for recovery ({{""livy.server.recovery.state-store"": ""zookeeper""}}).  In this case {{livy.server.recovery.state-store.url}} should point to the ZooKeeper quorum.

Setting {{""livy.server.recovery.state-store.url"": ""%HOSTGROUP::quorum%:2181""}} should be enough to specify ZK servers in a host group named {{quorum}}.  However, since it is not explicitly handled in {{BlueprintConfigurationProcessor}}, the placeholder is replaced only with a single address."	AMBARI	Resolved	4	4	1699	pull-request-available
13166709	Failed to force_non_member_install a stack version on hosts	"The ability to pre-install packages on hosts using the following API request is broken since 2.6.0:

{noformat}
$ curl -X POST -d '{ ""HostStackVersions"": { ""repository_version"": ""2.6.1.0-129"", ""stack"": ""HDP"", ""version"": ""2.6"", ""cluster_name"": ""TEST"", ""force_non_member_install"": true, ""components"": [ { ""name"" : ""ZOOKEEPER_SERVER"" }, { ""name"": ""ZOOKEEPER_CLIENT"" } ] } }' http://localhost:8080/api/v1/hosts/${hostname}/stack_versions
{noformat}

The request is accepted, but:

* on 2.6.0: package installation (1st task) fails due to missing {{stack_name}} and {{stack_version}}
* on 2.6.1: ""set all"" (2nd task) fails for the same reason"	AMBARI	Resolved	3	1	1699	pull-request-available
13194526	Make Grafana connection attempts and retry delay configurable	Metrics Grafana HTTP requests are attempted at most 15 times in case of failure, with 20 seconds delay each time.  The goal of this change is to make both the number of attempts and length of the delay configurable.  Shorter retry delay could be useful to reduce overall startup time.	AMBARI	Resolved	4	4	1699	pull-request-available
13213024	Scale hosts ignores rack_info	"The following request is accepted by Ambari, but the new host is assigned to {{/default-rack}}:

{noformat}
$ curl -X POST -d @- ""http://${AMBARI_SERVER}:8080/api/v1/clusters/TEST/hosts"" <<EOF
[
    {
        ""blueprint"": ""blue"",
        ""host_group"": ""node"",
        ""host_name"": ""c7402.ambari.apache.org"",
        ""rack_info"": ""/rack/a""
    }
]
EOF
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13175691	Unit test error in ambari-metrics-timelineservice: metric_blacklist.dat (No such file or directory)	"Many branch-2.7 test runs from Github are failing with 

{noformat}
2018-07-30 21:33:23,838 ERROR [main] timeline.TimelineMetricsFilter (TimelineMetricsFilter.java:readMetricWhitelistFromFile(132)) - Unable to parse metric file
java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder%402/ambari-metrics/ambari-metrics-timelineservice/target/test-classes/test_data/metric_blacklist.dat (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilter.readMetricWhitelistFromFile(TimelineMetricsFilter.java:117)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilter.initializeMetricFilter(TimelineMetricsFilter.java:85)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testMetricBlacklisting(TimelineMetricsFilterTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:344)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:269)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:240)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:184)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:286)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:240)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
2018-07-30 21:33:23,842 INFO  [main] timeline.TimelineMetricsFilter (TimelineMetricsFilter.java:initializeMetricFilter(86)) - Blacklisting 0 metrics
{noformat}
"	AMBARI	Resolved	2	1	1699	unit-test
13144834	Dependency check should ignore unknown services	"{{BlueprintValidatorImpl}} throws NPE if encounters an unknown component.

{noformat}
	at org.apache.ambari.server.topology.BlueprintValidatorImpl.validateHostGroup(BlueprintValidatorImpl.java:250)
	at org.apache.ambari.server.topology.BlueprintValidatorImpl.validateTopology(BlueprintValidatorImpl.java:70)
	at org.apache.ambari.server.topology.BlueprintImpl.validateTopology(BlueprintImpl.java:332)
{noformat}"	AMBARI	Resolved	2	1	1699	pull-request-available
13076202	Eliminate Maven warnings	"Get rid of as many Maven warnings as possible:

{noformat}
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-web:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:exec-maven-plugin @ line 161, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-admin:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:exec-maven-plugin is missing. @ line 91, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-common:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-hadoop-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-flume-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-kafka-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-storm-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-storm-sink-legacy:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 252, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-host-monitoring:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:exec-maven-plugin is missing. @ line 86, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 110, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-grafana:pom:2.1.0.0.0
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 64, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-assembly:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-host-aggregator:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-server:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-antrun-plugin @ line 699, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-antrun-plugin @ line 735, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:exec-maven-plugin @ line 824, column 15
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:properties-maven-plugin is missing. @ line 469, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-funtest:jar:2.0.0.0-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.httpcomponents:httpclient:jar -> version 4.2.5 vs 4.5.2 @ line 559, column 17
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-failsafe-plugin is missing. @ line 52, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-agent:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:properties-maven-plugin is missing. @ line 207, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13137634	Upgrade Apache Rat to 0.12	"Apache Rat 0.12 can automatically exclude patterns from SCM ignore files ({{.gitignore}} in Ambari's case) from its check.  See RAT-171 for details.

Benefits:

# No need to duplicate the exclude information in the {{apache-rat-plugin}}'s configuration.
# Improved run time for non-clean workdir, since it can exclude everything under {{target}}.
"	AMBARI	Resolved	3	4	1699	pull-request-available
13187182	Cannot deploy cluster without HDFS_CLIENT	"The attached blueprint cannot be deployed, because {{hadoop-env.sh}} is not saved by Ambari, because it requires {{HDFS_CLIENT}} component on at least one host:

https://github.com/apache/ambari/blob/8d145e0c04917866fd76690688826cf44065370e/ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/hook.py#L32-L33

Datanode start fails with:

{noformat}
ExecutionFailed: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/3.0.0.0-1634/hadoop/bin/hdfs --config /usr/hdp/3.0.0.0-1634/hadoop/conf --daemon start datanode'' returned 1. ERROR: JAVA_HOME is not set and could not be found.
{noformat}

This appears in output from {{after-INSTALL}} hook:

{noformat}
Parameter hadoop_conf_dir is missing or directory does not exist. This is expected if this host does not have any Hadoop components.
{noformat}

Additionally, host groups without {{HDFS_CLIENT}} cannot be scaled up."	AMBARI	Resolved	2	1	1699	pull-request-available
13193773	Set cloud storage tracking property for GCS	Similarly to AMBARI-23329, set {{fs.gs.application.name.suffix}} for GCS.	AMBARI	Resolved	3	3	1699	pull-request-available
13203736	Layout recommendation adds unwanted components	"STR:

# Install ZooKeeper
# Try to add Metrics Collector and Metrics Monitor using Add Service request

{noformat}
{
  ""operation_type"": ""ADD_SERVICE"",
  ""components"": [
    { ""name"": ""METRICS_COLLECTOR"", ""hosts"": [ { ""fqdn"": ""c7401.ambari.apache.org"" } ] },
    { ""name"": ""METRICS_MONITOR"", ""hosts"": [ { ""fqdn"": ""c7401.ambari.apache.org"" } ] }
  ]
}
{noformat}

Result: Metrics Grafana is also installed.  It is being added by the layout recommendation, which should not be invoked if full layout is explicitly specified in the request (ie. no {{""services""}} part is present)."	AMBARI	Resolved	3	1	1699	pull-request-available
13195477	NPE in default host group replacement	"Cluster deployment fails if some property value is null:

{noformat}
2018-10-31 17:38:44,579  INFO [pool-3-thread-1] AsyncCallableService:100 - Task ConfigureClusterTask exception during execution
java.lang.NullPointerException
	at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
	at java.util.regex.Matcher.reset(Matcher.java:309)
	at java.util.regex.Matcher.<init>(Matcher.java:229)
	at java.util.regex.Pattern.matcher(Pattern.java:1093)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor$HostGroupUpdater.updateForClusterCreate(BlueprintConfigurationProcessor.java:1754)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.updateValue(BlueprintConfigurationProcessor.java:739)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.lambda$null$7(BlueprintConfigurationProcessor.java:705)
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1044)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.lambda$applyDefaultUpdater$8(BlueprintConfigurationProcessor.java:700)
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1044)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.applyDefaultUpdater(BlueprintConfigurationProcessor.java:697)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doGeneralPropertyUpdatesForClusterCreate(BlueprintConfigurationProcessor.java:664)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doUpdateForClusterCreate(BlueprintConfigurationProcessor.java:443)
	at org.apache.ambari.server.topology.ClusterConfigurationRequest.process(ClusterConfigurationRequest.java:152)
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13231484	"Admin View build fails due to unavailable npm package ""ecstatic"""	"{{ambari-admin}} build started failing with:

{noformat:title=https://builds.apache.org/job/Ambari-branch-2.7/494/consoleText}
[ERROR] npm ERR! notarget No compatible version found: ecstatic@'>=0.4.0 <0.5.0'
[ERROR] npm ERR! notarget Valid install targets:
[ERROR] npm ERR! notarget [""4.1.2""]
...
[ERROR] npm ERR! notarget It was specified as a dependency of 'http-server'
{noformat}

It seems old versions of [ecstatic|https://www.npmjs.com/package/ecstatic?activeTab=versions] have been [removed|https://github.com/jfhbrook/node-ecstatic/issues/255] due to a security issue."	AMBARI	Resolved	3	1	1699	pull-request-available
13149983	Improve parallel start performance	Reduce the amount of work performed while holding {{configs_lock}}.	AMBARI	Resolved	4	4	1699	pull-request-available
13145085	SQL errors in Oracle schema create script	"During deploy on debian 7 with oracle database ambari server reported  many errors related to SQL queries:

{noformat}
14 Mar 2018 10:51:08,721 ERROR [ExecutionScheduler_QuartzSchedulerThread] JobStoreTX:3652 - Couldn't rollback jdbc connection. No more data to read from socket
java.sql.SQLRecoverableException: No more data to read from socket
{noformat}

{noformat}
Internal Exception: java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist

Error Code: 942
Call: INSERT INTO hostcomponentstate (id, current_state, last_live_state, upgrade_state, version, host_id, service_name, cluster_id, component_name) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        bind => [9 parameters bound]
{noformat}"	AMBARI	Resolved	1	1	1699	pull-request-available
13170160	Fix FindBugs warnings	"{noformat}
[INFO] --- findbugs-maven-plugin:3.0.3:check (default) @ ambari-server ---
...
[INFO] Total bugs: 1700
{noformat}"	AMBARI	Resolved	4	4	1699	pull-request-available
13200186	Support for complex Add Service request in secure cluster	Support adding services using the complex Add Service request in clusters that have Kerberos enabled.	AMBARI	Resolved	3	3	1699	pull-request-available
13189173	Ambari Server stops with Java 9 due to Guice error	"Ambari Server stopped a few seconds after start with the following error:

 
{code:title=/var/log/ambari-server/ambari-server.out}
Exception in thread ""main"" com.google.inject.CreationException: Unable to create injector, see the following errors:

1) No scope is bound to org.apache.ambari.server.AmbariService.
at org.apache.ambari.server.state.services.MetricsRetrievalService.class(MetricsRetrievalService.java:85)
while locating org.apache.ambari.server.state.services.MetricsRetrievalService
for field at org.apache.ambari.server.controller.jmx.JMXPropertyProvider.metricsRetrievalService(JMXPropertyProvider.java:88)
at org.apache.ambari.server.controller.metrics.MetricPropertyProviderFactory.createJMXPropertyProvider(MetricPropertyProviderFactory.java:1)
at com.google.inject.assistedinject.FactoryProvider2.initialize(FactoryProvider2.java:666)
at com.google.inject.assistedinject.FactoryModuleBuilder$1.configure(FactoryModuleBuilder.java:335) (via modules: org.apache.ambari.server.controller.ControllerModule -> com.google.inject.assistedinject.FactoryModuleBuilder$1)

1 error
at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)
at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:176)
at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)
at com.google.inject.Guice.createInjector(Guice.java:99)
at com.google.inject.Guice.createInjector(Guice.java:73)
at com.google.inject.Guice.createInjector(Guice.java:62)
at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:1079)
~
~
""/var/log/ambari-server/ambari-server.out"" 41L, 6558C{code}

{code}
cat /etc/ambari-server/conf/ambari.properties | grep 'java\.home'
java.home=/usr/java/jdk-9.0.4/
stack.java.home=/usr/java/jdk-9.0.4/
{code}"	AMBARI	Resolved	3	1	1699	pull-request-available
13202203	Support provision_action in complex Add Service request	Support {{provision_action}} in complex Add Service request, similarly to blueprints.	AMBARI	Resolved	3	4	1699	pull-request-available
13162208	Spark2 Thrift Server fails to start if LLAP and parallel execution are enabled	"STR:

# Enable parallel execution in Ambari Agent
# Deploy cluster via blueprint, placing Spark2 Thrift Server on the same node as Hive Server Interactive

Result: Spark2 Thrift Server runs into the following error during startup:

{noformat}
ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper
...
KeeperErrorCode = NoNode for /hiveserver2-hive2
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13167998	Error processing agent reports due to wrong stack usage	"{noformat}
22 May 2018 15:33:05,830 ERROR [agent-report-processor-0] AgentReportsProcessor:90 - Error processing agent reports
org.apache.ambari.server.StackAccessException: Stack data, stackName=HDPCORE, stackVersion=1.0.0-b368, stackServiceName=HBASE
        at org.apache.ambari.server.api.services.AmbariMetaInfo.getService(AmbariMetaInfo.java:604)
        at org.apache.ambari.server.api.services.AmbariMetaInfo.getComponent(AmbariMetaInfo.java:357)
        at org.apache.ambari.server.state.host.HostImpl.calculateHostStatus(HostImpl.java:1247)
        at org.apache.ambari.server.agent.HeartbeatProcessor.processHostStatus(HeartbeatProcessor.java:300)
        at org.apache.ambari.server.agent.HeartBeatHandler.handleCommandReportStatus(HeartBeatHandler.java:275)
        at org.apache.ambari.server.agent.AgentReportsProcessor$AgentReportProcessingTask.run(AgentReportsProcessor.java:83)
{noformat}
"	AMBARI	Resolved	3	1	1699	pull-request-available
13153361	Enable NameNode HA fails after Ambari Upgrade due to AttributeError	"STR

# Install a Ambari Cluster with Ambari 2.6.1
# Upgrade to Ambari 2.7
# Try to enable NameNode HA

Result:

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/journalnode.py"", line 143, in <module>
    JournalNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/journalnode.py"", line 39, in install
    import params
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py"", line 330, in <module>
    if hostname.lower() in nn_host.lower() or public_hostname.lower() in nn_host.lower():
AttributeError: 'NoneType' object has no attribute 'lower'
{noformat}

Problem:

{noformat:title=command*json}
""public_hostname"": null
{noformat}"	AMBARI	Resolved	2	1	1699	pull-request-available
13181605	Cannot start Hive Metastore without HDFS	"Starting Hive Metastore fails if HDFS is not present in the cluster with the error: {{JAVA_HOME is not set and could not be found.}}

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 211, in <module>
    HiveMetastore().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 61, in start
    create_metastore_schema() # execute without config lock
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive.py"", line 374, in create_metastore_schema
    user = params.hive_user
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-server2-hive2/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED] -verbose' returned 1. Error: JAVA_HOME is not set and could not be found.
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13212611	StackAdvisorAdapterTest result depends on method execution order	{{StackAdvisorAdapterTest}} result depends on method execution order, {{getLayoutRecommendationInfo}} fails if executed after {{recommendConfigurations_alwaysApply}}.	AMBARI	Resolved	3	1	1699	pull-request-available
13196744	Build error at Findbugs with Maven 3.6	"{noformat:title=mvn -am -pl ambari-server clean verify}
...
[INFO] Ambari Server 3.0.0.0-SNAPSHOT ..................... FAILURE [01:16 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
...
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:findbugs-maven-plugin:3.0.3:findbugs (findbugs) on project ambari-server: Unable to parse configuration of mojo org.codehaus.mojo:findbugs-maven-plugin:3.0.3:findbugs for parameter pluginArtifacts: Cannot assign configuration entry 'pluginArtifacts' with value '${plugin.artifacts}' of type java.util.Collections.UnmodifiableRandomAccessList to property of type java.util.ArrayList -> [Help 1]
{noformat}

{noformat:title=mvn -version}
Apache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T20:41:47+02:00)
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13194957	Improve copy of atlas.war	"Atlas Metadata Server's start script in Ambari includes a step to copy {{atlas.war}} from the default location to the {{expanded_webapp_dir}}.  This is performed using {{File(content=StaticFile)}}, which reads and writes the contents of the war file to/from memory.  It may take 1-3 seconds to perform, and is done even if the source and target files are the same, because there is no check on the paths.

This could be improved:

# Use {{cp}} to copy
# Skip the operation if source and target are the same"	AMBARI	Resolved	3	4	1699	pull-request-available
13199320	No need to create test jar if tests are skipped	Some test-related tasks ({{create-sample-upgrade-check-jar}}, {{generate-test-oozie2-checks-dir}}, {{generate-test-oozie2-server-actions-dir}}) can be skipped during build if tests are skipped.	AMBARI	Resolved	4	4	1699	pull-request-available
13130496	Ambari loads ambari.properties using ISO 8859-1 encoding	"Ambari Server loads {{ambari.properties}} using {{Properties.load(InputStream)}}, which [uses ISO 8859-1 character encoding|https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html#load-java.io.InputStream-].  This causes a problem for non-Latin1 characters that can occur in several properties, eg. in any directory or file name.

STR:

# Install and setup Ambari Server
# Update {{ambari.properties}} changing {{server.jdbc.user.passwd}} to {{/etc/ambari-server/conf/password.őőő}}
# {{mv -iv /etc/ambari-server/conf/password.dat /etc/ambari-server/conf/password.őőő}}
# Try to start Ambari Server

Result:

{noformat}
...
ERROR: Exiting with exit code 1.
REASON: Database check failed to complete. Please check /var/log/ambari-server/ambari-server.log and /var/log/ambari-server/ambari-server-check-database.log for more information.
{noformat}

{noformat:title=/var/log/ambari-server/ambari-server-check-database.log}
...
FileNotFoundException: File '/etc/ambari-server/conf/password.ÅÅÅ' does not exist
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13159971	Move user-related info to stack-level params	User-related information (users, groups, user-group mapping) is currently sent to agents in {{clusterLevelParams}}.  The problem is that the data reflects only a single stack, so users etc. from other stacks are not created, causing deployment failures.  In addition, {{stack_version}} should be moved, too, similar to how {{stack_name}} was moved in AMBARI-23746.	AMBARI	Resolved	2	3	1699	pull-request-available
13202801	Set unique configuration version tag	Add Service does not work for add-delete-add scenario, because configuration tag is constant {{ADD_SERVICE}}.  It should include some changing part, eg. timestamp suffix.	AMBARI	Resolved	2	1	1699	pull-request-available
13152744	Ambari Schema Upgrade Failing source 2.6.1.0 target 2.7.0.0	"{code:title=ambari-server.log}
16 Apr 2018 11:04:47,676 INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE stage ADD status VARCHAR2(255) NULL
16 Apr 2018 11:04:47,695 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed.
java.sql.SQLSyntaxErrorException: ORA-00904: ""PENDING"": invalid identifier

at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)
 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)
 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)
 at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:227)
 at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531)
 at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:195)
 at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1036)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1336)
 at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1845)
 at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1810)
 at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:294)
 at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:829)
 at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:632)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateStageTable(UpgradeCatalog270.java:842)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:283)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
16 Apr 2018 11:04:47,695 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: ORA-00904: ""PENDING"": invalid identifier

at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.sql.SQLSyntaxErrorException: ORA-00904: ""PENDING"": invalid identifier

at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)
 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)
 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)
 at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:227)
 at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531)
 at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:195)
 at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1036)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1336)
 at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1845)
 at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1810)
 at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:294)
 at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:829)
 at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:632)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateStageTable(UpgradeCatalog270.java:842)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:283)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 ... 1 more
{code}"	AMBARI	Resolved	2	1	1699	pull-request-available, upgrade
13202506	START_ONLY provision action may be applied to the wrong component	"If component XY is provisioned with {{START_ONLY}}, and component X is a prefix of XY, then the provision action is applied to both X and XY.

{noformat:title=blueprint}
{
  ""host_groups"": [
    {
      ""components"": [
        { ""name"": ""HIVE_SERVER"" },
        { ""name"": ""HIVE_SERVER_INTERACTIVE"", ""provision_action"": ""START_ONLY"" },
...
{noformat}

{noformat:title=ambari-server.log}
AmbariManagementControllerImpl:3149 - Skipping create of INSTALL task for HIVE_SERVER on c7401.ambari.apache.org.
AmbariManagementControllerImpl:3149 - Skipping create of INSTALL task for HIVE_SERVER_INTERACTIVE on c7401.ambari.apache.org.
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13151777	Ambari Metrics references outdated JARs	"GitHub PR builder is failing on some Jenkins nodes due to references to outdated Hadoop, etc. JARs that are no longer available.

{noformat}
[ERROR] Failed to execute goal on project ambari-metrics-timelineservice: Could not resolve dependencies for project org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.phoenix:phoenix-core:jar:5.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-common:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-annotations:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-common:jar:tests:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-common:jar:tests:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-common:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-api:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-server-common:jar:3.0.0.3.0.0.2-97, org.apache.phoenix:phoenix-core:jar:tests:5.0.0.3.0.0.2-97, org.apache.hbase:hbase-it:jar:tests:2.0.0.3.0.0.2-97, org.apache.hbase:hbase-testing-util:jar:2.0.0.3.0.0.2-97
{noformat}

https://builds.apache.org/view/A/view/Ambari/job/Ambari-Github-PullRequest-Builder/1744/console
https://builds.apache.org/view/A/view/Ambari/job/Ambari-Github-PullRequest-Builder/1791/console
etc."	AMBARI	Resolved	2	1	1699	pull-request-available
13124713	AmbariServer will throw internal server error in case of post existing version_definition	"The following curl will result in 500 if post it more than once:
{noformat}
curl -vvv -u admin:admin -k -H ""X-Requested-By:ambari"" -X POST https://$AMBARI_SERVER/api/v1/version_definitions -d '{
  ""VersionDefinition"": {
   ""version_url"":
""http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0/HDP-2.6.3.0-235.xml""
    }
  }'
{noformat}

The http response is:

{noformat}
{
  ""status"" : 500,
  ""message"" : ""An internal system exception occurred: Base url http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0 is already defined for another repository version. Setting up base urls that contain the same versions of components will cause stack upgrade to fail.""
}
{noformat}

It would be better, if in this case the http response status would be {noformat}409 - Conflict{noformat}

"	AMBARI	Resolved	3	1	1699	pull-request-available
13134576	Blueprint cluster creation using manually installed mpacks	Deploy a cluster via blueprint based on one or more MPacks that have already been manually installed.	AMBARI	Resolved	3	3	1699	blueprints, pull-request-available
13194475	Fix javadoc errors in ambari-utility	"Fix the following javadoc errors in {{ambari-utility}}:

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:jar (attach-javadocs) on project ambari-utility: MavenReportException: Error while generating Javadoc:
[ERROR] Exit code: 1 - ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:28: error: reference not found
[ERROR]  * {@link org.apache.ambari.swagger.NestedApiRecord} when {@link org.apache.ambari.swagger.AmbariSwaggerReader}
[ERROR]           ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:42: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:48: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:54: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:60: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerPreferredParent.java:37: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
{noformat}"	AMBARI	Resolved	4	1	1699	pull-request-available
13199817	Accept legacy JSON configuration in Add Service request	"Cluster creation via blueprint accepts configuration in the following format, where the {{""properties""}} level is omitted:

{noformat}
  ""configurations"": [
    {
      ""cluster-env"": {
        ""custom-property"": ""whatever""
      }
    },
    {
      ""zoo.cfg"": {
        ""syncLimit"": ""7""
      }
    },
...
{noformat}

""Add Service"" request should accept the same format, too, but currently it results in:

{noformat}
<h3>Caused by:</h3><pre>java.lang.IllegalArgumentException: Invalid fields in cluster-env configuration: [custom-property]
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)
	at org.apache.ambari.server.topology.ConfigurableHelper.lambda$parseConfigs$1(ConfigurableHelper.java:102)
	at java.util.ArrayList.forEach(ArrayList.java:1249)
	at org.apache.ambari.server.topology.ConfigurableHelper.parseConfigs(ConfigurableHelper.java:88)
	at org.apache.ambari.server.controller.AddServiceRequest.&lt;init&gt;(AddServiceRequest.java:88)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.call(AnnotatedConstructor.java:124)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:283)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator.createFromObjectWith(ValueInstantiator.java:229)
	at com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:195)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:488)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1280)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:159)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4001)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2992)
	at org.apache.ambari.server.controller.AddServiceRequest.of(AddServiceRequest.java:115)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.createAddServiceRequest(ServiceResourceProvider.java:1242)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.processAddServiceRequest(ServiceResourceProvider.java:1232)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.createResourcesAuthorized(ServiceResourceProvider.java:257)
{noformat}"	AMBARI	Resolved	3	4	1699	pull-request-available
13182104	Cannot deploy Hive Metastore with Kerberos without HDFS	"In order to enable Kerberos for Hive MetaStore we need a property in {{core-site}}, which is ignored by Ambari if HDFS is not present.  Hive Metastore start fails due to empty {{core-site}} with:

{noformat}
KeeperException$InvalidACLException: KeeperErrorCode = InvalidACL for /hive/cluster/delegationMETASTORE/keys
{noformat}"	AMBARI	Resolved	3	1	1699	pull-request-available
13203829	Allow skipping parts of Add Service request validation	Provide ability to disable parts of the validation for the Add Service request, eg. configuration validation and topology validation.  Some parts still need to be validated (eg. it probably makes no sense to add unknown services).	AMBARI	Resolved	3	4	1699	pull-request-available
13186542	Directory creation should be atomic	"If parallel execution is enabled on Ambari Agent, concurrent directory creation may fail with:

{noformat:title=errors-62.txt}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 375, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 31, in hook
    setup_hadoop_env()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py"", line 203, in setup_hadoop_env
    mode=01777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 191, in action_create
    sudo.makedir(path, self.resource.mode or 0755)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 121, in makedir
    os.mkdir(path)
OSError: [Errno 17] File exists: '/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'
{noformat}

or

{noformat:title=errors-63.txt}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 375, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 31, in hook
    setup_hadoop_env()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py"", line 203, in setup_hadoop_env
    mode=01777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 179, in action_create
    path = sudo.readlink(path)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 161, in readlink
    return os.readlink(path)
OSError: [Errno 22] Invalid argument: '/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'
{noformat}

The failed tasks need to be retried to succeed, causing delays."	AMBARI	Resolved	3	4	2212	pull-request-available
13273376	Deploy fails with 401:Unauthorized on HDP-GPL	"We are trying to deploy with paywalled repos of HDP and Ambari (ones supplied
in RELENG-7654) but deploy is failing at client install with {code}
RuntimeError: Failed to execute command '/usr/bin/yum -y install hdp-select',
exited with code '1', message: 'https://****:****@archive-test.cloudera.com/p
/HDP-GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml: [Errno 14] HTTPS
Error 401 - Unauthorized {code} This url is accessible with credentials RE has
supplied ie; https://1055c7c3-1b7b-
43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-test.cloudera.com/p/HDP-
GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml So not sure which
credential is Ambari picking as from the error it looks like the credentials
Ambari is using on this repo is incorrect. Could you please take a look Live
cluster : http://172.27.136.132:8080/ Repo urls used here for deploy: Ambari :
https://1055c7c3-1b7b-43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-
test.cloudera.com/p/ambari/2.7.5.0-64/centos7/ambaribn.repo HDP:
https://1055c7c3-1b7b-43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-
test.cloudera.com/p/HDP/centos7/3.x/BUILDS/3.1.5.0-139 cc
[~accountid:557058:af856db7-a0ad-
4e2b-b848-f11f481bf96f]/[~accountid:5dc59258b6e6b50c58af136b]

"	AMBARI	Patch Available	3	1	2212	pull-request-available
13144820	Ambari-agent fails to connect to server with two_way_auth enabled	"
    ERROR 2018-03-13 17:15:04,264 security.py:122 - Could not connect to wss://ctr-e138-1518143905142-94896-01-000002.hwx.site:8441/agent/stomp/v1
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/security.py"", line 113, in establish_connection
        conn.start()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/connect.py"", line 46, in start
        self.transport.start()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/transport.py"", line 109, in start
        self.attempt_connection()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/adapter/websocket.py"", line 89, in attempt_connection
        self.ws.connect()
      File ""/usr/lib/ambari-agent/lib/ambari_ws4py/client/__init__.py"", line 216, in connect
        self.sock.connect(self.bind_addr)
      File ""/usr/lib64/python2.7/ssl.py"", line 869, in connect
        self._real_connect(addr, False)
      File ""/usr/lib64/python2.7/ssl.py"", line 860, in _real_connect
        self.do_handshake()
      File ""/usr/lib64/python2.7/ssl.py"", line 833, in do_handshake
        self._sslobj.do_handshake()
    SSLError: [SSL: SSLV3_ALERT_BAD_CERTIFICATE] sslv3 alert bad certificate (_ssl.c:579)
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
12693139	Host registering failure from primary/agent os checking on centos6	"I am using Ambari (1.4.3.38) for hadoop cluster installation and management. All the cluster nodes are built on centos 6.0.

During the ambari server installation, ambari-server recognized the primary/cluster os as redhat6 (see ambari.properties). 
During the ambari agent bootstrap/host register, ambari-agent regonized the agent os as centos linux6 (see log). 

From log files (ambari-server.log, ambari-agent.log), I found the inconsistence caused the warning of ambari-agent bootstrapping and failure of host registering.

I'm still not sure why this happen, but I guess it's caused by the differene of os checking methods among ambari server side code, ambari-agent bootstrap script (os_type_check.sh,based on os release file) and registering script (Controller.py/Register.py based on os hardware profile) .

I just share to see if anyone can fix the issue.

BTW, for me, to solve the problem, I manually edited the script files to make it work temporarily:

To avoid warning of agent bootstrapping, in os_type_check.sh, add current_os=$RH6 above the echo line or add res=0 after case statement;
To make the node register work, in Controller.py, add data=data.replace('centos linux','redhat') before sending registering request;

Thanks."	AMBARI	Resolved	3	1	2212	patch
13179508	ambari-server setup fails with postgresql >= 9.3 	"ambari-server setup with default postgres has been failing with

    
    
    OSError: [Errno 2] No such file or directory: '/usr/bin/postgresql-setup'
    

Looks like postgres packages/dependencies are not installed along with
installing ambari server install.  
below is output from ambari-server install from ambari-2.7.1 cluster

    
    
    2018-08-07 11:17:56,106|executor.py.167|INFO|14260|MainThread|172.27.25.203|executing the command='yum -y install ambari-server'
    2018-08-07 11:17:56,432|executor.py.167|INFO|14260|Thread-191|stdout: Loaded plugins: ovl, priorities
    2018-08-07 11:17:57,971|executor.py.167|INFO|14260|Thread-191|stdout: 12711 packages excluded due to repository priority protections
    2018-08-07 11:17:58,754|executor.py.167|INFO|14260|Thread-191|stdout: Resolving Dependencies
    2018-08-07 11:17:58,754|executor.py.167|INFO|14260|Thread-191|stdout: --> Running transaction check
    2018-08-07 11:17:58,755|executor.py.167|INFO|14260|Thread-191|stdout: ---> Package ambari-server.x86_64 0:2.7.1.0-63 will be installed
    2018-08-07 11:17:58,974|executor.py.167|INFO|14260|Thread-191|stdout: --> Finished Dependency Resolution
    2018-08-07 11:17:59,146|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,146|executor.py.167|INFO|14260|Thread-191|stdout: Dependencies Resolved
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout:  Package            Arch        Version            Repository              Size
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: Installing:
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout:  ambari-server      x86_64      2.7.1.0-63         ambari-2.7.1.0-63      366 M
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: Transaction Summary
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: Install  1 Package
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: Total download size: 366 M
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: Installed size: 434 M
    2018-08-07 11:17:59,152|executor.py.167|INFO|14260|Thread-191|stdout: Downloading packages:
    2018-08-07 11:18:03,735|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction check
    2018-08-07 11:18:03,736|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction test
    2018-08-07 11:18:04,048|executor.py.167|INFO|14260|Thread-191|stdout: Transaction test succeeded
    2018-08-07 11:18:04,049|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction
    2018-08-07 11:18:21,843|executor.py.167|INFO|14260|Thread-191|stdout:   Installing : ambari-server-2.7.1.0-63.x86_64                              1/1
    2018-08-07 11:18:22,568|executor.py.167|INFO|14260|Thread-191|stdout:  
    2018-08-07 11:18:22,569|executor.py.167|INFO|14260|Thread-191|stdout:   Verifying  : ambari-server-2.7.1.0-63.x86_64                              1/1
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout:  
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout: Installed:
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout:   ambari-server.x86_64 0:2.7.1.0-63                                             
    2018-08-07 11:18:22,677|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:18:22,677|executor.py.167|INFO|14260|Thread-191|stdout: Complete!
    

where as on an ambari-2.7.0 cluster we do have postgres server packages and
lib and is being installed with ambari-server install

    
    
    [root@c7401 ~]# yum install -y ambari-server
    Loaded plugins: fastestmirror
    ambari-2.7.0.0                                                                                                                                             | 2.9 kB  00:00:00     
    ambari-2.7.0.0/primary_db                                                                                                                                  |  25 kB  00:00:00     
    Loading mirror speeds from cached hostfile
     * base: repos.lax.quadranet.com
     * extras: linux.mirrors.es.net
     * updates: mirror.keystealth.org
    Resolving Dependencies
    --> Running transaction check
    ---> Package ambari-server.x86_64 0:2.7.0.0-897 will be installed
    --> Processing Dependency: postgresql-server >= 8.1 for package: ambari-server-2.7.0.0-897.x86_64
    --> Running transaction check
    ---> Package postgresql-server.x86_64 0:9.2.23-3.el7_4 will be installed
    --> Processing Dependency: postgresql-libs(x86-64) = 9.2.23-3.el7_4 for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Processing Dependency: postgresql(x86-64) = 9.2.23-3.el7_4 for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Processing Dependency: libpq.so.5()(64bit) for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Running transaction check
    ---> Package postgresql.x86_64 0:9.2.23-3.el7_4 will be installed
    ---> Package postgresql-libs.x86_64 0:9.2.23-3.el7_4 will be installed
    --> Finished Dependency Resolution
    
    Dependencies Resolved
    
    ==================================================================================================================================================================================
     Package                                        Arch                                Version                                     Repository                                   Size
    ==================================================================================================================================================================================
    Installing:
     ambari-server                                  x86_64                              2.7.0.0-897                                 ambari-2.7.0.0                              367 M
    Installing for dependencies:
     postgresql                                     x86_64                              9.2.23-3.el7_4                              base                                        3.0 M
     postgresql-libs                                x86_64                              9.2.23-3.el7_4                              base                                        234 k
     postgresql-server                              x86_64                              9.2.23-3.el7_4                              base                                        3.8 M
    
    Transaction Summary
    ==================================================================================================================================================================================
    Install  1 Package (+3 Dependent packages)
    
    Total download size: 374 M
    Installed size: 468 M
    Downloading packages:
    (1/4): postgresql-libs-9.2.23-3.el7_4.x86_64.rpm                                                                                                           | 234 kB  00:00:00     
    (2/4): postgresql-server-9.2.23-3.el7_4.x86_64.rpm                                                                                                         | 3.8 MB  00:00:00     
    (3/4): postgresql-9.2.23-3.el7_4.x86_64.rpm                                                                                                                | 3.0 MB  00:00:04     
    warning: /var/cache/yum/x86_64/7/ambari-2.7.0.0/packages/ambari-server-2.7.0.0-897.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 07513cad: NOKEY17 MB/s | 371 MB  00:00:00 ETA 
    Public key for ambari-server-2.7.0.0-897.x86_64.rpm is not installed
    (4/4): ambari-server-2.7.0.0-897.x86_64.rpm                                                                                                                | 367 MB  00:00:23     
    ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    Total                                                                                                                                              16 MB/s | 374 MB  00:00:23     
    Retrieving key from http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.0.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    Importing GPG key 0x07513CAD:
     Userid     : ""Jenkins (HDP Builds) <jenkin@hortonworks.com>""
     Fingerprint: df52 ed4f 7a3a 5882 c099 4c66 b973 3a7a 0751 3cad
     From       : http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.0.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    Running transaction check
    Running transaction test
    Transaction test succeeded
    Running transaction
      Installing : postgresql-libs-9.2.23-3.el7_4.x86_64                                                                                                                          1/4 
      Installing : postgresql-9.2.23-3.el7_4.x86_64                                                                                                                               2/4 
      Installing : postgresql-server-9.2.23-3.el7_4.x86_64                                                                                                                        3/4 
      Installing : ambari-server-2.7.0.0-897.x86_64                                                                                                                               4/4 
      Verifying  : postgresql-9.2.23-3.el7_4.x86_64                                                                                                                               1/4 
      Verifying  : ambari-server-2.7.0.0-897.x86_64                                                                                                                               2/4 
      Verifying  : postgresql-libs-9.2.23-3.el7_4.x86_64                                                                                                                          3/4 
      Verifying  : postgresql-server-9.2.23-3.el7_4.x86_64                                                                                                                        4/4 
    
    Installed:
      ambari-server.x86_64 0:2.7.0.0-897                                                                                                                                              
    
    Dependency Installed:
      postgresql.x86_64 0:9.2.23-3.el7_4                     postgresql-libs.x86_64 0:9.2.23-3.el7_4                     postgresql-server.x86_64 0:9.2.23-3.el7_4                    
    
    Complete!
    


"	AMBARI	Resolved	3	1	2212	pull-request-available
13202253	Commands timeout if stdout has non-unicode symbols.	"
    
    ERROR 2018-12-03 18:08:08,694 ActionQueue.py:198 - Exception while processing EXECUTION_COMMAND command
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 191, in process_command
        self.execute_command(command)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 379, in execute_command
        self.commandStatuses.put_command_status(command, role_result)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CommandStatusDict.py"", line 77, in put_command_status
        is_sent, correlation_id = self.force_update_to_server({command['clusterId']: [report]})
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CommandStatusDict.py"", line 95, in force_update_to_server
        correlation_id = self.initializer_module.connection.send(message={'clusters':reports_dict}, destination=Constants.COMMANDS_STATUS_REPORTS_ENDPOINT, log_message_function=CommandStatusDict.log_sending)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/security.py"", line 137, in send
        body = json.dumps(message)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/__init__.py"", line 230, in dumps
        return _default_encoder.encode(obj)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 202, in encode
        chunks = list(chunks)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 426, in _iterencode
        for chunk in iterencode_dict(o, current_indent_level):
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 400, in _iterencode_dict
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 400, in _iterencode_dict
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 323, in _iterencode_list
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 382, in _iterencode_dict
        yield _encoder(value)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 48, in py_encode_basestring_ascii
        s = s.decode('utf-8')
      File ""/usr/lib64/python2.7/encodings/utf_8.py"", line 16, in decode
        return codecs.utf_8_decode(input, errors, True)
    UnicodeDecodeError: 'utf8' codec can't decode byte 0xea in position 90211: invalid continuation byte
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13148198	Fix formatZKFC	"resource_management.core.exceptions.Fail: Script '/var/lib/ambari-
agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py' has no
method 'format'

"	AMBARI	Resolved	3	1	2212	pull-request-available
13273682	VDF registration fails with SunCertPathBuilderException: unable to find valid certification path to requested target' on HTTPS cluster 	"Retrying deploy with latest build after BUG-122455 was fixed Here Stack
registration is failing with {code} An internal system exception occurred:
Could not load url from https://1055c7c3-1b7b-
43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-test.cloudera.com/p/HDP-
GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml.
sun.security.validator.ValidatorException: PKIX path building failed:
sun.security.provider.certpath.SunCertPathBuilderException: unable to find
valid certification path to requested target {code} 172.27.74.1 is a live node
where issue has occurred.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13133045	Ambari-agent puts Python scripts in 2.6 directory on OSes that use python 2.7.x by default	"User needs to upgrade all OSes for security purposes, and will need to remove all directories related to Python2.6 due to
TLS violations, they need this ASAP to test, as they have a hard deadline.  
OS: Red Hat Enterprise Linux Server release 7.4  
JDK version: 1.8.0_144  

"	AMBARI	Resolved	3	1	2212	pull-request-available
13151568	Fix TestCustomServiceOrchestrator.py and TestRegistration.py on branch-3.0-perf	"TestCustomServiceOchestrator requires rewriting since configs building routine
changed quite a lot on branch-3.0-perf

"	AMBARI	Resolved	3	1	2212	pull-request-available
13149733	Fix stack issues in HDFS to support Namenode Federation setup	"For example, here are 2 things I found to be probably wrong.

1\. Journal node restart failed because we cannot find hdfs-site :
dfs.journalnode.edits.dir. We delete that property in the wizard. We may have
to change that to dfs.journalnode.edits.dir.&lt;nameservice&gt;

2\. The following snippet in params_linux.py on HDFS 3.0 stack seems wrong. It
has been designed to work with only 1 nameservice.

    
    
    
    dfs_ha_enabled = False
    dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.internal.nameservices', None)
    if dfs_ha_nameservices is None:
      dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.nameservices', None)
    dfs_ha_namenode_ids = default(format(""/configurations/hdfs-site/dfs.ha.namenodes.{dfs_ha_nameservices}""), None)
    

3\. After setting up NN Fed, when I restart namenodes, I see the following
error.

    
    
    
        main_resource.resource.security_enabled, main_resource.resource.logoutput)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 154, in __init__
        security_enabled, run_user)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/namenode_ha_utils.py"", line 204, in get_property_for_active_namenode
        if INADDR_ANY in value and rpc_key in hdfs_site:
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'dfs.namenode.https-address.ns2.nn1' was not found in configurations dictionary!
    

This is probably because the namenode_ha_utils is not equipped to handle
multiple nameservices.

We may have to create to fix such stack errors when the wizard is done.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13167069	Agent fails to auto-start HDFS when using LZO	"Service auto-start for HDFS fails with the following error when using LZO libraries:
{noformat:title=auto_errors-...txt}
...
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs.py"", line 137, in hdfs
    install_lzo_if_needed()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/lzo_utils.py"", line 87, in install_lzo_if_needed
    Script.repository_util.create_repo_files()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/repository_util.py"", line 53, in create_repo_files
    if self.command_repository.version_id is None:
AttributeError: RepositoryUtil instance has no attribute 'command_repository'
{noformat}
{{repositoryFile}} entry is missing from auto-start commands."	AMBARI	Resolved	3	1	2212	pull-request-available
13216621	ClientComponentHasNoStatus exception clutters Operating System's /var/log/messages	"As part of the status check, exception ClientComponentHasNoStatus throws ""detected unhandled Python exception"" in /var/log/messages every 20 seconds for each client installed."	AMBARI	Resolved	3	1	2212	pull-request-available
13151975	Refactor base_alert to support multiple nameservices	"_NameNode Web UI_ alert is in critical state for all NameNodes with the
    
    
    
    [Alert][namenode_webui] HA nameservice value is present but there are no aliases for {{hdfs-site/dfs.ha.namenodes.ns1,ns2}}
    
"	AMBARI	Resolved	3	1	2212	pull-request-available
13138448	"Add Hosts is Failing with - Error while bootstrapping: Cannot run program ""/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py"""	"Add Hosts(UI/API) is broken and the following error is seen .
{code:java}
Error while bootstrapping:
Cannot run program ""/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py"": error=2, No such file or directory
{code}
 "	AMBARI	Resolved	1	1	2212	pull-request-available
13130991	Fix Namenode alerts broken due to enabling federation	"Namnode alerts need to be fixed.

<http://104.196.73.142:8080/#/main/alerts>

"	AMBARI	Resolved	3	1	2212	pull-request-available
13178213	Ambari server can not start on latest Amazon Linux 2	"On latest Amazon Linux 2 ambari-server cannot start with error message:

    
    
    [root@ip-10-0-222-155 cloudbreak]# ambari-server start
    Using python  /usr/bin/python
    Starting ambari-server
    Traceback (most recent call last):
      File ""/usr/sbin/ambari-server.py"", line 37, in <module>
        from ambari_server.dbConfiguration import DATABASE_NAMES, LINUX_DBMS_KEYS_LIST
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 30, in <module>
        from ambari_server.serverConfiguration import decrypt_password_for_alias, get_ambari_properties, get_is_secure, \
      File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 46, in <module>
        OS_VERSION = OSCheck().get_os_major_version()
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 322, in get_os_major_version
        return OSCheck.get_os_version().split('.')[0]
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 301, in get_os_version
        return OSCheck.get_alias(OSCheck._get_os_type(), OSCheck._get_os_version())[1]
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 313, in _get_os_version
        raise Exception(""Cannot detect os version. Exiting..."")
    Exception: Cannot detect os version. Exiting...
    

Additional infos from the machine:

    
    
    [root@ip-10-0-222-155 cloudbreak]# cat /etc/system-release
    Amazon Linux 2
    [root@ip-10-0-222-155 cloudbreak]# cat /etc/*release
    NAME=""Amazon Linux""
    VERSION=""2""
    ID=""amzn""
    ID_LIKE=""centos rhel fedora""
    VERSION_ID=""2""
    PRETTY_NAME=""Amazon Linux 2""
    ANSI_COLOR=""0;33""
    CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2""
    HOME_URL=""https://amazonlinux.com/""
    Amazon Linux 2
    [root@ip-10-0-222-155 cloudbreak]#
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13142846	Installation of MySQL fails on CentOS 7.2, should install mariadb	"I'm seeing the following error during HDP 3 beta installation.

    
    
    
    Traceback (most recent call last):
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 268, in _call_with_retries
        code, out = func(cmd, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    
    The above exception was the cause of the following exception:
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/HIVE/3.0.0.3.0/package/scripts/mysql_server.py"", line 68, in <module>
        MysqlServer().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/HIVE/3.0.0.3.0/package/scripts/mysql_server.py"", line 37, in install
        self.install_packages(env)
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 831, in install_packages
        retry_count=agent_stack_retry_count)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 53, in action_install
        self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py"", line 251, in install_package
        self.checked_call_with_retries(cmd, sudo=True, logoutput=self.get_logoutput())
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 251, in checked_call_with_retries
        return self._call_with_retries(cmd, is_checked=True, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 268, in _call_with_retries
        code, out = func(cmd, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    stdout:   /var/lib/ambari-agent/data/output-49.txt
    2018-02-16 18:25:54,593 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-16 18:25:54,605 - Using hadoop conf dir: /usr/hdp/3.0.0.0-814/hadoop/conf
    2018-02-16 18:25:54,608 - Group['livy'] {}
    2018-02-16 18:25:54,609 - Group['spark'] {}
    2018-02-16 18:25:54,610 - Group['hdfs'] {}
    2018-02-16 18:25:54,610 - Group['zeppelin'] {}
    2018-02-16 18:25:54,610 - Group['hadoop'] {}
    2018-02-16 18:25:54,611 - Group['users'] {}
    2018-02-16 18:25:54,611 - Group['knox'] {}
    2018-02-16 18:25:54,612 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,614 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,615 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,617 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,618 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,620 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-16 18:25:54,621 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,623 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,624 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,626 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-16 18:25:54,627 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,629 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,630 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,632 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,633 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,635 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-16 18:25:54,636 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,639 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-16 18:25:54,650 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-16 18:25:54,651 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-16 18:25:54,653 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,657 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,658 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-16 18:25:54,672 - call returned (0, '1015')
    2018-02-16 18:25:54,674 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1015'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-16 18:25:54,683 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1015'] due to not_if
    2018-02-16 18:25:54,684 - Group['hdfs'] {}
    2018-02-16 18:25:54,685 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-16 18:25:54,687 - FS Type: 
    2018-02-16 18:25:54,687 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-16 18:25:54,731 - File['/usr/hdp/3.0.0.0-814/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-16 18:25:54,732 - Writing File['/usr/hdp/3.0.0.0-814/hadoop/conf/hadoop-env.sh'] because contents don't match
    2018-02-16 18:25:54,733 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-16 18:25:54,771 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-16 18:25:54,786 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-16 18:25:54,787 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-16 18:25:54,788 - Repository with url http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-814 is not created due to its tags: set([u'GPL'])
    2018-02-16 18:25:54,788 - Repository['HDP-UTILS-1.1.0.22-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-16 18:25:54,796 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-1]\nname=HDP-UTILS-1.1.0.22-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-16 18:25:54,796 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-16 18:25:54,798 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,943 - Skipping installation of existing package unzip
    2018-02-16 18:25:54,944 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,960 - Skipping installation of existing package curl
    2018-02-16 18:25:54,961 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,976 - Skipping installation of existing package hdp-select
    2018-02-16 18:25:54,995 - Skipping stack-select on MYSQL_SERVER because it does not exist in the stack-select package structure.
    2018-02-16 18:25:55,435 - Using hadoop conf dir: /usr/hdp/3.0.0.0-814/hadoop/conf
    2018-02-16 18:25:55,471 - call['ambari-python-wrap /usr/bin/hdp-select status hive-server2'] {'timeout': 20}
    2018-02-16 18:25:55,523 - call returned (0, 'hive-server2 - 3.0.0.0-814')
    2018-02-16 18:25:55,525 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-16 18:25:55,567 - File['/var/lib/ambari-agent/cred/lib/CredentialUtil.jar'] {'content': DownloadSource('http://will-hdp-1.field.hortonworks.com:8080/resources/CredentialUtil.jar'), 'mode': 0755}
    2018-02-16 18:25:55,569 - Not downloading the file from http://will-hdp-1.field.hortonworks.com:8080/resources/CredentialUtil.jar, because /var/lib/ambari-agent/tmp/CredentialUtil.jar already exists
    2018-02-16 18:25:55,570 - checked_call[('/usr/jdk64/jdk1.8.0_112/bin/java', '-cp', u'/var/lib/ambari-agent/cred/lib/*', 'org.apache.ambari.server.credentialapi.CredentialUtil', 'get', 'javax.jdo.option.ConnectionPassword', '-provider', u'jceks://file/var/lib/ambari-agent/cred/conf/mysql_server/hive-site.jceks')] {}
    2018-02-16 18:25:56,859 - checked_call returned (0, 'SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\nFeb 16, 2018 6:25:56 PM org.apache.hadoop.util.NativeCodeLoader <clinit>\nWARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nhive')
    2018-02-16 18:25:56,873 - Package['mysql-community-release'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:57,038 - Installing package mysql-community-release ('/usr/bin/yum -d 0 -e 0 -y install mysql-community-release')
    2018-02-16 18:25:58,091 - Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    2018-02-16 18:25:58,091 - Failed to install package mysql-community-release. Executing '/usr/bin/yum clean metadata'
    2018-02-16 18:25:58,386 - Retrying to install package mysql-community-release after 30 seconds
    2018-02-16 18:26:37,003 - Skipping stack-select on MYSQL_SERVER because it does not exist in the stack-select package structure.
    
    Command failed after 1 tries
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13158669	Ambari Agent registration task is failing 	"Logs:

    
    
    
    INFO 2018-05-10 10:04:59,174 NetUtil.py:61 - Connecting to https://os-mv-07-test-3.openstacklocal:8440/ca
    WARNING 2018-05-10 10:04:59,174 NetUtil.py:92 - Failed to connect to https://os-mv-07-test-3.openstacklocal:8440/ca due to 'module' object has no attribute '_create_unverified_context'  
    WARNING 2018-05-10 10:04:59,174 NetUtil.py:115 - Server at https://os-mv-07-test-3.openstacklocal:8440 is not reachable, sleeping for 10 seconds...
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13159374	Timeline service is shown as stopped, while being started.	"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 108, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 87, in status
        for pid_file in self.get_pid_files():
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 99, in get_pid_files
        import params
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/params.py"", line 29, in <module>
        from params_linux import *
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/params_linux.py"", line 473, in <module>
        repo_name = str(config['clusterName']) + '_yarn'
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'clusterName' was not found in configurations dictionary!
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
12957505	HIVE service_check doesn't work properly	"In templetonSmoke.sh, there are
1) unnecessary `exit 0`
2) lack of redirect `>` command
3) unassigned variable

we should correct them all to check the HIVE service properly."	AMBARI	Resolved	3	1	2212	patch-available
13177496	Component Versions Are Not Reported On Initial Status Commands Anymore	"In Ambari 2.6, some status commands were able to supply the version of the
component they were running for. This was needed especially during upgrades to
correct situations where components would fail to start but the processes were
actually running and reporting.

  * During heartbeat registration, Ambari asked the agent to get the version in the next status command:  
<https://github.com/apache/ambari/blob/branch-2.6/ambari-
server/src/main/java/org/apache/ambari/server/agent/HeartBeatHandler.java#L416-L419>

  * The status command would place the version information from the structured out into the ""extra"" mapping, which would then be handled here via an event:  
<https://github.com/apache/ambari/blob/trunk/ambari-
server/src/main/java/org/apache/ambari/server/agent/HeartbeatProcessor.java#L599-L603>

We need to replace this functionality so that component versions are reported
on registration of agents.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13138991	Ignore python UT failing on branch-3.0-perf	"As discussed with Sid Wagle for now we should ignore UT failing on branch-3.0-perf as they take up a lot of
time rework.

The reason for this is that we need to merge into trunk soon, so the feature
code gets enough test cycles.  
The tests will be unignored and fixed after the merge is done.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13167806	Agent failed to process execution command	"Some execution commands were failed during blueprint deploy:

    
    
    
    ERROR 2018-06-19 22:29:28,058 ActionQueue.py:221 - Exception while processing EXECUTION_COMMAND command
     Traceback (most recent call last):
       File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 214, in process_command
         self.execute_command(command)
       File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 352, in execute_command
         commandresult['stdout'] += '\n\nCommand completed successfully!\n' if status == self.COMPLETED_STATUS else '\n\nCommand failed after ' + str(numAttempts) + ' tries\n'
     UnboundLocalError: local variable 'commandresult' referenced before assignment
     INFO 2018-06-19 22:29:28,100 ActionQueue.py:238 - Executing command with id = 4-0, taskId = 5 for role = MAPREDUCE2_CLIENT of cluster_id 2.
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13164883	Autostart is not working for TIMELINE_READER. 	"Steps to reproduce:-  
1\. Enable Autostart for all components.  
2.restart one host which contain TIMELINE_READER component.  
3\. wait for 5-10 minute for autostart to take an effect

verify from the logs  
tail -f /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py

    
    
    
    INFO 2018-06-07 08:32:32,487 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:34,488 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:36,489 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:38,490 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:40,491 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:42,496 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:44,501 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:46,505 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:48,518 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:50,519 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:52,521 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:54,533 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:56,534 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:58,535 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:33:00,537 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:33:02,538 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    

repro cluster:-<http://172.22.107.230:8080/>  
repro job:- <http://linux-jenkins.qe.hortonworks.com:8080/job/Nightly-Start-
EC2-Run-HDP/965535/>

"	AMBARI	Resolved	3	1	2212	pull-request-available
13166050	Canceling task during blueprint install results in agent not responding to any other tasks	"If failing operation which is in retry cycle is canceled during blueprint
install. The agent will still continue to execute the canceled task. Resulting
in ActionQueue being stuck and no other task being able to start execution

"	AMBARI	Resolved	3	1	2212	pull-request-available
13154434	Ambari-agent should handle delivery status responses from server	"This is required so that if connection between agent and server is lost, agent
knows what was lost and can resend it later.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13176252	Components start failing with 'Holder DFSClient_NONMAPREDUCE does not have any open files' while adding Namespace 	"STR: 
Add a namespace from UI. In the last step restart required services, hiveserver2 restart fails. Although on retrying it comes back up

{code}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.status(env)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 79, in status
 check_process_status(status_params.hive_pid)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/check_process_status.py"", line 43, in check_process_status
 raise ComponentIsNotRunning()
ComponentIsNotRunning

The above exception was the cause of the following exception:

Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 137, in <module>
 HiveServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 993, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 50, in start
 self.configure(env) # FOR SECURITY
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 45, in configure
 hive(name='hiveserver2')
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive.py"", line 119, in hive
 setup_hiveserver2()
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive.py"", line 167, in setup_hiveserver2
 skip=params.sysprep_skip_copy_tarballs_hdfs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 516, in copy_to_hdfs
 replace_existing_files=replace_existing_files,
 File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
 self.env.run()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
 self.run_action(resource, action)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
 provider_action()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 654, in action_create_on_execute
 self.action_delayed(""create"")
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 651, in action_delayed
 self.get_hdfs_resource_executor().action_delayed(action_name, self)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 354, in action_delayed
 self.action_delayed_for_nameservice(nameservice, action_name, main_resource)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 380, in action_delayed_for_nameservice
 self._create_resource()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 396, in _create_resource
 self._create_file(self.main_resource.resource.target, source=self.main_resource.resource.source, mode=self.mode)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 511, in _create_file
 self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 199, in run_command
 return self._run_command(*args, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 272, in _run_command
 raise WebHDFSCallException(err_msg, result_dict)
resource_management.libraries.providers.hdfs_resource.WebHDFSCallException: Execution of 'curl -sS -L -w '%\{http_code}' -X PUT --data-binary @/usr/hdp/3.0.1.0-30/hive/hive.tar.gz -H 'Content-Type: application/octet-stream' --negotiate -u : -k 'https://<HOST-FQDN>:50470/webhdfs/v1/hdp/apps/3.0.1.0-30/hive/hive.tar.gz?op=CREATE&overwrite=True&permission=444'' returned status_code=404. 
{
 ""RemoteException"": {
 ""exception"": ""FileNotFoundException"", 
 ""javaClassName"": ""java.io.FileNotFoundException"", 
 ""message"": ""File does not exist: /hdp/apps/3.0.1.0-30/hive/hive.tar.gz (inode 16450) Holder DFSClient_NONMAPREDUCE_-1764810327_120 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2800)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:597)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)\n""
 }
}
{code}

 "	AMBARI	Resolved	3	1	2212	pull-request-available
13165306	 NN cannot start due do not have permission for creation of the folde	"STR:  
1) Install ambari cluster with custom user configuration via BP  
Cluster: <http://172.27.14.154:8080>

Actual result: NN cannot start due do not have permission for creation of the
folder ""/var/run/hadoop/cstm-hdfs""  
Looks like some script changed permission for

    
    
    
    [root@ctr-e138-1518143905142-357962-01-000006 ~]# ls -la /var/run/ | grep ""hadoop""
    drwxr-xr-x  2 cstm-ams       hadoop   4096 Jun 11 01:56 ambari-metrics-monitor
    drwxr-xr-x  6 root           root     4096 Jun 11 01:56 hadoop
    

NN Logs:

    
    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 414, in 
        NameNode().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 138, in start
        upgrade_suspended=params.upgrade_suspended, env=env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 115, in namenode
        format_namenode()
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 369, in format_namenode
        logoutput=True
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
        returns=self.resource.returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hdfs --config /usr/hdp/3.0.0.0-1469/hadoop/conf namenode -format -nonInteractive' returned 1. ######## Hortonworks #############
    This is MOTD message, added for testing in qe infra
    WARNING: /var/run/hadoop/cstm-hdfs does not exist. Creating.
    mkdir: cannot create directory ‘/var/run/hadoop/cstm-hdfs’: Permission denied
    ERROR: Unable to create /var/run/hadoop/cstm-hdfs. Aborting.
     stdout:
    2018-06-11 10:00:42,196 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:42,308 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:43,323 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:43,353 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:43,358 - Group['cstm-users'] {}
    2018-06-11 10:00:43,364 - Group['cstm-ranger'] {}
    2018-06-11 10:00:43,364 - Group['cstm-zeppelin'] {}
    2018-06-11 10:00:43,365 - Group['hdfs'] {}
    2018-06-11 10:00:43,365 - Group['cstm-livy'] {}
    2018-06-11 10:00:43,365 - Group['hadoop'] {}
    2018-06-11 10:00:43,366 - Group['cstm-knox'] {}
    2018-06-11 10:00:43,366 - Group['cstm-spark'] {}
    2018-06-11 10:00:43,368 - User['yarn-ats'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,371 - User['cstm-ranger'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-ranger', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,373 - User['cstm-hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,376 - User['cstm-sqoop'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,378 - User['cstm-ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,381 - User['cstm-yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,384 - User['cstm-tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,386 - User['cstm-atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,389 - User['cstm-storm'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,391 - User['cstm-knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'cstm-knox'], 'uid': None}
    2018-06-11 10:00:43,394 - User['cstm-kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,397 - User['cstm-logsearch'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,399 - User['cstm-infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,402 - User['cstm-hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,404 - User['cstm-hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,407 - User['cstm-mr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,409 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,412 - User['cstm-zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-zeppelin', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,414 - User['cstm-zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,417 - User['cstm-livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-livy', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,419 - User['cstm-oozie'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,422 - User['cstm-spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'cstm-spark'], 'uid': None}
    2018-06-11 10:00:43,424 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,549 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-06-11 10:00:43,558 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-06-11 10:00:43,558 - Directory['/tmp/hbase-hbase'] {'owner': 'cstm-hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:43,696 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,819 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,942 - call['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase'] {}
    2018-06-11 10:00:43,953 - call returned (0, '1817')
    2018-06-11 10:00:43,954 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase /home/cstm-hbase,/tmp/cstm-hbase,/usr/bin/cstm-hbase,/var/log/cstm-hbase,/tmp/hbase-hbase 1817'] {'not_if': '(test $(id -u cstm-hbase) -gt 1000) || (false)'}
    2018-06-11 10:00:43,961 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase /home/cstm-hbase,/tmp/cstm-hbase,/usr/bin/cstm-hbase,/var/log/cstm-hbase,/tmp/hbase-hbase 1817'] due to not_if
    2018-06-11 10:00:43,962 - Group['hdfs'] {}
    2018-06-11 10:00:43,963 - User['cstm-hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-06-11 10:00:43,964 - FS Type: HDFS
    2018-06-11 10:00:43,964 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-06-11 10:00:44,026 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'root', 'group': 'hadoop'}
    2018-06-11 10:00:44,116 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-06-11 10:00:44,225 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}
    2018-06-11 10:00:44,235 - Skipping Execute[('setenforce', '0')] due to not_if
    2018-06-11 10:00:44,236 - Directory['/grid/0/log/hdfs'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:44,436 - Directory['/var/run/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'root', 'cd_access': 'a'}
    2018-06-11 10:00:44,595 - Directory['/tmp/hadoop-cstm-hdfs'] {'owner': 'cstm-hdfs', 'create_parents': True, 'cd_access': 'a'}
    2018-06-11 10:00:44,719 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'root'}
    2018-06-11 10:00:44,807 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'root'}
    2018-06-11 10:00:44,897 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:45,020 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-metrics2.properties'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:45,112 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}
    2018-06-11 10:00:45,233 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/configuration.xsl'] {'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:45,306 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'cstm-hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:45,409 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
    2018-06-11 10:00:45,545 - Skipping unlimited key JCE policy check and setup since the Java VM is not managed by Ambari
    2018-06-11 10:00:46,488 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:46,490 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:46,614 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:46,658 - Directory['/etc/security/limits.d'] {'owner': 'root', 'create_parents': True, 'group': 'root'}
    2018-06-11 10:00:46,721 - File['/etc/security/limits.d/hdfs.conf'] {'content': Template('hdfs.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}
    2018-06-11 10:00:46,836 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs_dn_jaas.conf'] {'content': Template('hdfs_dn_jaas.conf.j2'), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:46,925 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs_nn_jaas.conf'] {'content': Template('hdfs_nn_jaas.conf.j2'), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:47,011 - XmlConfig['hadoop-policy.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,026 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-policy.xml
    2018-06-11 10:00:47,026 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-policy.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,118 - XmlConfig['ssl-client.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,132 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-client.xml
    2018-06-11 10:00:47,133 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-client.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,234 - Directory['/usr/hdp/3.0.0.0-1469/hadoop/conf/secure'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'cd_access': 'a'}
    2018-06-11 10:00:47,489 - XmlConfig['ssl-client.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf/secure', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,503 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/secure/ssl-client.xml
    2018-06-11 10:00:47,504 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/secure/ssl-client.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,602 - XmlConfig['ssl-server.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,615 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-server.xml
    2018-06-11 10:00:47,615 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-server.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,712 - XmlConfig['hdfs-site.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {u'final': {u'dfs.datanode.failed.volumes.tolerated': u'true', u'dfs.datanode.data.dir': u'true', u'dfs.namenode.http-address': u'true', u'dfs.namenode.name.dir': u'true', u'dfs.webhdfs.enabled': u'true'}}, 'configurations': ...}
    2018-06-11 10:00:47,725 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs-site.xml
    2018-06-11 10:00:47,725 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs-site.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,889 - XmlConfig['core-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'xml_include_file': None, 'mode': 0644, 'configuration_attributes': {u'final': {u'fs.defaultFS': u'true'}}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:47,906 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml
    2018-06-11 10:00:47,906 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0644, 'encoding': 'UTF-8'}
    2018-06-11 10:00:48,038 - Writing File['/usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml'] because contents don't match
    2018-06-11 10:00:48,093 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/slaves'] {'content': Template('slaves.j2'), 'owner': 'root'}
    2018-06-11 10:00:48,181 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,214 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,287 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,304 - Repository['HDP-3.0-GPL-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469', 'action': ['create'], 'components': [u'HDP-GPL', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,329 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-1]\nname=HDP-3.0-GPL-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,397 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,417 - Repository['HDP-UTILS-1.1.0.22-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,446 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-1]\nname=HDP-3.0-GPL-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-1]\nname=HDP-UTILS-1.1.0.22-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,518 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,540 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:48,551 - Package['lzo'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,196 - Skipping installation of existing package lzo
    2018-06-11 10:00:49,196 - Package['hadooplzo_3_0_0_0_1469'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,402 - Skipping installation of existing package hadooplzo_3_0_0_0_1469
    2018-06-11 10:00:49,402 - Package['hadooplzo_3_0_0_0_1469-native'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,857 - Skipping installation of existing package hadooplzo_3_0_0_0_1469-native
    2018-06-11 10:00:49,861 - Directory['/grid/0/hadoop/hdfs/namenode'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
    2018-06-11 10:00:50,128 - Directory['/usr/lib/ambari-logsearch-logfeeder/conf'] {'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
    2018-06-11 10:00:50,315 - Generate Log Feeder config file: /usr/lib/ambari-logsearch-logfeeder/conf/input.config-hdfs.json
    2018-06-11 10:00:50,315 - File['/usr/lib/ambari-logsearch-logfeeder/conf/input.config-hdfs.json'] {'content': Template('input.config-hdfs.json.j2'), 'mode': 0644}
    2018-06-11 10:00:50,410 - Skipping setting up secure ZNode ACL for HFDS as it's supported only for NameNode HA mode.
    2018-06-11 10:00:50,415 - Called service start with upgrade_type: None
    2018-06-11 10:00:50,415 - HDFS: Setup ranger: command retry not enabled thus skipping if ranger admin is down !
    2018-06-11 10:00:50,417 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-client'] {'timeout': 20}
    2018-06-11 10:00:50,450 - call returned (0, 'hadoop-client - 3.0.0.0-1469')
    2018-06-11 10:00:50,451 - RangeradminV2: Skip ranger admin if it's down !
    2018-06-11 10:00:50,493 - checked_call['/usr/bin/kinit -c /var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147 -kt /etc/security/keytabs/nn.service.keytab nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM > /dev/null'] {'user': 'cstm-hdfs'}
    2018-06-11 10:00:50,611 - checked_call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,612 - call['ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'curl --location-trusted -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/b6b261de-4ab4-4c87-a271-bbaa9fc306f4 -c /var/lib/ambari-agent/tmp/cookies/b6b261de-4ab4-4c87-a271-bbaa9fc306f4 -w '""'""'%{http_code}'""'""' http://ctr-e138-1518143905142-357962-01-000006.hwx.site:6080/login.jsp --connect-timeout 10 --max-time 12 -o /dev/null 1>/tmp/tmpzBm4Ow 2>/tmp/tmprJvRDV''] {'quiet': False, 'env': {'KRB5CCNAME': '/var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'}}
    2018-06-11 10:00:50,729 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,729 - get_user_call_output returned (0, u'200', u'  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3630  100  3630    0     0  1954k      0 --:--:-- --:--:-- --:--:-- 3544k')
    2018-06-11 10:00:50,731 - call['/usr/bin/klist -s /var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'] {'user': 'cstm-hdfs'}
    2018-06-11 10:00:50,843 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,844 - call['ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'curl --location-trusted -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/49ac670c-d187-4b4c-8c13-32bc9f8ac060 -c /var/lib/ambari-agent/tmp/cookies/49ac670c-d187-4b4c-8c13-32bc9f8ac060 '""'""'http://ctr-e138-1518143905142-357962-01-000006.hwx.site:6080/service/public/v2/api/service?serviceName=cl1_hadoop&serviceType=hdfs&isEnabled=true'""'""' --connect-timeout 10 --max-time 12 -X GET 1>/tmp/tmpskeV7D 2>/tmp/tmpIUYUU7''] {'quiet': False, 'env': {'KRB5CCNAME': '/var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'}}
    2018-06-11 10:00:50,984 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,985 - get_user_call_output returned (0, u'[{""id"":2,""guid"":""92620a51-bd3f-44f1-aed6-29a7c80809ec"",""isEnabled"":true,""createdBy"":""cstm-hdfs"",""updatedBy"":""cstm-hdfs"",""createTime"":1528682426000,""updateTime"":1528682426000,""version"":1,""type"":""hdfs"",""name"":""cl1_hadoop"",""description"":""hdfs repo"",""configs"":{""commonNameForCertificate"":""-"",""dfs.secondary.namenode.kerberos.principal"":""nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""hadoop.security.authentication"":""kerberos"",""hadoop.security.auth_to_local"":""RULE:[1:$1@$0](ambari-qa@EXAMPLE.COM)s/.*/ambari-qa/\\nRULE:[1:$1@$0](cstm-hbase@EXAMPLE.COM)s/.*/cstm-hbase/\\nRULE:[1:$1@$0](cstm-hdfs@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[1:$1@$0](cstm-spark@EXAMPLE.COM)s/.*/cstm-spark/\\nRULE:[1:$1@$0](cstm-zeppelin@EXAMPLE.COM)s/.*/cstm-zeppelin/\\nRULE:[1:$1@$0](yarn-ats@EXAMPLE.COM)s/.*/yarn-ats/\\nRULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//\\nRULE:[2:$1@$0](activity_analyzer@EXAMPLE.COM)s/.*/activity_analyzer/\\nRULE:[2:$1@$0](activity_explorer@EXAMPLE.COM)s/.*/activity_explorer/\\nRULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](amsmon@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](atlas@EXAMPLE.COM)s/.*/cstm-atlas/\\nRULE:[2:$1@$0](ats-hbase@EXAMPLE.COM)s/.*/yarn-ats/\\nRULE:[2:$1@$0](cstm-knox@EXAMPLE.COM)s/.*/cstm-knox/\\nRULE:[2:$1@$0](cstm-livy@EXAMPLE.COM)s/.*/cstm-livy/\\nRULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](hbase@EXAMPLE.COM)s/.*/cstm-hbase/\\nRULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/cstm-hive/\\nRULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/cstm-mr/\\nRULE:[2:$1@$0](nfs@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/cstm-yarn/\\nRULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/cstm-oozie/\\nRULE:[2:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/cstm-ranger/\\nRULE:[2:$1@$0](rangertagsync@EXAMPLE.COM)s/.*/rangertagsync/\\nRULE:[2:$1@$0](rangerusersync@EXAMPLE.COM)s/.*/rangerusersync/\\nRULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/cstm-yarn/\\nRULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/cstm-yarn/\\nDEFAULT"",""dfs.datanode.kerberos.principal"":""dn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""tag.download.auth.users"":""cstm-hdfs"",""password"":""*****"",""policy.download.auth.users"":""cstm-hdfs"",""hadoop.rpc.protection"":""authentication"",""dfs.namenode.kerberos.principal"":""nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""fs.default.name"":""hdfs://ctr-e138-1518143905142-357962-01-000006.hwx.site:8020"",""hadoop.security.authorization"":""true"",""username"":""hadoop""},""policyVersion"":3,""policyUpdateTime"":1528682427000,""tagVersion"":1,""tagUpdateTime"":1528682426000}]', u'  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r  0     0    0  2603    0     0  92268      0 --:--:-- --:--:-- --:--:-- 92268')
    2018-06-11 10:00:50,986 - Hdfs Repository cl1_hadoop exist
    2018-06-11 10:00:50,989 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-security.xml'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:51,064 - Writing File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-security.xml'] because contents don't match
    2018-06-11 10:00:51,124 - Directory['/etc/ranger/cl1_hadoop'] {'owner': 'cstm-hdfs', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:51,306 - Directory['/etc/ranger/cl1_hadoop/policycache'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:51,515 - File['/etc/ranger/cl1_hadoop/policycache/hdfs_cl1_hadoop.json'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:51,605 - XmlConfig['ranger-hdfs-audit.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,617 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-audit.xml
    2018-06-11 10:00:51,618 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-audit.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:51,750 - XmlConfig['ranger-hdfs-security.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,767 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-security.xml
    2018-06-11 10:00:51,768 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-security.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:51,892 - XmlConfig['ranger-policymgr-ssl.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,905 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-policymgr-ssl.xml
    2018-06-11 10:00:51,905 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-policymgr-ssl.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:52,023 - Execute[(u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/ranger_credential_helper.py', '-l', u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/install/lib/*', '-f', '/etc/ranger/cl1_hadoop/cred.jceks', '-k', 'sslKeyStore', '-v', [PROTECTED], '-c', '1')] {'logoutput': True, 'environment': {'JAVA_HOME': u'/usr/lib/jvm/java-openjdk'}, 'sudo': True}
    Using Java:/usr/lib/jvm/java-openjdk/bin/java
    Alias sslKeyStore created successfully!
    2018-06-11 10:00:53,259 - Execute[(u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/ranger_credential_helper.py', '-l', u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/install/lib/*', '-f', '/etc/ranger/cl1_hadoop/cred.jceks', '-k', 'sslTrustStore', '-v', [PROTECTED], '-c', '1')] {'logoutput': True, 'environment': {'JAVA_HOME': u'/usr/lib/jvm/java-openjdk'}, 'sudo': True}
    Using Java:/usr/lib/jvm/java-openjdk/bin/java
    Alias sslTrustStore created successfully!
    2018-06-11 10:00:54,461 - File['/etc/ranger/cl1_hadoop/cred.jceks'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0640}
    2018-06-11 10:00:54,556 - File['/etc/ranger/cl1_hadoop/.cred.jceks.crc'] {'owner': 'cstm-hdfs', 'only_if': 'test -e /etc/ranger/cl1_hadoop/.cred.jceks.crc', 'group': 'hadoop', 'mode': 0640}
    2018-06-11 10:00:54,652 - File['/etc/hadoop/conf/dfs.exclude'] {'owner': 'cstm-hdfs', 'content': Template('exclude_hosts_list.j2'), 'group': 'hadoop'}
    2018-06-11 10:00:54,749 - call[('ls', u'/grid/0/hadoop/hdfs/namenode')] {}
    2018-06-11 10:00:54,756 - call returned (0, '')
    2018-06-11 10:00:54,757 - Execute['ls /grid/0/hadoop/hdfs/namenode | wc -l  | grep -q ^0$'] {}
    2018-06-11 10:00:54,765 - Execute['hdfs --config /usr/hdp/3.0.0.0-1469/hadoop/conf namenode -format -nonInteractive'] {'logoutput': True, 'path': ['/usr/hdp/3.0.0.0-1469/hadoop/bin'], 'user': 'cstm-hdfs'}
    ######## Hortonworks #############
    This is MOTD message, added for testing in qe infra
    WARNING: /var/run/hadoop/cstm-hdfs does not exist. Creating.
    mkdir: cannot create directory ‘/var/run/hadoop/cstm-hdfs’: Permission denied
    ERROR: Unable to create /var/run/hadoop/cstm-hdfs. Aborting.
    
    Command failed after 1 tries
    

Artifacts:

    
    
    
    http://testqelog.s3.amazonaws.com/qelogs/nat/107592/ambari-blueprints/split-6/nat-yc-r7-gfgs-ambari-blueprints-6/log_tree/index.html
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13163814	AutoStart Is not working for some of the components in the cluster	"Autostart is not working for the following components:-

TIMELINE_READER
 HIVE_SERVER
 SPARK2_THRIFTSERVER
 HISTORYSERVER
 HBASE_MASTER
 ZEPPELIN_MASTER
 OOZIE_SERVER
 APP_TIMELINE_SERVER
 SPARK2_JOBHISTORYSERVER

 {code}

[root@re-nat-yc-r7-nubs-ambari-autostart-1-1 ~]# tail -f /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py
 INFO 2018-06-04 11:31:51,233 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,236 RecoveryManager.py:213 - SPARK2_THRIFTSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,238 RecoveryManager.py:213 - ZEPPELIN_MASTER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,250 RecoveryManager.py:213 - HISTORYSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,251 RecoveryManager.py:213 - HIVE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,254 RecoveryManager.py:213 - HBASE_MASTER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,255 RecoveryManager.py:213 - APP_TIMELINE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,257 RecoveryManager.py:213 - HIVE_METASTORE needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,258 RecoveryManager.py:213 - SPARK2_JOBHISTORYSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,261 RecoveryManager.py:213 - OOZIE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.

 {code}"	AMBARI	Resolved	1	1	2212	pull-request-available
13282971	After node reboot autostart of components takes too much time.	"The reason for this is because ambari-agent does not know start order. And so
if INFRA_SOLR with timeout of 10 hours starts before ZOOKEEPER, which it
requires, it’s a big problem. As well as it blocks agent queue for any other
commands. The solution is to run the commands in parallel. Also for fast
autostart users might want to disable retry_gap: {noformat}/var/lib/ambari-
server/resources/scripts/configs.py -a set -l localhost -n cluster_name -u
admin -p admin -c cluster-env -k retry_gap_in_sec -v 0{noformat}

"	AMBARI	Resolved	3	1	2212	pull-request-available
13133340	Fix TestRecoveryManager ClusterConfigurationCache and a bunch of small tests.	"Fix the below tests on branch-3.0-perf:

    
    
    TestClusterConfigurationCache.py
    TestFileCache.py
    TestRecoveryManager.py
    TestScript.py
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13173437	install tasks sometimes fail	"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ZOOKEEPER/package/scripts/zookeeper_client.py"", line 81, in <module>
        ZookeeperClient().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ZOOKEEPER/package/scripts/zookeeper_client.py"", line 61, in install
        self.install_packages(env)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 836, in install_packages
        retry_count=agent_stack_retry_count)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/packaging.py"", line 30, in action_install
        self._pkg_manager.install_package(package_name, self.__create_context())
      File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/yum_manager.py"", line 219, in install_package
        shell.repository_manager_executor(cmd, self.properties, context)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 742, in repository_manager_executor
        call_result = subprocess_executor(cmd, timeout=-1, env=env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 446, in subprocess_executor
        lines = [line for line in output]
      File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
        self.gen.next()
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 528, in process_executor
        kill_timer.join()
      File ""/usr/lib64/python2.7/threading.py"", line 940, in join
        raise RuntimeError(""cannot join thread before it is started"")
    RuntimeError: cannot join thread before it is started
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13136552	Unable to enable hive interactive, LLAP, on our Ambari 2.5.2 managed cluster	"While trying to enable the hive interactive plugin identified
that Ambari was looking in the incorrect directory when trying to locate the
users keytab.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13250764	Hive Service Check fails during Rolling Upgrade from HDP-3.1.0.0 to HDP-3.1.4.0	"Hive Service Check fails during Rolling Upgrade from HDP-3.1.0.0 to
HDP-3.1.4.0-301 {code} 2019-08-11 17:24:29,680 - Using hadoop conf dir:
/usr/hdp/3.1.4.0-301/hadoop/conf 2019-08-11 17:24:29,698 - call['ambari-
python-wrap /usr/bin/hdp-select status hive-server2'] {'timeout': 20}
2019-08-11 17:24:29,732 - call returned (0, 'hive-server2 - 3.1.0.0-78')
2019-08-11 17:24:29,734 - Stack Feature Version Info: Cluster Stack=3.1,
Command Stack=None, Command Version=3.1.4.0-301, Upgrade Direction=upgrade ->
3.1.4.0-301 2019-08-11 17:24:29,775 - File['/var/lib/ambari-
agent/cred/lib/CredentialUtil.jar'] {'content':
DownloadSource('http://ctr-e141-1563959304486-21229-01-000007.hwx.site:8080/resources/CredentialUtil.jar'),
'mode': 0755} 2019-08-11 17:24:29,777 - Not downloading the file from
http://ctr-e141-1563959304486-21229-01-000007.hwx.site:8080/resources/CredentialUtil.jar,
because /var/lib/ambari-agent/tmp/CredentialUtil.jar already exists 2019-08-11
17:24:30,749 - Running Hive Server checks 2019-08-11 17:24:30,749 -
-------------------------- 2019-08-11 17:24:30,752 - Server Address List :
[u'ctr-e141-1563959304486-21229-01-000002.hwx.site',
u'ctr-e141-1563959304486-21229-01-000004.hwx.site'], Port : 10000, SSL
KeyStore : None 2019-08-11 17:24:30,752 - Waiting for the Hive Server to
start... 2019-08-11 17:24:30,753 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:30,863 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:36,712 - Successfully connected to
ctr-e141-1563959304486-21229-01-000002.hwx.site on port 10000 2019-08-11
17:24:36,713 - Successfully stayed connected to 'Hive Server' on host:
ctr-e141-1563959304486-21229-01-000007.hwx.site and port 10000 after
5.96049404144 seconds 2019-08-11 17:24:36,713 - Running Hive Server2 checks
2019-08-11 17:24:36,713 - -------------------------- 2019-08-11 17:24:36,719 -
Server Address List : [u'ctr-e141-1563959304486-21229-01-000002.hwx.site'],
Port : 10500, SSL KeyStore : None 2019-08-11 17:24:36,719 - Waiting for the
Hive Server2 to start... 2019-08-11 17:24:36,719 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:36,834 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:42,100 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:24:47,106 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:47,225 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:52,456 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:24:57,462 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:57,582 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:02,793 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:07,798 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:07,919 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:13,260 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:18,265 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:18,360 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:23,658 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:28,664 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:28,772 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:34,144 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:39,146 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:39,264 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:44,573 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:49,579 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:49,694 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:54,972 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:59,977 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:00,094 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:05,416 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:10,422 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:10,539 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:15,973 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:20,979 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:21,090 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:26,397 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:31,401 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:31,516 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:36,865 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:41,866 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:41,983 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:47,237 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:52,243 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:52,362 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:57,704 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:02,710 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:02,805 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:08,094 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:13,099 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:13,215 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:18,498 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:23,504 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:23,622 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:28,949 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:33,953 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:34,071 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:39,277 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:44,283 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:44,399 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:49,720 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:54,726 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:54,846 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:00,106 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:05,112 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:05,229 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:10,652 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:15,658 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:15,778 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:21,134 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:26,140 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:26,266 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:31,610 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:36,616 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:36,734 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:42,040 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:47,045 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:47,161 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:52,510 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:57,516 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:57,633 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:02,939 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:29:07,945 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:29:08,064 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:13,369 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:29:18,374 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:29:18,475 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:23,822 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed Command failed after 1 tries {code} Ambari was upgraded
from 2.7.3.0 to 2.7.4.0

"	AMBARI	Resolved	3	1	2212	pull-request-available
13273864	Credentials should not be shown on cleartext on Ambari UI	"Please see screenshot attached. Ambari UI - Stack and Versions page shows
username and password in cleartext . We should atleast hide the password Also
in Review page for UI install cc [~accountid:557058:af856db7-a0ad-
4e2b-b848-f11f481bf96f] / [~accountid:5dc59258b6e6b50c58af136b] /
[~accountid:557058:e797d548-8a74-4f63-a68d-616111201b1c]

"	AMBARI	Resolved	3	1	2212	pull-request-available
13140359	yum installation fails if there is any transaction files	"Starting from 2.6.1 Ambari started checking transaction files during the repo/service installation and it fails with below error.


{noformat}
2018-02-16 16:12:21,639 - File['/etc/yum.repos.d/ambari-hdp-104.repo'] {'content': '[HDP-2.6-repo-104]\nname=HDP-2.6-repo-104\nbaseurl=http://xxxxxxx/vcm_hadoop/HDP/centos6/2.6.4.0-91\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-104]\nname=HDP-UTILS-1.1.0.22-repo-104\nbaseurl=http://xxxxxx/vcm_hadoop/HDP-UTILS-1.1.0.22\n\npath=/\nenabled=1\ngpgcheck=0'} 
2.  2018-02-16 16:12:21,639 - Writing File['/etc/yum.repos.d/ambari-hdp-104.repo'] because contents don't match 
3.  2018-02-16 16:12:21,639 - Yum non-completed transactions check failed, found 1 non-completed transaction(s): 
4.  2018-02-16 16:12:21,640 - [2015-03-12.17:13.28.disabled] Packages broken: hue-common-2.6.1.2.2.0.0-2041.el6.x86_64; Packages not-installed hue-hcatalog-2.6.1.2.2.0.0-2041.el6.x86_64, hue-2.6.1.2.2.0.0-2041.el6.x86_64, hue-pig-2.6.1.2.2.0.0-2041.el6.x86_64, hue-oozie-2.6.1.2.2.0.0-2041.el6.x86_64, hue-beeswax-2.6.1.2.2.0.0-2041.el6.x86_64, hue-server-2.6.1.2.2.0.0-2041.el6.x86_64 
5.  2018-02-16 16:12:21,640 - *** Incomplete Yum Transactions *** 
6.  2018-02-16 16:12:21,640 - 
7.  2018-02-16 16:12:21,640 - Ambari has detected that there are incomplete Yum transactions on this host. This will interfere with the installation process and must be resolved before continuing. 
8.  2018-02-16 16:12:21,640 - 
9.  2018-02-16 16:12:21,640 - - Identify the pending transactions with the command 'yum history list <packages failed>' 
10. 2018-02-16 16:12:21,640 - - Revert each pending transaction with the command 'yum history undo' 
11. 2018-02-16 16:12:21,640 - - Flush the transaction log with 'yum-complete-transaction --cleanup-only' 
After cleaning up with 'yum-complete-transaction --cleanup-only' still Ambari could not able to proceed further with same error.

When user tried to do yum install it was going through successfully.

Finally had to delete /var/lib/yum/transaction* manually for ambari to proceed further with installation. these transactions files were very old and nothing to do with any latest installation but still ambari does not proceed further.
{noformat}


What is expected: If this validation was done intentionally then it should throw proper message guiding the user to remove those files.

OptionsAttachments"	AMBARI	Resolved	3	4	2212	pull-request-available
13081417	Python tests fail for ambari-server and ambari-agent on ppc64le	"There are 78 python test failures in Ambari agent for ppc64le

Error:

{code:java}
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-agent/src/test/python/resource_management/TestXmlConfigResource.py"", line 68, in test_action_create_empty_xml_config
    configuration_attributes={}
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/environment.py"", line 118, in run_action
    resource.provider)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/providers/__init__.py"", line 93, in find_provider
    if resource in os_family_provider:
UnboundLocalError: local variable 'os_family_provider' referenced before assignment
{code}

In Ambari-server, the below tests fail: 


{code:java}
ERROR: test_configure_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 37, in test_configure_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_install_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 79, in test_install_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 41, in install
    self.configure(env)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_start_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 48, in test_start_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 49, in start
    self.configure(env)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_start_default_22_with_phoenix_enabled (test_hbase_regionserver.TestHbaseRegionServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HBASE/test_hbase_regionserver.py"", line 427, in test_start_default_22_with_phoenix_enabled
    target = RMFTestCase.TARGET_COMMON_SERVICES)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 93, in start
    self.configure(env) # for security
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 49, in configure
    hbase(name='regionserver')
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase.py"", line 224, in hbase
    Package(params.phoenix_package,
AttributeError: 'module' object has no attribute 'phoenix_package'

FAIL: test_clean_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 124, in test_clean_default
    self.assert_clean_default()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 182, in assert_clean_default
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/removeMysqlUser.sh mysql hive c6402.ambari.apache.org' != u'bash -x /tmp/removeMysqlUser.sh mysqld hive c6402.ambari.apache.org'

FAIL: test_clean_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 135, in test_clean_secured
    self.assert_clean_secured()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 193, in assert_clean_secured
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/removeMysqlUser.sh mysql hive c6402.ambari.apache.org' != u'bash -x /tmp/removeMysqlUser.sh mysqld hive c6402.ambari.apache.org'

FAIL: test_configure_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 38, in test_configure_default
    self.assert_configure_default()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 153, in assert_configure_default
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/addMysqlUser.sh mysql hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org' != u'bash -x /tmp/addMysqlUser.sh mysqld hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org'

FAIL: test_configure_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 81, in test_configure_secured
    self.assert_configure_secured()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 171, in assert_configure_secured
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/addMysqlUser.sh mysql hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org' != u'bash -x /tmp/addMysqlUser.sh mysqld hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org'

FAIL: test_start_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 53, in test_start_default
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'start') != ('service', 'mysqld', 'start')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'start')
+ ('service', 'mysqld', 'start')
?                   +


FAIL: test_start_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 96, in test_start_secured
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'start') != ('service', 'mysqld', 'start')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'start')
+ ('service', 'mysqld', 'start')
?                   +


FAIL: test_stop_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 68, in test_stop_default
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'stop') != ('service', 'mysqld', 'stop')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'stop')
+ ('service', 'mysqld', 'stop')
?                   +


FAIL: test_stop_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 112, in test_stop_secured
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'stop') != ('service', 'mysqld', 'stop')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'stop')
+ ('service', 'mysqld', 'stop')
?                   +


FAIL: test_service_check_default (test_service_check.TestServiceCheck)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 40, in test_service_check_default
    self.assert_service_check()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 153, in assert_service_check
    try_sleep = 5,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: '/tmp/oozieSmoke2.sh suse /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False' != u'/tmp/oozieSmoke2.sh suse-ppc /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False'

FAIL: test_service_check_secured (test_service_check.TestServiceCheck)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 53, in test_service_check_secured
    self.assert_service_check()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 153, in assert_service_check
    try_sleep = 5,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: '/tmp/oozieSmoke2.sh suse /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False' != u'/tmp/oozieSmoke2.sh suse-ppc /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False'

FAIL: test_hook_default (test_before_install.TestHookBeforeInstall)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/hooks/before-INSTALL/test_before_install.py"", line 42, in test_hook_default
    repo_template='[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 282, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6 [truncated]... != {'base_url': u'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0. [truncated]...
  {'action': ['create'],
-  'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0',
+  'base_url': u'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0',
?              +

-  'components': ['HDP', 'main'],
+  'components': [u'HDP', 'main'],
?                 +

   'mirror_list': None,
-  'repo_file_name': 'HDP',
+  'repo_file_name': u'HDP',
?                    +

-  'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'}
+  'repo_template': u'{{package_type}} {{base_url}} {{components}}'}

FAIL: test_hook_default_repository_file (test_before_install.TestHookBeforeInstall)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/hooks/before-INSTALL/test_before_install.py"", line 80, in test_hook_default_repository_file
    append_to_file=False)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 282, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'append_to_file': False, 'base_url': 'http://repo1/HDP/centos5/2.x/updates/2.2. [truncated]... != {'append_to_file': False, 'base_url': u'http://repo1/HDP/centos5/2.x/updates/2.2 [truncated]...
  {'action': ['create'],
   'append_to_file': False,
-  'base_url': 'http://repo1/HDP/centos5/2.x/updates/2.2.0.0',
+  'base_url': u'http://repo1/HDP/centos5/2.x/updates/2.2.0.0',
?              +

-  'components': ['HDP', 'main'],
+  'components': [u'HDP', 'main'],
?                 +

   'mirror_list': None,
   'repo_file_name': 'ambari-hdp-4',
-  'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'}
+  'repo_template': u'{{package_type}} {{base_url}} {{components}}'}

FAIL: testTransparentHugePage (TestCheckHost.TestCheckHost)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/custom_actions/TestCheckHost.py"", line 407, in testTransparentHugePage
    self.assertEquals(structured_out_mock.call_args[0][0], {'transparentHugePage' : {'message': 'never', 'exit_code': 0}})
AssertionError: {'transparentHugePage': {'message': '', 'exit_code': 0}} != {'transparentHugePage': {'message': 'never', 'exit_code': 0}}
- {'transparentHugePage': {'exit_code': 0, 'message': ''}}
+ {'transparentHugePage': {'exit_code': 0, 'message': 'never'}}
?                                                      +++++

{code}



"	AMBARI	Patch Available	3	1	2212	powerpc, ppc64le
13135406	Fix TestAlertSchedulerHandler.py and TestAlerts.py	"How the configs and alerts definitions are consumed by the alerts framework
has completely changed on branch-3.0-perf so most alert-related tests require
rewriting.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13260554	Ambari is indexing all subdirectories contents under /resources folder via API.	GET /resources gives back the directory content of /var/lib/ambari-server/resources. The directory doesn't contain any sensitive information, only files which are already visible on github. But it might freak out security guys therefore it's best to disable the listing.	AMBARI	Resolved	2	1	2212	pull-request-available
13190622	Ambari-agent takes up too many cpu on perf	"   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    14129 1426.122    0.101 1426.122    0.101 {time.sleep}
        1    0.337    0.337 1426.769 1426.769 main.py:358(run_threads)
      331    0.219    0.001    0.219    0.001 {method 'acquire' of 'thread.lock' objects}
       11    0.181    0.016    0.181    0.016 {built-in method poll}
        1    0.108    0.108    0.108    0.108 {method 'do_handshake' of '_ssl._SSLSocket' objects}
    14151    0.042    0.000    0.042    0.000 threading.py:571(isSet)
      125    0.028    0.000    0.028    0.000 {method 'flush' of 'file' objects}
     5078    0.027    0.000    0.052    0.000 decoder.py:65(py_scanstring)
       15    0.020    0.001    0.020    0.001 {posix.read}
     5093    0.020    0.000    0.024    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}
        1    0.019    0.019    0.019    0.019 {method 'connect' of '_socket.socket' objects}
    15365    0.018    0.000    0.018    0.000 {method 'match' of '_sre.SRE_Pattern' objects}
55424/13131    0.016    0.000    0.057    0.000 encoder.py:332(_iterencode_dict)
    38241    0.014    0.000    0.014    0.000 {isinstance}
       21    0.013    0.001    0.022    0.001 collections.py:282(namedtuple)
    473/5    0.012    0.000    0.073    0.015 decoder.py:148(JSONObject)
     5078    0.009    0.000    0.034    0.000 encoder.py:43(py_encode_basestring_ascii)
        5    0.006    0.001    0.070    0.014 __init__.py:122(dump)
    13251    0.005    0.000    0.005    0.000 {method 'write' of 'file' objects}
        7    0.004    0.001    0.004    0.001 {ambari_commons.libs.x86_64._posixsubprocess.fork_exec}
     5638    0.004    0.000    0.004    0.000 encoder.py:49(replace)
   3167/5    0.003    0.000    0.073    0.015 scanner.py:27(_scan_once)
13353/9909    0.003    0.000    0.030    0.000 encoder.py:279(_iterencode_list)
       75    0.003    0.000    0.003    0.000 {method 'read' of '_ssl._SSLSocket' objects}
   3177/8    0.003    0.000    0.008    0.001 Utils.py:124(make_immutable)
    13131    0.003    0.000    0.060    0.000 encoder.py:409(_iterencode)
    11128    0.003    0.000    0.003    0.000 {method 'groups' of '_sre.SRE_Match' objects}
  2202/45    0.003    0.000    0.004    0.000 Utils.py:135(get_mutable_copy)
   474/10    0.002    0.000    0.008    0.001 Utils.py:170(__init__)
      238    0.002    0.000    0.002    0.000 {time.localtime}
      119    0.002    0.000    0.004    0.000 __init__.py:242(__init__)
     3759    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
    14752    0.002    0.000    0.002    0.000 {method 'end' of '_sre.SRE_Match' objects}
    16854    0.002    0.000    0.002    0.000 {method 'append' of 'list' objects}
     5098    0.002    0.000    0.002    0.000 {method 'join' of 'unicode' objects}
      238    0.002    0.000    0.007    0.000 __init__.py:451(format)
       13    0.002    0.000    0.004    0.000 metric_alert.py:286(__init__)
        7    0.001    0.000    0.026    0.004 subprocess32.py:1153(_execute_child)
       41    0.001    0.000    0.001    0.000 {open}
      616    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      238    0.001    0.000    0.004    0.000 __init__.py:404(formatTime)
       18    0.001    0.000    0.001    0.000 {posix.listdir}
        5    0.001    0.000    0.072    0.014 ClusterCache.py:131(persist_cache)
     4032    0.001    0.000    0.003    0.000 collections.py:323(<genexpr>)
       18    0.001    0.000    0.001    0.000 {method 'sort' of 'list' objects}
       90    0.001    0.000    0.001    0.000 {built-in method now}
      238    0.001    0.000    0.001    0.000 {time.strftime}
      119    0.001    0.000    0.001    0.000 __init__.py:1215(findCaller)
     5706    0.001    0.000    0.001    0.000 {method 'group' of '_sre.SRE_Match' objects}
      281    0.001    0.000    0.001    0.000 threading.py:146(acquire)
      281    0.001    0.000    0.001    0.000 threading.py:186(release)
      119    0.001    0.000    0.001    0.000 {method 'seek' of 'file' objects}
        1    0.001    0.001    0.001    0.001 ClusterTopologyCache.py:58(on_cache_update)
      119    0.001    0.000    0.005    0.000 handlers.py:144(shouldRollover)
     50/5    0.001    0.000    0.035    0.007 decoder.py:223(JSONArray)
Major cpu cosumers:
1) Regexp operation: 
As we can see a lot of time is took for regexp operations. This happens because we use non-compiled regular expressions.
2) Json operations:
Another major cpu consumer is json module, because _speedups.so is not compiled for python2.7 currently. We have this situation. This is tackled by other issue
3) Main thread waking up/sleeping too often.
This seems to create quite a bit cpu usage.
The approach was implemented so agent can check for SIGTERM (ambari-agent stop). A proper solution should be a usage signal.pause() instead of sleep/wakeup."	AMBARI	Resolved	3	1	2212	pull-request-available
13139371	[PERF] Deployment of PERF stack leads to Out of Sync	"While deploying PERF stack, PERF 1.0 stack is not installed and shows up as
Out of sync  

"	AMBARI	Resolved	3	1	2212	pull-request-available
13211473	/var/lib/ambari-agent/cache not updating (Ambari 2.7)	"As of Ambari 2.7, any changes to /var/lib/ambari-server/resources are not
pushed to ambari-agents cache (/var/lib/ambari-agent/cache/. This is not an
issue in Ambari 2.6.

Also, if you remove files or need to wipe ambari-agent cache(s), the files
will never repopulate.

Another example, is custom ambari host_scripts not being pushed to the ambari-
agents as should be done based on this article, and works in Ambari 2.6:
<https://community.hortonworks.com/articles/38149/how-to-create-and-register-
custom-ambari-alerts.html>

To reproduce:

  * 1\. deploy hosts with Ambari 2.7.3 and have them connected to an ambari-server
  * 2\. turn on DEBUG logging in ambari-agent
  * 3\. go to an ambari-agent. Notice that there is no /var/lib/ambari-agent/cache/host_scripts/.hash but there are .hash files for the stack directories.
  * 4\. on the ambari-server, add files into /var/lib/ambari-server/resources/host_scripts
  * 5\. restart ambari-agent and notice that their host_scripts directory is never updated and .hash is never generated
  * 6\. check ambari-agent.log and notice that other paths were checked (by FileCache.py) but host_scripts were not

"	AMBARI	Resolved	3	1	2212	pull-request-available
13155068	Ambari Metrics Service Check Fails Post Ambari Upgarde with error : Configuration parameter not found in configurations dictionary!	"In an unkerberized cluster , after ambari upgrade from 2.6.X to 2.7.0.0 , the
service check for Ambari Metrics fail with below error.

    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/service_check.py"", line 304, in <module>
        AMSServiceCheck().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/service_check.py"", line 170, in service_check
        import params
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/params.py"", line 119, in <module>
        'files', 'grafana-dashboards', stack_name))
      File ""/usr/lib64/python2.7/posixpath.py"", line 75, in join
        if b.startswith('/'):
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'service_package_folder' was not found in configurations dictionary!
"	AMBARI	Resolved	3	1	2212	pull-request-available
13147911	When credential store is enabled status commands should not generate jceks	"Status commands should not generate jceks as they don't need them. This was
the behavior in previous release.

Currently jceks are generated for every status command, flooding ambari-agent
logs and making status commands run unnecessary longer.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13168277	Remove reference to JDK 1.7 in ambari-server setup	"We need to remove the reference to JDK 1.7, as it's not supported with HDP 3.0. We still see the following in ambari-server
setup:
    
    
    [2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13200038	Dir creation fails if webhdfs is enabled	"The previous clusters had webhdfs disabled as a workaround.

With webHDFS turned back on, dir creation continues to fail.

    
    
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 90, in <module>
        AmsCollector().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 345, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 48, in start
        self.configure(env, action = 'start') # for security
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 43, in configure
        hbase('master', action)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/hbase.py"", line 228, in hbase
        dfs_type=params.dfs_type
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 712, in action_create_on_execute
        self.action_delayed(""create"")
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 709, in action_delayed
        self.get_hdfs_resource_executor().action_delayed(action_name, self)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 385, in action_delayed
        self.action_delayed_for_nameservice(None, action_name, main_resource)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 407, in action_delayed_for_nameservice
        self._assert_valid()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 346, in _assert_valid
        self.target_status = self._get_file_status(target)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 509, in _get_file_status
        list_status = self.util.run_command(target, 'GETFILESTATUS', method='GET', ignore_status_codes=['404'], assertable_result=False)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 226, in run_command
        return self._run_command(*args, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 294, in _run_command
        _, out, err = get_user_call_output(cmd, user=self.run_user, logoutput=self.logoutput, quiet=False)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/get_user_call_output.py"", line 62, in get_user_call_output
        raise ExecutionFailed(err_msg, code, files_output[0], files_output[1])
    resource_management.core.exceptions.ExecutionFailed: Execution of 'curl -sS -L -w '%{http_code}' -X GET -d '' -H 'Content-Length: 0' 'http://fakelocalhost:50070/webhdfs/v1s3a:/cloudhdp-dl-s3-2/c01/amshbase?op=GETFILESTATUS&user.name=hdfs' 1>/tmp/tmpsSkYRy 2>/tmp/tmpYQM8tv' returned 6. curl: (6) Could not resolve host: fakelocalhost; Unknown error
    000
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13182692	HDI Livy2 fails to restart	"Livy2 restart fails from Ambari due to Ambari could not fetch some Hadoop
configs?

StdErr:
{code}    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 148, in <module>
        LivyServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 351, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 62, in start
        self.configure(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 52, in configure
        setup_livy(env, 'server', upgrade_type=upgrade_type, action = 'config')
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/setup_livy2.py"", line 53, in setup_livy
        params.HdfsResource(None, action=""execute"")
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 681, in action_execute
        self.get_hdfs_resource_executor().action_execute(self)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 164, in action_execute
        logoutput=logoutput,
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
        returns=self.resource.returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hadoop --config /usr/hdp/3.0.1.0-175/hadoop/conf jar /var/lib/ambari-agent/lib/fast-hdfs-resource.jar /var/lib/ambari-agent/tmp/hdfs_resources_1535895647.58.json' returned 1. Initializing filesystem uri: hdfs://mycluster
    Creating: Resource [source=null, target=wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, type=directory, action=create, owner=livy, group=null, mode=700, recursiveChown=false, recursiveChmod=false, changePermissionforParents=false, manageIfExists=true] in hdfs://mycluster
    Exception occurred, Reason: Wrong FS: wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, expected: hdfs://mycluster
    java.lang.IllegalArgumentException: Wrong FS: wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, expected: hdfs://mycluster
    	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:781)
    	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:240)
    	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1583)
    	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1580)
    	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1595)
    	at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1768)
    	at org.apache.ambari.fast_hdfs_resource.Resource.checkResourceParameters(Resource.java:193)
    	at org.apache.ambari.fast_hdfs_resource.Runner.main(Runner.java:112)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
    	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
    
{code}

Please note that the system test sets the

property | from | to  
---|---|---  
`livy.server.recovery.state-store` | `zookeeper` | `filesystem`  
`livy.server.recovery.state-store.url` |
`zk1-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181,zk2-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181,zk5-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181`
| `wasb://spark2l-at30wu-
livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-
69e01f25-3b7b-4af4-a787-37664ab45f0c`  
  
and then the restart fails.

"	AMBARI	Resolved	3	1	2212	pull-request-available
12669081	Ambar-client updates for JIRA-3201	"Some issue with the patch given in JIRA-3201

add testcase for get_hosts , remove create_hosts/create_hosts as of now.
"	AMBARI	Resolved	4	4	2212	client
12972325	Any start command fails if AMS is installed.	"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 39, in <module>
        BeforeStartHook().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 254, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 28, in hook
        import params
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/params.py"", line 145, in <module>
        metric_collector_hosts += host + ':' + metric_collector_port + ','
    TypeError: unsupported operand type(s) for +=: 'NoneType' and 'str'
    Error: Error: Unable to run the custom hook script ['/usr/bin/python', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py', 'START', '/var/lib/ambari-agent/data/command-9.json', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START', '/var/lib/ambari-agent/data/structured-out-9.json', 'INFO', '/var/lib/ambari-agent/tmp']
    

"	AMBARI	Resolved	3	1	2212	ambari-metrics, ambari-server
13134015	NFS Gateway is not logging at the correct location	"/grid/0/log/**hdfs//hadoop-hdfs-root-
nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out**

    
    
    
    [root@ctr-e137-1514896590304-8236-01-000002 ~]# cd /grid/0/log/hdfs/
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# ls -ltr
    total 500
    -rw------- 1 root root        0 Jan  9 22:52 hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out
    -rw------- 1 hdfs hadoop      0 Jan  9 22:52 SecurityAuth.audit
    -rw-r--r-- 1 root root      714 Jan  9 22:52 privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out
    -rw------- 1 root root      117 Jan  9 23:00 privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err
    drwxrwxr-x 2 root hadoop   4096 Jan  9 23:54 root
    drwxr-xr-x 2 hdfs hadoop   4096 Jan  9 23:59 hdfs
    -rw------- 1 hdfs hadoop 493804 Jan 10 00:58 hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# ps -ef| grep nfs
    root       10637       1  0 Jan09 ?        00:00:00 jsvc.exec -Dproc_nfs3 -outfile /grid/0/log/hdfs//hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out -errfile /grid/0/log/hdfs//privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err -pidfile /var/run/hadoop//hadoop-hdfs-root-nfs3.pid -nodetach -user hdfs -cp /usr/hdp/3.0.0.0-691/hadoop/conf:/usr/hdp/3.0.0.0-691/hadoop/lib/*:/usr/hdp/3.0.0.0-691/hadoop/.//*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/./:/usr/hdp/3.0.0.0-691/hadoop-hdfs/lib/*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/.//*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/lib/*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/.//*:/usr/hdp/3.0.0.0-691/hadoop-yarn/./:/usr/hdp/3.0.0.0-691/hadoop-yarn/lib/*:/usr/hdp/3.0.0.0-691/hadoop-yarn/.//*:/usr/hdp/3.0.0.0-691/tez/*:/usr/hdp/3.0.0.0-691/tez/lib/*:/usr/hdp/3.0.0.0-691/tez/conf:/usr/hdp/3.0.0.0-691/tez/doc:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-2.8-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib:/usr/hdp/3.0.0.0-691/tez/man:/usr/hdp/3.0.0.0-691/tez/tez-api-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-common-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-dag-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-examples-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-ext-service-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-history-parser-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-javadoc-tools-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-job-analyzer-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-mapreduce-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-internals-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-library-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-cache-plugin-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-acls-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-fs-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/ui:/usr/hdp/3.0.0.0-691/tez/lib/RoaringBitmap-0.4.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/async-http-client-1.8.16.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-cli-1.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-codec-1.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections-3.2.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections4-4.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-io-2.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-lang-2.6.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-math3-3.1.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/guava-11.0.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-hdfs-client-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-common-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-core-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-client-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-json-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jettison-1.3.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-util-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jsr305-3.0.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/metrics-core-3.1.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/netty-3.6.2.Final.jar:/usr/hdp/3.0.0.0-691/tez/lib/protobuf-java-2.5.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/servlet-api-2.5.jar:/usr/hdp/3.0.0.0-691/tez/lib/slf4j-api-1.7.10.jar:/usr/hdp/3.0.0.0-691/tez/lib/tez.tar.gz -Dhdp.version=3.0.0.0-691 -Djava.net.preferIPv4Stack=true -Dhdp.version=3.0.0.0-691 -Xmx1024m -Dhadoop.security.logger=ERROR,DRFAS -jvm server -Dyarn.log.dir=/grid/0/log/hdfs/ -Dyarn.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dyarn.home.dir=/usr/hdp/3.0.0.0-691/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=:/usr/hdp/3.0.0.0-691/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.0.0.0-691/hadoop/lib/native -Dhadoop.log.dir=/grid/0/log/hdfs/ -Dhadoop.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dhadoop.home.dir=/usr/hdp/3.0.0.0-691/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter
    hdfs       25658   10637  0 Jan09 ?        00:06:37 jsvc.exec -Dproc_nfs3 -outfile /grid/0/log/hdfs//hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out -errfile /grid/0/log/hdfs//privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err -pidfile /var/run/hadoop//hadoop-hdfs-root-nfs3.pid -nodetach -user hdfs -cp /usr/hdp/3.0.0.0-691/hadoop/conf:/usr/hdp/3.0.0.0-691/hadoop/lib/*:/usr/hdp/3.0.0.0-691/hadoop/.//*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/./:/usr/hdp/3.0.0.0-691/hadoop-hdfs/lib/*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/.//*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/lib/*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/.//*:/usr/hdp/3.0.0.0-691/hadoop-yarn/./:/usr/hdp/3.0.0.0-691/hadoop-yarn/lib/*:/usr/hdp/3.0.0.0-691/hadoop-yarn/.//*:/usr/hdp/3.0.0.0-691/tez/*:/usr/hdp/3.0.0.0-691/tez/lib/*:/usr/hdp/3.0.0.0-691/tez/conf:/usr/hdp/3.0.0.0-691/tez/doc:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-2.8-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib:/usr/hdp/3.0.0.0-691/tez/man:/usr/hdp/3.0.0.0-691/tez/tez-api-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-common-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-dag-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-examples-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-ext-service-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-history-parser-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-javadoc-tools-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-job-analyzer-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-mapreduce-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-internals-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-library-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-cache-plugin-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-acls-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-fs-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/ui:/usr/hdp/3.0.0.0-691/tez/lib/RoaringBitmap-0.4.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/async-http-client-1.8.16.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-cli-1.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-codec-1.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections-3.2.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections4-4.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-io-2.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-lang-2.6.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-math3-3.1.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/guava-11.0.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-hdfs-client-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-common-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-core-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-client-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-json-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jettison-1.3.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-util-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jsr305-3.0.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/metrics-core-3.1.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/netty-3.6.2.Final.jar:/usr/hdp/3.0.0.0-691/tez/lib/protobuf-java-2.5.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/servlet-api-2.5.jar:/usr/hdp/3.0.0.0-691/tez/lib/slf4j-api-1.7.10.jar:/usr/hdp/3.0.0.0-691/tez/lib/tez.tar.gz -Dhdp.version=3.0.0.0-691 -Djava.net.preferIPv4Stack=true -Dhdp.version=3.0.0.0-691 -Xmx1024m -Dhadoop.security.logger=ERROR,DRFAS -jvm server -Dyarn.log.dir=/grid/0/log/hdfs/ -Dyarn.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dyarn.home.dir=/usr/hdp/3.0.0.0-691/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=:/usr/hdp/3.0.0.0-691/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.0.0.0-691/hadoop/lib/native -Dhadoop.log.dir=/grid/0/log/hdfs/ -Dhadoop.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dhadoop.home.dir=/usr/hdp/3.0.0.0-691/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter
    root      449951  449877  0 21:36 pts/0    00:00:00 grep --color=auto nfs
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# 
    


"	AMBARI	Resolved	3	1	2212	pull-request-available
13181872	"Diff in Downloaded client config: Host file has Stack info where as downloaded file has 'None' in ""user.agent.prefix"" properties"	"  * Download client config for HDFS from UI
  * Compare the contents of core-site.xml between downloaded config and the one in /etc/hadoop/conf/core-site.xml

Following properties have a difference:

  * fs.azure.user.agent.prefix
  * fs.s3a.user.agent.prefix

Both of them have value **User-Agent: APN/1.0 Hortonworks/1.0 HDP/None** in
downloaded file where as in host config file it is **User-Agent: APN/1.0
Hortonworks/1.0 HDP/3.0.0.0-1453**

Could you please check

"	AMBARI	Resolved	3	1	2212	pull-request-available
13170247	Kafka failed to stop	"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka_broker.py"", line 145, in <module>
        KafkaBroker().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka_broker.py"", line 99, in stop
        ensure_base_directories()
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka.py"", line 266, in ensure_base_directories
        recursive_ownership = True,
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 125, in __new__
        cls(names_list.pop(0), env, provider, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 199, in action_create
        recursion_follow_links=self.resource.recursion_follow_links, safemode_folders=self.resource.safemode_folders)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 73, in _ensure_metadata
        sudo.chown_recursive(path, _user_entity, _group_entity, recursion_follow_links)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 55, in chown_recursive
        os.lchown(os.path.join(root, name), uid, gid)
    OSError: [Errno 2] No such file or directory: '/grid/0/log/kafka/controller.log'
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13169196	Component with recovery Enabled are not coming up when autostart is enabled 	"STR:-  
1\. test enable autostart for all component  
2\. test restart all the host at once.  
3\. test expect within 45 min. each and every component should be up.  
4\. test failed with the following exception

    
    
     ||Host:- nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal|| 
    
    
     |ComponentName : RANGER_TAGSYNC Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : DRUID_ROUTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
     ComponentName : HIVE_SERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : SPARK2_THRIFTSERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : HISTORYSERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : DRUID_BROKER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
      |ComponentName : HBASE_MASTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : HIVE_METASTORE Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : ZEPPELIN_MASTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : ACTIVITY_ANALYZER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : DRUID_OVERLORD Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : TIMELINE_READER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED
      |ComponentName : RANGER_ADMIN Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : OOZIE_SERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
      ComponentName : METRICS_COLLECTOR Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : METRICS_GRAFANA Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
    

That means these components are not up untill the waiting time completes.

I checked the agent logs:-  
attching the ambari-agent logs generated from  
cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py  
[agent-
autostart.log![](/images/icons/link_attachment_7.gif)](/secure/attachment/159734
/159734_agent-autostart.log ""agent-autostart.log attached to BUG-106407"")  
the relevent part if

    
    
    cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py
    
    INFO 2018-06-28 11:14:31,282 RecoveryManager.py:454 - RecoverConfig = {u'components': ()}
    INFO 2018-06-28 11:14:34,208 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'METRICS_COLLECTOR',
    INFO 2018-06-28 11:14:34,208 RecoveryManager.py:178 - New status, desired status is set to INIT for METRICS_COLLECTOR
    INFO 2018-06-28 11:14:50,695 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HBASE_REGIONSERVER
    INFO 2018-06-28 11:14:51,404 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SUPERVISOR
    INFO 2018-06-28 11:14:51,954 RecoveryManager.py:157 - New status, current status is set to INSTALLED for TEZ_CLIENT
    INFO 2018-06-28 11:14:52,672 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_CLIENT
    INFO 2018-06-28 11:14:53,467 RecoveryManager.py:157 - New status, current status is set to INSTALLED for PIG
    INFO 2018-06-28 11:14:54,241 RecoveryManager.py:157 - New status, current status is set to INSTALLED for KERBEROS_CLIENT
    INFO 2018-06-28 11:14:54,683 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HST_AGENT
    INFO 2018-06-28 11:14:55,367 RecoveryManager.py:157 - New status, current status is set to INSTALLED for METRICS_MONITOR
    INFO 2018-06-28 11:14:55,920 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ZOOKEEPER_CLIENT
    INFO 2018-06-28 11:14:56,458 RecoveryManager.py:157 - New status, current status is set to INSTALLED for LOGSEARCH_LOGFEEDER
    INFO 2018-06-28 11:14:57,017 RecoveryManager.py:157 - New status, current status is set to INSTALLED for INFRA_SOLR_CLIENT
    INFO 2018-06-28 11:14:57,823 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DATANODE
    INFO 2018-06-28 11:14:58,443 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HST_SERVER
    INFO 2018-06-28 11:14:59,205 RecoveryManager.py:157 - New status, current status is set to INSTALLED for TIMELINE_READER
    INFO 2018-06-28 11:15:00,015 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_ROUTER
    INFO 2018-06-28 11:15:00,617 RecoveryManager.py:157 - New status, current status is set to INSTALLED for NAMENODE
    INFO 2018-06-28 11:15:01,151 RecoveryManager.py:157 - New status, current status is set to INSTALLED for YARN_REGISTRY_DNS
    INFO 2018-06-28 11:15:01,745 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRPC_SERVER
    INFO 2018-06-28 11:15:02,250 RecoveryManager.py:157 - New status, current status is set to INSTALLED for STORM_UI_SERVER
    INFO 2018-06-28 11:15:02,812 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HBASE_MASTER
    INFO 2018-06-28 11:15:03,353 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_USERSYNC
    INFO 2018-06-28 11:15:03,825 RecoveryManager.py:157 - New status, current status is set to INSTALLED for NIMBUS
    INFO 2018-06-28 11:15:04,413 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SUPERSET
    INFO 2018-06-28 11:15:04,996 RecoveryManager.py:157 - New status, current status is set to INSTALLED for APP_TIMELINE_SERVER
    INFO 2018-06-28 11:15:05,552 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ACTIVITY_EXPLORER
    INFO 2018-06-28 11:15:06,202 RecoveryManager.py:157 - New status, current status is set to INSTALLED for LOGSEARCH_SERVER
    INFO 2018-06-28 11:15:07,436 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SPARK2_JOBHISTORYSERVER
    INFO 2018-06-28 11:15:07,962 RecoveryManager.py:157 - New status, current status is set to INSTALLED for INFRA_SOLR
    INFO 2018-06-28 11:15:08,494 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_METASTORE
    INFO 2018-06-28 11:15:09,508 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RESOURCEMANAGER
    INFO 2018-06-28 11:15:10,197 RecoveryManager.py:157 - New status, current status is set to INSTALLED for OOZIE_SERVER
    INFO 2018-06-28 11:15:10,840 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 11:15:11,517 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SPARK2_THRIFTSERVER
    INFO 2018-06-28 11:15:12,028 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_OVERLORD
    INFO 2018-06-28 11:15:12,998 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_BROKER
    INFO 2018-06-28 11:15:13,650 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_SERVER
    INFO 2018-06-28 11:15:14,321 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_TAGSYNC
    INFO 2018-06-28 11:15:14,852 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ACTIVITY_ANALYZER
    INFO 2018-06-28 11:15:14,956 RecoveryManager.py:163 - current status is set to INSTALLED for METRICS_COLLECTOR
    INFO 2018-06-28 11:15:15,655 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ZEPPELIN_MASTER
    INFO 2018-06-28 11:15:16,199 RecoveryManager.py:157 - New status, current status is set to INSTALLED for METRICS_GRAFANA
    INFO 2018-06-28 11:15:16,859 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HISTORYSERVER
    INFO 2018-06-28 11:17:36,038 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'METRICS_COLLECTOR',
    INFO 2018-06-28 11:17:36,042 RecoveryManager.py:183 - desired status is set to STARTED for METRICS_COLLECTOR
    INFO 2018-06-28 11:27:33,451 RecoveryManager.py:183 - desired status is set to INSTALLED for METRICS_COLLECTOR
    INFO 2018-06-28 11:27:33,451 RecoveryManager.py:620 - Received EXECUTION_COMMAND (STOP/INSTALL), desired state of METRICS_COLLECTOR to INSTALLED
    INFO 2018-06-28 11:29:10,328 RecoveryManager.py:588 - After EXECUTION_COMMAND (STOP/INSTALL), with taskId=63, current state of METRICS_COLLECTOR to INSTALLED
    INFO 2018-06-28 11:45:03,661 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 11:45:12,168 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 11:45:20,469 RecoveryManager.py:163 - current status is set to STARTED for HST_SERVER
    INFO 2018-06-28 11:45:26,682 RecoveryManager.py:163 - current status is set to STARTED for RANGER_TAGSYNC
    INFO 2018-06-28 11:45:46,474 RecoveryManager.py:163 - current status is set to STARTED for YARN_REGISTRY_DNS
    INFO 2018-06-28 11:45:54,622 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    INFO 2018-06-28 11:46:27,459 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_SERVER
    INFO 2018-06-28 11:48:53,946 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 14:34:17,142 RecoveryManager.py:583 - After EXECUTION_COMMAND (START), with taskId=1530194603, current state of ACTIVITY_EXPLORER to STARTED
    INFO 2018-06-28 14:34:18,039 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,105 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,170 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,259 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,303 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,409 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,478 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,570 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,616 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,644 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,674 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,730 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,764 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,780 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,831 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,875 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,888 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,960 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,982 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,015 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,051 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,090 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,108 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,125 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,220 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,290 RecoveryManager.py:454 - RecoverConfig = {u'components': ()}
    INFO 2018-06-28 14:35:36,027 RecoveryManager.py:163 - current status is set to INSTALLED for SUPERVISOR
    INFO 2018-06-28 14:36:05,838 RecoveryManager.py:163 - current status is set to STARTED for SUPERSET
    INFO 2018-06-28 14:36:45,585 RecoveryManager.py:163 - current status is set to STARTED for YARN_REGISTRY_DNS
    INFO 2018-06-28 14:37:18,681 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 14:37:55,603 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 14:38:14,872 RecoveryManager.py:163 - current status is set to STARTED for RANGER_TAGSYNC
    INFO 2018-06-28 14:38:29,806 RecoveryManager.py:163 - current status is set to STARTED for DATANODE
    INFO 2018-06-28 14:38:43,789 RecoveryManager.py:163 - current status is set to STARTED for RANGER_USERSYNC
    INFO 2018-06-28 14:38:54,727 RecoveryManager.py:163 - current status is set to INSTALLED for METRICS_MONITOR
    INFO 2018-06-28 14:39:40,572 RecoveryManager.py:163 - current status is set to STARTED for STORM_UI_SERVER
    INFO 2018-06-28 14:39:42,898 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 14:40:49,742 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 14:41:13,508 RecoveryManager.py:163 - current status is set to STARTED for HBASE_REGIONSERVER
    INFO 2018-06-28 14:44:02,873 RecoveryManager.py:163 - current status is set to STARTED for NIMBUS
    INFO 2018-06-28 14:45:17,040 RecoveryManager.py:163 - current status is set to STARTED for METRICS_COLLECTOR
    INFO 2018-06-28 14:45:20,928 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_LOGFEEDER
    INFO 2018-06-28 14:46:24,130 RecoveryManager.py:163 - current status is set to INSTALLED for INFRA_SOLR
    INFO 2018-06-28 14:46:24,598 RecoveryManager.py:163 - current status is set to STARTED for ACTIVITY_ANALYZER
    INFO 2018-06-28 14:47:14,171 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 14:49:22,384 RecoveryManager.py:163 - current status is set to STARTED for NAMENODE
    INFO 2018-06-28 14:49:31,552 RecoveryManager.py:163 - current status is set to STARTED for DRPC_SERVER
    INFO 2018-06-28 15:10:55,994 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_SERVER
    INFO 2018-06-28 15:11:01,316 RecoveryManager.py:163 - current status is set to STARTED for SPARK2_JOBHISTORYSERVER
    INFO 2018-06-28 15:11:11,251 RecoveryManager.py:163 - current status is set to STARTED for METRICS_GRAFANA
    INFO 2018-06-28 15:11:37,007 RecoveryManager.py:163 - current status is set to STARTED for HST_SERVER
    INFO 2018-06-28 15:11:57,521 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    INFO 2018-06-28 15:12:03,321 RecoveryManager.py:163 - current status is set to STARTED for OOZIE_SERVER
    INFO 2018-06-28 15:13:24,798 RecoveryManager.py:163 - current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 15:14:13,744 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 15:15:03,889 RecoveryManager.py:163 - current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 15:15:50,119 RecoveryManager.py:163 - current status is set to INSTALLED for OOZIE_SERVER
    INFO 2018-06-28 15:15:50,451 RecoveryManager.py:163 - current status is set to INSTALLED for NAMENODE
    INFO 2018-06-28 15:15:51,459 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 15:15:57,678 RecoveryManager.py:163 - current status is set to STARTED for NAMENODE
    INFO 2018-06-28 15:16:22,782 RecoveryManager.py:163 - current status is set to STARTED for HBASE_MASTER
    INFO 2018-06-28 15:16:40,512 RecoveryManager.py:163 - current status is set to STARTED for APP_TIMELINE_SERVER
    INFO 2018-06-28 15:16:43,582 RecoveryManager.py:163 - current status is set to STARTED for OOZIE_SERVER
    INFO 2018-06-28 15:17:45,952 RecoveryManager.py:163 - current status is set to STARTED for HISTORYSERVER
    INFO 2018-06-28 15:37:57,495 RecoveryManager.py:163 - current status is set to INSTALLED for HST_AGENT
    INFO 2018-06-28 15:38:47,195 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    

I checked is this because of memory issue. but

    
    
    [root@nat-yc-r7-irrs-ambari-autostart-1-re-re-3 data]# df -h
    Filesystem             Size  Used Avail Use% Mounted on
    /dev/mapper/rhel-root   17G   11G  6.0G  65% /
    devtmpfs               7.8G     0  7.8G   0% /dev
    tmpfs                  7.8G     0  7.8G   0% /dev/shm
    tmpfs                  7.8G  361M  7.5G   5% /run
    tmpfs                  7.8G     0  7.8G   0% /sys/fs/cgroup
    /dev/vda1             1014M  172M  843M  17% /boot
    /dev/vdb               246G   19G  216G   8% /grid/0
    tmpfs                  1.6G     0  1.6G   0% /run/user/1037
    tmpfs                  1.6G     0  1.6G   0% /run/user/0
    

I checked some of auto_errors

    
    
    cat auto_errors-1530190498.txt
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 132, in <module>
        HistoryServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 100, in start
        skip=params.sysprep_skip_copy_tarballs_hdfs) or resource_created
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 497, in copy_to_hdfs
        source_file = prepare_function()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 97, in _prepare_tez_tarball
        hadoop_lib_native_lzo_dir = os.path.join(stack_root, service_version, ""hadoop"", ""lib"", ""native"")
      File ""/usr/lib64/python2.7/posixpath.py"", line 75, in join
        if b.startswith('/'):
    AttributeError: 'NoneType' object has no attribute 'startswith'
    

attching the ambari-agent logs generated from  
cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py  
[agent-
autostart.log![](/images/icons/link_attachment_7.gif)](/secure/attachment/159734
/159734_agent-autostart.log ""agent-autostart.log attached to BUG-106407"")

Repro cluster:- <http://linux-jenkins.qe.hortonworks.com:8080/job/Nightly-
Start-EC2-Run-HDP/985907/>  
lifetime extended to 72 hours <http://linux-
jenkins.qe.hortonworks.com:8080/job/update-openstack-lifetime/12085/>

"	AMBARI	Resolved	3	1	2212	pull-request-available
13281670	Components autostart does not work sometimes after ambari-agent restart	If configurations where cached and didn't change during restart of agent (for some people they always change, they will not see the issue). Recovery didn't get enabled. Until some changes configs/topology changes where done.	AMBARI	Resolved	3	1	2212	pull-request-available
13134423	Agent commands hang even after freeing up disk space on the host	"*STR*
# Install a cluster with Ambari-2.6.2 and HDP-2.6.4.0
# Go the host (say host1) running Nimbus component and restart Nimbus
# Fill up the disk space on host1 (in my test, the disk space was filled up on the host running Nimbus component)
# Try to restart Nimbus. Nimbus restart expectedly fails with error:
{code}
Caught an exception while executing custom service command: <type 'exceptions.IOError'>: [Errno 28] No space left on device; [Errno 28] No space left on device
{code}
# Now free up the disk space on host1 and try to restart Nimbus

 

*Result*
Nimbus restart command hangs and eventually times out

Looks like the issue is because the action queue is unable to create new command for Nimbus restart."	AMBARI	Resolved	2	1	2212	pull-request-available, system_test
13150538	NN Federation Wizard: Bootstrap NameNode failed	"Bootstrap NameNode command fails due to stopped NameNode. Actually NameNode
should be started and command to start it was sent correctly from UI, but was
ignored by BE due to:

    
    
    Ignoring ServiceComponentHost as the current state matches the new desired state, clusterName=c, serviceName=HDFS, componentName=NAMENODE, hostname=c7404.ambari.apache.org, currentState=STARTED, newDesiredState=STARTED

This is because Format NameNode command starts NameNode for a little period of
time and then stops it back, but component's current state is not updated
immediately and BE thinks that NameNode is still started.  
From Format NameNode log:

    
    
    
    18/04/04 11:44:31 INFO namenode.NameNode: STARTUP_MSG: 
    /************************************************************
    STARTUP_MSG: Starting NameNode
    
    ...
    
    18/04/04 11:44:35 INFO namenode.NameNode: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down NameNode at c7404.ambari.apache.org/192.168.74.104
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13174129	Rescheduled and canceled tasks stay in progress forever	"1\. Ambari-server reschedules task (timeout #1)  
2\. Task did yet got rescheduled (due to something else being in queue)  
3\. but, ambari-server cancels it (timeout #2)  
4\. The tasks keeps reprorting that's it's in progess forever, however being
canceled

"	AMBARI	Resolved	3	1	2212	pull-request-available
13137130	"Debian stackdeploys failing with ""ambari-agent: command not found"" errors"	"Debian stackdeploys failing with ""ambari-agent: command not found"" errors even
though ambari-agent is installed.

From a debian cluster:

    
    
    
    root@ctr-e137-1514896590304-63273-01-000006:~# dpkg-query -l | grep ambari
    ii  ambari-agent                                  2.6.2.0-45                        amd64        Ambari Agent
    root@ctr-e137-1514896590304-63273-01-000006:~# find / -name ambari-agent
    /run/ambari-agent
    /usr/lib/ambari-agent
    /var/log/ambari-agent
    /var/lib/ambari-agent
    /var/lib/ambari-agent/bin/ambari-agent
    /etc/init.d/ambari-agent
    /etc/ambari-agent
    /grid/0/log/ambari-agent
"	AMBARI	Resolved	3	1	2212	pull-request-available
13157848	Make server/agent connection with no cert verification possible with agent python 2.7.5	"By reading <https://bugzilla.redhat.com/show_bug.cgi?id=1173041> and also the
last update of AMBARI-14149, I think the python fix is also backported into
python 2.7.5  
If so, could you change ""(2, 7, 9)"" to a lower version please?

Editing /etc/python/cert-verification.cfg wouldn't be ideal workaround as it
would affect to all other python applications in the system.  
And today I had a case which system didn't have this file (SUSE and Anaconda2
python)

Thank you

"	AMBARI	Resolved	3	1	2212	pull-request-available
13191894	Ambari-agent cannot register sometimes	"
    ERROR 2018-10-11 13:45:57,401 HeartbeatThread.py:108 - Exception in HeartbeatThread. Re-running the registration
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 95, in run
        self.register()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 163, in register
        self.force_component_status_update()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 173, in force_component_status_update
        self.component_status_executor.force_send_component_statuses()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ComponentStatusExecutor.py"", line 206, in force_send_component_statuses
        service_name, component_name = service_and_component_name.split(""/"")
    ValueError: need more than 1 value to unpack
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13029116	ATS reports as down in Ambari UI after upgrade	"Steps
1. Deploy HDP-2.5.0 cluster with Ambari-2.4.1.0
2. Upgrade Ambari to 2.5.0.0-419
3. Restart ATS and observe the status of ATS after sometime in Ambari UI

Result:
ATS shows down. Ambari-agent log shows below:
{code}
INFO 2016-12-08 16:14:44,419 ActionQueue.py:105 - Adding STATUS_COMMAND for component APP_TIMELINE_SERVER of service YARN of cluster cl1 to the queue.
INFO 2016-12-08 16:14:44,621 ActionQueue.py:105 - Adding STATUS_COMMAND for component NODEMANAGER of service YARN of cluster cl1 to the queue.
INFO 2016-12-08 16:14:46,429 PythonReflectiveExecutor.py:65 - Reflective command failed with exception:
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ambari_agent/PythonReflectiveExecutor.py"", line 57, in run_file
    imp.load_source('__main__', script)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 155, in <module>
    ApplicationTimelineServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 82, in status
    only_if = format(""test -e {yarn_historyserver_pid_file_old}"", user=status_params.yarn_user))
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 143, in run
    Logger.info_resource(resource)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 87, in info_resource
    Logger.info(Logger.filter_text(Logger._get_resource_repr(resource)))
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 110, in _get_resource_repr
    return Logger.get_function_repr(repr(resource), resource.arguments, resource)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 180, in get_function_repr
    return unicode(""{0} {{{1}}}"", 'UTF-8').format(name, arguments_str)
  File ""/usr/lib64/python2.6/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
AttributeError: 'NoneType' object has no attribute 'utf_8_decode'
INFO 2016-12-08 16:14:53,190 Controller.py:283 - Heartbeat (response id = 15365) with server is running...
INFO 2016-12-08 16:14:55,083 Heartbeat.py:90 - Adding host info/state to heartbeat message.
{code}
"	AMBARI	Resolved	2	1	2212	upgrade
13149939	Fix connection drop on ambari-agent by locking the write code	"This solution can possibly work to fix connection drop

    
    
    ERROR 2018-04-04 00:22:26,771 websocket.py:272 - Failed to receive data
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 394, in once
        b = self.sock.recv(self.reading_buffer_size)
      File ""/usr/lib64/python2.7/ssl.py"", line 759, in recv
        return self.read(buflen)
      File ""/usr/lib64/python2.7/ssl.py"", line 653, in read
        v = self._sslobj.read(len or 1024)
    SSLError: [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:1783)
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13178883	ambair-agent floods data directory with files created for status commands	"
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-001de461-9c2f-417a-b323-6233ed86eb37.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00209852-ad33-4026-90a0-4da2aac71403.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00333009-87b2-4649-80d1-c97b639a31f0.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0042da27-dae6-4e9b-9ffb-336cd17787c5.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-005b6b80-baf7-43a5-aa7f-2e4695b3f478.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0066dcd4-9406-49c7-a2b3-effd9a3a4f2b.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-006a1999-d33d-4d48-a74f-8a7830b449ec.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0094b368-c5e1-496e-ba5b-b5080767dbc0.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-00b2a497-0088-464c-a4e4-855534d082ed.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00bbd469-370f-4eca-aee9-109db61edd75.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00bbe2f2-aeb2-4eb7-9bfb-6ce5fe9950b0.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00c341b7-5c4c-4dc5-a5ba-a550b0cabf67.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00d80541-e6a5-4bc4-9020-c018a928c6b9.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00da8bca-ac68-4d47-b492-2a978f368a1c.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00ed37ff-a9b8-4fe6-a46d-7d792586ac62.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00f49f50-cd1e-498f-ac97-5cc9724adf56.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00f74fcb-8cc1-46f8-b74a-6ea3089e19d9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-010174eb-dee9-41a7-9afb-25c5b16e185e.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01024c7d-beb5-40b2-914c-aece83011cea.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0106b69f-9343-40d1-a225-65112470f884.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0109e024-4cbf-49a8-8496-9d05787ed58d.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-011e9c15-5531-43c9-aabf-a9efa5123d2f.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-012f2630-46ad-4608-b7ac-971d3ee35f59.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0130258e-5c15-4147-a348-c86fa0ae5a1c.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01357110-1577-4d4f-908d-7956232ecd73.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-013a04e5-d2e8-4d9e-8cb6-3fe3ac6f75ca.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01456ca3-f135-4fd8-8b58-83ea513f7c06.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01473538-7a02-4df9-92c4-728d158d3e0f.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-014bdd5a-899d-43f6-a375-d65e30e0a2c3.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-014e0a58-fdc5-4d0a-8b0c-ac62129ba0ce.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-015c87f3-cdaa-4cf5-91ee-bf9af09e4262.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-015c972a-9210-4854-a49b-a1d591f799e6.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-016d1956-8076-4722-829d-e10748420432.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-017e22f1-561a-41ca-a59e-1473f3ec3065.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-017fe18b-5dc7-4c97-891a-737810ec5416.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01d63c54-9033-4932-974f-f6c756a2d1d4.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01ff5f2e-4572-4f52-b77c-12e3aea1c6fa.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02216bb7-9bac-4c5b-a074-f3e2b33c7a86.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02239432-9ba1-4899-923f-2f94c5facb89.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0225d7ba-22c5-412e-bea1-931732bf98f2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-023d005d-d253-4445-951b-22b903e9ff85.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-023e4124-bd5d-4e0d-8f1e-b146b5f9bb03.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-024c5c89-56c0-4470-8d64-2ab9e48707fa.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-025bb933-f6a5-4495-8df4-342e0c695845.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-026dcffa-015b-4ac0-be4e-70c2092a091a.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0272568c-5492-46be-8c6d-26edebb06f92.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0283f8dd-0d90-4978-9b44-c982a0ee78e9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0291a3b2-9b11-4e60-9f36-d63e11d48803.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02a4a54a-fc9c-4839-94b3-fd29cdbaeda6.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02a781a3-4db4-468b-a180-964d4fcdf176.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02bdfe4c-a2ce-4e38-990b-12c889a06a32.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02c873ea-60ce-444a-8b67-05c648d4c8d5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02d706ec-4b9d-44ad-baf6-67ec62dbdcdd.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02de1f1d-d3e8-4c74-b5dc-550540f4277e.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-02e34a4b-8f2d-489b-acd3-3c77b07e4de9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-02e42e6d-8756-45f6-8097-16430aebb4e3.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02e48587-00ff-48e4-81d4-a8535b3d6cd7.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02eebc26-43ed-4b36-99df-091e66ed6317.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0304d392-7471-422c-9229-2222ca7be62a.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-031091e8-b6cd-47d8-804c-29947ad34c87.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03140883-e4d2-4cc3-9731-7406762199da.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03203cad-edfc-4293-90db-8edd305f470b.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-032c1a82-386b-43de-9e17-d6228ce4ca40.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-03351bb9-9cc3-4d80-9bbb-4edf49b93b9c.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0348d610-18bf-42cc-8f3b-e16671f0f7b8.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-034d698d-7946-4503-b64b-20c9592c363b.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03640ef9-54f3-45c9-b9de-a0a211524093.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-036ad39f-9ef1-4967-b023-316e3429af34.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0377c628-708b-41f5-b3cf-1dda26686fda.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-037b5ae4-966c-4815-82fb-d10d7e7947c5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-037ce4fe-4c96-4614-bca1-ddb4d702ce03.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-039472ba-ea55-4a68-922b-dd0d799c1c07.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03990281-da3a-4d79-8998-5ae573d62fae.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03f2239b-f8d9-4bb4-a043-a675b91dd2e2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03f70e9b-549f-4019-9bd6-19a511ce67e1.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0403a633-8967-4408-bd94-e8a5d1e1b4b2.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0416dae8-52da-48c6-9451-e3bd10f8a0c7.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-041eebf1-8908-4484-aa5b-67c22101942c.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0452e168-012a-4b77-b73e-89f900fd9ce5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-045c0c2a-4a93-4ccb-8b02-e31f32f297c4.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-046d50dd-f5de-4ef9-a0eb-36b6946a3ce9.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0490547f-88dd-446c-aa64-1897521e2eba.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-049a2eca-90a1-4ca4-9216-6065f90bf855.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-049f27b7-f073-41a3-a415-1d8975b2d608.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04a989b1-2d78-4f35-bef9-9be4ab6c13e0.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04b2d2ee-f5d4-4c98-8011-db57ec5c28da.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-04b3c7f8-5c80-418c-a4f6-c6148e0c8a92.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04be6118-ae18-4d2d-80ca-5fc79dacf86d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04bedda7-53ab-40b4-8c7d-33242b660f06.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04bfc4e2-aa8f-4a24-a79f-b483bb80d210.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04c7e1dc-3846-453e-be10-75955445088c.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04dd82ce-25da-4dea-8b20-cda14df347fd.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04f65567-6f8c-4da8-8505-80664a4ee439.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-050f7e3c-6460-4045-9bdb-1eb3b602136d.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-051e0ccb-d6b8-4158-a1a9-9d7d0d52224d.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-052e32da-f71b-4f09-b62b-b3f30b25ec32.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-056e9da7-5a96-4646-bae6-2d080d0b2a82.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-056f3673-aa47-4efc-b0ad-e0ab96812654.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05708b0e-0df8-4b16-8bd9-874a72607186.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0577ca36-f1f2-49ce-852a-43b56a6656ca.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-059f1d92-1419-42c4-a7fd-d0f85a85f240.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05af50fb-e48f-40a9-9877-962d1718bbff.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05b2dcef-6aea-400d-992d-f0443d4c56d2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05b706f4-cd06-49b1-8d9c-c356f38c99d3.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05f829b6-8340-4673-97a2-94eaf6290e23.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05ff36a6-c39a-412e-b6c4-58f0f50cbab0.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0609de51-1254-4385-98f8-0bc027510908.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06123cae-b89d-496b-8a26-13c3e208b087.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0625192e-495d-4a2f-8226-54befd87daea.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-062ab0e0-9854-46ff-91a0-47e6d920e055.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-062bef42-9ef9-4808-896d-4b77a124458d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06510548-2629-480b-99e0-9769a4a3e254.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-066265b2-56e9-4629-8c13-309fd5b9bedb.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-066f2f1e-337e-4dca-a7aa-51b4efcb2b4d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-067d3e69-333a-424f-a846-c6411d8e8e87.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068226fb-f9d8-46bb-8bd7-3a13056bb983.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0688025a-d92d-4a8d-b14d-f7486a3f0d59.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068c77f3-ad4f-4986-836f-f4b9f44b53bf.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068f3c75-a176-49c2-96ba-48a4a6cc76ed.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06adba4c-9a60-432a-80f4-780602f476a7.json
    ...
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13148206	Create topic handler for small agent actions. Like restart_agent, clean_caches etc.	"For bigger actions containing a lot of info or special workflow and a new
topic would be required. Small actions like restart_agent/clean_cache make
sense to be sent in one general topic

"	AMBARI	Resolved	3	1	2212	pull-request-available
13188490	Ambari-agent does for save data hashes correctly	"This causes all the data to be re-send during registration. Which can be very
dramatic for perf clusters.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13208700	deploy-gce-perf-cluster.py fails after upgrade on gce controller	"'gce fqdn' command was removed. While the script relied on it.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13202729	Directory/File creation hangs if relative path is supplied with cd_access	"
    Directory('hadoofs/fs1/ams-hbase-wal', owner='root', group='root', create_parents=True, recursive_ownership=True, cd_access='a')
    

Hangs as seen on s3a clusters when one of paths was set to relative by an
accident.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13147419	Remove python3 files from ws4py since they can cause build failure	"On some envs (not reproducible for me) it can result in failure like this:

    
    
    2018/03/22 23:49:21 INFO    : [INFO]   File ""/usr/lib/ambari-agent/lib/ambari_ws4py/async_websocket.py"", line 85
    2018/03/22 23:49:21 INFO    : [INFO]     yield from self.proto.writer.drain()
    2018/03/22 23:49:21 INFO    : [INFO]              ^
    2018/03/22 23:49:21 INFO    : [INFO] SyntaxError: invalid syntax
    2018/03/22 23:49:21 INFO    : [INFO] 
    2018/03/22 23:49:21 INFO    : [INFO] Compiling /grid/0/jenkins/workspace/Zuul_Ambari_Build_Job/build-support/SOURCES/ambari/ambari-agent/target/rpm/ambari-agent/buildroot/usr/lib/ambari-agent/lib/ambari_ws4py/server/tulipserver.py ...
    2018/03/22 23:49:21 INFO    : [INFO]   File ""/usr/lib/ambari-agent/lib/ambari_ws4py/server/tulipserver.py"", line 101
    2018/03/22 23:49:21 INFO    : [INFO]     request_line = yield from self.next_line()
    2018/03/22 23:49:21 INFO    : [INFO]                             ^
    2018/03/22 23:49:21 INFO    : [INFO] SyntaxError: invalid syntax
    2018/03/22 23:49:21 INFO    : [INFO] 
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13167576	Metric Collector goes down after HDFS restart post EU	"
**STR**

  1. Deployed cluster with Ambari version: 2.6.1.5-3 and HDP version: 2.6.1.0-129
  2. Upgrade Ambari to Target Version: 2.7.0.0-709
  3. Upgrade AMS and Smartsense (keeping them stopped)
  4. Perform EU to HDP-3.0 and let it complete
  5. Restart HDFS
  6. Observe state of Metrics Collectors (AMS is configured in distributed mode)

**Result**  
Both metrics collectors are down (auto start is enabled for Metrics Collector)

From logs:

    
    
    
    2018-06-13 16:45:05,620 ERROR org.apache.ambari.metrics.core.timeline.discovery.TimelineMetricMetadataManager: TimelineMetricMetadataKey is null for : [-8, 31, -72, 32, 88, -8, -51, -88, -104, 12, -123, 99, 55, -90, 45, -12, 115, 0, -6, 13]
    2018-06-13 16:45:05,622 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
    java.lang.NullPointerException
            at org.apache.ambari.metrics.core.timeline.aggregators.TimelineMetricReadHelper.getTimelineMetricCommonsFromResultSet(TimelineMetricReadHelper.java:116)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getLastTimelineMetricFromResultSet(PhoenixHBaseAccessor.java:446)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getLatestMetricRecords(PhoenixHBaseAccessor.java:1134)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getMetricRecords(PhoenixHBaseAccessor.java:953)
            at org.apache.ambari.metrics.core.timeline.HBaseTimelineMetricsService.getTimelineMetrics(HBaseTimelineMetricsService.java:288)
            at org.apache.ambari.metrics.webapp.TimelineWebServices.getTimelineMetrics(TimelineWebServices.java:261)
            at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
    
    2018-06-13 16:45:07,887 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=ctr-e138-1518143905142-361872-01-000005.hwx.site:2181,ctr-e138-1518143905142-361872-01-000006.hwx.site:2181,ctr-e138-1518143905142-361872-01-000003.hwx.site:2181 sessionTimeout=120000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$13/572967831@60474c94
    2018-06-13 16:45:07,889 INFO org.apache.zookeeper.client.ZooKeeperSaslClient: Client will use GSSAPI as SASL mechanism.
    2018-06-13 16:45:07,891 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
    2018-06-13 16:45:07,891 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181, initiating session
    2018-06-13 16:45:07,894 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181, sessionid = 0x363f94c8d6d0059, negotiated timeout = 90000
    2018-06-13 16:45:11,938 INFO org.apache.hadoop.hbase.client.RpcRetryingCallerImpl: Call exception, tries=6, retries=6, started=4153 ms ago, cancelled=false, msg=Call to ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320, details=row 'SYSTEM.CATALOG' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=ctr-e138-1518143905142-361872-01-000007.hwx.site,61320,1528896330963, seqNum=-1
    2018-06-13 16:45:15,954 INFO org.apache.hadoop.hbase.client.RpcRetryingCallerImpl: Call exception, tries=7, retries=7, started=8169 ms ago, cancelled=false, msg=Call to ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320, details=row 'SYSTEM.CATALOG' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=ctr-e138-1518143905142-361872-01-000007.hwx.site,61320,1528896330963, seqNum=-1
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13155342	Stack scripts should only try to reach corresponding Namenode pair in the context rather than all NNs	"A good example is when we start up a specific Namenode, say NN1. In that case,
we try to get the HAState of all the NNs (even NN3 and NN4) to find out the
active namenode. This is not really needed since starting up NN1 should not
care about NN3 and NN4's state.

I see usages in following places.

  * ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py
  * ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode_ha_state.py
  * ambari-common/src/main/python/resource_management/libraries/functions/namenode_ha_utils.py

Some of the above might not need any change.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13159945	New Alert JSON Is Invalid When Sent To Agents	"STR:

  * Set a simple cluster with HDFS
  * Attempt to create a new Alert:

    
    
    
    POST http://{{ambari-server}}:8080/api/v1/clusters/c1/alert_definitions
    
    {
      ""AlertDefinition"": {
        ""component_name"": ""NAMENODE"",
        ""description"": ""This service-level alert is triggered if the total number of volume failures across the cluster is greater than the configured critical threshold."",
        ""enabled"": true,
        ""help_url"": null,
        ""ignore_host"": false,
        ""interval"": 2,
        ""label"": ""NameNode Volume Failures"",
        ""name"": ""namenode_volume_failures"",
        ""scope"": ""ANY"",
        ""service_name"": ""HDFS"",
        ""source"": {
          ""jmx"": {
            ""property_list"": [
              ""Hadoop:service=NameNode,name=FSNamesystemState/VolumeFailuresTotal""
            ],
            ""value"": ""{0}""
          },
          ""reporting"": {
            ""ok"": {
              ""text"": ""There are {0} volume failures""
            },
            ""warning"": {
              ""text"": ""There are {0} volume failures"",
              ""value"": 1
            },
            ""critical"": {
              ""text"": ""There are {0} volume failures"",
              ""value"": 1
            },
            ""units"": ""Volume(s)""
          },
          ""type"": ""METRIC"",
          ""uri"": {
            ""http"": ""{{hdfs-site/dfs.namenode.http-address}}"",
            ""https"": ""{{hdfs-site/dfs.namenode.https-address}}"",
            ""https_property"": ""{{hdfs-site/dfs.http.policy}}"",
            ""https_property_value"": ""HTTPS_ONLY"",
            ""kerberos_keytab"": ""{{hdfs-site/dfs.web.authentication.kerberos.keytab}}"",
            ""kerberos_principal"": ""{{hdfs-site/dfs.web.authentication.kerberos.principal}}"",
            ""default_port"": 0,
            ""connection_timeout"": 5,
            ""high_availability"": {
              ""nameservice"": ""{{hdfs-site/dfs.internal.nameservices}}"",
              ""alias_key"": ""{{hdfs-site/dfs.ha.namenodes.{{ha-nameservice}}}}"",
              ""http_pattern"": ""{{hdfs-site/dfs.namenode.http-address.{{ha-nameservice}}.{{alias}}}}"",
              ""https_pattern"": ""{{hdfs-site/dfs.namenode.https-address.{{ha-nameservice}}.{{alias}}}}""
            }
          }
        }
      }
    }
    

This alert will not be scheduled on the agent correctly:

    
    
    
    ERROR 2018-05-16 20:11:55,186 AlertSchedulerHandler.py:307 - [AlertScheduler] Unable to load an invalid alert definition. It will be skipped.
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/AlertSchedulerHandler.py"", line 287, in __json_to_callable
        alert = MetricAlert(json_definition, source, self.config)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/alerts/metric_alert.py"", line 52, in __init__
        self.metric_info = JmxMetric(alert_source_meta['jmx'])
      File ""/usr/lib/ambari-agent/lib/ambari_agent/alerts/metric_alert.py"", line 288, in __init__
        self.property_list = jmx_info['property_list']
    KeyError: 'property_list'
    

Looking at `/var/lib/ambari-agent/cache/cluster_cache/alerts.json`, we can see
that `property_list` was changed into `propertyList`.

    
    
    
            ""name"": ""namenode_volume_failures"",
            ""componentName"": ""NAMENODE"",
            ""description"": ""This service-level alert is triggered if the total number of volume failures across the cluster is greater than the configured critical threshold."",
            ""interval"": 2,
            ""clusterId"": 2,
            ""label"": ""NameNode Volume Failures"",
            ""ignore_host"": false,
            ""source"": {
              ""jmx"": {
                ""urlSuffix"": ""/jmx"",
                ""propertyList"": [
                  ""Hadoop:service=NameNode,name=FSNamesystemState/VolumeFailuresTotal""
                ],
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13121915	Need to address HDP-GPL repo update after user accepts license in post-install scenario	"  * User denies the GPL license agreement, UI will still issue PUT call to create the repo
  * Ambari should not write HDP-GPL repo info to the ambari-hdp-1.repo because it will break yum in local repo world.
  * Then the user accepts the license by running ambari-server setup
  * The repo file does not get updated on existing hosts
  * We need to push the HDP-GPL repo to the repo files on existing hosts only when the license is accepted
"	AMBARI	Resolved	3	1	2212	pull-request-available
13141224	Ambari does not manage repositories	"While running CI tests

    
    
     
    Traceback (most recent call last): 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/package/__init__.py"", line 283, in _call_with_retries 
        code, out = func(cmd, **kwargs) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner 
        result = function(command, **kwargs) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call 
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper 
        result = _call(command, **kwargs_copy) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 303, in _call 
        raise ExecutionFailed(err_msg, code, out, err) 
    ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install hadooplzo_3_0_0_2_87' returned 1. Error: Nothing to do 
    
    
    
    2018-02-26 12:16:14,479 - Repository for HDP/3.0.0.2-87/HDP-3.0 is not managed by Ambari
    2018-02-26 12:16:14,479 - Repository for HDP/3.0.0.2-87/HDP-3.0-GPL is not managed by Ambari
    2018-02-26 12:16:14,480 - Repository for HDP/3.0.0.2-87/HDP-UTILS-1.1.0.22 is not managed by Ambari
    

I checked in DB and indeed manage_repos was false, however I didn't found any
signs in logs etc. that it was set to false during cluster setup.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13138758	Timeline Service V2.0 reader install fails if wget is not already installed on the host	"Encountered while testing Atlantic Beta 1.  
Timeline Service V2.0 reader install fails if wget is already not installed on
the host beforehand.

    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 101, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 45, in install
        hbase_service.install_hbase(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/hbase_service.py"", line 82, in install_hbase
        Execute(hbase_download_cmd, user=""root"", logoutput=True)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
        tries=self.resource.tries, try_sleep=self.resource.try_sleep)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase' returned 127. -bash: wget: command not found
     stdout:
    2018-02-13 07:39:31,705 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:31,711 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:31,713 - Group['livy'] {}
    2018-02-13 07:39:31,714 - Group['spark'] {}
    2018-02-13 07:39:31,714 - Group['hdfs'] {}
    2018-02-13 07:39:31,714 - Group['zeppelin'] {}
    2018-02-13 07:39:31,714 - Group['hadoop'] {}
    2018-02-13 07:39:31,715 - Group['users'] {}
    2018-02-13 07:39:31,715 - Group['knox'] {}
    2018-02-13 07:39:31,716 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,717 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,718 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,719 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,720 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,721 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,722 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,723 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,725 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,726 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,727 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,728 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,729 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,731 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-13 07:39:31,732 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,735 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-13 07:39:31,744 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-13 07:39:31,744 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-13 07:39:31,746 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,748 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,749 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-13 07:39:31,759 - call returned (0, '1014')
    2018-02-13 07:39:31,760 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-13 07:39:31,766 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] due to not_if
    2018-02-13 07:39:31,766 - Group['hdfs'] {}
    2018-02-13 07:39:31,767 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-13 07:39:31,767 - FS Type: 
    2018-02-13 07:39:31,767 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-13 07:39:31,786 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-13 07:39:31,787 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-13 07:39:31,803 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,811 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,812 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,813 - Repository['HDP-UTILS-1.1.0.21-repo-1'] {'append_to_file': True, 'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,816 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.21-repo-1]\nname=HDP-UTILS-1.1.0.21-repo-1\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,817 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,817 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,902 - Skipping installation of existing package unzip
    2018-02-13 07:39:31,902 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,912 - Skipping installation of existing package curl
    2018-02-13 07:39:31,912 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,922 - Skipping installation of existing package hdp-select
    2018-02-13 07:39:32,207 - Looking for matching packages in the following repositories: HDP-3.0-repo-1, HDP-UTILS-1.1.0.21-repo-1
    2018-02-13 07:39:34,268 - Package['hadoop_3_0_0_0_809-yarn'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,352 - Skipping installation of existing package hadoop_3_0_0_0_809-yarn
    2018-02-13 07:39:34,354 - Package['hadoop_3_0_0_0_809-mapreduce'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,363 - Skipping installation of existing package hadoop_3_0_0_0_809-mapreduce
    2018-02-13 07:39:34,365 - Package['hadoop_3_0_0_0_809-hdfs'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,380 - Skipping installation of existing package hadoop_3_0_0_0_809-hdfs
    2018-02-13 07:39:34,393 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,394 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:34,394 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-yarn-resourcemanager'] {'timeout': 20}
    2018-02-13 07:39:34,421 - call returned (0, 'hadoop-yarn-resourcemanager - 3.0.0.0-809')
    2018-02-13 07:39:34,461 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,472 - Execute['umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase'] {'logoutput': True, 'user': 'root'}
    -bash: wget: command not found
    
    Command failed after 1 tries
    
    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 101, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 45, in install
        hbase_service.install_hbase(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/hbase_service.py"", line 82, in install_hbase
        Execute(hbase_download_cmd, user=""root"", logoutput=True)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
        tries=self.resource.tries, try_sleep=self.resource.try_sleep)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase' returned 127. -bash: wget: command not found
     stdout:
    2018-02-13 07:39:31,705 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:31,711 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:31,713 - Group['livy'] {}
    2018-02-13 07:39:31,714 - Group['spark'] {}
    2018-02-13 07:39:31,714 - Group['hdfs'] {}
    2018-02-13 07:39:31,714 - Group['zeppelin'] {}
    2018-02-13 07:39:31,714 - Group['hadoop'] {}
    2018-02-13 07:39:31,715 - Group['users'] {}
    2018-02-13 07:39:31,715 - Group['knox'] {}
    2018-02-13 07:39:31,716 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,717 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,718 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,719 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,720 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,721 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,722 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,723 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,725 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,726 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,727 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,728 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,729 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,731 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-13 07:39:31,732 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,735 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-13 07:39:31,744 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-13 07:39:31,744 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-13 07:39:31,746 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,748 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,749 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-13 07:39:31,759 - call returned (0, '1014')
    2018-02-13 07:39:31,760 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-13 07:39:31,766 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] due to not_if
    2018-02-13 07:39:31,766 - Group['hdfs'] {}
    2018-02-13 07:39:31,767 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-13 07:39:31,767 - FS Type: 
    2018-02-13 07:39:31,767 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-13 07:39:31,786 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-13 07:39:31,787 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-13 07:39:31,803 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,811 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,812 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,813 - Repository['HDP-UTILS-1.1.0.21-repo-1'] {'append_to_file': True, 'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,816 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.21-repo-1]\nname=HDP-UTILS-1.1.0.21-repo-1\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,817 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,817 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,902 - Skipping installation of existing package unzip
    2018-02-13 07:39:31,902 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,912 - Skipping installation of existing package curl
    2018-02-13 07:39:31,912 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,922 - Skipping installation of existing package hdp-select
    2018-02-13 07:39:32,207 - Looking for matching packages in the following repositories: HDP-3.0-repo-1, HDP-UTILS-1.1.0.21-repo-1
    2018-02-13 07:39:34,268 - Package['hadoop_3_0_0_0_809-yarn'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,352 - Skipping installation of existing package hadoop_3_0_0_0_809-yarn
    2018-02-13 07:39:34,354 - Package['hadoop_3_0_0_0_809-mapreduce'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,363 - Skipping installation of existing package hadoop_3_0_0_0_809-mapreduce
    2018-02-13 07:39:34,365 - Package['hadoop_3_0_0_0_809-hdfs'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,380 - Skipping installation of existing package hadoop_3_0_0_0_809-hdfs
    2018-02-13 07:39:34,393 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,394 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:34,394 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-yarn-resourcemanager'] {'timeout': 20}
    2018-02-13 07:39:34,421 - call returned (0, 'hadoop-yarn-resourcemanager - 3.0.0.0-809')
    2018-02-13 07:39:34,461 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,472 - Execute['umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase'] {'logoutput': True, 'user': 'root'}
    -bash: wget: command not found
    
    Command failed after 1 tries
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13135824	Not able to register new HDP version after upgrading to Ambari2.6.1	"Steps to reproduce the issue

1. Install Ambari2.5.x and install HDP on that.

2. Now Upgrade ambari to 2.6.1 version.

3. Now try to register HDP version with VDF file - Save button is not enabled (also UI does not load repo URLs and)

4. In JS it fails with below error

Cannot read property 'gpl.license.accepted' of undefined
What is expected: UI should properly show the message that gpl license needs to be enabled. also Upgrade should take care of adding this property if it is needed.

Workaround for this is : add ""gpl.license.accepted=true"" to ambari.proeprties and then restart should fix the problem"	AMBARI	Resolved	1	1	2212	pull-request-available
13143856	Metrics Collector Install failed on HA cluster	"We have the very first run on Atlantic run using Ambari 2.7.0.2-60 and
HDP-3.0.0.2-130.  
Noticed in HA cluster that deploy has failed at Metrics collector install with
below error

    
    
    
    Caught an exception while executing custom service command: <class 'ambari_agent.AgentException.AgentException'>: 'Script /var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py does not exist'; 'Script /var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py does not exist'
    
    

Artifacts can be found
[here](http://logserver.eng.hortonworks.com/?prefix=qelogs/nat/86939/yarn-
ha/split-1/nat-yc-r7-schs-yarn-ha-re-
re/deploy_logs/logs/ctr-e138-1518143905142-62501-01-000002.hwx.site/)  
Live cluster: <http://172.27.15.145:8080/> (lifetime 48hrs)  
Could you please help take a look

"	AMBARI	Resolved	3	1	2212	pull-request-available
13137446	Heartbeat gets lost due to subprocess lock	"Subprocess has problem when run in multithreaded environment. As stated by developers it should not be used in such env.
As a result Ambari done multiple patches to subprocess. However still we are having multithreading problems with it.

This jira targets moving from Subprocess to Subprocess32 (a port of Python3.0 subprocess) which support multithreaded execution."	AMBARI	Resolved	3	1	2212	pull-request-available
13152679	Register VDF failing for AmazonLinux2 	"
    
    -bash-4.2# curl -v -k -u admin:admin -H ""X-Requested-By:ambari"" -X POST http://ctr-e138-1518143905142-226218-01-000002.hwx.site:8080/api/v1/version_definitions -d '{ ""VersionDefinition"": { ""version_url"": ""http://s3.amazonaws.com/dev.hortonworks.com/HDP/amazonlinux2/3.x/BUILDS/3.0.0.0-1189/HDP-3.0.0.0-1189.xml"" } }'
    Note: Unnecessary use of -X or --request, POST is already inferred.
    *   Trying 172.27.80.4...
    * TCP_NODELAY set
    * Connected to ctr-e138-1518143905142-226218-01-000002.hwx.site (172.27.80.4) port 8080 (#0)
    * Server auth using Basic with user 'admin'
    > POST /api/v1/version_definitions HTTP/1.1
    > Host: ctr-e138-1518143905142-226218-01-000002.hwx.site:8080
    > Authorization: Basic YWRtaW46YWRtaW4=
    > User-Agent: curl/7.55.1
    > Accept: */*
    > X-Requested-By:ambari
    > Content-Length: 151
    > Content-Type: application/x-www-form-urlencoded
    >
    * upload completely sent off: 151 out of 151 bytes
    < HTTP/1.1 500 Internal Server Error
    < Date: Fri, 13 Apr 2018 10:09:15 GMT
    < X-Frame-Options: DENY
    < X-XSS-Protection: 1; mode=block
    < X-Content-Type-Options: nosniff
    < Cache-Control: no-store
    < Pragma: no-cache
    < X-Content-Type-Options: nosniff
    < X-Frame-Options: DENY
    < Set-Cookie: AMBARISESSIONID=node018hn2df5vsjqjuo7g1yr9mvno8.node0;Path=/;HttpOnly
    < Expires: Thu, 01 Jan 1970 00:00:00 GMT
    < User: admin
    < Content-Type: text/plain;charset=utf-8
    < Transfer-Encoding: chunked
    <
    {
      ""status"" : 500,
      ""message"" : ""An internal system exception occurred: Could not load url from http://s3.amazonaws.com/dev.hortonworks.com/HDP/amazonlinux2/3.x/BUILDS/3.0.0.0-1189/HDP-3.0.0.0-1189.xml.  null""
    * Connection #0 to host ctr-e138-1518143905142-226218-01-000002.hwx.site left intact
    }-bash-4.2#
    -bash-4.2#
    

Build Used:

    
    
    
    -bash-4.2# cat /etc/yum.repos.d/ambari.repo
    #VERSION_NUMBER=2.7.0.0-309
    [ambari-2.7.0.0-309]
    name=ambari Version - ambari-2.7.0.0-309
    baseurl=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-309
    gpgcheck=1
    gpgkey=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-309/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    enabled=1
    priority=1
    

"	AMBARI	Resolved	3	1	2212	pull-request-available
13138982	"""Stale Alerts"" alert appears on cluster after some time"	"Previously. Every x seconds any alert was updated with information from agent.
Even if nothing changed.  
Due to perf reasons this behavior was changed so that agent reports alerts
only when their status changes.

Due this change, ""stale alerts"" alert started appearing on clusters, showing
that alerts were not updated for a long time.

We should consider alert up-to-date to the point of last heartbeat. Which
shows that the connection between server and agent is okay due to
last_hb_timestamp, and so agent could send alert updates.

"	AMBARI	Resolved	3	1	2212	pull-request-available
13207459	Cover HA wizard controller with unit tests	"Following files to cover:
* app/controllers/main/admin/highAvailability/hawq/addStandby/step3_controller.js
* app/controllers/main/admin/highAvailability/hawq/addStandby/wizard_controller.js"	AMBARI	Resolved	3	3	2400	pull-request-available
13136350	Dashboard-Metrics page style edits	See screenshots	AMBARI	Resolved	3	1	2400	pull-request-available
13143205	Unable to add a service in second go from Ambari-UI	"Installed Ambari using build 2.7.0.2-70 and HDP-3.0.0.2-150.
Added services HDFS, Yarn, Hive, HBase, Zookeeper and Solr.
Now trying to Add Knox / Ranger, after customizing the configurations and clicked on the {{Next}} button UI is stuck and unable to reach the deploy screen.
Browser console shows error {{unable to load modification handler for RANGER}}."	AMBARI	Resolved	1	1	2400	pull-request-available
13170872	All input fields are disabled after validation - cancel - filter for something on advanced tab	"STR: (see attached video)

1. try to do some property modifications, and save it
2. when some warnings appear, click cancel
3. filter for a variable that is not on the Advanced tab
4. everything is disabled on all tabs, even after discarding the modifications"	AMBARI	Resolved	2	1	2400	pull-request-available
13165871	Delete Button not enabled on Druid Router component	"facing issue while trying to delete Druid Router, where the delete button is disabled. 

The Test performs the following:
* Select component from list (after clicking on add button)
* Start component
* Restart all component with stale config
* Run Service check against service
* Stop component
* Delete component
* Restart all component with stale config
* Run Service check
* Repeat the above steps for all possible components.

Noticed that the delete button was not enabled for Druid Router only. When I accessed the cluster later, the delete button was enabled. This looks like an intermittent issue. Only once I could reproduce this on my local after doing various actions related to add/delete component."	AMBARI	Resolved	2	1	2400	pull-request-available
13140193	Background Ops during wizard pages do not show all the ops running by default	The Background Ops opened inside an install wizard do not show all the operations running by default. They are shown only after selecting the dropdown.	AMBARI	Resolved	2	1	2400	pull-request-available
13198604	"Service display name on left navigation bar should be suffixed with ""Client"" if only client service component is present for a service"	"When a service only contains client service component then check service's displayname. If service's displayname does not and with ""Client"" then suffix service's displayname  with ""Client""

Current behavior:   !Dsplayname HDFS.png! 
Expected behavior:   !Displayname HDFS Client.png! "	AMBARI	Resolved	3	3	2400	pull-request-available
13160038	Install Wizard: fix markup issues	"Fix small markup issues on Install Wizard:
* decrease space after checkboxes on step3
* increase table width on step6
* change logic for bell animation on configs
* use scrollable tabs for service selection on All Configurations tab
* update warning color for bell and labels
* add title for deploy popup on step8
* change progress bar width on step9"	AMBARI	Resolved	1	1	2400	pull-request-available
13184008	Host Level Maintenance mode is not working through Ambari UI	"Steps to reproduce:-
1. Login
2.Open host page.
3. select on host.
4. click on ""Host-Action"" Tab and select ""Turn On Maintenance mode ""

The message is showing 
{code}
Are you sure you want to Turn Off Maintenance Mode for ctr-e138-1518143905142-471677-01-000006.hwx.site?
{code}

I am trying to Turn On Maintenance Mode.

after that 
 !Screen Shot 2018-09-10 at 2.31.42 PM.png|thumbnail! 
{code}
Maintenance Mode has been turned off. It may take a few minutes for the alerts to be enabled.
{code}"	AMBARI	Resolved	1	1	2400	pull-request-available
13199299	"If service does not have client service component created then ""Run Service Check"" option should be made hidden"	"If service does not have client service component created then ""Run Service Check"" option should be made hidden"	AMBARI	Resolved	3	3	2400	pull-request-available
13182337	Quicklinks URL overflow outside the UI box	"Quicklinks in HDI environment are overflowing in UI. See attached.

"	AMBARI	Resolved	2	1	2400	pull-request-available
13235737	Persistent Cross Site Scripting (XSS) in Ambari	"Below is the HTTP Request and Response issued when a user submits a note containing a JavaScript
after modifying some configuration in ""Tez"" service.
HTTP Request:
PUT /api/v1/clusters/<env> HTTP/1.1
Host: xyz601:8080
Content-Length: 199
Accept: application/json, text/javascript, /; q=0.01
Origin: http://xyz601:8080
X-Requested-With: XMLHttpRequest
X-Requested-By: X-Requested-By
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/70.0.3538.102 Safari/537.36
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
Referer: http://xyz:8080/
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9
Cookie: AMBARISESSIONID=vfiy4336mxwl1k5ehd6jrz43i
Connection: close
{""Clusters"":{""desired_service_config_versions"":

{""service_config_version"":4,""service_name"":""TEZ"",""service_config_version_note"":""Creat ed from service config version V4\n<img src=x onerror=alert(1)>""}
}}

Remediation Recommendations
Restrict all input passed to the application to valid, whitelisted content, and ensure that all
response/output sent by the server is HTML/URL/JavaScript encoded, depending on the context in
which the data is used by the application.
The remediation should not attempt to blacklist content and remove, filter, or sanitize it. There are
too many types of encoding it to get around filters for such content.
We strongly recommend a positive security policy that specifies what is allowed.
Negative or attack signature based policies are difficult to maintain and are likely to be incomplete."	AMBARI	Resolved	2	1	2400	pull-request-available
13139803	Alert filter with services having space is not working	"Alert filter with services having space is not working.
STR:
1. Navigate to Alerts page
2. Try to filter using Service as filter key with services like ""Ambari Infra""/""Log Search""
3. Alerts are not filtered."	AMBARI	Resolved	3	1	2400	pull-request-available
13181286	Implement Notifications/Coasters from Fluid Design	See screenshot	AMBARI	Resolved	3	1	2400	pull-request-available
13138942	Ranger LB URL is pointing to ranger port by default when we click on Quick link	"For Ranger HA, when we give External url as LB url and port (LB:port), but when we click on quick link is pointing to LB url, but it is using ranger port(6080 or 6182 depend it's http or https).

Below code snippet - reading the host name from policymgr_external_url and then using default ranger port.


{noformat}
} else if (serviceName === 'RANGER') {
      var siteConfigs = this.get('configProperties').findProperty('type', 'admin-properties').properties;
      if (siteConfigs['policymgr_external_url']) {
        host = siteConfigs['policymgr_external_url'].split('://')[1].split(':')[0];
         var newItem = {};
        newItem.url = siteConfigs['policymgr_external_url'];
        newItem.label = link.label;
         return newItem;
        }
    }
{noformat}
"	AMBARI	Resolved	3	1	2400	pull-request-available
13147453	Admin View: Users and Groups style tweaks	"See screenshots
"	AMBARI	Resolved	3	1	2400	pull-request-available
13152001	Alerts Are Not Updated in the UI	"Alerts in the UI are not updated automatically. You must hard refresh the page or constantly navigate around to get them to show up correctly.

STR:
- Deploy a simple cluster, just with ZK. 
- Kill one of the ZK servers manually (or just stop it) and wait.
- The APIs will update with the newly triggered alert, but the UI will not

In some cases, navigating around will show inconsistent alerts:
- The alerts hidden under the alert icon never change without a hard refresh
- Visiting the page of an alert definition does show updated data, but it no longer changes to reflect the current state."	AMBARI	Resolved	2	1	2400	pull-request-available
13156618	JS error after installing Ranger from Install Wizard - Smart Configs are broken - page refresh fixed the issue	"See attached.
Modifed FE code (doNotShowAndInstall computed property) so that Ranger can be installed from Install Wizard.  Installation succeeded.  However, post-Install, there were some JS errors (see attached) that prevented the Smart Config pages from loading.  See attached. !Screen Shot 2018-04-27 at 2.17.22 PM.png|thumbnail! 
After page refresh, the problem went away and Smart Configs can be viewed just fine."	AMBARI	Resolved	2	1	2400	pull-request-available
13204219	Cover host component view with unit tests	"Following files to cover:
app/views/main/host/details/host_component_view.js

"	AMBARI	Resolved	3	3	2400	pull-request-available
13130549	ambari-web unit test is failing at apache ambari jenkins job	"{code}
Error loading resource file:///api/v1/clusters/c1/upgrades?fields=Upgrade&_=1515682880321 (203). Details: Error opening /api/v1/clusters/c1/upgrades: No such file or directory

  30514 passing (31s)
  157 pending
  1 failing

  1) Ambari Web Unit tests test/utils/date/timezone_test timezoneUtils #detectUserTimezone Detect UTC+1:
     expected '0-60|Africa' to include '0-60|Atlantic'
{code}"	AMBARI	Resolved	1	1	2400	pull-request-available
13145687	For Ranger Service /spnego reference principal is not getting updated in config	For UI based installation, /spnego reference principal used in Ranger Service kerberos descriptor is not getting updated in ranger.spnego.kerberos.keytab configuration.	AMBARI	Resolved	1	1	2400	pull-request-available
13168058	Database tab disabled if Setup Database and Database User	"Although 'Database' tab doesn't allow to go to the next step if the required properties are empty it is not allowing even if there are no required properties left on the screen.

STR
1. Go to Customize Services --> Databases --> Ranger.
2. Enter the Ranger DB host.
3. Set/Toggle 'Setup Database and Database Use' to No.

Although no required properties are present but the 'Next' button is disabled."	AMBARI	Resolved	2	1	2400	pull-request-available
13153018	Config's tags should be cached	"UI often make calls to fetch current tags, which can be cached and updated on WebSocket event.

"	AMBARI	Resolved	2	3	2400	pull-request-available
13133401	Standardize precision when expressing durations	"Duration should have the following format ""1d 3h 46m""."	AMBARI	Resolved	3	3	2400	pull-request-available
13147886	Service page layout updates	See screenshots	AMBARI	Resolved	3	3	2400	pull-request-available
13175179	How alert count is presented for config errors/warnings/suggestions confusing and misleading	See screenshots	AMBARI	Resolved	2	3	2400	pull-request-available
13141000	Target host shows all the hosts available in Move Wizard	The target host selection in the Move Wizard shows all the available hosts in the cluster instead of the hosts where the component is not installed. 	AMBARI	Resolved	2	1	2400	pull-request-available
13144370	Kerberos 'Edit' Properties not conforming to the new style	The 'Edit' link of Kerberos properties page does not confirm to the new UI design styles. All the actions are buttons on this page, and it is easy to miss the edit link.	AMBARI	Resolved	3	1	2400	pull-request-available
13139802	Admin user can delete own user from Users page	"Admin user can delete own user from Users page.

 

STR:
 # Login to ambari UI as admin user
 # Navigate to Users page
 # Try to delete the same user (admin) from the users list

The delete option is disabled in the individual user page, but not from the all Users page."	AMBARI	Resolved	3	1	2400	pull-request-available
13155677	Config Version Comparison tool shows diff text merging into each other if config values are long.	"While comparing two config versions , if config values are long , than the diff shows values merging into each other and text is not wrapped.

See Attached Screenshot"	AMBARI	Resolved	2	1	2400	pull-request-available
13152252	Navigation sub-menu style changes	"The additional top and bottom padding is for the sub-menu container should be added (5px).
There needs to be more padding between the Service name and the status indicator, they're too tight now

"	AMBARI	Resolved	3	3	2400	pull-request-available
13149469	Dashboard: add Yarn Containers widget	"Title: ""YARN containers""
Content:   <#allocated>/<#pending>/<#reserved>"	AMBARI	Resolved	3	3	2400	pull-request-available
13182670	'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' is not equal to the value of yarn_hierarchy in UI Deploy	"The property 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' does not show up in the UI install wizard even when {{yarn_cgroups_enabled}} and {{gpu_module_enabled}} are enabled in YARN configs. But this property is available after deployment of the cluster.  

 !Screen Shot 2018-09-02 at 10.20.43 PM.png|thumbnail! 
 !Screen Shot 2018-09-02 at 10.22.33 PM.png|thumbnail! 
When trying to set a value for the config 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy', Stack advisor displays the following:
{code}
yarn.nodemanager.linux-container-executor.cgroups.hierarchy and yarn_hierarchy should always have same value
yarn.nodemanager.linux-container-executor.cgroups.hierarchy and yarn_hierarchy should always have same value
Name of the Cgroups hierarchy under which all YARN jobs will be launched
{code}
 !Screen Shot 2018-09-02 at 10.25.08 PM.png|thumbnail! 
From the stack advisor description, looks like 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' property needs to be same as yarn_hierarchy. As part of UI Deploy, the property {{yarn_hierarchy}} is set to {{hadoop-yarn-tmp-ctr-e138-1518143905142-461959-01-000002.hwx.site}}. But after deploy, the {{yarn.nodemanager.linux-container-executor.cgroups.hierarchy}} is set to {{/yarn}} and not the property set while UI Deploy. This is causing Deploy failures due to Nodemanagers not getting started. When this property is set correctly, Nodemanagers start successfully. "	AMBARI	Resolved	1	1	2400	pull-request-available
13196630	Delete host confirm popup does not contain all master components	"# Navigate to host with one or more masters
# Click Delete Host from the Host Actions menu

Observed: All the masters are not mentioned in the popup. This is a frontend bug
 !Screen Shot 2018-11-05 at 2.47.04 PM.png! 
"	AMBARI	Resolved	3	1	2400	pull-request-available
13148583	Duplicate websocket subscription on Configs page	"Steps to reproduce:
# Open ambari web
# Go to HDFS
# Open Configs page
# Leave page

Actual Result: UI has subscribed to all topics again."	AMBARI	Resolved	3	1	2400	pull-request-available
13173194	"Service disappear from the UI when Going back to ""Customize Services"" page or doing page refresh"	"Here is the STR:
- install isilon mpack
- go through the cluster creation wizard and select onefs service
- on the configuration page check that there are onefs related properties (for example smart connect zone name)
- either hit f5 or go to the next page then go back
- on the configuration page there will be no onefs related properties any more
- if you continue the deployment this way it will fail at the end
"	AMBARI	Resolved	3	1	2400	pull-request-available
13207246	Select Service page: Show Yarn and MR2 as separate services for selection instead of single selection (Yarn+MapReduce2)	"This needs to be done so that user can select YARN without MR2 using UI installer wizard

"	AMBARI	Resolved	3	3	2400	pull-request-available
13151388	Usability: Generate blueprint should download a single Zip file and not 2 files.	During the installation wizard, just before deploy, the user has the opportunity to export a blueprint. This will download two files, but the behavior in the browser is odd as the user has to accept that multiple files will be downloaded. It would be better if it just downloaded a single zip file with both the blueprint and cluster creation template included in it. Much like our client configuration export.	AMBARI	Resolved	2	4	2400	pull-request-available
13144342	The tooltip for Overriden properties shown under manage Config Groups page has '<br/>' in the tooltip text	"The tooltip for Overriden properties shown under manage Config Groups page has '<br/>' in the tooltip text
Also if there are more than one properties overriden then the they are not shown in new lines.


Attached Screenshot"	AMBARI	Resolved	3	1	2400	pull-request-available
13134465	Add new state for Not Available data in Heatmap widget	"# We should add a new state ""Data Not Available""
# Roundup metric values based on data type"	AMBARI	Resolved	3	1	2400	pull-request-available
13213631	Cover widget mixin with unit tests	"Following files to cover:
* app/mixins/common/widgets/widget_mixin.js"	AMBARI	Resolved	3	3	2400	pull-request-available
13148592	Ambari Requests Too Much Data From Hosts When Logging In	"When first logging into Ambari, the web client makes the following request:
GET api/v1/clusters/<clusterName>/hosts?fields=
- Hosts/cpu_count
- Hosts/host_name
- Hosts/host_status
- Hosts/ip
- Hosts/last_agent_env
- Hosts/last_heartbeat_time
- Hosts/maintenance_state
- Hosts/ph_cpu_count
- Hosts/public_host_name
- Hosts/rack_info
- Hosts/total_mem
- alerts_summary
- host_components/HostRoles/desired_admin_state
- host_components/HostRoles/display_name
- host_components/HostRoles/maintenance_state
- host_components/HostRoles/service_name
- host_components/HostRoles/stale_configs
- host_components/HostRoles/state
- host_components/logging
- stack_versions/HostStackVersions
- stack_versions/repository_versions/RepositoryVersions/display_name
- stack_versions/repository_versions/RepositoryVersions/id
- stack_versions/repository_versions/RepositoryVersions/repository_version

In a cluster with 100 hosts, this can cause a payload of over 200MB to be returned. The culprit seems to be the {{Hosts/last_agent_env}} property, specifically the {{activeJavaProcs}} key. If the hosts in the cluster are running a lot of Java processes, this can account for over 90% of the payload (roughly 180MB). This data takes 30 or more seconds for the server to serialize and stream to the socket.

It looks like {{Hosts/last_agent_env}} isn't even needed after initially provisioning a host during the host checks, so it probably can be removed."	AMBARI	Resolved	2	1	2400	pull-request-available
13136285	Host details page: components not reconfigured after deleting their host	"# When deleting a host, after components deletion config changes not triggered, as opposed to when they removed individually.
 # When adding/deleting component config modification triggered before actual call to add/delete a component, so even if the call fails then configs would be modified"	AMBARI	Resolved	3	1	2400	pull-request-available
13144767	Customise Services : Alignment not proper for Select Config group Pop up and overriden text box	"There are two issues w.r.t alignment

*Issue#1*
1) Go to All Configurations Tab under Customise Services Step of Install Wizard.
2) Select any property say under HDFS 'DataNode directories permission' . Click '+' to override.
3) Select a new config group
4) The new text box which appears to provide overridden value is misaligned with original one.

*Issue#2*
Following same steps as in Issue#1. Check the warning 'You are changing not default group, please select config group to which you want to save dependent configs from other services +Show Details+' at page top. 
Click Show Details link.
A pop up opens. The text which appears on the pop up  is not aligned properly.

Attaching screenshots for both issues."	AMBARI	Resolved	3	1	2400	pull-request-available
13139646	Not able to delete livy component from a host	"I am not able to delete livy(only one instance of livy is present on the cluster) from a host.
After stopping livy, when I try to delete, its showing below message :
""WARNING! Delete the last Livy for Spark2 Server component in the cluster?
Deleting the last component in the cluster could result in permanent loss of service data."" (Attaching the screenshot)

But there's not way to enable 'Confirm Delete' button.

Note : In earlier versions of ambari there used to be a check box and user has to check it to enable delete button."	AMBARI	Resolved	2	1	2400	pull-request-available
13217149	"Rack ""Config Refresh"" behaviour is different in Ambari 2.6 and 2.7.3"	"In the new ambari-2.7.3 version, there is no way (via web ui) to generally refresh configs on a host . There is a way to refresh HDFS_CLIENT config on the Namenode host, but refreshing HDFS_CLIENT config *does not* refresh the topology_mappings.data file. 


*In Ambari 2.6.2: (During the ""Refresh Configs"")*
When we refresh ""Refresh Configs"" then it refreshes all client configs which also includes HDFS and MapReduce2 config refresh. During the ""Restart MapReduce2 Client"" step it actually performs the ""/etc/hadoop/conf/topology_mappings.data"" file refresh with correct rack info which is updated via ambari ui.
Example : 
{code}
2019-02-09 21:31:05,642 - File['/usr/hdp/2.6.5.0-292/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}
2019-02-09 21:31:05,647 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
2019-02-09 21:31:05,655 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('P@.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
{code}


*In Ambari 2.7.3:  (During the ""Refresh Configs"")*
Even after refreshing HDFS Client (AND) MapReduce Clients individually using ""Refresh Configs"" iwe do not see above kind of message. Also no changes happens inside the ""/etc/hadoop/conf/topology_mappings.data"" is not updated until we restart daemon components like DataNode. So the behavior of client config refresh seems to be slightly changed from ambari side."	AMBARI	Resolved	3	1	2400	pull-request-available
13186332	JS errors during adding hosts	"Steps:
# Deploy any cluster.
# Do not refresh the page.
# Go through add hosts wizard to deploy step.

Result: JS errors in console after deploy button click. Also some persist requests with errors were sent to the server. ""App.get('clusterId')"" returns null. For perf cluster deploy the number of errors increases to several hundred."	AMBARI	Resolved	3	1	2400	pull-request-available
13154789	issue with service auto restart service categorization	"The service and component categorization are incorrect for e.x.

NameNode, Nodemanager and and Nimbus are under ""Ambari Metrics""."	AMBARI	Resolved	2	1	2400	pull-request-available
13151644	UI Performance Tuning	"Since we've made a ton of change to the UI, I'm noticing some slowness in page load times and interaction. 
Specific Areas I'm noticing slowness in:
* Initial Ambari UI Page Load
* Initial Dashboard UI Page Load
* Service Actions Menu Loading
* Quicklinks Loading
* Service Configurations Tab Loading"	AMBARI	Resolved	2	1	2400	pull-request-available
13201189	Start Namenode failing during Move master NN wizard on non-HA cluster with custom hdfs service user	"Start Namenode failing during Move master NN wizard.

From NN logs:

{code:java}
2018-11-21 19:43:57,126 WARN Encountered exception loading fsimage java.io.IOException: NameNode is not formatted. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:237) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1090) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2018-11-21 19:43:57,131 INFO Stopped o.e.j.w.WebAppContext@58359ebd{/,null,UNAVAILABLE}{/hdfs}
2018-11-21 19:43:57,135 INFO Stopped ServerConnector@75e91545{HTTP/1.1,[http/1.1]}{ctr-e139-1542663976389-4877-02-000004.hwx.site:50070}
2018-11-21 19:43:57,136 INFO Stopped o.e.j.s.ServletContextHandler@2f4205be{/static,file:///usr/hdp/3.1.0.0-13/hadoop-hdfs/webapps/static/,UNAVAILABLE}
2018-11-21 19:43:57,136 INFO Stopped o.e.j.s.ServletContextHandler@319bc845{/logs,file:///grid/0/log/hdfs/cstm-hdfs/,UNAVAILABLE}
2018-11-21 19:43:57,138 INFO Stopping NameNode metrics system...
2018-11-21 19:43:57,139 INFO timeline thread interrupted.
2018-11-21 19:43:57,140 INFO NameNode metrics system stopped.
2018-11-21 19:43:57,141 INFO NameNode metrics system shutdown complete.
2018-11-21 19:43:57,141 ERROR Failed to start namenode. java.io.IOException: NameNode is not formatted. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:237) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1090) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2018-11-21 19:43:57,143 INFO No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times.
2018-11-21 19:43:57,143 INFO Exiting with status 1: java.io.IOException: NameNode is not formatted.
2018-11-21 19:43:57,145 INFO SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at ctr-e139-1542663976389-4877-02-000004.hwx.site/172.27.25.135 ************************************************************/
{code}

Ambari task logs

{code:java}
2018-11-22 03:31:30,588 - The NameNode is still in Safemode. Please be careful with commands that need Safemode OFF.
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 408, in <module>
    NameNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 352, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 138, in start
    upgrade_suspended=params.upgrade_suspended, env=env)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 264, in namenode
    create_hdfs_directories(name_service)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 336, in create_hdfs_directories
    nameservices=name_services
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 677, in action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 674, in action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 373, in action_delayed
    self.action_delayed_for_nameservice(None, action_name, main_resource)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 395, in action_delayed_for_nameservice
    self._assert_valid()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 334, in _assert_valid
    self.target_status = self._get_file_status(target)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 497, in _get_file_status
    list_status = self.util.run_command(target, 'GETFILESTATUS', method='GET', ignore_status_codes=['404'], assertable_result=False)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 214, in run_command
    return self._run_command(*args, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 282, in _run_command
    _, out, err = get_user_call_output(cmd, user=self.run_user, logoutput=self.logoutput, quiet=False)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/get_user_call_output.py"", line 62, in get_user_call_output
    raise ExecutionFailed(err_msg, code, files_output[0], files_output[1])
resource_management.core.exceptions.ExecutionFailed: Execution of 'curl -sS -L -w '%{http_code}' -X GET -d '' -H 'Content-Length: 0' --negotiate -u : 'http://ctr-e139-1542663976389-4877-02-000004.hwx.site:50070/webhdfs/v1/tmp?op=GETFILESTATUS' 1>/tmp/tmpC3TR3n 2>/tmp/tmpTlrH_1' returned 7. curl: (7) Failed to connect to ctr-e139-1542663976389-4877-02-000004.hwx.site port 50070: Connection refused
000
{code}

Customized service users and ambari agent user is enabled in the test.
"	AMBARI	Resolved	1	1	2400	pull-request-available
13186607	The operation 'Install HDP-{$version} version' change name after ending and page reloading	"STR:
1) Deploy ambari 2.7.1 cluster
2) Make registration of new version HDP 

Actual result: The operation 'Install HDP-{$version} version' change name (from 'Install HDP-{$version} version' on 'Install Version') after ending of operation and page reloading. 
Note: take a look on screenshots

Expected result: The operation for installing of new HDP version must have the constant name."	AMBARI	Resolved	2	1	2400	pull-request-available
13149525	JS error while deploying new service	"{noformat}
""<DS.StateManager:ember27929> could not respond to event setProperty in state rootState.loading.""
{noformat}
"	AMBARI	Resolved	3	1	2400	pull-request-available
13147505	Install Wizard: Add a Cancel button to go back to Admin View.	"See screenshot.

"	AMBARI	Resolved	3	3	2400	pull-request-available
13150283	UI option to stop HDFS service doesnt work - JS Error	"HDFS Stop option from UI doesn't work. No operation is triggered

Below Js Error is seen in the console.
Ambari Build - 2.7.0.0-263

{code:java}
app.js:28031 Uncaught TypeError: Cannot read property 'set' of undefined
    at app.js:28031
    at Class.onPrimary (app.js:211716)
    at Class.newFunc [as onPrimary] (vendor.js:12954)
    at handler (vendor.js:31554)
    at HTMLButtonElement.<anonymous> (vendor.js:23346)
    at HTMLDivElement.dispatch (vendor.js:3178)
    at HTMLDivElement.elemData.handle (vendor.js:2854)
{code}
"	AMBARI	Resolved	1	1	2400	pull-request-available
13131063	Unsightly artifacts during Login	See attached.	AMBARI	Resolved	3	1	2400	pull-request-available
13210150	Cover views of the modals with unit tests	"Following files to cover app/views/common/modal_popups/*.

"	AMBARI	Resolved	3	3	2400	pull-request-available
13161037	"Disabled alert status isn't ""NONE"""	"Disabled alert status isn't ""NONE""

STR:
1)Stop Hbase
2)Navigate to alerts page
3)Move state of first CRITICALalert to ""Disabled""

Expected:
Alert status is NONE

Actual:
Alert status is CRITICAL
"	AMBARI	Resolved	1	1	2400	pull-request-available
13168344	WebSockets traffic does not work between Ambari Web UI and Ambari Server when it is accessed via Knox Proxy (UI side changes)	When connected via Knox proxy ambari server web socket URL should be changed from <ambari-server protocol>://<ambari-server host>:<ambari-server port>/api/stomp/v1/websocket to <knox protocol>://<knox host>:<knox port>/gateway/default/ambari/websocket	AMBARI	Resolved	1	1	2400	pull-request-available
13157866	Manage Ambari UI issues	"See screenshots

"	AMBARI	Resolved	2	1	2400	pull-request-available
13193502	Add column to show which service recommended changes during upgrade refer to	"During the stack upgrade from HDP 2.6 -> HDP 3.x, the list of changes Ambari wants to make to each service is very difficult to interpret. Below is an example of the config changes Ambari recommends when attempting to upgrade fresh-install HDP2.6.2 cluster. 

Here's an example of how these configs look like when the user presses the ""Open"" button:
open_recommended_config_changes.png 
Note the highlighting is not intentional - it is automatically done by the browser (chrome) due to the special characters in the configs.

A small step to improve the user experience would be to add a column that shows which service each config applies."	AMBARI	Resolved	3	3	2400	pull-request-available
13142855	Configurations attributes reset after running Add Service wizard	"h3. STEPS TO REPRODUCE
# Install the latest Sandbox docker (to re-reproduce the issue docker is easier)
# Add ""hadoop.security.group.mapping.ldap.bind.password"" in Custom core-site with the value ""admin-password"" with Property Type *""PASSWORD""*, and save
!image-2018-02-27-20-00-12-533.png|thumbnail! 
# From Add Service Wizard, add some service (I tested with Nifi and Solr but any service should be OK)
# After Wizard, go to HDFS => Configs and search ldap.bind.password, it shows text input box rather than password input box"	AMBARI	Resolved	2	1	2400	pull-request-available
13184596	"""Host is in Maintenance mode"" text is not displayed in UI after maintenance mode is turned on"	"Behavior in ambari-2.7.0
1. login
2. navigate host details page.
3. click host action
4. click ""Turn On Maintenance Mode""
after the first confirm window. we can see the text which displays ""Host is in Maintenance mode""
 
But in ambari-2.7.1
The UI Tab which display ""Turn On Maintenance Mode"" 
It is not getting displayed."	AMBARI	Resolved	2	1	2400	pull-request-available
13170029	Host details page: remove duplicate title	"See screenshot

"	AMBARI	Resolved	3	1	2400	pull-request-available
13159397	Manage Ambari UI style fixes	"* Add a tooltip on config version compare button: ""Compare this version with current""
* User edit page: remove extra space between columns
* Group edit: Group Access select alignment"	AMBARI	Resolved	1	1	2400	pull-request-available
13131023	Login page lost Ambari branding	"See attached.

1. There's no Ambari branding on trunk. There's nothing that says this is a login page for Ambari.
2. The spacing is odd.

See attached for trunk vs 2.6.1 comparison."	AMBARI	Resolved	2	1	2400	pull-request-available
13162861	Express Upgrade: Clicking on upgrade item shows no tasks on large clusters	"There are two issues over here.
*1st issue:*
This seems to be a functional bug. As it seems from the attached screenshot that even after API call was completed content was not rendered.

*2nd issue:*
There seems to be a performance issue. UI asks for all tasks fields instead of only ones that are needed making api call really expensive. Expected behavior is that only status should be asked for and when user clicks on a task then  log for that task should be requested. "	AMBARI	Resolved	1	1	2400	pull-request-available
13147871	Update combo filter style	"Remove shadow outside input box.

"	AMBARI	Resolved	4	3	2400	pull-request-available
13197800	Heatmap tab, metrics tab and QuickLinks section in summary tab should be hidden if service has only client service component	"HDFS client and YARN client service component is created but other HDFS/YARN components are not created via blueprint deploy

*Expected behavior:*
Heatmap tab, metrics tab and QuickLinks section should be hidden if a service does not have any non-client servicecomponent created in the cluster

*Actual behavior:*
Heatmap and metrics tab are shown with empty metrics and no graphs"	AMBARI	Resolved	3	3	2400	pull-request-available
13173827	Background Operations: minor UX changes	"See screenshots

"	AMBARI	Resolved	3	3	2400	pull-request-available
13167147	Customize Service step issues	"There are some issues in Step 7 of the Installer Wizard
1. Even if there are some *required* changes in the 'Database' tab it allows to go to the next step. Next should only be enabled if the required property has been provided with an input
2. If there are some CRITICAL errors, the top notification bell should be 'Red' (similar to what we see if there is an empty required property) 
3. If I click on any of the above critical error properties it takes me to a wrong page. "	AMBARI	Resolved	1	1	2400	pull-request-available
13166328	Add Ranger Kms Server doesn't not trigger Install request while enabling HA	"Steps to reproduce:
- Install Cluster with Ambari-2.7.0.0
- Add Ranger service +  Ranger KMS service
- Go to Action option of Ranger KMS service > Click on Add Ranger KMS server > Confirmation Pop up shows up, after clicking Confirm ADD option Install request is not triggered.
"	AMBARI	Resolved	1	1	2400	pull-request-available
13200605	Counter installedClients for AMBARI_INFRA_SOLR is null	"App.Service.find('AMBARI_INFRA_SOLR').get('installedClients') = null,
Should return number of installed clients."	AMBARI	Resolved	3	1	2400	pull-request-available
13195645	Cluster user can't modify shared widgets	"If a user with role ""Cluster User"" tries to ""Edit Shared"" widgets, the server will return an error:
{noformat}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Only cluster operator can create widgets with cluster scope""
}
{noformat}"	AMBARI	Resolved	3	1	2400	pull-request-available
13198055	Restart option should not be shown if service components are not created in a cluster	"In the attached screenshot ""Restart Nodemanager"" should not be shown because NodeManager service component is not present in the cluster"	AMBARI	Resolved	3	1	2400	pull-request-available
12651319	YARN/MR2 do not start after reconfiguring	"After successfully installing a cluster, I saved YARN configs (changed only 1 property). YARN service would not start with ResourceManager saying it cannot bind to port.

Turns out that certain properties in {{yarn-site.xml}} were set to null value.
{noformat}
""yarn.resourcemanager.admin.address"" : ""null"",
""yarn.resourcemanager.resource-tracker.address"" : ""null"",
""yarn.resourcemanager.scheduler.address"" : ""null"",
""yarn.resourcemanager.address"" : ""null"",
""yarn.log.server.url"" : ""null"",
{noformat}
Similar problem with MR2 also. The {{mapred-site.xml}} had 19 properties which were null.

{noformat}
""mapred.jobtracker.taskScheduler"" : ""null"",
""mapred.tasktracker.map.tasks.maximum"" : ""null"",
""mapred.hosts.exclude"" : ""null"",
....
{noformat}"	AMBARI	Resolved	2	1	2400	pull-request-available
13148845	Problems while selecting the property uniquely in Configurations screen while installation	"While scripting the automation code for the new UI installer, could not select some properties uniquely and some problems faced:
# Unable to select some properties by their label in xpath:
{panel}
* In Advanced zeppelin-config panel under Zeppelin configurations, there are a bunch of configurations related to zeppelin.ssl. As seen in screenshot-1, all the labels have zeppelin.ssl in them and if one were to type textbox with label zeppelin.ssl using XPATH strict text match, i.e. //*[text()=’zeppelin.ssl’], it fails. This is because within the label there are some html elements (see screenshot-1) like <wbr> which do not allow a strict match to the label.
* A simple workaround would be to use xpath contains() operator, but it would return multiple fields for the same label, which is not good.
{panel}
# Try to select the property by ‘data-qa’ also has similar problems to above
{panel}
As per screenshot-2, the data-qa attribute has the value ‘service-config-zeppelin-ssl-zeppelin-config-xml-default’ which can only be covered by xpath //*[contains(@data-qa,’zeppelin-ssl’)]. But this would again lead to multiple matches in the UI.
{panel}

Two asks from a test automation point of view:
# Remove any HTML entity from in between the label text
{panel}
Example: zeppelin.<wbr>ssl to zeppelin.ssl
{panel}
# Set the data-qa attribute for the input fields to just the property name as specified in Blueprint. For example, if the Blueprint property is ‘zeppelin.ssl’, the data-qa attribute can be ‘zeppelin-ssl’.
 
This would enable the automation code to uniquely identify the property fields."	AMBARI	Resolved	1	1	2400	pull-request-available
13143233	Config dependencies alert bar not displayed properly while scrolling down the page	"Config dependencies alert bar not displayed properly while scrolling down the page.

Attaching screenshot"	AMBARI	Resolved	2	1	2400	pull-request-available
13173220	Inconsistent Ambari warnings	"STR:

#  Go to Hive/Configs
#  Enable Interactive Query, select the host, Save
#  6 Warnings are coming up, though the REST API call returned only 2, the other 4 are produced by Ambari Web
#  Click on cancel, discard the changes.
#  Go to the Summary, tab, then back to the Configs tab, and do steps #1-#3 again
#  This time only the two warnings are displayed, Ambari Web doesn't generates those extra 4 (desired outcome)
#  Log out and log in again, and the same starts again, for the first modification you'll receive 4 extra warnings."	AMBARI	Resolved	2	1	2400	pull-request-available
13216658	"Dashboard is unable to load . Common console error : ""SEVERE TypeError: widgetGroups is undefined"""	"I found a corner scenario where the dashboard never loads.
If we click on Dashboard link on landing page before dashboard completely loads upon login, dashboard never loads."	AMBARI	Resolved	2	1	2400	pull-request-available
13205373	JS error after starting stack downgrade	"{code:java}
Uncaught TypeError: Cannot convert undefined or null to object
    at Function.keys (<anonymous>)
    at Object.<anonymous> (app.js:17261)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at Object.deferred.(:8080/anonymous function) [as resolve] (http://104.196.93.139:8080/javascripts/vendor.js:1341:40)
    at Class.complete (app.js:17012)
    at app.js:206411
{code}
"	AMBARI	Resolved	3	1	2400	pull-request-available
12780985	Kerberos: Run ambari-server using non-root causes issues with AD velocity engine	"setup ambari-server to run with non-root daemon (ambari-server setup, select a non-root daemon account) and the following exception will be thrown when creating identities in an Active Directory when enabling Kerberos.

{code}
java.lang.RuntimeException: Velocity could not be initialized!
	at org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:307)
	at org.apache.velocity.runtime.RuntimeInstance.parse(RuntimeInstance.java:1196)
	at org.apache.velocity.runtime.RuntimeInstance.parse(RuntimeInstance.java:1181)
	at org.apache.velocity.runtime.RuntimeInstance.evaluate(RuntimeInstance.java:1297)
	at org.apache.velocity.runtime.RuntimeInstance.evaluate(RuntimeInstance.java:1265)
	at org.apache.velocity.app.Velocity.evaluate(Velocity.java:180)
	at org.apache.ambari.server.view.ViewContextImpl.parameterize(ViewContextImpl.java:381)
	at org.apache.ambari.server.view.ViewContextImpl.getProperties(ViewContextImpl.java:194)
	at org.apache.ambari.view.filebrowser.HdfsService.getApi(HdfsService.java:67)
	at org.apache.ambari.view.filebrowser.FileOperationService.listdir(FileOperationService.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:708)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:652)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1329)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:166)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:445)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:559)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1038)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:374)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:972)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.FailsafeHandlerList.handle(FailsafeHandlerList.java:132)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:363)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.velocity.exception.VelocityException: Error initializing log: Failed to initialize an instance of org.apache.velocity.runtime.log.Log4JLogChute with the current runtime configuration.
	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:875)
	at org.apache.velocity.runtime.RuntimeInstance.init(RuntimeInstance.java:262)
	at org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:302)
	... 93 more
Caused by: org.apache.velocity.exception.VelocityException: Failed to initialize an instance of org.apache.velocity.runtime.log.Log4JLogChute with the current runtime configuration.
	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:220)
	at org.apache.velocity.runtime.log.LogManager.updateLog(LogManager.java:269)
	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:871)
	... 95 more
Caused by: java.lang.RuntimeException: Error configuring Log4JLogChute : 
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.velocity.util.ExceptionUtils.createWithCause(ExceptionUtils.java:67)
	at org.apache.velocity.util.ExceptionUtils.createRuntimeException(ExceptionUtils.java:45)
	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:133)
	at org.apache.velocity.runtime.log.Log4JLogChute.init(Log4JLogChute.java:85)
	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:157)
	... 97 more
Caused by: java.io.FileNotFoundException: velocity.log (Permission denied)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:142)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)
	at org.apache.log4j.FileAppender.<init>(FileAppender.java:110)
	at org.apache.log4j.RollingFileAppender.<init>(RollingFileAppender.java:79)
	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:118)
	... 99 more
{code}"	AMBARI	Resolved	1	1	8661	kerberos
12754141	Provide a way to get service-specific Kerberos descriptor via REST API	Provide a way for a caller via the REST API to get information about a service's Kerberos descriptor.  This information should probably be attached to a service resource response.	AMBARI	Resolved	3	2	8661	api, service, stack
13147198	Remove LDAP Synchronization Process	"The existing LDAP synchronization process has the following challenges:

* Common annoyance among Operators
* Difficult to schedule as it’s interactive
* Introduces delay in entitlements being granted to users (in LDAP), and more importantly from revocation.
* Introduces issues with remove users that are no longer active, or with the company which == compliance concerns

and benefits: 
* Simplifies auto-complete for adding users/groups to cluster roles and view permissions
* Shields users from LDAP performance issues on login, and during user/group permission mapping

Given that, Ambari's LDAP sync process should be removed to allow for users and groups to be dynamically synchronized with a configured LDAP server.  Users should be added to the Ambari DB if necessary and groups are to be dynamically assigned and mapped to Ambari roles upon successful authentication with the configured LDAP server (via Ambari).  A scheduled job may need to execute to clean out any orphaned data.

The requirements will be broken out into categories of capabilities:

1.) Permission Mapping
2.) Permission Resolution
3.) User management

*Permission Mapping*: Ability to map an individual LDAP User DN to a permission, as well as an individual Group DN to a permission

*Permission Resolution*: Ability to resolve DN of user, DN of all directly mapped groups, and DN of all in-directly mapped groups (nested groups)
* If the user logging in has no permissions they should not be allowed to login, but shown a message stating that they have no mapped permissions in Ambari and to contact their administrator, no Ambari DB user should be auto-created
* If the user logging in has permissions, we should:
** Auto-create the Ambari user in the DB if it does not exist
** Check if we were asked to auto-create home directories on login, and if so check if the user has a home directory, if they don't, then auto-create it

*User Management*: Because users will be auto-created in the Ambari DB, and because they will be authenticated against LDAP before being able to login we have to appropriately deal with two types of users:

# Orphaned Users: Users without any mapped permissions - users enter this state by being in a group ""HadoopOps"" lets say and then six months later they are removed from this group. Or were directly mapped to a permission by name, and were removed from that permission. If they try logging into Ambari they will not be allowed to login and will be shown the message stating that they have no permissions.  These users need to be removed from Ambari individually through the Ambari UI with a ""Remove LDAP User"" button on the user.
# Deprovisioned Users: Users who have been either removed or inactivated in the upstream LDAP server due to termination or other reasons - In most situations we'll never see these users again.

For both types of users having the following capabilities would be extremely helpful:

* Ability to remove individual LDAP users from Ambari ""Remove LDAP User"" button
* Ability to remove all users who haven't logged in in more than x days.
"	AMBARI	Open	3	15	8661	authentication, ldap
13207537	Enable Kerberos fails when Ambari server is not on a registered host	"Enable Kerberos fails when Ambari server is not on a registered host.  

The following error is seen in /var/log/ambari-server.log

{noformat}
2019-01-03 15:28:34,238  WARN [Server Action Executor Worker 39] ServerActionExecutor:471 - Task #39 failed to complete execution due to thrown exception: org.apache.ambari.server.HostNotFoundException:Host not found, hostname=c7401.ambari.apache.org
org.apache.ambari.server.HostNotFoundException: Host not found, hostname=c7401.ambari.apache.org
        at org.apache.ambari.server.state.cluster.ClustersImpl.getHost(ClustersImpl.java:456)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:190)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:174)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.findConfigurationTagsWithOverrides(AmbariManagementControllerImpl.java:2431)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.google.inject.internal.DelegatingInvocationHandler.invoke(DelegatingInvocationHandler.java:50)
        at com.sun.proxy.$Proxy134.findConfigurationTagsWithOverrides(Unknown Source)
        at org.apache.ambari.server.state.ConfigHelper.calculateExistingConfigurations(ConfigHelper.java:2158)
        at org.apache.ambari.server.controller.KerberosHelperImpl.calculateConfigurations(KerberosHelperImpl.java:1722)
        at org.apache.ambari.server.controller.KerberosHelperImpl.getActiveIdentities(KerberosHelperImpl.java:1797)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.calculateServiceIdentities(KerberosServerAction.java:512)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:456)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.execute(CreatePrincipalsServerAction.java:92)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

This is caused when Ambari tried to find the host-specific configuration values when processing the Kerberos identities and the host is not registered for the relevant cluster. This can happen when the Ambari server Kerberos identity is being processed when the Ambari server host is not registered with the cluster. 

To solve this, host specific configuration values should not be obtained for the non-registered Ambari server host. 
"	AMBARI	Resolved	2	1	8661	pull-request-available
12774667	When adding the Oozie service to a kerberized cluster OOZIE_SERVER doesn't start	"Oozie server fails to start with the error: ""Fail: Configuration parameter 'oozie.service.HadoopAccessorService.kerberos.principal' was not found in configurations dictionary!""

Steps to reproduce:
Create a non-kerberized cluster
I used the following blueprint
{code}
{   
  ""host_groups"" : [
    {
      ""name"" : ""host_group_1"",
      ""components"" : [      
        {
          ""name"" : ""NODEMANAGER""
        },
        {
          ""name"" : ""NAMENODE""
        },
        {
          ""name"" : ""HISTORYSERVER""
        },
        {
          ""name"" : ""ZOOKEEPER_SERVER""
        },
        {
          ""name"" : ""SECONDARY_NAMENODE""
        },
        {
          ""name"" : ""RESOURCEMANAGER""
        },  
        {
          ""name"" : ""APP_TIMELINE_SERVER""
        },        
        {
          ""name"" : ""DATANODE""
        },
        {
          ""name"" : ""YARN_CLIENT""
        },
        {
          ""name"" : ""ZOOKEEPER_CLIENT""
        },
        {
          ""name"" : ""MAPREDUCE2_CLIENT""
        }     
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""2.2""
  }
}
{code}
- manually unzip UnlimitedJCEPolicy
- manually install MIT KDC
- Using UI, kerberize the existing cluster
- Using the UI, add the Oozie service

OOZIE_SERVER failed to start and the above noted exception was from the log that is exposed via the UI for the oozie start operation.
According to Robert Levas this property is in the kerberos descriptor it should be set.
"	AMBARI	Resolved	2	1	8661	keberos
12762453	Replace ${host} variable with relevant host in Kerberos Descriptors	Replace {{host}} variable with relevant host in Kerberos Descriptors	AMBARI	Resolved	3	3	8661	kerberos, kerberos_descriptor
12769963	Kerberos: when unable to connect to KDC admin, need to inform user	"This appears to be happening when the KDC and/or KDC Admin hosts do not point to a valid KDC, and when the relevant KDC does not _handle_ the indicated realm.

For the *host* issue, (in the cases that I tested), a {{org.apache.ambari.server.serveraction.kerberos.KerberosKDCConnectionException}} is being thrown but not caught to re-throw a {{java.lang.IllegalArgumentException}}, which would generate the expected 400 error.

For the *realm* issue, the following error string is not being captured by the logic to produce a {{org.apache.ambari.server.serveraction.kerberos.KerberosRealmException}}. 
{code}
kadmin: Cannot find KDC for requested realm while initializing kadmin interface
{code}
In any case, {{org.apache.ambari.server.serveraction.kerberos.KerberosRealmException}} is not being caught to re-throw a {{java.lang.IllegalArgumentException}}, which would generate the expected 400 error.
"	AMBARI	Resolved	3	3	8661	kerberos
12767438	Get kdc_type from kerberos-env rather than krb5-conf configuration	Get {{kdc_type}} from {{kerberos-env}} rather than {{krb5-conf}} configuration	AMBARI	Resolved	3	3	8661	kerberos
12914157	Create base infrastructure to allow for granular role based access control	"Create base infrastructure to allow for granular role based access control. 

This entails creating a base class to help with authorization checks.  The base class is to contain a default authorization check implementation but allow derived classes to override the logic to implement more sophisticated checks. 
"	AMBARI	Resolved	3	3	8661	rbac
12743514	Create Kerberos Service	"Create a service to manage the (optional) Kerberos server (managed KDC) and client components.

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12671235/AmbariClusterKerberization.pdf] for more information."	AMBARI	Resolved	3	2	8661	component, kdc, kerberos, stack
12766515	Add principal type to Kerberos descriptor	"Add principal _type_ to Kerberos descriptor to declare whether is principal is a service principal or a user principal.

This is needed for Active Directory since service principals needs to be created differently than user principals. 
 "	AMBARI	Resolved	3	3	8661	kerberos, kerberos_descriptor
12821690	NameNode Restart fails after attempt to Kerberize Cluster	"When attempting to restart the HDFS NameNode after running the Kerberos wizard to enable Kerberos, the NameNode fails to startup.  

The underlying failure in the ambari-agent appears to be:

""Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 298, in <module>
    NameNode().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 214, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 72, in start
    namenode(action=""start"", rolling_restart=rolling_restart, env=env)
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py"", line 38, in namenode
    setup_ranger_hdfs()
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py"", line 66, in setup_ranger_hdfs
    hdfs_repo_data = hdfs_repo_properties()
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py"", line 194, in hdfs_repo_properties
    config_dict['dfs.datanode.kerberos.principal'] = params._dn_principal_name
AttributeError: 'module' object has no attribute '_dn_principal_name'""

This keeps the HDFS NameNode from starting up properly after Kerberos is Enabled, and this seems to keep the process of Enabling Kerberos from completing.  

The problem appears to be a Python coding issue where _private_ variables (declared with a leading underscore) are not imported from {{common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py}} into {{common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py}}."	AMBARI	Resolved	2	1	8661	kerberos
12749306	ShellCommandUtil.Results class should be public, not package private	"{{ShellCommandUtil.Results}} class should be _public_, _not package_ private. Because it is _package private_, any {{ShellCommandUtil}} public methods returning a {{ShellCommandUtil.Results}} is basically unusable outside of the {{org.apache.ambari.server.utils}} package.

Solution:  Make {{ShellCommandUtil.Results}} and its methods explicitly _public_"	AMBARI	Resolved	4	1	8661	utils
13027933	Add permission for Service Auto Start	"Add permission to give a role the ability to set services to auto-start. This permission should be allowed at the cluster-level to toggle the feature for the cluster (Manage Service Auto Start Configuration) and at the service-level to toggle the feature for specific services (Manage Service Auto Start). However the service-level feature may not be available via all interfaces. 

The following roles should be able to toggle auto-start at the cluster level:
* Ambari Administrator
* Cluster Administrator
* Cluster Operator

The following roles should be able to toggle auto-start at the service level:
* Ambari Administrator
* Cluster Administrator
* Cluster Operator
* Service Administrator"	AMBARI	Resolved	3	3	8661	rbac
12952995	RBAC based user access to view instances are not honoured	"Problem:
1. Create a cluster 
2. Create some view instances in amber
3. Create a local non-admin ambari user
4. Grant the newly created user access to one of the view instances

Log-in with the non-admin user. The user should only see the view instances it has permission instead of all view instances.

This seems to have been introduced by https://issues.apache.org/jira/browse/AMBARI-14194"	AMBARI	Resolved	2	1	8661	rbac, security
13011057	Ambari should be able to create arbitrary Kerberos identities for itself as declared in the Kerberos Descriptor	"Ambari should be able to create arbitrary Kerberos identities for itself as declared in the Kerberos Descriptor.

Currently, Ambari is hard-coded to create identities for itself and SPNEGO, but that may not be good enough for all scenarios. Therefore, there needs to be an {{AMBARI}} service block in the Kerberos descriptor to allow for arbitrary identities to be defined for the Ambari server - similar to how any other service  is defined in the Kerberos descriptor. 

"	AMBARI	Resolved	2	1	8661	kerberos, kerberos_descriptor
12769795	Kerberos: Need stdout to show info on Kerberos-related tasks	"There's no stdout to indicate progress / success for the tasks for principal generation / keytab generation, etc.

Also, even just to know what command is actually being run to create the principal, that will help folks with debugging. Or is that only shown in case of a failure. Just want to make sure in the case of a problem, there is output there to help folks.
"	AMBARI	Resolved	3	3	8661	kerberos
13150446	Update Kerberos service documentation for Ambari 2.7.0	Update Kerberos service documentation for Ambari 2.7.0	AMBARI	Resolved	4	3	8661	pull-request-available
12991103	Kerberos Client fails to install	"Log
{noformat}
Traceback (most recent call last):
    File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 80, in <module>
      KerberosClient().execute()
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 280, in execute
      method(env)
    File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 28, in install
      self.install_packages(env)
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 545, in install_packages
      if Script.check_package_condition(package):
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 583, in check_package_condition
      return chooser_method()
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/package_conditions.py"", line 93, in should_install_kerberos_server
      return 'role' in config and not _has_applicable_local_component(""KERBEROS_CLIENT"")
  TypeError: _has_applicable_local_component() takes exactly 2 arguments (1 given)
{noformat}"	AMBARI	Resolved	1	1	8661	kerberos
13146815	Add SSO-related configuration recommendations to the stack advisor	"Add SSO-related configuration recommendations to the stack advisor.

# Add a new action - {{recommend-configurations-for-sso}} - to query services for only SSO-related configuration changes
# Append to the stack advisor input data, the Ambari-stored SSO integration data (list of services that should enable SSO integration, proxy url, public key details, etc...).

"	AMBARI	Resolved	3	3	8661	pull-request-available, stack_advisor
12771915	Kerberos: Do not validate host health or maintenance state when enabling Kerberos	Do not validate host health or maintenance state when enabling Kerberos	AMBARI	Resolved	3	3	8661	kerberos
13067086	BE: Extend Ambari REST API to Support User Account Management Improvements	"Update the Ambari REST API to allow for GET, POST, PUT, and DELETE operations on the authentication sources related to an Ambari user account.

* * {{/api/v1/users/:USERNAME/sources}}
** List a user’s authentication sources
** Add a new authentication source for a user

* {{/api/v1/users/:USERNAME/sources/:SOURCE_ID}}
** Get details on a specific authentication source for a user
** Modify details for a specific authentication source for a user

Update the following entry points, ensuring backwards compatibility where possible:

* {{/api/v1/users}}
** List all users
** Add a new user
** Backward compatibility: Set password should create or update the appropriate user_authentication record. 

"	AMBARI	Resolved	3	3	8661	rest_api, security
12959876	Kerberos wizard stuck trying to schedule service check operation	"Attached jstack after firing up the curl call.

{code}
curl -u admin:admin -H ""X-Requested-By:ambari"" -i -X POST -d'{""RequestInfo"":{""context"":""Kerberos Service Check"",""command"":""KERBEROS_SERVICE_CHECK"",""operation_level"":{""level"":""CLUSTER"",""cluster_name"":""c1""}},""Requests/resource_filters"":[{""service_name"":""KERBEROS""}]}' http://104.196.89.51:8080/api/v1/clusters/c1/requests
{code}

Behavior:
- Call timedout on the UI and wizard cannot proceed further.
- Exception in the server log after long wait:
{code}
12 Apr 2016 23:01:45,231  INFO [qtp-ambari-client-818] AmbariManagementControllerImpl:3376 - Received action execution request, clusterName=c1, request=isCommand :true, action :null, command :KERBEROS_SERVICE_CHECK, inputs :{}, resourceFilters: [RequestResourceFilter{serviceName='KERBEROS', componentName='null', hostNames=[]}], exclusive: false, clusterName :c1
12 Apr 2016 23:01:45,409  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HIVE
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: AMBARI_METRICS
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HDFS
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: MAPREDUCE2
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: OOZIE
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: TEZ
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HBASE
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: ZOOKEEPER
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: KERBEROS
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: YARN
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: PIG
12 Apr 2016 23:03:49,984  WARN [qtp-ambari-client-818] MITKerberosOperationHandler:434 - Failed to execute kadmin:
        Command: /usr/bin/kadmin -s perf-b-3.c.pramod-thangali.internal -p admin -w ******** -r EXAMPLE.COM -q ""get_principal admin""
        ExitCode: 1
        STDOUT: Authenticating as principal admin with password.

        STDERR: kadmin: Client not found in Kerberos database while initializing kadmin interface
{code}"	AMBARI	Resolved	1	1	8661	kerberos, kerberos-wizard, kerberos_descriptor
12917354	Enforce granular role-based access control for group functions	"Enforce granular role-based access control for alert functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Manage groups                 |   |   |   |   |   |(+)|

Entry points affected: 
* GET /api/v1/groups
* GET /api/v1/groups/:group_name
* PUT /api/v1/groups/:group_name
* POST /api/v1/groups/:group_name
* DELETE /api/v1/groups/:group_name
* GET /api/v1/groups/:group_name/members
* PUT /api/v1/groups/:group_name/members
* POST /api/v1/groups/:group_name/members
* DELETE /api/v1/groups/:group_name/members"	AMBARI	Resolved	3	3	8661	rbac
13180705	"Kerberos ""Additional Realms"" should not require keytab re-generation and cluster restart"	"""Admin -> Kerberos -> Additonal Realms""
* Currently requires keytab re-generation which in turn requires restarting the cluster. *But it is completely unrelated to keytabs*.

Fix:
* Move ""Additional Realms"" to the ""Kerberos"" service configs where it belongs, along with the ""auth_to_local"" setting which is what it is used for.
* When it is changed:
   ** No keytab re-generation is then required.
   ** Instead of silently altering ""auth_to_local"" rules, they should come up as ""Recommendations""."	AMBARI	Open	3	1	8661	auth_to_local, kerberos
12764542	Use cluster property rather than cluster-env/security_enabled to enable or disable Kerberos	"Use a cluster property rather than {{cluster-env/security_enabled}} to enable or disable Kerberos.  Since {{cluster-env/security_enabled}} is used by services to determine if Kerberos is enabled or not, it should not be set before completing the process of enabling or disabling Kerberos.  To declare whether the cluster enable or disable Kerberos, a property on the cluster should be set.  The property should be called {{security_type}} and must have one of the following values:
* NONE
* KERBEROS 

By using {{cluster-env/security_enabled}}, the configuration property gets set to ""true"" before Kerberos is filly enabled.   This is causing issues with stopping services so that the updated Kerberos-related configurations can be set.

Example API call to enable Kerberos
{code:title=PUT /api/v1/clusters/c1}
{
  ""Clusters"" : {
    ""security_type"" : ""KERBEROS""
  }
}
{code}

Example API call to disable Kerberos
{code:title=PUT /api/v1/clusters/c1}
{
  ""Clusters"" : {
    ""security_type"" : ""NONE""
  }
}
{code}"	AMBARI	Resolved	1	3	8661	kerberos
13145464	Update service metainfo to declare SSO integration support	"Update service metainfo to declare SSO integration support. The following tag may be optionally set in a service's {{metainfo.xml}} file:
{code:java}
<sso>
 <supported>true</supported>
 <enabledConfiguration>config-type/sso.enabled.property</enabledConfiguration>
</sso>
{code}
 "	AMBARI	Resolved	2	3	8661	pull-request-available, sso
12755616	Components should indicate Security State (via ambari-agent)	"In order to properly handle the automated installation or removal of a security infrastructure (like Kerberos) in the cluster, Ambari needs to know whether each component on the hosts of the cluster is properly _secured_ or not.  This information may be compared with data on the Ambari server to help determine what steps should be taken to ensure the cluster is in the correct _secured_ state.

To do this, the current and desired component security state is maintained in the Ambari database.  The Ambari server will update the desired state details according to whether the cluster is to be secured or not and whether the relevant service has enough metadata to be secured.  If the desired and actual security state details do not match, the Ambari server will take the necessary steps to work towards synchronization. 

In order for a component to indicate its security status, a new property needs to be returned in the {{STATUS_COMMAND}} response message (from the Ambari agent).  This property should be named ‘securityState’ and should have one of the following values:

* {{UNKNOWN}} - Indicates that it is not known whether the service or component is secured or not
* {{UNSECURED}} - Indicates service or component is not or should not be secured
* {{SECURED_KERBEROS}} - Indicates component is or should be secured using Kerberos
* {{ERROR}} - Indicates the component is not secured due to an error condition

To properly set this state value, a call needs to be executed per component querying for its specific state.  Due to the differences on how each component is secured and how it may be determined if security is setup what type is configured, and working is it properly, it is necessary for each component to have its own logic for determining this state. Therefore the ambari-agent process will need to call into the component’s configured (lifecycle) script and wait for its response - not unlike how it determines whether the component is up and running.

After the infrastructure is in place, each service definition needs to be updated to implement the new security status check function.  The function should perform the following steps:

* Determine if security is enabled or disabled
** If disabled, return ""UNSECURED""
** If enabled, determine what type of security is enabled
*** If Kerberos is configured
**** Perform tests (kinit?, ping KDC?) to determine if the configuration appears to be working
***** If working, return “SECURED_KERBEROS”
***** If not working, return “ERROR”
*** Else, return ""UNKNOWN""

If no function is available, the Ambari agent should return “UNKNOWN”."	AMBARI	Resolved	2	2	8661	kerberos, states
12960863	Auth-to-local rule generation duplicates default rules when adding case-insensitive default rules	"When re-generating auth-to-local rules where existing rules are already set, the default (or fallback) rule for the default and additional realms is duplicated but the extra instance(s) have the case-insensitive flag:

Example:
{noformat:title=Was}
...
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
...
{noformat}
{noformat:title=Becomes}
...
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*///L
...
{noformat}

*Steps to Reproduce*
# Create cluster with (at least) HDFS
# Enable Kerberos (do not check the box next to ""Enable case insensitive username rules""; kerberos-env/case_insensitive_username_rules should be false
# Edit Kerberos configuration and check ""Enable case insensitive username rules"" to set kerberos-env/case_insensitive_username_rules to true
# Regenerate Keytabs
# See duplicate entry in HDFS configs (core-site/hadoop.security.auth_to_local)

"	AMBARI	Resolved	2	1	8661	auth_to_local, kerberos
12924678	Failed to deploy Kerberized cluster via blueprint with custom principal name	"Failed to deploy Kerberized cluster via Blueprint with custom principal name set in Kerberos descriptor declared in _Cluster Creation Template_ like 

{code}
...
  ""security"": {
    ""type"": ""KERBEROS"",
    ""kerberos_descriptor"": {
      ""identities"": [
        {
          ""name"": ""smokeuser"",
          ""principal"": {
            ""value"": ""smokeuser9jJevBQAYGQWnRkuapSEp@${realm}""			
          }
        }
      ]
    }
  },
...
{code}

The following error (shown in ambari-server.log) was encountered while the cluster was being built: 
{noformat}
Failed to create keytab for smokeuser9jJevBQAYGQWnRkuapSEp@EXAMPLE.COM, missing cached file
{noformat}

h3. Cause
This was caused because the Kerberos descriptor in the _Blueprint_ or _Cluster Creation Template_ did not declare the type property of the principal being updated.  This caused the logic in Ambari to assume the principal was a _service_ principal rather than a _user_ (or headless) principal.  Because of this, when merging the updates to the default Kerberos descriptor (from the stack), the {{smokeuser}} principal type was changed _user_ to _service_.  Thus it was skipped over when the _Blueprints_ process executed the phase to ensure (and cache) headless identities.  

The bug is in the logic parsing the Kerberos descriptor.  By not specifying a principal type, the logic assumes the principal type is _service_; however in this case, the principal type needs to be {{null}}, so that when the user-specified Kerberos descriptor is merged with the default Kerberos descriptor, the default principal type is not changed. 

*NOTE:* This is not limited to _Blueprints_, the issue will cause issues if the specified Kerberos descriptor artifact is missing {{principal/type}} properties as well.  See [Set the Kerberos Descriptor|https://cwiki.apache.org/confluence/display/AMBARI/Automated+Kerberizaton#AutomatedKerberizaton-SettheKerberosDescriptor]

h3. Solution
Change the logic it the Kerberos descriptor parser to allow for the principal type to be {{null}}. Handle this value being {{null}} by consumers of this data such that {{null}} indicates the default value of {{service}}. This will keep the current behavior consistent, and also allow for the merging facility to properly merge principal updates - like changing the principal name pattern without needing to specify the principal type as well. 

h3. Workaround
Explicitly set the {{type}} property of _user_ (or headless) principals in the Kerberos descriptor:
{code}
...
  ""security"": {
    ""type"": ""KERBEROS"",
    ""kerberos_descriptor"": {
      ""identities"": [
        {
          ""name"": ""smokeuser"",
          ""principal"": {
            ""type"" : ""USER"",
            ""value"": ""smokeuser9jJevBQAYGQWnRkuapSEp@${realm}""			
          }
        }
      ]
    }
  },
...
{code}

"	AMBARI	Resolved	2	1	8661	kerberos_descriptor
13129251	Loosely store users authenticating from remote sources (LDAP, PAM, etc)	"Loosely store users authenticating from remote sources (LDAP, PAM, etc) such that minimal information is store in the Ambari database, relying on information from the remote sources to provide details like group membership and username.

Group membership, consecutive authentication failure count, and etc... should not be stored in the Ambari database for user accounts that are not authenticated locally. 

To do this, convert the {{users}} table into the following tables:

*user*
* user_id  (primary key)
* principal_id (foreign key to adminprincipal table)
* user_name
* authentication_source (LOCAL, LDAP, PAM)
* active_widget_layouts
* create_time

*local_user_authentication*
* user_id (foreign key to user table)
* password
* active
* consecutive_failures
* create_time
* update_time
"	AMBARI	Open	3	1	8661	authentication
12774314	Set kdc_type in kerberos-env rather than krb5-conf configuration	"Currently, the {{kdc_type}} value is being set in the {{krb5-conf}} configuration, it needs to be set in the {{kerberos-env}} since the data lies outside the realm of the krb5.conf file. 
  "	AMBARI	Resolved	3	3	8661	kerberos
13087272	"NPE during ""Update Kerberos Descriptor"""	"Ambari-server.log:- 
{code}
java.lang.NullPointerException
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processIdentity(KerberosDescriptorUpdateHelper.java:360)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processIdentities(KerberosDescriptorUpdateHelper.java:321)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processComponent(KerberosDescriptorUpdateHelper.java:230)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processService(KerberosDescriptorUpdateHelper.java:195)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processServices(KerberosDescriptorUpdateHelper.java:122)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.updateUserKerberosDescriptor(KerberosDescriptorUpdateHelper.java:78)
	at org.apache.ambari.server.serveraction.upgrades.UpgradeUserKerberosDescriptor.execute(UpgradeUserKerberosDescriptor.java:139)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:517)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:454)
	at java.lang.Thread.run(Thread.java:748)

{code}

*Cause*
This is caused by having a _custom/unexpected_ identity in the user-supplied Kerberos descriptor that is not in the Kerberos descriptor from the initial stack.  

*Solution*
Ignore the _custom/unexpected_ identity since the user must have added that manually and it is expected that it should remain untouched after the upgrade process. 

"	AMBARI	Resolved	2	1	8661	express_upgrade, rolling_upgrade, upgrade
12781015	Kerberos: Password generator needs to generate passwords based on rules to satisfy password policy	"The password generator used to generate passwords for identities needs to generate passwords based on a rule set rather than just a random sequence of characters. 

In a KDC (MIT or Active Directory), there may be a policy in place requiring a certain characteristics for the password. By creating a password consisting if 18 characters pulled randomly from {{abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890?.!$%^*()-_+=~}}, there is no guarantee that any specific policy will be met. 

The following rules should be settable:
* Length
* Minimum number of lowercase letters (a-z)
* Minimum number of uppercase letters (A-Z)
* Minimum number of digits (0-9)
* Minimum number of punctuation characters ({{?.!$%^*()-_+=~}})"	AMBARI	Resolved	3	1	8661	kerberos
12915084	Fix translation of different view resource type entities to all be of type VIEW	"Fix translation of different view resource type entities to all be of type VIEW. 

When an Ambari view is added to Ambari, a new resource type ({{adminresourcetype}}) is added.  Each of these types need to resolve to represent a _view_  resource ({{org.apache.ambari.server.security.authorization.ResourceType#VIEW}} so that authorization checks can be performed properly. 

For example:
{noformat}
 resource_type_id |    resource_type_name
------------------+---------------------------
                1 | AMBARI
                2 | CLUSTER
                3 | VIEW
                5 | ADMIN_VIEW{2.1.2}
                6 | FILES{1.0.0}
                7 | PIG{1.0.0}
                8 | CAPACITY-SCHEDULER{1.0.0}
                9 | TEZ{0.7.0.2.3.2.0-377}
               10 | SLIDER{2.0.0}
               11 | HIVE{1.0.0}
               55 | ADMIN_VIEW{2.2.0.0}
               56 | TEZ{0.7.0.2.3.2.0-3539}
{noformat}

The translation needs to be be:

{noformat}
AMBARI                    | ResourceType.AMBARI
CLUSTER                   | ResourceType.CLUSTER
VIEW                      | ResourceType.VIEW
ADMIN_VIEW{2.1.2}         | ResourceType.VIEW
FILES{1.0.0}              | ResourceType.VIEW
PIG{1.0.0}                | ResourceType.VIEW
CAPACITY-SCHEDULER{1.0.0} | ResourceType.VIEW
TEZ{0.7.0.2.3.2.0-377}    | ResourceType.VIEW
SLIDER{2.0.0}             | ResourceType.VIEW
HIVE{1.0.0}               | ResourceType.VIEW
ADMIN_VIEW{2.2.0.0}       | ResourceType.VIEW
TEZ{0.7.0.2.3.2.0-3539}   | ResourceType.VIEW
{noformat}"	AMBARI	Resolved	1	1	8661	rbac
12821979	Add the ability to obtain details about required Kerberos identities	"Add the ability to obtain details about required Kerberos identities for the cluster.   These details should be obtained using a REST API call formatted as a JSON structure.  

Resulting JSON block per Kerberos identity:
{code}
    ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
    }
{code}

The data will be converted into CSV-formatted data similar to the file exported from Ambari 1.7 (as follows):

||host||description||principal||keytab file name||keytab file base path||keytab file owner||keytab file group||keytab file mode||
|host1|Ambari Smoke Test User|ambari-qa@EXAMPLE.COM|smokeuser.headless.keytab|/etc/security/keytabs|ambari-qa|hadoop|440|
|host1|HDFS User|hdfs@EXAMPLE.COM|hdfs.headless.keytab|/etc/security/keytabs|hdfs|hadoop|440|
|host1|HDFS SPNEGO User|HTTP/host1@EXAMPLE.COM|spnego.service.keytab|/etc/security/keytabs|root|hadoop|440|
|host1|HDFS SPNEGO User|HTTP/host1@EXAMPLE.COM|spnego.service.keytab|/etc/security/keytabs|root|hadoop|440|
|host1|DataNode|dn/host1@EXAMPLE.COM|dn.service.keytab|/etc/security/keytabs|hdfs|hadoop|400|
|host1|NameNode|nn/host1@EXAMPLE.COM|nn.service.keytab|/etc/security/keytabs|hdfs|hadoop|400|
|host1|ZooKeeper Server|zookeeper/host1@EXAMPLE.COM|zk.service.keytab|/etc/security/keytabs|zookeeper|hadoop|400|


*Solution*
The following API calls are to be used to obtain the data:

{code:title=GET /api/v1/clusters/c1/hosts?fields=kerberos_identities/*}
{
  ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts?fields=kerberos_identities/*"",
  ""items"" : [
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host1""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/HTTP%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/dn%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/nm%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/nn%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""namenode_nn"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""nn/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/zookeeper%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""zookeeper_zk"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""zookeeper"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""zookeeper/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host2""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/HTTP%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/dn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/jhs%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""history_server_jhs"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""mapred"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/jhs.service.keytab"",
            ""principal_local_username"" : ""mapred"",
            ""principal_name"" : ""jhs/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/nm%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/nn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""secondary_namenode_nn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""nn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/rm%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""resource_manager_rm"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/rm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""rm/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/yarn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""app_timeline_server_yarn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/yarn.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""yarn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/zookeeper%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""zookeeper_zk"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""zookeeper"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""zookeeper/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host3""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/HTTP%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/amshbase%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""ams_hbase_master_hbase"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""ams"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/ams-hbase.master.keytab"",
            ""principal_local_username"" : ""ams"",
            ""principal_name"" : ""amshbase/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/dn%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/nm%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/zookeeper%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""ams_zookeeper"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""ams"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.ams.keytab"",
            ""principal_local_username"" : ""ams"",
            ""principal_name"" : ""zookeeper/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    }
  ]
}
{code}

{code:title=GET /api/v1/clusters/c1/kerberos_identities?fields=*}
{
  ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities?fields=*"",
  ""items"" : [
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nn%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""namenode_nn"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""nn/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""zookeeper_zk"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""zookeeper"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""zookeeper/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/jhs%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""history_server_jhs"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""mapred"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/jhs.service.keytab"",
        ""principal_local_username"" : ""mapred"",
        ""principal_name"" : ""jhs/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""secondary_namenode_nn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""nn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/rm%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""resource_manager_rm"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/rm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""rm/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/yarn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""app_timeline_server_yarn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/yarn.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""yarn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""zookeeper_zk"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""zookeeper"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""zookeeper/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/amshbase%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""ams_hbase_master_hbase"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""ams"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/ams-hbase.master.keytab"",
        ""principal_local_username"" : ""ams"",
        ""principal_name"" : ""amshbase/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""ams_zookeeper"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""ams"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.ams.keytab"",
        ""principal_local_username"" : ""ams"",
        ""principal_name"" : ""zookeeper/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    }
  ]
}
{code}

{code:title=GET /api/v1/clusters/c1/kerberos_identities?fields=*&format=csv}
host,description,principal name,principal type,local username,keytab file path,keytab file owner,keytab file owner access,keytab file group,keytab file group access,keytab file mode,keytab file installed
host1,/spnego,HTTP/host1@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host1,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host1,datanode_dn,dn/host1@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host1,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host1,nodemanager_nm,nm/host1@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host1,namenode_nn,nn/host1@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/nn.service.keytab,hdfs,r,hadoop,,400,true
host1,zookeeper_zk,zookeeper/host1@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/zk.service.keytab,zookeeper,r,hadoop,,400,true
host2,/spnego,HTTP/host2@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host2,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host2,datanode_dn,dn/host2@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host2,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host2,history_server_jhs,jhs/host2@EXAMPLE.COM,SERVICE,mapred,/etc/security/keytabs/jhs.service.keytab,mapred,r,hadoop,,400,true
host2,nodemanager_nm,nm/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host2,secondary_namenode_nn,nn/host2@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/nn.service.keytab,hdfs,r,hadoop,,400,true
host2,resource_manager_rm,rm/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/rm.service.keytab,yarn,r,hadoop,,400,true
host2,app_timeline_server_yarn,yarn/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/yarn.service.keytab,yarn,r,hadoop,,400,true
host2,zookeeper_zk,zookeeper/host2@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/zk.service.keytab,zookeeper,r,hadoop,,400,true
host3,/spnego,HTTP/host3@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host3,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host3,ams_hbase_master_hbase,amshbase/host3@EXAMPLE.COM,SERVICE,ams,/etc/security/keytabs/ams-hbase.master.keytab,ams,r,hadoop,,400,true
host3,datanode_dn,dn/host3@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host3,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host3,nodemanager_nm,nm/host3@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host3,ams_zookeeper,zookeeper/host3@EXAMPLE.COM,SERVICE,ams,/etc/security/keytabs/zk.service.ams.keytab,ams,r,hadoop,,400,true
{code}
"	AMBARI	Resolved	2	1	8661	kerberos
12776742	Kerberos: fails when entering admin principal with blank password 	"Note: I don't believe the below is specific to add host, but related to the prompting and how the set admin cred works in case of a blank password. I hit this during testing of add host though.

- install cluster, kerberize
- add host (be sure to use a new browser so you know it will prompt for kerb admin credentials)
- got to the review part of add host, click deploy
- prompted for admin creds (as expected)
- tried messing around by putting in bad creds and that seemed to work...
- expect when I put in the right admin cred principal name (admin/admin) but a blank password. I was surprised it allowed me to click save (because the password field was blank)
- so I click save, dialog disappears and I am cannot get it to re-prompt.
- this is what it PUT and the response was blank...
 
{code}
[{""session_attributes"":{""kerberos_admin"":{""principal"":""admin/admin"",""password"":""""}}}]:
Response Headersview source
{code}

in ambari-server.log, nothing

{code}
17:58:05,860  INFO [qtp1257282095-603] AmbariManagementControllerImpl:1171 - Received a updateCluster request, clusterId=2, clusterName=MyCluster, securityType=null, request={ clusterName=MyCluster, clusterId=2, provisioningState=null, securityType=null, stackVersion=HDP-2.2, desired_scv=null, hosts=[] }
{code}

- back in wizard doesn't solve it. had to completely exit wizard and ambari web to start again

The overall issue is how the credentials are being validated.  If no password is being set, the command to test the credentials when using a MIT KDC generates the following command:
{code}
kadmin -p admin/admin -w """" -r EXAMPLE.COM -q 'get_principal admin/admin'
{code}

The empty password ({{-w """"}}) in the command creates an interactive session where the command is waiting for data on STDIN, thus hanging the process.

An empty password should not cause the same behavior when using Active Directory."	AMBARI	Resolved	2	1	8661	kerberos
12915829	Change Anonymous API Authentication To A Declared User	"When using {{api.authenticate=false}}, REST requests to the Ambari APIs don't need to contain any user information. As a result, new code being placed which assumes an authenticated user will throw NPE exceptions:

{code}
      // Ensure that the authenticated user has authorization to get this information
      if (!isUserAdministrator && !AuthorizationHelper.getAuthenticatedName().equalsIgnoreCase(userName)) {
        throw new AuthorizationException();
      }
{code}

{code}
java.lang.NullPointerException
	at org.apache.ambari.server.controller.internal.ActiveWidgetLayoutResourceProvider.getResources(ActiveWidgetLayoutResourceProvider.java:156)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:946)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:132)
	at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:512)
	at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:381)
	at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:217)
{code}

Recommend changing this option to something like

{code}
api.authenticated.user=admin
{code}

This will preserve the existing functionality while allowing the new code to continue to assume authenticated users."	AMBARI	Resolved	3	1	8661	api, authentication, rbac
13134115	First prereq not displaying for Free IPA method in Enable Kerberos Wizard	"First prereq not displaying for Free IPA method in Enable Kerberos Wizard.

 !image-2018-01-26-16-32-14-802.png! 

This is due to a typo at 
{code:title=ambari-web/app/controllers/main/admin/kerberos/step1_controller.js:88}
          dsplayText: Em.I18n.t('admin.kerberos.wizard.step1.option.ipa.condition.1'),
{code}

The code should be 
{code:title=ambari-web/app/controllers/main/admin/kerberos/step1_controller.js:88}
          displayText: Em.I18n.t('admin.kerberos.wizard.step1.option.ipa.condition.1')
{code}

Notice ""dsplayText"" vs ""displayText"".
"	AMBARI	Resolved	3	1	8661	pull-request-available
13067078	BE: Improve User Account Management 	"Update the backend for improved user management.  

User management tables in the DB should be:

*{{users}}*
||Name||Type||Description||
|user_id|INTEGER|Internal unique identifier|
|principal_id|INTEGER|Foreign key from adminprincipal table|
|user_name|VARCHAR|Unique, case-insensitive, login identifier expected to be used when logging into Ambari|
|create_time|TIMESTAMP|Creation time for this account in Ambari|
|active|BOOLEAN|Active/not active flag|
|consecutive_failed_auth_attemps|INTEGER|The number a failed authorization attempts since the last successful authentication|
|active_widgets_layout|VARCHAR| |
|display_name|VARCHAR|Cosmetic name value to show the user in user interfaces|
|local_username|VARCHAR|Case-sensitive username to use when impersonating user in facilities like Ambari Views|
* Primary Key: {{user_id}}
* Foreign Key: {{principal_id}} -> {{adminprincipal.principal_id}}

*{{user_authentication}}*
||Name||Type||Description||
|user_authentication_id|INTEGER|Primary key for this table|
|user_id|INTEGER|Foreign key from users table|
|authentication_type|VARCHAR|Type of authentication system - LOCAL, LDAP,  KERBEROS, JTW, PAM, etc...
|authentication_key|VARCHAR|Type-specific key (or identifier):
* LOCAL: the user's password (digest)
* LDAP: the user’s distinguished name
* KERBEROS: the user’s principal
* etc...|
|create_time|TIMESTAMP|Creation time of this record
|update_time|TIMESTAMP|Update time for this record, can be used to enforce password retention times|
* Primary Key: {{user_authentication_id}}
* Foreign Key: {{user_id}} -> {{users.user_id}}

Java code needs to change accordingly."	AMBARI	Resolved	3	3	8661	authentication, security
12937979	Provide explicit ordering for roles	"Since it may be desired to order roles in ways other than alphabetically, each role should have an explicit numerical order that may be used by UI's. 

Roles should be explicitly ordered by the amount of access privileges they have. 


||Role Name||Explicit Order Value||
|Ambari Administrator|1|
|Cluster Administrator|2|
|Cluster Operator|3|
|Service Administrator|4|
|Service Operator|5|
|Cluster User|6|
|View User|7|


"	AMBARI	Resolved	3	3	8661	rbac
12835919	Invalid property value set in core-site.xml when KNOX HA is enabled	"In KNOX-HA cluster, noticed that the “hadoop.proxyuser.knox.hosts"" property in /etc/hadoop/conf/core-site.xml
 refers to only one node and knox is running on multiple nodes.
{code}
 <property>
      <name>hadoop.proxyuser.knox.hosts</name>
      <value>host1</value>
    </property>
{code}
The value for this property should include all the knox hosts
{code}
 <property>
      <name>hadoop.proxyuser.knox.hosts</name>
      <value>host1,host4,etc...</value>
    </property>
{code}"	AMBARI	Resolved	2	1	8661	kerberos, kerberos_descriptor
12828073	Ambari uses users' interactive ticket cache	"It appears that it is necessary to kinit prior to starting ambari-server, even after ambari-server setup-security (#3). It seems that this should be automatically handled by Ambari. 

Ambari-server should NOT use the same ticket cache as the interactive user. 

STR:
1. kinit
2. ambari-server start
3. verify that ambari-server can authenticate with ticket specified in #1
4. kdestroy
5. try to authenticate through Ambari again (it will not work)

*Solution*
Ensure JAAS Login works properly such that the Kerberos tickets for the account that executes Ambari is not relevant.

"	AMBARI	Resolved	2	1	8661	JAAS
13024634	Fix NPE in UpgradeCatalog250Test.testExecuteDMLUpdates	"Fix NPE in UpgradeCatalog250Test.testExecuteDMLUpdates

{noformat}
Running org.apache.ambari.server.upgrade.UpgradeCatalog250Test
Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.14 sec <<< FAILURE! - in org.apache.ambari.server.upgrade.UpgradeCatalog250Test
testExecuteDMLUpdates(org.apache.ambari.server.upgrade.UpgradeCatalog250Test)  Time elapsed: 0.272 sec  <<< ERROR!
java.lang.NullPointerException
	at org.apache.ambari.server.upgrade.UpgradeCatalog250Test.testExecuteDMLUpdates(UpgradeCatalog250Test.java:234)
{noformat}
"	AMBARI	Resolved	1	1	8661	unit-test
12975730	Fix description of SERVICE.ADD_DELETE_SERVICES permission	"The description of the SERVICE.ADD_DELETE_SERVICES permission currently reads

{quote}
Add Service to cluster
{quote}

This should be changed to

{quote}
Add/delete services
{quote}
"	AMBARI	Resolved	3	1	8661	rbac
13201894	Handle requests from a configured trusted proxy to identify a proxied user using Kerberos	"Handle requests from a configured trusted proxy to identify a proxied user using Kerberos.

Upon receiving a request where that caller is identified using Kerberos, check to see of the request was from a (trusted) proxy.  If so, validate the trusted proxy and set the authenticated user to the proxied user specified in the ""{{doAs}}"" query parameter. 

After receiving a request where the user is to be authenticated using Kerberos, perform the following steps:
# Determine if a proxied user is specified using a ""{{doAs}}"" query parameter.  
# Using the following Ambari configuration property, determine if a proxied user can be specified from the requesting host:
** {{ambari.tproxy.proxyuser.$username.hosts}}, where $username is the username of the authenticated user (not the user specified in the doAs query parameter)
# Obtain the proxied username from the {{doAs}} query parameter
# Using the following Ambari configuration property, determine if the proxied user can be specified based on the user's username:
** {{ambari.tproxy.proxyuser.$username.users}}, where $username is the username of the authenticated user 
# Using the following Ambari configuration property, determine if the proxied user can be specified based on the groups the proxied user belong to:
** {{ambari.tproxy.proxyuser.$username.groups}}, where $username is the username of the authenticated user t"	AMBARI	Resolved	3	3	8661	pull-request-available, tproxy
12838173	Kerberos Wizard: Moving between steps 2 and 3 back/forward cause API error.	"STR:
* Install cluster with HDFS, Hive and dependcies.
* Start Kerberos Wizard
* Proceed to step 3 *Install and Test Kerberos Client*
* Go back to step2 *Configure Kerberos*
* Click on next button

AR: Popup with request error shown
{noformat}
500 status code recieved on POST method for API: /api/v1/clusters/c1/hosts

Error message: org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: ServiceComponent not found, clusterName=c1, serviceName=KERBEROS, serviceComponentName=KERBEROS_CLIENT
{noformat}
After popup confirmation click on Next button doesn't do anything.
ER: No errors shown, user can proceed to the next step.

Workaround: Refresh page on step2 and hit Next button."	AMBARI	Resolved	2	1	8661	kerberos, kerberos-wizard
13168836	Python unit test failure on 2.7.6	"Python unit tests fail on Python 2.7.6, because [SSLContext|https://docs.python.org/2/library/ssl.html#ssl-contexts] was only added in 2.7.9.

{noformat:title=https://builds.apache.org/job/Ambari-trunk-Commit/9543/consoleText}
ERROR: test_ldap_sync_ssl (TestAmbariServer.TestAmbariServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/TestAmbariServer.py"", line 7804, in test_ldap_sync_ssl
    sync_ldap(options)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/main/python/ambari_server/setupSecurity.py"", line 405, in sync_ldap
    raise FatalException(1, err)
FatalException: ""Fatal exception: Sync event creation failed. Error details: 'module' object has no attribute 'SSLContext', exit code 1""

ERROR: test_get_ssl_context (TestServerUtils.TestServerUtils)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/TestServerUtils.py"", line 124, in test_get_ssl_context
    context = get_ssl_context(properties)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/main/python/ambari_server/serverUtils.py"", line 274, in get_ssl_context
    context = ssl.SSLContext(protocol)
AttributeError: 'module' object has no attribute 'SSLContext'
{noformat}

CC [~rlevas]"	AMBARI	Resolved	3	1	8661	pull-request-available
12763165	Configuration keys in Kerberos descriptors should allow for variable replacement	"Configuration keys in Kerberos descriptors should allow for variable replacement. For example a typical {{configurations}} block may look like 
{code}
""configurations"" : [
 ""core-site"": {
     ""hadoop.proxyuser.hive.groups"":""${hadoop-env/proxyuser_group}""
   },
  ""hive-site"": {
   .....
  },
  ""webhcat-site"": {
  ....
 }
]
{code}

However some configuration keys need to be dynamically generated and should be generated using the existing variable replacement feature in {{org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor}}.  For example: 
{code}
""configurations"": [
  ""core-site"": {
     ""hadoop.proxyuser.${hive-env/hive_user}.groups"":""${hadoop-env/proxyuser_group}""
   },
  ""hive-site"": {
   .....
  },
  ""webhcat-site"": {
  ....
 }
] 
{code}

The configuration key {{hadoop.proxyuser.$\{hive-env/hive_user\}.groups}} should be processed such that _$\{hive-env/hive_user\}_ is replaced some value like {{hive}} to yield  {{hadoop.proxyuser.hive.groups}} 
"	AMBARI	Resolved	3	3	8661	kerberos, kerberos_descriptor
12842349	hive-site/hive.metastore.sasl.enabled value incorrect when adding Hive to a Kerberized Cluster	"When *adding* Hive to an existing Kerberized cluster, the {{hive-site/hive.metastore.sasl.enabled}} value is set to {{false}} when it should be {{true}}.  If Hive was installed before enabling Kerberos,  
{{hive-site/hive.metastore.sasl.enabled}} is set to {{true}} after enabling Kerberos.

If {{hive-site/hive.metastore.sasl.enabled}} is {{false}} in a Kerberized cluster, the following error can be seen in the hiverserver2.log:

{noformat:title=/var/log/hive/hiveserver2.log}
2015-07-01 23:35:16,128 ERROR [HiveServer2-Handler-Pool: Thread-37]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Unsupported mechanism type GSSAPI
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.thrift.transport.TTransportException: Unsupported mechanism type GSSAPI
        at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)
        at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:138)
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
        at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
        ... 4 more
{noformat}

*Cause*
It appears that the front end is updating the Kerberos Descriptor _artifact_ with _old_ data rather than the data the is specified on the stack's Kerberos Descriptor. This occurs during the transition between the ""Review"" and ""Install, Start, Test"" pages of the ""Add Service Wizard"".

*Solution*
Use the current Kerberos Descriptor's values as default value for the updated Kerberos Descriptor and update only what the user changes in the relevant fields."	AMBARI	Resolved	2	1	8661	kerberos
12777738	Agent is spamming logs with exceptions 	"{code}
INFO 2015-01-19 14:00:32,786 PythonExecutor.py:114 - Result: {'structuredOut': {u'securityIssuesFound': u'Configuration file hbase-site did not pass the vali
dation. Reason: Property hbase.regionserver.keytab.file does not exist', u'processes': [], u'securityState': u'UNSECURED'}, 'stdout': '2015-01-19 10:10:12,89
4 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/scr
ipt/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 
90, in status\n    check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/ch
eck_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:10:50,579 - Error while exe
cuting command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", lin
e 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    
check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.
py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:12:46,728 - Error while executing command \'st
atus\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n
    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    check_process_statu
s(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in ch
eck_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:12:47,149 - Error while executing command \'status\':\nTraceback 
(most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  
File ""/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py"", line 86, in status\n    raise ComponentIsNotRunning()\nC
omponentIsNotRunning\n2015-01-19 10:13:46,533 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/sit
e-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/
0.96.0.2.0/package/scripts/hbase_master.py"", line 73, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_manage
ment/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:
13:47,003 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libra
ries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py
"", line 90, in status\n    check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/func
tions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:13:47,438 - Error w
hile executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.
py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line
 76, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py""
, line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:13:47,872 - Error while executing command \'statu
s\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n   
 method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py"", line 86, in status\n    raise ComponentI
sNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:39,410 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/l
ib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common
-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 110, in status\n    check_process_status(status_params.namenode_pid_file)\n  File ""/usr/lib/pytho
n2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nCom
ponentIsNotRunning\n2015-01-19 10:19:40,004 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-
packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.
96.0.2.0/package/scripts/hbase_master.py"", line 73, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_manageme
nt/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19
:40,420 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/librari
es/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"",
 line 110, in status\n    check_process_status(status_params.namenode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/funct
ions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:40,852 - Error wh
ile executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.p
y"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_master.py"", line 73, in
 status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 
41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:41,278 - Error while executing command \'status\':\nT
raceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method
(env)\n  File ""/var/lib/ambari-agent/cache/common-services/KAFKA/0.8.1.2.2/package/scripts/kafka_broker.py"", line 70, in status\n    check_process_status(sta
tus_params.kafka_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_proc
ess_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:41,676 - Error while executing command \'status\':\nTraceback (most re
cent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/v
ar/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    check_process_status(status_params.datanode_pi
d_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    rai
se ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:42,112 - Error while executing command \'status\':\nTraceback (most recent call last):\n 
 File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent
{code}"	AMBARI	Resolved	2	1	8661	kerberos
13170730	Suppress log messages from the credential_store_helper	Suppress log messages from the credential_store_helper since they are not necessary.	AMBARI	Resolved	3	1	8661	pull-request-available
12832381	Kerberos: Creating principals in AD when special characters are involved causes failures	"Creating principals in AD when special characters are involved causes failures.

The following characters in the CN need to be escaped:
{noformat}
/ , \ # + < > ; "" =
{noformat}

*Note:* javax.naming.ldap.Rdn will properly escape relative distinguished name parts.


The following characters in the sAMAccountName need to be removed or replaced:
{noformat}
[ ] : ; | = + * ? < > / \
{noformat}

*Note:* This needs to be done explicitly within the attributes set if a relevant entry exists.
{code}
// Replace the following _illegal_ characters: [ ] : ; | = + * ? < > / \
value = value.toString().replaceAll(""\\[|\\]|\\:|\\;|\\||\\=|\\+|\\*|\\?|\\<|\\>|\\/|\\\\"", ""_"");
{code}
"	AMBARI	Resolved	2	1	8661	active-directory, active_directory, kerberos
13057423	Disabling security fails with AttributeError	"Disabling security is failing with :
{code}
Stack Advisor reported an error: AttributeError: 'NoneType' object has no attribute 'replace'
StdOut file: /var/run/ambari-server/stack-recommendations/26/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/26/stackadvisor.err
{code}

Error file shows :
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 166, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 116, in main
    result = stackAdvisor.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 775, in recommendConfigurations
    calculation(configurations, clusterSummary, services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/./../stacks/HDP/2.5/services/stack_advisor.py"", line 480, in recommendStormConfigurations
    storm_nimbus_impersonation_acl.replace('{{storm_bare_jaas_principal}}', storm_bare_jaas_principal)
AttributeError: 'NoneType' object has no attribute 'replace'
{code}"	AMBARI	Resolved	1	1	8661	kerberos, stack_advisor
13068235	Create Database Schema for Improved User Account Management	"User management tables in the DB should be:

*{{users}}*
||Name||Type||Description||
|user_id|INTEGER|Internal unique identifier|
|principal_id|INTEGER|Foreign key from adminprincipal table|
|user_name|VARCHAR|Unique, case-insensitive, login identifier expected to be used when logging into Ambari|
|active|BOOLEAN|Active/not active flag|
|consecutive_failures|INTEGER|The number a failed authorization attempts since the last successful authentication|
|active_widgets_layout|VARCHAR| |
|display_name|VARCHAR|Cosmetic name value to show the user in user interfaces|
|local_username|VARCHAR|Case-sensitive username to use when impersonating user in facilities like Ambari Views|
|create_time|TIMESTAMP|Creation time for this account in Ambari|
* Primary Key: {{user_id}}
* Foreign Key: {{principal_id}} -> {{adminprincipal.principal_id}}

*{{user_authentication}}*
||Name||Type||Description||
|user_authentication_id|INTEGER|Primary key for this table|
|user_id|INTEGER|Foreign key from users table|
|authentication_type|VARCHAR|Type of authentication system - LOCAL, LDAP,  KERBEROS, JTW, PAM, etc...
|authentication_key|VARCHAR|Type-specific key (or identifier):
* LOCAL: the user's password (digest)
* LDAP: the user’s distinguished name
* KERBEROS: the user’s principal
* etc...|
|create_time|TIMESTAMP|Creation time of this record
|update_time|TIMESTAMP|Update time for this record, can be used to enforce password retention times|
* Primary Key: {{user_authentication_id}}
* Foreign Key: {{user_id}} -> {{users.user_id}}
"	AMBARI	Resolved	2	3	8661	user_management
12758434	HBase service components should indicate security state	"The HBase service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. HBASE_MASTER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hbase_conf_dir + ‘/hbase-site.xml’
** hbase.security.authentication
*** = “kerberos”
*** required
** hbase.security.authorization
*** = “true”
*** required
** hbase.master.keytab.file
*** not empty
*** path exists and is readable
*** required
** hbase.master.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hbase master principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. HBASE_REGIONSERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hbase_conf_dir + ‘hbase-site.xml’
** hbase.security.authentication
*** = “kerberos”
*** required
** hbase.security.authorization
*** = “true”
*** required
** hbase.regionserver.keytab.file
*** not empty
*** path exists and is readable
*** required
** hbase.regionserver.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hbase region server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477

_*Note*_: There may be additional work related to _REST gateway impersonation_"	AMBARI	Resolved	3	4	8661	hbase, kerberos, security
12777381	Root user has spnego (HTTP) kerberos ticket set after Kerberos is enabled, root should have no ticket.	"After enabling Kerberos, the root user has the spnego user set for it 

{code}
[root@c6501 ~]# klist
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: HTTP/c6501.ambari.apache.org@EXAMPLE.COM

Valid starting     Expires            Service principal
02/18/15 22:14:51  02/19/15 22:14:51  krbtgt/EXAMPLE.COM@EXAMPLE.COM
	renew until 02/18/15 22:14:51
{code}

It appears that the issue is related to the agent-side scheduler and/or some job that is scheduled to run periodically. Apparently some job is kinit-ing with the SPNEGO identity as the running user (root in this case) without changing the ticket cache. Thus whenever the job runs the root user's ticket cache gets changed to contain the SPNEGO identity's ticket.
"	AMBARI	Resolved	1	1	8661	kerberos, keytabs
13075924	Update Database Access Layer to Support New Database Schema for Improved User Account Management	"Update Database Access Layer to Support New Database Schema for Improved User Account Management.  

* Update {{org.apache.ambari.server.orm.entities.UserEntity}}
* Update {{org.apache.ambari.server.orm.dao.UserDAO}}
* Add {{org.apache.ambari.server.orm.entities.UserAuthenticationEntity}}
* Add {{org.apache.ambari.server.orm.dao.UserAuthenticationDAO}}
"	AMBARI	Resolved	3	1	8661	user_management
12988072	Changes to stack advisor framework to help with service advisors	"The following changes are needed in the stack advisor framework to help with service advisors:

* Add additional logging to show why a service advisor implementation was not loaded
* Move {{isSecurityEnabled}} from {{stacks/HDP/2.0.6/services/stack_advisor.py}} to a class member of {{DefaultStackAdvisor}} in {{stacks/stack_advisor.py}} so that all stack and service advisors may be able to use it
"	AMBARI	Resolved	3	1	8661	service_advisor, stack_advisor
12903233	Security-related HTTP headers should be set separately for Ambari Views then for Ambari server UI	"The security-related HTTP headers should be set separately for the Ambari Views then for the Ambari server UI. This is because they have different requirements.  For example the Ambari server UI should not be allowed to execute in an iframe (by default) where Ambari View must be able to execute in an iframe invoked from the same origin.

The relevant headers are:
* Strict-Transport-Security
* X-Frame-Options
* X-XSS-Protection

These headers should be configurable via the ambari.properties such that they may be turned on or off - and set to some custom value.

The default value for this headers should be as follows:
* Strict-Transport-Security: max-age=31536000
* X-Frame-Options: SAMEORIGIN
* X-XSS-Protection: 1; mode=block

Strict-Transport-Security should only be turned on if SSL is enabled.

The relevant Ambari properties should be:
* Strict-Transport-Security: views.http.strict-transport-security
* X-Frame-Options: views.http.x-frame-options
* X-XSS-Protection: views.http.x-xss-protection

By setting any of these to be empty, the header is to be turned off (or not set).

For example:
{code:title=Sets Strict-Transport-Security to a custom value}
views.http.strict-transport-security=max-age=31536000; includeSubDomains
{code}

{code:title=Turns Strict-Transport-Security off}
views.http.strict-transport-security=
{code}
"	AMBARI	Resolved	3	1	8661	security
13147818	Enable or disable SSO for services upon setting sso-configuration values 	"Enable or Disable SSO for services upon setting sso-configuration values.

The action performed by the user via the REST API to set value for the Ambari server sso-configuration value should trigger a call to the stack advisor and, using the returned recommendations, set properties needed to enable or disable SSO integration for the relevant services."	AMBARI	Resolved	3	3	8661	pull-request-available
12826040	Kerberos: Password generator needs to generate passwords based on a pattern to satisfy password policy	"The password generator used to generate passwords for identities needs to generate passwords based on a pattern rather than just a random sequence of characters. 

Within the KDC, there may be a policy in place requiring a certain characteristics for the password. By creating a password consisting if 18 characters pulled randomly from {{abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890?.!$%^*()-_+=~}}, there is no guarantee that any specific policy will be met. 
"	AMBARI	Resolved	3	3	8661	kerberos
13004430	Create authentication filter to encapsulate the various Ambari authentication methods	"Create a Spring authentication filter to encapsulate the various Ambari authentication methods since the Spring filter chain allows for a single authentication filter and Ambari needs to allow for multiple, optional, authentication filters to handle one of (but not limited to) the following authentication methods:

* Basic Auth 
* SSO (JWT)
* Kerberos token

"	AMBARI	Resolved	3	3	8661	authentication, kerberos, security
12938869	KerberosDescriptorTest failed due to moved/missing test directory	"Unit tests in {{org.apache.ambari.server.stack.KerberosDescriptorTest}} fail due to moved/missing directory - {{.../ambari/ambari-server/target/classes/stacks}}.  

{{org.apache.ambari.server.stack.KerberosDescriptorTest}} should use {{{.../ambari/ambari-server/src/main/resources/stacks}} instead. "	AMBARI	Resolved	3	1	8661	unit-test
13050320	Atlas MetaData server start fails while granting permissions to HBase tables after unkerberizing the cluster	"STR
1. Deploy HDP-2.5.0.0 with Ambari-2.5.0.0 (secure MIT cluster installed via blueprint)
2. Express Upgrade the cluster to 2.6.0.0
3. Disable Kerberos
4. Observed that Atlas Metadata server start failed with below errors:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata_server.py"", line 249, in <module>
    MetadataServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 720, in restart
    self.start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata_server.py"", line 102, in start
    user=params.hbase_user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
atlas_titan
ATLAS_ENTITY_AUDIT_EVENTS
atlas
TABLE
ATLAS_ENTITY_AUDIT_EVENTS
atlas_titan
2 row(s) in 0.2000 seconds

nil
TABLE
ATLAS_ENTITY_AUDIT_EVENTS
atlas_titan
2 row(s) in 0.0030 seconds

nil
java exception
ERROR Java::OrgApacheHadoopHbaseIpc::RemoteWithExtrasException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: No registered coprocessor service found for name AccessControlService in region hbase:acl,,1480905643891.19e697cf0c4be8a99c54e39aea069b29.
	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:7692)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execServiceOnRegion(RSRpcServices.java:1897)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(RSRpcServices.java:1879)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32299)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2141)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:187)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:167)
{code}

*Cause*
When disabling Kerberos, the stack advisor recommendations are not properly applied due to the order of operations and various conditionals.

*Solution*
Ensure that the stack advisor recommendations are properly applied when disabling Kerberos. 
"	AMBARI	Resolved	2	1	8661	system_test
12896073	kdc_type lost when updating kerberos-env via Kerberos service configuration page	"After editing the kerberos-env configuration using Ambari's Kerberos service configuration page and saving the new configuration, the {{kdc_type}} property is lost and not saved with the new configuration.

*By loosing this value, any future Kerberos-related operations will fail with errors since the mandatory kerberos-env/kdc_type property will be missing.*

{code:title=kerberos-env before update}
{
  ""kdc_type"": ""mit-kdc"",
  ""password_min_uppercase_letters"": ""1"",
  ""password_min_whitespace"": ""0"",
  ""password_min_punctuation"": ""1"",
  ""password_min_digits"": ""1"",
  ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
  ""kdc_create_attributes"": """",
  ""admin_server_host"": ""host1"",
  ""password_min_lowercase_letters"": ""1"",
  ""container_dn"": """",
  ""password_length"": ""20"",
  ""case_insensitive_username_rules"": ""false"",
  ""manage_identities"": ""true"",
  ""service_check_principal_name"": ""${cluster_name}-${short_date}"",
  ""kdc_host"": ""host1"",
  ""ad_create_attributes_template"": ""\n{\n  \""objectClass\"": [\""top\"", \""person\"", \""organizationalPerson\"", \""user\""],\n  \""cn\"": \""$principal_name\"",\n  #if( $is_service )\n  \""servicePrincipalName\"": \""$principal_name\"",\n  #end\n  \""userPrincipalName\"": \""$normalized_principal\"",\n  \""unicodePwd\"": \""$password\"",\n  \""accountExpires\"": \""0\"",\n  \""userAccountControl\"": \""66048\""\n}"",
  ""install_packages"": ""true"",
  ""realm"": ""EXAMPLE.COM"",
  ""ldap_url"": """",
  ""executable_search_paths"": ""/usr/bin, /usr/kerberos/bin, /usr/sbin, /usr/lib/mit/bin, /usr/lib/mit/sbin""
}
{code}

{code:title=kerberos-env after update}
{
  ""password_min_uppercase_letters"": ""1"",
  ""password_min_whitespace"": ""0"",
  ""password_min_punctuation"": ""1"",
  ""password_min_digits"": ""1"",
  ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
  ""kdc_create_attributes"": """",
  ""admin_server_host"": ""hist1:88"",
  ""password_min_lowercase_letters"": ""1"",
  ""container_dn"": """",
  ""password_length"": ""20"",
  ""case_insensitive_username_rules"": ""false"",
  ""manage_identities"": ""true"",
  ""service_check_principal_name"": ""${cluster_name}-${short_date}"",
  ""kdc_host"": ""host1:88"",
  ""ad_create_attributes_template"": ""\n{\n  \""objectClass\"": [\""top\"", \""person\"", \""organizationalPerson\"", \""user\""],\n  \""cn\"": \""$principal_name\"",\n  #if( $is_service )\n  \""servicePrincipalName\"": \""$principal_name\"",\n  #end\n  \""userPrincipalName\"": \""$normalized_principal\"",\n  \""unicodePwd\"": \""$password\"",\n  \""accountExpires\"": \""0\"",\n  \""userAccountControl\"": \""66048\""\n}"",
  ""install_packages"": ""true"",
  ""realm"": ""EXAMPLE.COM"",
  ""ldap_url"": """",
  ""executable_search_paths"": ""/usr/bin, /usr/kerberos/bin, /usr/sbin, /usr/lib/mit/bin, /usr/lib/mit/sbin""
}
{code}

{code:title=Javascript Error}
Uncaught TypeError: Cannot read property 'get' of undefined
App.MainServiceInfoConfigsController.Em.Controller.extend.prepareConfigObjects @ app.js:22525
App.MainServiceInfoConfigsController.Em.Controller.extend.parseConfigData @ app.js:22490
App.ConfigsLoader.Em.Mixin.create.loadCurrentVersionsSuccess @ app.js:61506
Em.Object.extend.send.opt.success @ app.js:154010
f.Callbacks.o @ vendor.js:125
f.Callbacks.p.fireWith @ vendor.js:125
w @ vendor.js:127
f.support.ajax.f.ajaxTransport.c.send.d @ vendor.js:127
app.js:55160 App.componentConfigMapper execution time: 1.048ms
{code}

*Steps to reproduce*
# Create cluster (Zookeeper-only is fine)
# Enable Kerberos (any KDC, MIT KDC is fine)
# Browse to Kerberos service configuration page
# Change a value (maybe add or remove the port for the KDC server value)
# Save the configuration
# After view refreshes, the waiting icon appears and does not go away

*Workaround*
Manually add the {{kerberos-env/kdc_type}} property back to the current kerberos-env configuration.  The value must be either ""mit-kdc"" or ""active-directory"" and must be the correct one for the configuration.  Once this is done, Ambari should be restarted so that any cached configuration data is refreshed. 

This can also be fixed using {{/var/lib/ambari-server/resources/scripts/configs.sh}}."	AMBARI	Resolved	2	1	8661	regression
13145465	Allow Ambari Server to Setup SSO for the entire stack using the CLI	"Today enabling SSO requires visiting each component that supports SSO and adding configuration entries to each. This task is to enable a single entry point via the Ambari CLI to configure SSO for each service that supports it.

Changes to the ambari-server setup-sso CLI are needed allow configuration of all SSO-capable services using that single CLI. This facility can be used to enable, disable, and reconfigure SSO integration.

*Proposed implementation:*

Services are to declare they support SSO integration by indicating in the service's \{{metainfo.xml}} file as follows: 
{code}
<sso>
 <supported>true</supported>
 <enabledConfiguration>config-type/sso.enabled.property</enabledConfiguration>
</sso>
{code}

The stack/service advisor will be used to retrieve the recommended configurations needed by a service to set up SSO integration. A special stack advisor action will be added to ensure only SSO-related recommendations are returned upon request. The new action name is ""{{recommend-configurations-for-sso}}"". Ambari (or common) SSO information will be provided to the stack advisor via the input data under the label ""sso-configuration"". This information may be used by the stack advisor when creating recommendations.

Ambari will store details on which services should be enabled for SSO so it _knows_ how to behave when SSO integration is enabled and new services are added. This data will be stored within Ambari's configuration data under the category of {{sso-configuration}}. The list of services to have SSO integration turned on will be stored in the property named {{ambari.sso.enabled_services}}. The value will be a comma-delimited list of service names, or ""{{*}}"" to indicate all services that support SSO integration.

The Ambari REST API entry point for installed services ({{/api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME}}) is to be enhance by adding the following properties:
* *\{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not
* *\{{sso_integration_enabled}}* - Indicates whether the service is configured for SSO integration or not
* *\{{sso_integration_desired}}* - Indicates whether the service is chosen for SSO integration or not

The Ambari REST API entry point for stack services ({{/api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME}}) is to be enhance by adding the following properties:
* *\{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not

When producing a list of installed services that support SSO integration in the CLI, the Ambari REST API is to be used to query for the relevant service names. Once the user selects the set of services to enable SSO for (or all), the Ambari REST API is to be used to set the value of the Ambari configuration \{{sso-configuration/ambari.sso.enabled_services}}. Upon setting this, logic is triggered in the backend to query the stack advisor for SSO-related configuration recommendations which will be automatically applied. This will potentially yield new configuration versions and require services to be manually restarted.

When adding new services, the \{{sso-configuration/ambari.sso.enabled_services}} value is to be checked to see if the new service is on the list of services to have SSO integration enabled. If so, and the service has a SSO descriptor, its configuration will be updated as needed before the service is started.

In a Blueprint scenario, it is expected that the user first sets up Ambari for SSO integration using the {{ambari-server setup-sso}} CLI. The Blueprint is expected to set the relevant properties needed to enable SSO integration per service. However, if SSO details were set up, the stack advisor may recommend relevant changes which may be applied depending on the Blueprint settings."	AMBARI	Resolved	3	15	8661	SSO, sso
12826445	Kerberos Identity data is empty when Cluster.security_type != KERBEROS	"Kerberos Identity data is empty when Cluster.security_type does not equal ""KERBEROS"".  

Technically this seems to make sense since there shouldn't be any relevant Kerberos identities if the cluster isn't configured for Kerberos; however a _chicken-and-the-egg_ condition is encountered when manually enabled Kerberos and a listing of the needed Kerberos identities is needed before Kerberos is to be fully enabled. 

*Solution*
Allow the keberos_identities API end points to generate data even if Cluster.security_type is not equal to ""KERBEROS""."	AMBARI	Resolved	3	1	8661	kerberos
12896371	Kerberos: Allow multiple KDC hosts to be set while enabling Kerberos	"Because multiple KDCs may exist for an installation (failover, high availability, etc...), Ambari should allow a user to specify multiple KDC hosts to be set while enabling Kerberos and updating the Kerberos service's configuration.

This should be done by allowing {{kerberos-env/kdc_host}} to accept a (comma-)delimited list of hosts and then parsing that list properly when building the krb5.conf file where each {{kdc_host}} item generates an entry in the relevant realm block.  For example:

{noformat:title=kerberos-env}
{
  ...
 ""kdc_hosts"" : ""kdc1.example.com, kdc2.example.com""
  ...
}
{noformat}

{noformat:title=krb5.conf}
[realms]
  EXAMPLE.COM = {
    ...
    kdc = kdc1.example.com
    kdc = kdc2.example.com
    ...
  }
{noformat}"	AMBARI	Resolved	4	1	8661	kerberos
13173394	Multiple alerts after HDFS service only regenerate keytabs, as keytabs are out of sync	"*STR*
 # Install Ambari 2.7.0
 # Deploy a cluster with some services
 # Execute to HDFS / Actions / Regenerate Keytabs
 # Restart all required components

*Result*

Multiple alerts have been shown saying they receive 403 when trying to connect to different web UIs

*Expected result*

No alerts; everything should work as expected"	AMBARI	Resolved	2	1	8661	pull-request-available
12828373	Kerberos: cleanup Manual kerb req text	"Update checkbox ""reqs"" on Page 1 of wizard with choice of Manual Kerb:

* Cluster hosts have network access to the KDC
* Kerberos client utilities (such as kinit) have been installed on every cluster host
* The Java Cryptography Extensions (JCE) have been setup on the Ambari Server host and all hosts in the cluster
* The Service and Ambari Principals will be manually created in the KDC before completing this wizard
* The keytabs for the Service and Ambari Principals will be manually created and distributed to cluster hosts before completing this wizard
"	AMBARI	Resolved	2	1	8661	kerberos, kerberos-wizard
13021846	NPE when authenticating via a Centrify LDAP proxy	"When authenticating using LDAP where the LDAP server is a Centrify LDAP proxy, a {{NullPointerException}} is being thrown due to unexpected missing LDAP user object attributes. 

{noformat}
10 Nov 2016 08:23:38,789 ERROR [ambari-client-thread-25] AmbariLdapBindAuthenticator:95 - Caught exception
java.lang.NullPointerException
	at org.apache.ambari.server.security.authorization.AmbariLdapBindAuthenticator.authenticate(AmbariLdapBindAuthenticator.java:83)
	at org.springframework.security.ldap.authentication.LdapAuthenticationProvider.doAuthentication(LdapAuthenticationProvider.java:178)
	at org.springframework.security.ldap.authentication.AbstractLdapAuthenticationProvider.authenticate(AbstractLdapAuthenticationProvider.java:61)
	at org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProvider.authenticate(AmbariLdapAuthenticationProvider.java:73)
	at org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:156)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:168)
	at org.apache.ambari.server.security.authentication.AmbariAuthenticationFilter.doFilter(AmbariAuthenticationFilter.java:88)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

*Cause*
The cause for this {{NPE}} is related to missing data from the attribute search request made to a Centrify LDAP proxy after a bind has occurred.  Since the query filter at this point is ""{(objectClass=*}}"", the Centrify LDAP proxy does not have enough data to determine what information to return to the caller.  However, the filter was something like ""{(objectClass=posixAccount}}"", it will be able to build a set of attributes to return to the caller since it would determine that the call wants data for a specific _profile_.

This can be seen by the following {{tcpdump}} entry:
{code}
LDAPMessage searchRequest(2) ""uid=user1,ou=people,ou=dev,dc=apache,dc=org"" baseObject
    messageID: 2
    protocolOp: searchRequest (3)
        searchRequest
            baseObject: uid=user1,ou=people,ou=dev,dc=apache,dc=org
            scope: baseObject (0)
            derefAliases: derefAlways (3)
            sizeLimit: 0
            timeLimit: 0
            typesOnly: False
            Filter: (objectClass=*)
            attributes: 0 items
    [Response In: 2]
    controls: 1 item
{code}

Note the filter line above: *{{Filter: (objectClass=*)}}*

From the Centrify LDAP proxy logs, the following lines can be seen showing that no mapping is avaialbe:

{noformat}
Nov  8 12:13:45 host1 slapd: cdc search start with filterstr: (objectClass=*)
Nov  8 12:13:45 host1 slapd: cdc search: objectType =  ( is mapped to NONE)
Nov  8 12:13:45 host1 slapd: cdc search after translation filter = (objectClass=*)
{noformat}

This search filter is hardcoded in {{com.sun.jndi.ldap.LdapCtx}} as seen in http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/com/sun/jndi/ldap/LdapCtx.java#1308.

This code is invoked from the Spring LDAP library after attempting to authenticate using the Spring org.springframework.security.ldap.authentication.BindAuthenticator class. 

*Solution*
To solve this, Ambari should avoid using {{org.springframework.security.ldap.authentication.BindAuthenticator}} to authenticate users via LDAP so that more control can be had over when and how user attributes are obtained. 
"	AMBARI	Resolved	3	1	8661	ldap
12911874	Add (descriptive) label to permission resource and database schema	"Add (descriptive) label to permission resource and database schema to avoid having to hardcode a descriptive name for a permission. For example:


||Permission Name||Permission Label||
|VIEW.USER|View User|
|CLUSTER.USER|Cluster User|
|SERVICE.OPERATOR|Service Operator|
|SERVICE.ADMINISTRATOR|Service Administrator|
|CLUSTER.OPERATOR|Cluster Operator|
|CLUSTER.ADMINISTRATOR|Cluster Administrator|

This descriptive label can be used in user interfaces so that a descriptive (or friendly) name does not have to be hardcoded. 

{noformat:title=API Call Example}
GET api/v1/permissions/1

200 OK
{
  ""href"" : ""http://your.ambari.server/api/v1/permissions/1"",
  ""PermissionInfo"" : {
    ""permission_id"" : 1,
    ""permission_name"" : ""AMBARI.ADMIN"",
    ""permission_label"" : ""Administrator"",
    ""resource_name"" : ""AMBARI""
  }
}
{noformat}"	AMBARI	Resolved	3	3	8661	rbac
12967161	Add conditional constraints for Kerberos identities to control when they are created	"Add conditional constraints for Kerberos identities to control when they are created. For example if Kerberos Identity should only be created (and distributed) for a component when some other component or service is installed. 

An example of this would be
{code}
{
  ""name"": ""/HIVE/HIVE_SERVER/hive_server_hive"",
  ""principal"": {
    ""configuration"": ""hive-interactive-site/hive.llap.daemon.service.principal""
  },
  ""keytab"": {
    ""configuration"": ""hive-interactive-site/hive.llap.daemon.keytab.file""
  },
  ""when"" : {
      ""contains"" : [""services"", ""HIVE""]
  }
}
{code}

Note the ""{{when}}"" clause. This indicates that this identity should only be processed when the set of services contains ""HIVE"".  An alternative to this would be to test the set of components for a certain component. 
"	AMBARI	Resolved	2	3	8661	kerberos, kerberos_descriptor
13136968	Deactivate unsupported stack definitions in Ambari	"Deactivate unsupported stack definitions in Ambari.  This includes HDP versions before 2.6.

{noformat:title=Active Stacks}
./3.0/metainfo.xml:    <active>true</active>
./2.6/metainfo.xml:    <active>true</active>
./2.5/metainfo.xml:    <active>true</active>
./2.4/metainfo.xml:    <active>true</active>
./2.3/metainfo.xml:    <active>true</active>
{noformat}

There is no enforcement of minimum stack level during the upgrade process. The user is expected to have performed the necessary actions to meet the _documented_ minimum stack level. 

"	AMBARI	Resolved	3	3	8661	pull-request-available
13149730	Remove unnecessary properties from the Ranger SSO configuration updates via the stack advisor	"Remove unnecessary properties from the Ranger SSO configuration updates via the stack advisor

* {{ranger.sso.cookiename}}
* {{ranger.sso.query.param.originalurl}}"	AMBARI	Resolved	3	3	8661	pull-request-available
13048699	Kerberos identity reference not working for ranger-audit property in hbase	"From stack 2.5 onwards {{xasecure.audit.jaas.Client.option.principal/ranger-hbase-audit}} needs to have principal value available under {{hbase.master.kerberos.principal/hbase-site}}

To achieve that added below block of code under hbase [kerberos.json|https://github.com/apache/ambari/blob/branch-2.5/ambari-server/src/main/resources/stacks/HDP/2.5/services/HBASE/kerberos.json]
{noformat}
{
              ""name"": ""/HBASE/HBASE_MASTER/hbase_master_hbase"",
              ""principal"": {
                ""configuration"": ""ranger-hbase-audit/xasecure.audit.jaas.Client.option.principal""
              },
              ""keytab"": {
                ""configuration"": ""ranger-hbase-audit/xasecure.audit.jaas.Client.option.keyTab""
              }
}
{noformat}

But on test cluster, {{xasecure.audit.jaas.Client.option.principal/ranger-hbase-audit}} property is not showing the expected value. It is showing the principal/keytab values of {{ams_hbase_master_hbase}} identity. 

Because of wrong reference of principal audit to solr is not working in kerberos environment, as security.json have below entry instead of {{hbase@EXAMPLE.COM}}
{noformat}
""amshbase@EXAMPLE.COM"":[
        ""ranger_audit_user"",
        ""dev""]
{noformat}
"	AMBARI	Resolved	3	1	8661	kerberos, kerberos_descriptor
12770462	Implement unkerberize for kerberized cluster	"Implement the ability to disable Kerberos from a cluster that was previously configured for Kerberos.

This entails reverting configuration properties set when Kerberos was enabled to default values found in the stack. 

Principals will not be destroyed in the KDC and ketyab files will not be removed from hosts."	AMBARI	Resolved	3	3	8661	kerberos, kerberos_descriptor
12923615	SERVICE.MANAGE_CONFIG_GROUPS missing from CLUSTER.ADMINISTRATOR and AMBARI.ADMINISTRATOR roles in MySQL create script	"SERVICE.MANAGE_CONFIG_GROUPS missing from CLUSTER.ADMINISTRATOR and AMBARI.ADMINISTRATOR roles in MySQL create script.
"	AMBARI	Resolved	1	1	8661	rbac
13134848	Long cannot be cast to String error when changing a user's password	"Long cannot be cast to String error when changing a user's password:
{noformat}
30 Jan 2018 18:21:11,308 ERROR [ambari-client-thread-38] AbstractResourceProvider:353 - Caught AmbariException when modifying a resource
org.apache.ambari.server.AmbariException: java.lang.Long cannot be cast to java.lang.String
at org.apache.ambari.server.controller.internal.UserResourceProvider.addOrUpdateLocalAuthenticationSource(UserResourceProvider.java:559)
at org.apache.ambari.server.controller.internal.UserResourceProvider.updateUsers(UserResourceProvider.java:486)
at org.apache.ambari.server.controller.internal.UserResourceProvider.access$200(UserResourceProvider.java:69)
at org.apache.ambari.server.controller.internal.UserResourceProvider$3.invoke(UserResourceProvider.java:264)
at org.apache.ambari.server.controller.internal.UserResourceProvider$3.invoke(UserResourceProvider.java:261)
at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:465)
at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:346)
at org.apache.ambari.server.controller.internal.UserResourceProvider.updateResources(UserResourceProvider.java:261)
at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:317)
...
{noformat}

*Steps to reproduce*
 # Create a {{LOCAL}} user account (using either the Ambari UI or REST API)
{noformat}
POST /api/v1/users
{noformat}
{code:title=Payload}
{ 
  ""Users"" : {
    ""user_name"" : ""myuser"",
    ""password"" : ""hadoop""
  }
}
{code}
 # Change the user's password (using either the Ambari UI or REST API via the users entry point)
{noformat}
PUT /api/v1/users/myuser
{noformat}
{code:title=Payload}
{ 
  ""Users"" : {
    ""old_password"" : ""hadoop""
    ""password"" : ""hadoop1234""
  }
}
{code}
{code:title=Response}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: java.lang.Long cannot be cast to java.lang.String""
}
{code}

*Cause*
When building the internal request to set the user's password via the UserAuthenticationSource resource provider, the authentication source key is set as a {{Long}}. The UserAuthenticationSource resource provider expects this value to be a {{String}}.

*Solution*
The User resource provider should set the {{AuthenticationSourceInfo/source_id}} as a {{String}} value.

"	AMBARI	Resolved	2	1	8661	pull-request-available
