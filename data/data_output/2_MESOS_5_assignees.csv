id	title	description	project_name	status_name	priority_id	type_id	assignee_id	labels
12985035	AuthenticationTest.UnauthenticatedSlave fails with clang++3.8	"With {{clang++-3.8}}, {{make check}} fails with the following message:

{noformat}
[ RUN      ] AuthenticationTest.UnauthenticatedSlave
*** Aborted at 1467208613 (unix time) try ""date -d @1467208613"" if you are using GNU date ***
PC: @        0x10b7f5a8b std::__1::__tree<>::__assign_multi<>()
*** SIGSEGV (@0x0) received by PID 40053 (TID 0x7fff73aaf000) stack trace: ***
    @     0x7fff8af4252a _sigtramp
    @        0x110216a00 (unknown)
    @        0x10b7f5881 mesos::internal::logging::Flags::operator=()
    @        0x10b7f3076 mesos::internal::slave::Flags::operator=()
    @        0x10b7f1cbf mesos::internal::tests::cluster::Slave::start()
    @        0x10bf1a2d1 mesos::internal::tests::MesosTest::StartSlave()
    @        0x10b7511b9 mesos::internal::tests::AuthenticationTest_UnauthenticatedSlave_Test::TestBody()
    @        0x10c703caa testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x10c703b0a testing::Test::Run()
    @        0x10c704b02 testing::TestInfo::Run()
    @        0x10c7053c3 testing::TestCase::Run()
    @        0x10c70cefb testing::internal::UnitTestImpl::RunAllTests()
    @        0x10c70ca43 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x10c70c95e testing::UnitTest::Run()
    @        0x10bbe44f3 main
    @     0x7fff9071a5ad start
make[3]: *** [check-local] Segmentation fault: 11
make[2]: *** [check-am] Error 2
make[1]: *** [check] Error 2
make: *** [check-recursive] Error 1
{noformat}"	MESOS	Resolved	3	1	2531	mesosphere
13084577	Update master to handle updates to agent total resources	"With MESOS-7755 we update the allocator interface to support updating the total resources on an agent. These allocator invocations are driven by the master when it receives an update the an agent's total resources.

We could transport the updates from agents to the master either as update to {{UpdateSlaveMessage}}, e.g., by adding a {{repeated Resource total}} field; in order to distinguish updates to {{oversubscribed}} to updates to {{total}} we would need to introduce an additional tag field (an empty list of {{Resource}} has the same representation as an absent list of {{Resource}}). Alternatively we could introduce a new message transporting just the update to {{total}}; it should be possible to reuse such a message for external resource providers which we will likely add at a later point."	MESOS	Resolved	3	3	2531	mesosphere, storage
13258620	Nvml isolator cannot be disabled which makes it impossible to exclude non-free code	"We currently do not allow disabling of the link against {{libnvml}} which is probably not under a free license. This makes it hard to include Mesos at all in distributions requiring only free licenses, see e.g., https://bugzilla.redhat.com/show_bug.cgi?id=1749383.

We should add a configuration time flag to disable this feature completely until we can provide a free replacement."	MESOS	Resolved	3	1	2531	autotools, cmake
12902288	FetcherCacheTest.LocalUncachedExtract is flaky	"From ASF CI:
https://builds.apache.org/job/Mesos/866/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/console

{code}
[ RUN      ] FetcherCacheTest.LocalUncachedExtract
Using temporary directory '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA'
I0925 19:15:39.541198 27410 leveldb.cpp:176] Opened db in 3.43934ms
I0925 19:15:39.542362 27410 leveldb.cpp:183] Compacted db in 1.136184ms
I0925 19:15:39.542428 27410 leveldb.cpp:198] Created db iterator in 35866ns
I0925 19:15:39.542448 27410 leveldb.cpp:204] Seeked to beginning of db in 8807ns
I0925 19:15:39.542459 27410 leveldb.cpp:273] Iterated through 0 keys in the db in 6325ns
I0925 19:15:39.542505 27410 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0925 19:15:39.543143 27438 recover.cpp:449] Starting replica recovery
I0925 19:15:39.543393 27438 recover.cpp:475] Replica is in EMPTY status
I0925 19:15:39.544373 27436 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0925 19:15:39.544791 27433 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0925 19:15:39.545284 27433 recover.cpp:566] Updating replica status to STARTING
I0925 19:15:39.546155 27436 master.cpp:376] Master c8bf1c95-50f4-4832-a570-c560f0b466ae (f57fd4291168) started on 172.17.1.195:41781
I0925 19:15:39.546257 27433 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 747249ns
I0925 19:15:39.546288 27433 replica.cpp:323] Persisted replica status to STARTING
I0925 19:15:39.546483 27434 recover.cpp:475] Replica is in STARTING status
I0925 19:15:39.546187 27436 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/master"" --zk_session_timeout=""10secs""
I0925 19:15:39.546567 27436 master.cpp:423] Master only allowing authenticated frameworks to register
I0925 19:15:39.546617 27436 master.cpp:428] Master only allowing authenticated slaves to register
I0925 19:15:39.546632 27436 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials'
I0925 19:15:39.546931 27436 master.cpp:467] Using default 'crammd5' authenticator
I0925 19:15:39.547044 27436 master.cpp:504] Authorization enabled
I0925 19:15:39.547276 27441 whitelist_watcher.cpp:79] No whitelist given
I0925 19:15:39.547320 27434 hierarchical.hpp:468] Initialized hierarchical allocator process
I0925 19:15:39.547471 27438 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0925 19:15:39.548318 27443 recover.cpp:195] Received a recover response from a replica in STARTING status
I0925 19:15:39.549067 27435 recover.cpp:566] Updating replica status to VOTING
I0925 19:15:39.549115 27440 master.cpp:1603] The newly elected leader is master@172.17.1.195:41781 with id c8bf1c95-50f4-4832-a570-c560f0b466ae
I0925 19:15:39.549162 27440 master.cpp:1616] Elected as the leading master!
I0925 19:15:39.549190 27440 master.cpp:1376] Recovering from registrar
I0925 19:15:39.549342 27434 registrar.cpp:309] Recovering registrar
I0925 19:15:39.549666 27430 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418187ns
I0925 19:15:39.549753 27430 replica.cpp:323] Persisted replica status to VOTING
I0925 19:15:39.550089 27442 recover.cpp:580] Successfully joined the Paxos group
I0925 19:15:39.550320 27442 recover.cpp:464] Recover process terminated
I0925 19:15:39.550904 27430 log.cpp:661] Attempting to start the writer
I0925 19:15:39.551955 27434 replica.cpp:477] Replica received implicit promise request with proposal 1
I0925 19:15:39.552351 27434 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 380746ns
I0925 19:15:39.552372 27434 replica.cpp:345] Persisted promised to 1
I0925 19:15:39.552896 27436 coordinator.cpp:231] Coordinator attemping to fill missing position
I0925 19:15:39.554003 27432 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0925 19:15:39.554534 27432 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 510572ns
I0925 19:15:39.554558 27432 replica.cpp:679] Persisted action at 0
I0925 19:15:39.555516 27443 replica.cpp:511] Replica received write request for position 0
I0925 19:15:39.555595 27443 leveldb.cpp:438] Reading position from leveldb took 65355ns
I0925 19:15:39.556177 27443 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 542757ns
I0925 19:15:39.556200 27443 replica.cpp:679] Persisted action at 0
I0925 19:15:39.556813 27429 replica.cpp:658] Replica received learned notice for position 0
I0925 19:15:39.557251 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 422272ns
I0925 19:15:39.557281 27429 replica.cpp:679] Persisted action at 0
I0925 19:15:39.557315 27429 replica.cpp:664] Replica learned NOP action at position 0
I0925 19:15:39.558061 27442 log.cpp:677] Writer started with ending position 0
I0925 19:15:39.559294 27434 leveldb.cpp:438] Reading position from leveldb took 56536ns
I0925 19:15:39.560333 27432 registrar.cpp:342] Successfully fetched the registry (0B) in 10.936064ms
I0925 19:15:39.560469 27432 registrar.cpp:441] Applied 1 operations in 41340ns; attempting to update the 'registry'
I0925 19:15:39.561244 27441 log.cpp:685] Attempting to append 176 bytes to the log
I0925 19:15:39.561378 27436 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0925 19:15:39.562126 27439 replica.cpp:511] Replica received write request for position 1
I0925 19:15:39.562515 27439 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 364968ns
I0925 19:15:39.562539 27439 replica.cpp:679] Persisted action at 1
I0925 19:15:39.563160 27438 replica.cpp:658] Replica received learned notice for position 1
I0925 19:15:39.563699 27438 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 455933ns
I0925 19:15:39.563730 27438 replica.cpp:679] Persisted action at 1
I0925 19:15:39.563753 27438 replica.cpp:664] Replica learned APPEND action at position 1
I0925 19:15:39.564749 27434 registrar.cpp:486] Successfully updated the 'registry' in 4.214016ms
I0925 19:15:39.564893 27434 registrar.cpp:372] Successfully recovered registrar
I0925 19:15:39.564950 27442 log.cpp:704] Attempting to truncate the log to 1
I0925 19:15:39.565039 27429 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0925 19:15:39.565172 27430 master.cpp:1413] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register
I0925 19:15:39.565946 27429 replica.cpp:511] Replica received write request for position 2
I0925 19:15:39.566349 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 375473ns
I0925 19:15:39.566371 27429 replica.cpp:679] Persisted action at 2
I0925 19:15:39.566994 27431 replica.cpp:658] Replica received learned notice for position 2
I0925 19:15:39.567440 27431 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 437095ns
I0925 19:15:39.567483 27431 leveldb.cpp:401] Deleting ~1 keys from leveldb took 31954ns
I0925 19:15:39.567498 27431 replica.cpp:679] Persisted action at 2
I0925 19:15:39.567514 27431 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0925 19:15:39.576660 27410 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0925 19:15:39.577055 27410 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges
I0925 19:15:39.583020 27443 slave.cpp:190] Slave started on 46)@172.17.1.195:41781
I0925 19:15:39.583062 27443 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4""
I0925 19:15:39.583472 27443 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential'
I0925 19:15:39.583752 27443 slave.cpp:321] Slave using credential for: test-principal
I0925 19:15:39.584249 27443 slave.cpp:354] Slave resources: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0925 19:15:39.584344 27443 slave.cpp:390] Slave hostname: f57fd4291168
I0925 19:15:39.584362 27443 slave.cpp:395] Slave checkpoint: true
I0925 19:15:39.585180 27428 state.cpp:54] Recovering state from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta'
I0925 19:15:39.585383 27440 status_update_manager.cpp:202] Recovering status update manager
I0925 19:15:39.585636 27435 containerizer.cpp:386] Recovering containerizer
I0925 19:15:39.586380 27438 slave.cpp:4110] Finished recovery
I0925 19:15:39.586845 27438 slave.cpp:4267] Querying resource estimator for oversubscribable resources
I0925 19:15:39.587059 27430 status_update_manager.cpp:176] Pausing sending status updates
I0925 19:15:39.587064 27438 slave.cpp:705] New master detected at master@172.17.1.195:41781
I0925 19:15:39.587139 27438 slave.cpp:768] Authenticating with master master@172.17.1.195:41781
I0925 19:15:39.587163 27438 slave.cpp:773] Using default CRAM-MD5 authenticatee
I0925 19:15:39.587321 27438 slave.cpp:741] Detecting new master
I0925 19:15:39.587357 27434 authenticatee.cpp:115] Creating new client SASL connection
I0925 19:15:39.587574 27438 slave.cpp:4281] Received oversubscribable resources  from the resource estimator
I0925 19:15:39.587739 27442 master.cpp:5138] Authenticating slave(46)@172.17.1.195:41781
I0925 19:15:39.587853 27441 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(139)@172.17.1.195:41781
I0925 19:15:39.588052 27439 authenticator.cpp:92] Creating new server SASL connection
I0925 19:15:39.588248 27431 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0925 19:15:39.588297 27431 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0925 19:15:39.588443 27437 authenticator.cpp:197] Received SASL authentication start
I0925 19:15:39.588506 27437 authenticator.cpp:319] Authentication requires more steps
I0925 19:15:39.588677 27443 authenticatee.cpp:252] Received SASL authentication step
I0925 19:15:39.588814 27436 authenticator.cpp:225] Received SASL authentication step
I0925 19:15:39.588855 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0925 19:15:39.588876 27436 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0925 19:15:39.588937 27436 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0925 19:15:39.588979 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0925 19:15:39.588997 27436 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.589011 27436 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.589036 27436 authenticator.cpp:311] Authentication success
I0925 19:15:39.589126 27443 authenticatee.cpp:292] Authentication success
I0925 19:15:39.589192 27437 master.cpp:5168] Successfully authenticated principal 'test-principal' at slave(46)@172.17.1.195:41781
I0925 19:15:39.589238 27433 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(139)@172.17.1.195:41781
I0925 19:15:39.589412 27440 slave.cpp:836] Successfully authenticated with master master@172.17.1.195:41781
I0925 19:15:39.589540 27440 slave.cpp:1230] Will retry registration in 13.562027ms if necessary
I0925 19:15:39.589745 27436 master.cpp:3862] Registering slave at slave(46)@172.17.1.195:41781 (f57fd4291168) with id c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.590121 27438 registrar.cpp:441] Applied 1 operations in 70627ns; attempting to update the 'registry'
I0925 19:15:39.590831 27430 log.cpp:685] Attempting to append 345 bytes to the log
I0925 19:15:39.590927 27439 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0925 19:15:39.591809 27430 replica.cpp:511] Replica received write request for position 3
I0925 19:15:39.592072 27430 leveldb.cpp:343] Persisting action (364 bytes) to leveldb took 221734ns
I0925 19:15:39.592099 27430 replica.cpp:679] Persisted action at 3
I0925 19:15:39.592643 27442 replica.cpp:658] Replica received learned notice for position 3
I0925 19:15:39.593215 27442 leveldb.cpp:343] Persisting action (366 bytes) to leveldb took 560946ns
I0925 19:15:39.593237 27442 replica.cpp:679] Persisted action at 3
I0925 19:15:39.593255 27442 replica.cpp:664] Replica learned APPEND action at position 3
I0925 19:15:39.594663 27433 registrar.cpp:486] Successfully updated the 'registry' in 4.472832ms
I0925 19:15:39.594874 27431 log.cpp:704] Attempting to truncate the log to 3
I0925 19:15:39.595407 27429 slave.cpp:3138] Received ping from slave-observer(45)@172.17.1.195:41781
I0925 19:15:39.595450 27433 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0925 19:15:39.596017 27442 replica.cpp:511] Replica received write request for position 4
I0925 19:15:39.596029 27429 hierarchical.hpp:675] Added slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0925 19:15:39.595952 27441 master.cpp:3930] Registered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0925 19:15:39.596240 27429 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:39.596263 27439 slave.cpp:880] Registered with master master@172.17.1.195:41781; given slave ID c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.596341 27439 fetcher.cpp:77] Clearing fetcher cache
I0925 19:15:39.596345 27429 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.596367 27429 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 299337ns
I0925 19:15:39.596524 27434 status_update_manager.cpp:183] Resuming sending status updates
I0925 19:15:39.596571 27442 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 575374ns
I0925 19:15:39.596662 27442 replica.cpp:679] Persisted action at 4
I0925 19:15:39.596984 27439 slave.cpp:903] Checkpointing SlaveInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/slave.info'
I0925 19:15:39.597522 27434 replica.cpp:658] Replica received learned notice for position 4
I0925 19:15:39.597553 27410 sched.cpp:164] Version: 0.26.0
I0925 19:15:39.597746 27439 slave.cpp:939] Forwarding total oversubscribed resources 
I0925 19:15:39.598021 27429 master.cpp:4272] Received update of slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with total oversubscribed resources 
I0925 19:15:39.598070 27434 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 531503ns
I0925 19:15:39.598162 27434 leveldb.cpp:401] Deleting ~2 keys from leveldb took 79081ns
I0925 19:15:39.598170 27428 sched.cpp:262] New master detected at master@172.17.1.195:41781
I0925 19:15:39.598206 27434 replica.cpp:679] Persisted action at 4
I0925 19:15:39.598238 27434 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0925 19:15:39.598276 27428 sched.cpp:318] Authenticating with master master@172.17.1.195:41781
I0925 19:15:39.598296 27428 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0925 19:15:39.598950 27430 hierarchical.hpp:735] Slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) updated with oversubscribed resources  (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0925 19:15:39.599242 27430 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:39.599282 27430 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.599341 27430 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 327742ns
I0925 19:15:39.599632 27437 authenticatee.cpp:115] Creating new client SASL connection
I0925 19:15:39.600005 27428 master.cpp:5138] Authenticating scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.600170 27435 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(140)@172.17.1.195:41781
I0925 19:15:39.600518 27433 authenticator.cpp:92] Creating new server SASL connection
I0925 19:15:39.600788 27436 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0925 19:15:39.600831 27436 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0925 19:15:39.600944 27433 authenticator.cpp:197] Received SASL authentication start
I0925 19:15:39.601019 27433 authenticator.cpp:319] Authentication requires more steps
I0925 19:15:39.601150 27436 authenticatee.cpp:252] Received SASL authentication step
I0925 19:15:39.601284 27436 authenticator.cpp:225] Received SASL authentication step
I0925 19:15:39.601326 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0925 19:15:39.601341 27436 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0925 19:15:39.601387 27436 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0925 19:15:39.601413 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0925 19:15:39.601421 27436 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.601428 27436 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.601439 27436 authenticator.cpp:311] Authentication success
I0925 19:15:39.601508 27433 authenticatee.cpp:292] Authentication success
I0925 19:15:39.601644 27433 master.cpp:5168] Successfully authenticated principal 'test-principal' at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.601671 27436 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(140)@172.17.1.195:41781
I0925 19:15:39.601842 27434 sched.cpp:407] Successfully authenticated with master master@172.17.1.195:41781
I0925 19:15:39.601869 27434 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.1.195:41781
I0925 19:15:39.601955 27434 sched.cpp:747] Will retry registration in 749.975107ms if necessary
I0925 19:15:39.602046 27443 master.cpp:2179] Received SUBSCRIBE call for framework 'default' at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
W0925 19:15:39.602128 27443 master.cpp:2186] Framework at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0925 19:15:39.602149 27443 master.cpp:1642] Authorizing framework principal '' to receive offers for role '*'
I0925 19:15:39.602375 27437 master.cpp:2250] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0925 19:15:39.602712 27429 hierarchical.hpp:515] Added framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.602859 27437 sched.cpp:641] Framework registered with c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.602905 27437 sched.cpp:655] Scheduler::registered took 30086ns
I0925 19:15:39.603204 27429 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.603234 27429 hierarchical.hpp:1221] Performed allocation for 1 slaves in 506104ns
I0925 19:15:39.603520 27438 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.603962 27431 sched.cpp:811] Scheduler::resourceOffers took 123790ns
I0925 19:15:39.605443 27432 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O0 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.605485 27432 master.cpp:2714] Authorizing framework principal '' to launch task 0 as user 'mesos'
I0925 19:15:39.606487 27432 master.hpp:176] Adding task 0 with resources cpus(*):1; mem(*):1 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168)
I0925 19:15:39.606586 27432 master.cpp:3248] Launching task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 with resources cpus(*):1; mem(*):1 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168)
I0925 19:15:39.606875 27440 slave.cpp:1270] Got assigned task 0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.607050 27439 hierarchical.hpp:1103] Recovered cpus(*):999; mem(*):999; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):1) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.607087 27440 slave.cpp:4773] Checkpointing FrameworkInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/framework.info'
I0925 19:15:39.607103 27439 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:39.607573 27440 slave.cpp:4784] Checkpointing framework pid 'scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781' to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/framework.pid'
I0925 19:15:39.608544 27440 slave.cpp:1386] Launching task 0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.615109 27440 slave.cpp:5209] Checkpointing ExecutorInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/executor.info'
I0925 19:15:39.616000 27440 slave.cpp:4852] Launching executor 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.616510 27441 containerizer.cpp:640] Starting container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' for executor '0' of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000'
I0925 19:15:39.616612 27440 slave.cpp:5232] Checkpointing TaskInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a/tasks/0/task.info'
I0925 19:15:39.617144 27440 slave.cpp:1604] Queuing task '0' for executor 0 of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.617277 27440 slave.cpp:658] Successfully attached file '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.619359 27437 launcher.cpp:132] Forked child with pid '30069' for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.619583 27437 containerizer.cpp:873] Checkpointing executor's forked pid 30069 to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a/pids/forked.pid'
I0925 19:15:39.622011 27441 fetcher.cpp:299] Starting to fetch URIs for container: 4bc31eb2-709b-4b09-a5a9-21a8387e355a, directory: /tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a
I0925 19:15:39.633872 27441 fetcher.cpp:756] Fetching URIs using command '/mesos/mesos-0.26.0/_build/src/mesos-fetcher'
E0925 19:15:39.724884 27430 fetcher.cpp:515] Failed to run mesos-fetcher: Failed to fetch all URIs for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' with exit status: 256
Failed to synchronize with slave (it's probably exited)
E0925 19:15:39.725486 27443 slave.cpp:3342] Container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' for executor '0' of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' failed to start: Failed to fetch all URIs for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' with exit status: 256
I0925 19:15:39.725620 27430 containerizer.cpp:1097] Destroying container '4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.725651 27430 containerizer.cpp:1126] Waiting for the isolators to complete for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.825744 27443 containerizer.cpp:1284] Executor for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' has exited
I0925 19:15:39.827075 27429 slave.cpp:3440] Executor '0' of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 exited with status 1
I0925 19:15:39.827324 27429 slave.cpp:2717] Handling status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 from @0.0.0.0:0
I0925 19:15:39.827514 27429 slave.cpp:5147] Terminating task 0
W0925 19:15:39.827745 27436 containerizer.cpp:988] Ignoring update for unknown container: 4bc31eb2-709b-4b09-a5a9-21a8387e355a
I0925 19:15:39.828073 27440 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.828168 27440 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.828661 27440 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.830041 27440 status_update_manager.cpp:376] Forwarding update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to the slave
I0925 19:15:39.830292 27434 slave.cpp:3016] Forwarding the update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to master@172.17.1.195:41781
I0925 19:15:39.830492 27434 slave.cpp:2940] Status update manager successfully handled status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.830641 27432 master.cpp:4415] Status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 from slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168)
I0925 19:15:39.830682 27432 master.cpp:4454] Forwarding status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.830842 27432 master.cpp:6081] Updating the latest state of task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to TASK_FAILED
I0925 19:15:39.831075 27431 sched.cpp:918] Scheduler::statusUpdate took 176815ns
I0925 19:15:39.831204 27439 hierarchical.hpp:1103] Recovered cpus(*):1; mem(*):1 (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.831357 27432 master.cpp:6149] Removing task 0 with resources cpus(*):1; mem(*):1 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168)
I0925 19:15:39.831491 27432 master.cpp:3606] Processing ACKNOWLEDGE call 6bb8651c-0668-4724-8fbd-76db8a91adb7 for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.831763 27437 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.831957 27437 status_update_manager.cpp:826] Checkpointing ACK for status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833057 27437 status_update_manager.cpp:530] Cleaning up status update stream for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833407 27432 slave.cpp:2319] Status update manager successfully handled status update acknowledgement (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833447 27432 slave.cpp:5188] Completing task 0
I0925 19:15:39.833470 27432 slave.cpp:3544] Cleaning up executor '0' of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833768 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' for gc 6.99999035100741days in the future
I0925 19:15:39.833933 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0' for gc 6.99999034949333days in the future
I0925 19:15:39.834005 27432 slave.cpp:3633] Cleaning up framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.834031 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' for gc 6.99999034847111days in the future
I0925 19:15:39.834106 27430 status_update_manager.cpp:284] Closing status update streams for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.834121 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0' for gc 6.99999034757926days in the future
I0925 19:15:39.834266 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' for gc 6.99999034594963days in the future
I0925 19:15:39.834360 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' for gc 6.99999034517333days in the future
I0925 19:15:40.549545 27428 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:40.549640 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 849712ns
I0925 19:15:40.550092 27442 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:40.550679 27442 sched.cpp:811] Scheduler::resourceOffers took 157498ns
I0925 19:15:40.551633 27432 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O1 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:40.552602 27432 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:40.552672 27432 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:41.551115 27428 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:41.551200 27428 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:41.551224 27428 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:41.551239 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 595589ns
I0925 19:15:42.552183 27433 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:42.552254 27433 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:42.552271 27433 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:42.552281 27433 hierarchical.hpp:1221] Performed allocation for 1 slaves in 496429ns
I0925 19:15:43.553062 27442 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:43.553134 27442 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:43.553151 27442 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:43.553163 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 482544ns
I0925 19:15:44.554844 27443 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:44.554930 27443 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:44.554954 27443 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:44.554970 27443 hierarchical.hpp:1221] Performed allocation for 1 slaves in 699469ns
I0925 19:15:45.556754 27442 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:45.556805 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 702577ns
I0925 19:15:45.557119 27437 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:45.557569 27435 sched.cpp:811] Scheduler::resourceOffers took 122887ns
I0925 19:15:45.558279 27433 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O2 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:45.559015 27441 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:45.559070 27441 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:46.558176 27439 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:46.558245 27439 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:46.558262 27439 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:46.558274 27439 hierarchical.hpp:1221] Performed allocation for 1 slaves in 509658ns
I0925 19:15:47.559289 27429 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:47.559360 27429 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:47.559376 27429 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:47.559386 27429 hierarchical.hpp:1221] Performed allocation for 1 slaves in 495131ns
I0925 19:15:48.560979 27442 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:48.561064 27442 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:48.561087 27442 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:48.561101 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 710782ns
I0925 19:15:49.562594 27431 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:49.562666 27431 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:49.562683 27431 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:49.562695 27431 hierarchical.hpp:1221] Performed allocation for 1 slaves in 525867ns
I0925 19:15:50.564564 27428 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:50.564620 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 621850ns
I0925 19:15:50.565004 27432 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:50.565457 27428 sched.cpp:811] Scheduler::resourceOffers took 110220ns
I0925 19:15:50.566159 27437 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O3 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:50.566815 27428 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:50.566869 27428 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:51.565913 27433 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:51.565981 27433 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:51.565999 27433 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:51.566009 27433 hierarchical.hpp:1221] Performed allocation for 1 slaves in 504883ns
I0925 19:15:52.567260 27432 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:52.567333 27432 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:52.567350 27432 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:52.567361 27432 hierarchical.hpp:1221] Performed allocation for 1 slaves in 513500ns
I0925 19:15:53.568176 27438 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:53.568248 27438 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:53.568266 27438 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:53.568281 27438 hierarchical.hpp:1221] Performed allocation for 1 slaves in 522293ns
I0925 19:15:54.570142 27430 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.570226 27430 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:54.570250 27430 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:54.570264 27430 hierarchical.hpp:1221] Performed allocation for 1 slaves in 626798ns
I0925 19:15:54.588251 27442 slave.cpp:4267] Querying resource estimator for oversubscribable resources
I0925 19:15:54.588673 27443 slave.cpp:4281] Received oversubscribable resources  from the resource estimator
I0925 19:15:54.596678 27428 slave.cpp:3138] Received ping from slave-observer(45)@172.17.1.195:41781
../../src/tests/fetcher_cache_tests.cpp:681: Failure
Failed to wait 15secs for awaitFinished(task.get())
I0925 19:15:54.606274 27410 sched.cpp:1771] Asked to stop the driver
I0925 19:15:54.606623 27439 master.cpp:1119] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 disconnected
I0925 19:15:54.606679 27439 master.cpp:2475] Disconnecting framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.606855 27439 master.cpp:2499] Deactivating framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.607441 27439 master.cpp:1143] Giving framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 0ns to failover
I0925 19:15:54.607770 27433 hierarchical.hpp:599] Deactivated framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.609256 27432 master.cpp:4815] Framework failover timeout, removing framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.609297 27432 master.cpp:5571] Removing framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.609501 27433 slave.cpp:1980] Asked to shut down framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 by master@172.17.1.195:41781
W0925 19:15:54.609549 27433 slave.cpp:1995] Cannot shut down unknown framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.609881 27432 master.cpp:919] Master terminating
I0925 19:15:54.610255 27440 hierarchical.hpp:552] Removed framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.610627 27440 hierarchical.hpp:706] Removed slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:54.611197 27436 slave.cpp:3184] master@172.17.1.195:41781 exited
W0925 19:15:54.611233 27436 slave.cpp:3187] Master disconnected! Waiting for a new master to be elected
I0925 19:15:54.616207 27410 slave.cpp:585] Slave terminating
[  FAILED  ] FetcherCacheTest.LocalUncachedExtract (15091 ms)
{code}
"	MESOS	Resolved	3	1	2531	flaky-test, mesosphere
13004523	Resource leak in libevent_ssl_socket.cpp.	"Coverity detected the following resource leak.
IMO {code} if (fd == -1) {code} should be  {code} if (owned_fd == -1) {code}.


{code}
 // Duplicate the file descriptor because Libevent will take ownership
754  // and control the lifecycle separately.
755  //
756  // TODO(josephw): We can avoid duplicating the file descriptor in
757  // future versions of Libevent. In Libevent versions 2.1.2 and later,
758  // we may use `evbuffer_file_segment_new` and `evbuffer_add_file_segment`
759  // instead of `evbuffer_add_file`.
   	3. open_fn: Returning handle opened by dup.
   	4. var_assign: Assigning: owned_fd = handle returned from dup(fd).
760  int owned_fd = dup(fd);
   	CID 1372873: Argument cannot be negative (REVERSE_NEGATIVE) [select issue]
   	5. Condition fd == -1, taking true branch.
761  if (fd == -1) {
   	
CID 1372872 (#1 of 1): Resource leak (RESOURCE_LEAK)
6. leaked_handle: Handle variable owned_fd going out of scope leaks the handle.
762    return Failure(ErrnoError(""Failed to duplicate file descriptor""));
763  }
{code}

https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881747&defectInstanceId=28450468"	MESOS	Resolved	3	1	2531	coverity
13074495	Add resource provider IDs to the registry	"To support resource provider re-registration following a master fail-over, the IDs of registered resource providers need to be kept in the registry.
An operation to commit those IDs using the registrar needs to be added as well."	MESOS	Resolved	3	3	2531	mesosphere, storage
13156745	`leveldb::PosixEnv::DeleteFile()` can segfault.	"Patch https://reviews.apache.org/r/66310/ instantiate a {{LevelDBStorage}} for the registrar of the agent-side resource provider manager. This change seems to introduce flaky segfaults to various tests, including {{MasterAPITest.Subscribe}}, {{MasterTest.UnacknowledgedTerminalTask}}, {{ReconciliationTest.ImplicitTerminalTask}} and {{MasterAuthorizerTest/1.FilterStateEndpoint}}. The segfault usually happens after successfully attached the virtual path of a launched default executor:
{noformat}
I0502 21:47:31.329772  4511 paths.cpp:745] Creating sandbox '/tmp/ContentType_MasterAPITest_Subscribe_1_HzYh4s/slaves/82b400f9-82ed-4fdc-9f34-8c652dc502a8-S0/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/398c5e60-5141-43f0-9856-98c05d501b33' for user 'root'
I0502 21:47:31.330245  4511 slave.cpp:8964] Launching executor 'default' of framework 82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000 with resources [] in work directory '/tmp/ContentType_MasterAPITest_Subscribe_1_HzYh4s/slaves/82b400f9-82ed-4fdc-9f34-8c652dc502a8-S0/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/398c5e60-5141-43f0-9856-98c05d501b33'
I0502 21:47:31.330423  4511 slave.cpp:3044] Queued task '7e968920-a780-4265-9e4b-e56e091ff27e' for executor 'default' of framework 82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000
I0502 21:47:31.330549  4511 slave.cpp:3523] Launching container 398c5e60-5141-43f0-9856-98c05d501b33 for executor 'default' of framework 82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000
I0502 21:47:31.330747  4515 slave.cpp:1008] Successfully attached '/tmp/ContentType_MasterAPITest_Subscribe_1_HzYh4s/slaves/82b400f9-82ed-4fdc-9f34-8c652dc502a8-S0/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/398c5e60-5141-43f0-9856-98c05d501b33' to virtual path '/tmp/ContentType_MasterAPITest_Subscribe_1_HzYh4s/slaves/82b400f9-82ed-4fdc-9f34-8c652dc502a8-S0/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/latest'
I0502 21:47:31.330814  4515 slave.cpp:1008] Successfully attached '/tmp/ContentType_MasterAPITest_Subscribe_1_HzYh4s/slaves/82b400f9-82ed-4fdc-9f34-8c652dc502a8-S0/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/398c5e60-5141-43f0-9856-98c05d501b33' to virtual path '/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/latest'
I0502 21:47:31.331126  4511 slave.cpp:1008] Successfully attached '/tmp/ContentType_MasterAPITest_Subscribe_1_HzYh4s/slaves/82b400f9-82ed-4fdc-9f34-8c652dc502a8-S0/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/398c5e60-5141-43f0-9856-98c05d501b33' to virtual path '/tmp/ContentType_MasterAPITest_Subscribe_1_HzYh4s/slaves/82b400f9-82ed-4fdc-9f34-8c652dc502a8-S0/frameworks/82b400f9-82ed-4fdc-9f34-8c652dc502a8-0000/executors/default/runs/398c5e60-5141-43f0-9856-98c05d501b33'
*** Aborted at 1525297651 (unix time) try ""date -d @1525297651"" if you are using GNU date ***
I0502 21:47:31.331786  4515 executor.cpp:192] Version: 1.6.0
W0502 21:47:31.331804  4515 process.cpp:2824] Attempted to spawn already running process version@172.16.10.128:37498
I0502 21:47:31.332619  4515 executor.cpp:414] Connected with the agent
I0502 21:47:31.332749  4515 executor.cpp:311] Sending SUBSCRIBE call to http://172.16.10.128:37498/slave(945)/api/v1/executor
PC: @     0x7f9239ecd0ed __GI_getenv
*** SIGSEGV (@0x171) received by PID 13317 (TID 0x7f922de28700) from PID 369; stack trace: ***
    @     0x7f922cca2155 (unknown)
    @     0x7f922cca6e41 (unknown)
    @     0x7f922cc9a678 (unknown)
    @     0x7f923ae935e0 (unknown)
    @     0x7f9239ecd0ed __GI_getenv
    @     0x7f9239ec4d61 __dcigettext
    @     0x7f9239f1ba9e __GI___strerror_r
    @     0x7f9239f1b9df strerror
    @     0x7f923dde4e23 leveldb::(anonymous namespace)::IOError()
    @     0x7f923dde5832 leveldb::(anonymous namespace)::PosixEnv::DeleteFile()
    @     0x7f923dde1628 leveldb::BuildTable()
    @     0x7f923ddbd5b4 leveldb::DBImpl::WriteLevel0Table()
    @     0x7f923ddbf6c8 leveldb::DBImpl::CompactMemTable()
    @     0x7f923ddc0756 leveldb::DBImpl::BackgroundCompaction()
    @     0x7f923ddc1172 leveldb::DBImpl::BackgroundCall()
    @     0x7f923dde55cb leveldb::(anonymous namespace)::PosixEnv::BGThreadWrapper()
    @     0x7f923ae8be25 start_thread
    @     0x7f9239f8d34d __clone
{noformat}

leveldb maintains a global singleton of {{PosixEnv}}, whose {{DeleteFile()}} callback sometimes fails to unlink a {{ldb}} file, and it uses {{strerror()}} to report the error, which seems to call {{getenv}}, and this increase the likelihood of some setenv-getenv race described in https://issues.apache.org/jira/browse/MESOS-2407.

It is worth investigating why {{DeleteFile()}} fails."	MESOS	Resolved	3	1	2531	flaky-test, mesosphere, storage
13197999	Resource providers reported by master should reflect connected resource providers	"Currently, the master will remember any resource provider it saw ever and report it e.g., in {{GET_AGENTS}} responses, regardless of whether the resource provider is currently connected or not. This is not very intuitive.

The master should instead only report resource providers which are currently connected. Agents can still report even disconnected resource providers."	MESOS	Resolved	3	4	2531	mesosphere
13132332	CHECK failure in DRFSorter due to invalid framework id.	"A framework registering with a custom {{FrameworkID}} containing slashes such as {{/foo/bar}} will trigger a CHECK failure at https://github.com/apache/mesos/blob/177a2221496a2caa5ad25e71c9982ca3eed02fd4/src/master/allocator/sorter/drf/sorter.cpp#L167:
{noformat}
master.cpp:6618] Updating info for framework /foo/bar 
sorter.cpp:167] Check failed: clientPath == current->clientPath() (/foo/bar vs. foo/bar)
{noformat}
The sorter should be defensive with any {{FrameworkID}} containing slashes."	MESOS	Resolved	3	1	2531	allocator, mesosphere, techdebt
12929164	SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation	"Executing {{SlaveTest.LaunchTaskInfoWithContainerInfo}} from {{468b8ec}} under OS X 10.10.5 in isolation fails due to missing cleanup,
{code}
% ./bin/mesos-tests.sh --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
Source directory: /ABC/DEF/src/mesos
Build directory: /ABC/DEF/src/mesos/build
-------------------------------------------------------------
We cannot run any Docker tests because:
Docker tests not supported on non-Linux systems
-------------------------------------------------------------
/usr/bin/nc
/usr/bin/curl
Note: Google Test filter = SlaveTest.LaunchTaskInfoWithContainerInfo-HealthCheckTest.ROOT_DOCKER_DockerHealthyTask:HealthCheckTest.ROOT_DOCKER_DockerHealthStatusChange:HierarchicalAllocator_BENCHMARK_Test.DeclineOffers:HookTest.ROOT_DOCKER_VerifySlavePreLaunchDockerHook:SlaveTest.ROOT_RunTaskWithCommandInfoWithoutUser:SlaveTest.DISABLED_ROOT_RunTaskWithCommandInfoWithUser:DockerContainerizerTest.ROOT_DOCKER_Launch:DockerContainerizerTest.ROOT_DOCKER_Kill:DockerContainerizerTest.ROOT_DOCKER_Usage:DockerContainerizerTest.ROOT_DOCKER_Recover:DockerContainerizerTest.ROOT_DOCKER_SkipRecoverNonDocker:DockerContainerizerTest.ROOT_DOCKER_Logs:DockerContainerizerTest.ROOT_DOCKER_Default_CMD:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Override:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Args:DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer:DockerContainerizerTest.DISABLED_ROOT_DOCKER_SlaveRecoveryExecutorContainer:DockerContainerizerTest.ROOT_DOCKER_NC_PortMapping:DockerContainerizerTest.ROOT_DOCKER_LaunchSandboxWithColon:DockerContainerizerTest.ROOT_DOCKER_DestroyWhileFetching:DockerContainerizerTest.ROOT_DOCKER_DestroyWhilePulling:DockerContainerizerTest.ROOT_DOCKER_ExecutorCleanupWhenLaunchFailed:DockerContainerizerTest.ROOT_DOCKER_FetchFailure:DockerContainerizerTest.ROOT_DOCKER_DockerPullFailure:DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard:DockerTest.ROOT_DOCKER_interface:DockerTest.ROOT_DOCKER_parsing_version:DockerTest.ROOT_DOCKER_CheckCommandWithShell:DockerTest.ROOT_DOCKER_CheckPortResource:DockerTest.ROOT_DOCKER_CancelPull:DockerTest.ROOT_DOCKER_MountRelative:DockerTest.ROOT_DOCKER_MountAbsolute:CopyBackendTest.ROOT_CopyBackend:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/0:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/1:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/2:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/3:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/4:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/5:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/6:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/7:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/8:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/9:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/10:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/11:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/12:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/13:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/14:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/15:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/16:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/17:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/18:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/19:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/20:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/21:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/22:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/23:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/24:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/25:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/26:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/27:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/28:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/29:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/30:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/31:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/32:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/33:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/34:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/35:SlaveCount/Registrar_BENCHMARK_Test.Performance/0:SlaveCount/Registrar_BENCHMARK_Test.Performance/1:SlaveCount/Registrar_BENCHMARK_Test.Performance/2:SlaveCount/Registrar_BENCHMARK_Test.Performance/3
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from SlaveTest
[ RUN      ] SlaveTest.LaunchTaskInfoWithContainerInfo
[       OK ] SlaveTest.LaunchTaskInfoWithContainerInfo (79 ms)
[----------] 1 test from SlaveTest (79 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:569: Failure
Failed
Tests completed with child processes remaining:
-+- 54487 /ABC/DEF/src/mesos/build/src/.libs/mesos-tests --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
 \--- 54503 /bin/sh /ABC/DEF/src/mesos/build/src/mesos-containerizer launch --command={""shell"":true,""value"":""\/ABC\/DEF\/src\/mesos\/build\/src\/mesos-executor""} --commands={""commands"":[]} --directory=/tmp --help=false --pipe_read=10 --pipe_write=13 --user=test
[==========] 1 test from 1 test case ran. (87 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
{code}
"	MESOS	Resolved	3	1	2531	mesosphere, tech-debt
13247100	Retain agent draining start time in master	"The master should store in memory the last time that a {{DrainSlaveMessage}} was sent to the agent so that this time can be displayed in the web UI. This would help operators determine the expected time at which the agent should transition to DRAINED.

We should update the webui to use that time as a starting point and the {{DrainConfig}}'s {{max_grace_period}} to calculate the expected maximum time until the agent is drained."	MESOS	Resolved	3	3	2531	foundations
12954230	Expose per-role dominant share	"A client's dominant share is crucial measure for how likely it is to receive offers in the future. We should expose it in a dedicated allocator metric.

As currently the {{HierarchicalAllocatorProcess}} does work with generic {{Sorters}} which have no notion of DRF share we need to decide whether and where we would need to limit generality in order to expose the innards of the currently used {{DRFSorter}}.
"	MESOS	Resolved	3	1	2531	mesosphere
13090515	Propagate resource updates from local resource providers to master	"When a resource provider registers with a resource provider manager, the manager should sent a message to its subscribers informing them on the changed resources.

For the first iteration where we add agent-specific, local resource providers, the agent would be subscribed to the manager. It should be changed to handle such a resource update by informing the master about its changed resources. In order to support master failovers, we should make sure to similarly inform the master on agent reregistration."	MESOS	Resolved	3	4	2531	mesosphere, storage
13022175	Some tests use hardcoded port numbers.	"DockerContainerizerTest.ROOT_DOCKER_NoTransitionFromKillingToRunning and many HealthCheckTests use hardcoded port numbers. This can create false failures if these tests are run in parallel on the same machine.

It appears instead we should use random port numbers."	MESOS	Resolved	4	1	2531	mesosphere, parallel-tests
13135780	Pending offer operations on resource provider resources not properly accounted for in allocator	"The master currently does not accumulate the resources used by offer operations on master failover. While we create a datastructure to hold this information, we missed updating it.
{code}
hashmap<FrameworkID, Resources> usedByOperations;

if (provider.newOperations.isSome()) {
  foreachpair (const id::UUID& uuid,
               const Operation& operation,
               provider.newOperations.get()) {
    // Update to bookkeeping of operations.
    CHECK(!slave->operations.contains(uuid))
      << ""New operation "" << uuid.toString() << "" is already known"";

    Framework* framework = nullptr;
    if (operation.has_framework_id()) {
      framework = getFramework(operation.framework_id());
    }

    addOperation(framework, slave, new Operation(operation));
  }
}

allocator->addResourceProvider(
    slaveId,
    provider.newTotal.get(),
    usedByOperations);
{code}

Here {{usedByOperations}} is not updated.

This leads to problems when the operation becomes terminal and we try to recover the used resources which might not be known to the framework sorter inside the hierarchical allocator."	MESOS	Resolved	3	1	2531	mesosphere, operations, storage
13019718	MesosContainerizer/DefaultExecutorTest.KillTask/0 failing on ASF CI	"{noformat:title=}
[ RUN      ] MesosContainerizer/DefaultExecutorTest.KillTask/0
I1110 01:20:11.482097 29700 cluster.cpp:158] Creating default 'local' authorizer
I1110 01:20:11.485241 29700 leveldb.cpp:174] Opened db in 2.774513ms
I1110 01:20:11.486237 29700 leveldb.cpp:181] Compacted db in 953614ns
I1110 01:20:11.486299 29700 leveldb.cpp:196] Created db iterator in 24739ns
I1110 01:20:11.486325 29700 leveldb.cpp:202] Seeked to beginning of db in 2300ns
I1110 01:20:11.486344 29700 leveldb.cpp:271] Iterated through 0 keys in the db in 378ns
I1110 01:20:11.486399 29700 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1110 01:20:11.486933 29733 recover.cpp:451] Starting replica recovery
I1110 01:20:11.487289 29733 recover.cpp:477] Replica is in EMPTY status
I1110 01:20:11.488503 29721 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(7318)@172.17.0.3:52462
I1110 01:20:11.488855 29727 recover.cpp:197] Received a recover response from a replica in EMPTY status
I1110 01:20:11.489398 29729 recover.cpp:568] Updating replica status to STARTING
I1110 01:20:11.490223 29723 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 575135ns
I1110 01:20:11.490284 29732 master.cpp:380] Master d28fbae1-c3dc-45fa-8384-32ab9395a975 (3a31be8bf679) started on 172.17.0.3:52462
I1110 01:20:11.490317 29732 master.cpp:382] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/k50x7x/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.2.0/_inst/share/mesos/webui"" --work_dir=""/tmp/k50x7x/master"" --zk_session_timeout=""10secs""
I1110 01:20:11.490696 29732 master.cpp:432] Master only allowing authenticated frameworks to register
I1110 01:20:11.490712 29732 master.cpp:446] Master only allowing authenticated agents to register
I1110 01:20:11.490720 29732 master.cpp:459] Master only allowing authenticated HTTP frameworks to register
I1110 01:20:11.490730 29732 credentials.hpp:37] Loading credentials for authentication from '/tmp/k50x7x/credentials'
I1110 01:20:11.490281 29723 replica.cpp:320] Persisted replica status to STARTING
I1110 01:20:11.491210 29732 master.cpp:504] Using default 'crammd5' authenticator
I1110 01:20:11.491225 29720 recover.cpp:477] Replica is in STARTING status
I1110 01:20:11.491394 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I1110 01:20:11.491621 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I1110 01:20:11.491770 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I1110 01:20:11.491937 29732 master.cpp:584] Authorization enabled
I1110 01:20:11.492276 29725 whitelist_watcher.cpp:77] No whitelist given
I1110 01:20:11.492310 29723 hierarchical.cpp:149] Initialized hierarchical allocator process
I1110 01:20:11.492569 29721 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(7319)@172.17.0.3:52462
I1110 01:20:11.492830 29719 recover.cpp:197] Received a recover response from a replica in STARTING status
I1110 01:20:11.493371 29720 recover.cpp:568] Updating replica status to VOTING
I1110 01:20:11.494002 29721 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 367673ns
I1110 01:20:11.494032 29721 replica.cpp:320] Persisted replica status to VOTING
I1110 01:20:11.494218 29734 recover.cpp:582] Successfully joined the Paxos group
I1110 01:20:11.494469 29734 recover.cpp:466] Recover process terminated
I1110 01:20:11.495633 29733 master.cpp:2033] Elected as the leading master!
I1110 01:20:11.495685 29733 master.cpp:1560] Recovering from registrar
I1110 01:20:11.495880 29720 registrar.cpp:329] Recovering registrar
I1110 01:20:11.496842 29730 log.cpp:553] Attempting to start the writer
I1110 01:20:11.498610 29725 replica.cpp:493] Replica received implicit promise request from __req_res__(7320)@172.17.0.3:52462 with proposal 1
I1110 01:20:11.499179 29725 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 524192ns
I1110 01:20:11.499213 29725 replica.cpp:342] Persisted promised to 1
I1110 01:20:11.500258 29726 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1110 01:20:11.501874 29731 replica.cpp:388] Replica received explicit promise request from __req_res__(7321)@172.17.0.3:52462 for position 0 with proposal 2
I1110 01:20:11.502413 29731 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 484138ns
I1110 01:20:11.502457 29731 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.503885 29720 replica.cpp:537] Replica received write request for position 0 from __req_res__(7322)@172.17.0.3:52462
I1110 01:20:11.503985 29720 leveldb.cpp:436] Reading position from leveldb took 56800ns
I1110 01:20:11.504534 29720 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 467426ns
I1110 01:20:11.504566 29720 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.505470 29721 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1110 01:20:11.505988 29721 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 479078ns
I1110 01:20:11.506021 29721 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.506706 29732 log.cpp:569] Writer started with ending position 0
I1110 01:20:11.508041 29734 leveldb.cpp:436] Reading position from leveldb took 50010ns
I1110 01:20:11.509210 29733 registrar.cpp:362] Successfully fetched the registry (0B) in 13.068032ms
I1110 01:20:11.509356 29733 registrar.cpp:461] Applied 1 operations in 27124ns; attempting to update the registry
I1110 01:20:11.510251 29732 log.cpp:577] Attempting to append 168 bytes to the log
I1110 01:20:11.510457 29724 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1110 01:20:11.511355 29728 replica.cpp:537] Replica received write request for position 1 from __req_res__(7323)@172.17.0.3:52462
I1110 01:20:11.511828 29728 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 423890ns
I1110 01:20:11.511859 29728 replica.cpp:708] Persisted action APPEND at position 1
I1110 01:20:11.512572 29734 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1110 01:20:11.513051 29734 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 368122ns
I1110 01:20:11.513087 29734 replica.cpp:708] Persisted action APPEND at position 1
I1110 01:20:11.514302 29726 registrar.cpp:506] Successfully updated the registry in 4.862976ms
I1110 01:20:11.514503 29726 registrar.cpp:392] Successfully recovered registrar
I1110 01:20:11.514593 29728 log.cpp:596] Attempting to truncate the log to 1
I1110 01:20:11.514760 29730 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1110 01:20:11.515249 29723 master.cpp:1676] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I1110 01:20:11.515534 29722 hierarchical.cpp:176] Skipping recovery of hierarchical allocator: nothing to recover
I1110 01:20:11.516068 29722 replica.cpp:537] Replica received write request for position 2 from __req_res__(7324)@172.17.0.3:52462
I1110 01:20:11.516619 29722 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 497823ns
I1110 01:20:11.516652 29722 replica.cpp:708] Persisted action TRUNCATE at position 2
I1110 01:20:11.517526 29734 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1110 01:20:11.518040 29734 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 384129ns
I1110 01:20:11.518111 29734 leveldb.cpp:399] Deleting ~1 keys from leveldb took 39398ns
I1110 01:20:11.518138 29734 replica.cpp:708] Persisted action TRUNCATE at position 2
I1110 01:20:11.525027 29700 containerizer.cpp:201] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W1110 01:20:11.525806 29700 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W1110 01:20:11.526018 29700 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I1110 01:20:11.527331 29700 cluster.cpp:435] Creating default 'local' authorizer
I1110 01:20:11.528741 29725 slave.cpp:208] Mesos agent started on (571)@172.17.0.3:52462
I1110 01:20:11.528789 29725 slave.cpp:209] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.2.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD""
I1110 01:20:11.529228 29725 credentials.hpp:86] Loading credential for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential'
I1110 01:20:11.529305 29700 scheduler.cpp:176] Version: 1.2.0
I1110 01:20:11.529436 29725 slave.cpp:346] Agent using credential for: test-principal
I1110 01:20:11.529464 29725 credentials.hpp:37] Loading credentials for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials'
I1110 01:20:11.529747 29725 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I1110 01:20:11.529855 29729 scheduler.cpp:469] New master detected at master@172.17.0.3:52462
I1110 01:20:11.529884 29729 scheduler.cpp:478] Waiting for 0ns before initiating a re-(connection) attempt with the master
I1110 01:20:11.531039 29725 slave.cpp:533] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1110 01:20:11.531113 29725 slave.cpp:541] Agent attributes: [  ]
I1110 01:20:11.531126 29725 slave.cpp:546] Agent hostname: 3a31be8bf679
I1110 01:20:11.532897 29723 state.cpp:57] Recovering state from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta'
I1110 01:20:11.533222 29727 status_update_manager.cpp:203] Recovering status update manager
I1110 01:20:11.533269 29721 scheduler.cpp:353] Connected with the master at http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.533627 29734 containerizer.cpp:557] Recovering containerizer
I1110 01:20:11.534519 29725 scheduler.cpp:235] Sending SUBSCRIBE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.535482 29732 provisioner.cpp:253] Provisioner recovery complete
I1110 01:20:11.535652 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.535815 29724 slave.cpp:5411] Finished recovery
I1110 01:20:11.536440 29724 slave.cpp:5585] Querying resource estimator for oversubscribable resources
I1110 01:20:11.536898 29721 slave.cpp:915] New master detected at master@172.17.0.3:52462
I1110 01:20:11.536906 29731 status_update_manager.cpp:177] Pausing sending status updates
I1110 01:20:11.536941 29721 slave.cpp:974] Authenticating with master master@172.17.0.3:52462
I1110 01:20:11.537076 29721 slave.cpp:985] Using default CRAM-MD5 authenticatee
I1110 01:20:11.537214 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54635
I1110 01:20:11.537353 29719 authenticatee.cpp:121] Creating new client SASL connection
I1110 01:20:11.537256 29721 slave.cpp:947] Detecting new master
I1110 01:20:11.537591 29733 master.cpp:2329] Received subscription request for HTTP framework 'default'
I1110 01:20:11.537611 29721 slave.cpp:5599] Received oversubscribable resources {} from the resource estimator
I1110 01:20:11.537701 29733 master.cpp:2069] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1110 01:20:11.538077 29733 master.cpp:6745] Authenticating slave(571)@172.17.0.3:52462
I1110 01:20:11.538208 29732 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1121)@172.17.0.3:52462
I1110 01:20:11.538291 29733 master.cpp:2427] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1110 01:20:11.538508 29731 authenticator.cpp:98] Creating new server SASL connection
I1110 01:20:11.538782 29720 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1110 01:20:11.538823 29720 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1110 01:20:11.539227 29730 hierarchical.cpp:275] Added framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.539317 29722 master.hpp:2161] Sending heartbeat to d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.539331 29730 hierarchical.cpp:1694] No allocations performed
I1110 01:20:11.539696 29730 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.539818 29730 hierarchical.cpp:1286] Performed allocation for 0 agents in 554795ns
I1110 01:20:11.540354 29720 authenticator.cpp:204] Received SASL authentication start
I1110 01:20:11.540361 29719 scheduler.cpp:675] Enqueuing event SUBSCRIBED received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.540438 29720 authenticator.cpp:326] Authentication requires more steps
I1110 01:20:11.540750 29720 authenticatee.cpp:259] Received SASL authentication step
I1110 01:20:11.541038 29721 authenticator.cpp:232] Received SASL authentication step
I1110 01:20:11.541081 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1110 01:20:11.541112 29721 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1110 01:20:11.541147 29719 scheduler.cpp:675] Enqueuing event HEARTBEAT received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.541178 29721 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1110 01:20:11.541260 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1110 01:20:11.541285 29721 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1110 01:20:11.541307 29721 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1110 01:20:11.541342 29721 authenticator.cpp:318] Authentication success
I1110 01:20:11.541517 29733 authenticatee.cpp:299] Authentication success
I1110 01:20:11.541586 29720 master.cpp:6775] Successfully authenticated principal 'test-principal' at slave(571)@172.17.0.3:52462
I1110 01:20:11.541826 29721 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1121)@172.17.0.3:52462
I1110 01:20:11.542129 29730 slave.cpp:1069] Successfully authenticated with master master@172.17.0.3:52462
I1110 01:20:11.542362 29730 slave.cpp:1483] Will retry registration in 9.532818ms if necessary
I1110 01:20:11.542577 29733 master.cpp:5154] Registering agent at slave(571)@172.17.0.3:52462 (3a31be8bf679) with id d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.543083 29731 registrar.cpp:461] Applied 1 operations in 60476ns; attempting to update the registry
I1110 01:20:11.543926 29729 log.cpp:577] Attempting to append 337 bytes to the log
I1110 01:20:11.544077 29723 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1110 01:20:11.545238 29731 replica.cpp:537] Replica received write request for position 3 from __req_res__(7325)@172.17.0.3:52462
I1110 01:20:11.546116 29731 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 825474ns
I1110 01:20:11.546169 29731 replica.cpp:708] Persisted action APPEND at position 3
I1110 01:20:11.547427 29725 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1110 01:20:11.547969 29725 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 483290ns
I1110 01:20:11.548005 29725 replica.cpp:708] Persisted action APPEND at position 3
I1110 01:20:11.550129 29732 registrar.cpp:506] Successfully updated the registry in 6.962944ms
I1110 01:20:11.550396 29726 log.cpp:596] Attempting to truncate the log to 3
I1110 01:20:11.550614 29720 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1110 01:20:11.551375 29723 slave.cpp:4263] Received ping from slave-observer(531)@172.17.0.3:52462
I1110 01:20:11.551326 29734 master.cpp:5225] Registered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1110 01:20:11.551720 29730 replica.cpp:537] Replica received write request for position 4 from __req_res__(7326)@172.17.0.3:52462
I1110 01:20:11.551892 29723 slave.cpp:1115] Registered with master master@172.17.0.3:52462; given agent ID d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.551975 29723 fetcher.cpp:86] Clearing fetcher cache
I1110 01:20:11.552170 29732 hierarchical.cpp:485] Added agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I1110 01:20:11.552338 29721 status_update_manager.cpp:184] Resuming sending status updates
I1110 01:20:11.552486 29730 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 709727ns
I1110 01:20:11.552655 29723 slave.cpp:1138] Checkpointing SlaveInfo to '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/slave.info'
I1110 01:20:11.552609 29730 replica.cpp:708] Persisted action TRUNCATE at position 4
I1110 01:20:11.553383 29731 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1110 01:20:11.553409 29723 slave.cpp:1175] Forwarding total oversubscribed resources {}
I1110 01:20:11.553653 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.553671 29727 master.cpp:5624] Received update of agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with total oversubscribed resources {}
I1110 01:20:11.553755 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 1.528714ms
I1110 01:20:11.553975 29731 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 525057ns
I1110 01:20:11.554072 29731 leveldb.cpp:399] Deleting ~2 keys from leveldb took 59750ns
I1110 01:20:11.554065 29732 hierarchical.cpp:555] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1110 01:20:11.554105 29731 replica.cpp:708] Persisted action TRUNCATE at position 4
I1110 01:20:11.554260 29732 hierarchical.cpp:1694] No allocations performed
I1110 01:20:11.554314 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.554345 29727 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:11.554379 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 239597ns
I1110 01:20:11.556370 29724 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.559177 29730 scheduler.cpp:235] Sending ACCEPT call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.560282 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.561323 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.562417 29733 master.cpp:3581] Processing ACCEPT call for offers: [ d28fbae1-c3dc-45fa-8384-32ab9395a975-O0 ] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:11.562584 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task c96eb523-0365-49b2-8b3b-78976ff28797
I1110 01:20:11.563097 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
I1110 01:20:11.567248 29733 master.cpp:8337] Adding task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679)
I1110 01:20:11.567651 29733 master.cpp:8337] Adding task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679)
I1110 01:20:11.567845 29733 master.cpp:4438] Launching task group { 08848440-4c0e-4ad6-a0a9-b5947c5d21ba, c96eb523-0365-49b2-8b3b-78976ff28797 } of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) with resources cpus(*):0.2; mem(*):64; disk(*):64 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.568495 29724 slave.cpp:1547] Got assigned task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.569128 29729 hierarchical.cpp:1018] Recovered cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.3; mem(*):96; disk(*):96) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.569211 29729 hierarchical.cpp:1055] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 filtered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for 5secs
I1110 01:20:11.570461 29724 slave.cpp:1709] Launching task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.571297 29724 paths.cpp:530] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' to user 'mesos'
I1110 01:20:11.580168 29724 slave.cpp:6319] Launching executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 with resources cpus(*):0.1; mem(*):32; disk(*):32 in work directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.580930 29734 containerizer.cpp:940] Starting container a283035b-25d3-4b48-b59a-964e5a4dfa06 for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.581110 29724 slave.cpp:2031] Queued task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.581214 29724 slave.cpp:868] Successfully attached file '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.585572 29722 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-default-executor"",""--launcher_dir=\/mesos\/mesos-1.2.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.2.0\/_build\/src\/mesos-default-executor""}"" --help=""false"" --pipe_read=""60"" --pipe_write=""61"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06""'
I1110 01:20:11.588587 29722 launcher.cpp:127] Forked child with pid '10191' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.592996 29734 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:11.777189 10229 executor.cpp:189] Version: 1.2.0
I1110 01:20:11.786099 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.787382 29722 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54638
I1110 01:20:11.787693 29722 slave.cpp:3086] Received Subscribe request for HTTP executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.790436 29730 slave.cpp:2276] Sending queued task group task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] to executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP)
I1110 01:20:11.795663 10221 default_executor.cpp:130] Received SUBSCRIBED event
I1110 01:20:11.797111 10221 default_executor.cpp:134] Subscribed executor on 3a31be8bf679
I1110 01:20:11.797611 10221 default_executor.cpp:130] Received LAUNCH_GROUP event
I1110 01:20:11.801981 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.802435 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.803306 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640
I1110 01:20:11.803452 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
I1110 01:20:11.803827 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.803865 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640
I1110 01:20:11.803978 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
I1110 01:20:11.804236 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d' to user 'mesos'
I1110 01:20:11.814858 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.815129 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611' to user 'mesos'
I1110 01:20:11.824666 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""sleep 1000""}"" --help=""false"" --pipe_read=""63"" --pipe_write=""64"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d""'
I1110 01:20:11.826855 29727 launcher.cpp:127] Forked child with pid '10240' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d'
I1110 01:20:11.828918 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""sleep 1000""}"" --help=""false"" --pipe_read=""65"" --pipe_write=""66"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611""'
I1110 01:20:11.831428 29727 launcher.cpp:127] Forked child with pid '10241' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611'
I1110 01:20:11.834421 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.837882 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.847651 10227 default_executor.cpp:470] Successfully launched child containers [ a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 ] for tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ]
I1110 01:20:11.849225 29728 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.850085 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797'
I1110 01:20:11.850145 29734 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.850405 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba'
I1110 01:20:11.850746 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.851114 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.851552 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.851727 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.852295 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.852826 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.853938 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.854076 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.854460 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54641
I1110 01:20:11.854559 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER
I1110 01:20:11.854610 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:11.855126 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.855190 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.855200 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54642
I1110 01:20:11.855409 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER
I1110 01:20:11.855608 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:11.855803 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:11.856199 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856346 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.856439 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856598 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:11.856828 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856998 29725 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1110 01:20:11.857322 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.857386 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.857787 29725 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1110 01:20:11.858124 10226 default_executor.cpp:130] Received ACKNOWLEDGED event
I1110 01:20:11.858530 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.859519 29732 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.859676 10233 default_executor.cpp:130] Received ACKNOWLEDGED event
../../src/tests/default_executor_tests.cpp:338: Failure
Value of: runningUpdate1->status().task_id()
  Actual: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
Expected: taskInfo1.task_id()
Which is: c96eb523-0365-49b2-8b3b-78976ff28797
../../src/tests/default_executor_tests.cpp:342: Failure
Value of: runningUpdate2->status().task_id()
  Actual: c96eb523-0365-49b2-8b3b-78976ff28797
Expected: taskInfo2.task_id()
Which is: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
I1110 01:20:11.861587 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.861948 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.862280 29733 scheduler.cpp:235] Sending KILL call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.862632 29721 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.863528 29719 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.863664 29719 master.cpp:4870] Processing ACKNOWLEDGE call 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.864003 29732 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
W1110 01:20:11.864294 29732 status_update_manager.cpp:769] Unexpected status update acknowledgement (received 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87, expecting d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
E1110 01:20:11.864575 29726 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement
I1110 01:20:11.864804 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.865231 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.866297 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.866420 29722 master.cpp:4870] Processing ACKNOWLEDGE call d91c7deb-4646-4b4e-ba1a-5650a256e8d2 for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.867036 29726 status_update_manager.cpp:395] Received status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.867076 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.867291 29722 master.cpp:4762] Telling agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
W1110 01:20:11.867297 29726 status_update_manager.cpp:769] Unexpected status update acknowledgement (received d91c7deb-4646-4b4e-ba1a-5650a256e8d2, expecting 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.867449 29733 slave.cpp:2344] Asked to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
E1110 01:20:11.867710 29733 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement
I1110 01:20:11.869391 10228 default_executor.cpp:130] Received KILL event
I1110 01:20:11.869498 10228 default_executor.cpp:810] Received kill for task 'c96eb523-0365-49b2-8b3b-78976ff28797'
I1110 01:20:11.869544 10228 default_executor.cpp:694] Shutting down
I1110 01:20:11.870112 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.870338 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.870965 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.871399 29730 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.871984 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643
I1110 01:20:11.872088 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER
I1110 01:20:11.872284 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d in RUNNING state
I1110 01:20:11.872340 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643
I1110 01:20:11.872416 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER
I1110 01:20:11.872597 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.877090 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 in RUNNING state
I1110 01:20:11.877320 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.962539 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d has exited
I1110 01:20:11.963811 29733 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.963851 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 has exited
I1110 01:20:11.964437 29729 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d/termination'
I1110 01:20:11.965940 29728 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.966202 29732 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611/termination'
I1110 01:20:11.970046 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797' in state TASK_KILLED
I1110 01:20:11.970501 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba' in state TASK_KILLED
I1110 01:20:11.970559 10231 default_executor.cpp:768] Terminating after 1secs
I1110 01:20:11.971288 29723 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.972218 29728 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.972488 29728 slave.cpp:3740] Handling status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.974179 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.975229 29726 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.975278 29729 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.975517 29729 slave.cpp:3740] Handling status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.976048 29729 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.978132 29720 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.978482 29725 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:12.494274 29725 hierarchical.cpp:1880] Filtered offer with cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:12.494357 29725 hierarchical.cpp:1694] No allocations performed
I1110 01:20:12.494402 29725 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:12.494491 29725 hierarchical.cpp:1286] Performed allocation for 1 agents in 915780ns
I1110 01:20:13.071280 29734 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06 has exited
I1110 01:20:13.071339 29734 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06 in RUNNING state
I1110 01:20:13.071746 29734 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:13.076637 29723 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:13.077929 29721 slave.cpp:4672] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 exited with status 0
I1110 01:20:13.078433 29732 master.cpp:5884] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679): exited with status 0
I1110 01:20:13.078538 29732 master.cpp:7840] Removing executor 'default' with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:13.079448 29730 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.2; mem(*):64; disk(*):64) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:13.081049 29723 scheduler.cpp:675] Enqueuing event FAILURE received from http://172.17.0.3:52462/master/api/v1/scheduler

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: failure(0x7fff1aa9a950, @0x2ab91c02dd10 48-byte object <90-62 27-EC B8-2A 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 10-CE 01-1C B9-2A 00-00 70-CE 02-1C B9-2A 00-00 00-00 00-00 B8-2A 00-00>)
Stack trace:
I1110 01:20:13.496551 29730 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:13.496680 29730 hierarchical.cpp:1286] Performed allocation for 1 agents in 1.498625ms
I1110 01:20:13.497339 29729 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:13.499797 29721 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:14.497707 29732 hierarchical.cpp:1694] No allocations performed
I1110 01:20:14.497795 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:14.497895 29732 hierarchical.cpp:1286] Performed allocation for 1 agents in 410313ns
I1110 01:20:15.499423 29728 hierarchical.cpp:1694] No allocations performed
I1110 01:20:15.499526 29728 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:15.499658 29728 hierarchical.cpp:1286] Performed allocation for 1 agents in 547651ns
I1110 01:20:16.500463 29729 hierarchical.cpp:1694] No allocations performed
I1110 01:20:16.500581 29729 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:16.500699 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 505442ns
I1110 01:20:17.502176 29727 hierarchical.cpp:1694] No allocations performed
I1110 01:20:17.502262 29727 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:17.502367 29727 hierarchical.cpp:1286] Performed allocation for 1 agents in 464526ns
I1110 01:20:18.503680 29723 hierarchical.cpp:1694] No allocations performed
I1110 01:20:18.503762 29723 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:18.503851 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 425163ns
I1110 01:20:19.505476 29723 hierarchical.cpp:1694] No allocations performed
I1110 01:20:19.505586 29723 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:19.505705 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 590762ns
I1110 01:20:20.507310 29724 hierarchical.cpp:1694] No allocations performed
I1110 01:20:20.507390 29724 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:20.507477 29724 hierarchical.cpp:1286] Performed allocation for 1 agents in 411721ns
I1110 01:20:21.508368 29729 hierarchical.cpp:1694] No allocations performed
I1110 01:20:21.508458 29729 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:21.508564 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 432440ns
W1110 01:20:21.855908 29728 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.856066 29728 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:21.856652 29734 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
W1110 01:20:21.857002 29727 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.857069 29727 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:21.857378 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.857475 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.857662 29722 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:21.858206 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING)
I1110 01:20:21.858988 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.859259 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.859647 29725 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.9; mem(*):992; disk(*):992; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.859845 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:21.859979 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING)
I1110 01:20:21.860970 29729 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.861687 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
../../src/tests/default_executor_tests.cpp:400: Failure
Value of: killedUpdate1->status().state()
  Actual: TASK_RUNNING
Expected: TASK_KILLED
I1110 01:20:21.864666 29721 master.cpp:1297] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) disconnected
I1110 01:20:21.864765 29721 master.cpp:2918] Disconnecting framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.864820 29721 master.cpp:2942] Deactivating framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.865016 29732 hierarchical.cpp:386] Deactivated framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
W1110 01:20:21.865586 29721 master.hpp:2264] Master attempted to send message to disconnected framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
W1110 01:20:21.865691 29721 master.hpp:2270] Unable to send event to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default): connection closed
I1110 01:20:21.865777 29721 master.cpp:1310] Giving framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) 0ns to failover
I1110 01:20:21.866277 29728 hierarchical.cpp:1018] Recovered cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {}) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.867295 29733 master.cpp:6426] Framework failover timeout, removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.867328 29733 master.cpp:7170] Removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.867539 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I1110 01:20:21.867559 29731 slave.cpp:2575] Asked to shut down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 by master@172.17.0.3:52462
I1110 01:20:21.867617 29731 slave.cpp:2600] Shutting down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.867585 29733 master.cpp:7811] Removing task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.867707 29731 slave.cpp:4776] Cleaning up executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP)
I1110 01:20:21.867904 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I1110 01:20:21.868042 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' for gc 6.99998999732444days in the future
I1110 01:20:21.867939 29733 master.cpp:7811] Removing task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.868232 29731 slave.cpp:4864] Cleaning up framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868252 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default' for gc 6.99998999732444days in the future
I1110 01:20:21.868422 29725 status_update_manager.cpp:285] Closing status update streams for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868484 29725 status_update_manager.cpp:531] Cleaning up status update stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868777 29721 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000' for gc 6.99998999732444days in the future
I1110 01:20:21.868926 29720 hierarchical.cpp:337] Removed framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.869235 29725 status_update_manager.cpp:531] Cleaning up status update stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.870568 29730 slave.cpp:787] Agent terminating
I1110 01:20:21.870795 29732 master.cpp:1258] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) disconnected
I1110 01:20:21.870825 29732 master.cpp:2977] Disconnecting agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.870930 29732 master.cpp:2996] Deactivating agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.871158 29726 hierarchical.cpp:584] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 deactivated
I1110 01:20:21.876986 29733 master.cpp:1097] Master terminating
I1110 01:20:21.877754 29724 hierarchical.cpp:517] Removed agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
[  FAILED  ] MesosContainerizer/DefaultExecutorTest.KillTask/0, where GetParam() = ""mesos"" (10406 ms)
{noformat}"	MESOS	Resolved	3	1	2531	flaky, mesosphere, newbie
13005024	Remove stout's Set type	"stout provides a {{Set}} type which wraps a {{std::set}}. As only addition it provides new constructors,
{code}
Set(const T& t1);
Set(const T& t1, const T& t2);
Set(const T& t1, const T& t2, const T& t3);
Set(const T& t1, const T& t2, const T& t3, const T& t4);
{code}
which simplified creation of a {{Set}} from (up to four) known elements.

C++11 brought {{std::initializer_list}} which can be used to create a {{std::set}} from an arbitrary number of elements, so it appears that it should be possible to retire {{Set}}."	MESOS	Resolved	4	1	2531	tech-debt
13216221	Reviewbot jenkins jobs stops validating any reviews as soon as it sees a patch which does not apply	"The reviewbot Jenkins setup fetches all Mesos reviews since some time stamp, filters that list down to reviews which need to be validated, and then one by one validates each of the remaining review requests.

In doing that it applies patches with {{support/apply-reviews.py}} which is invoked by shelling out wth a function {{shell}} in {{support/verify-reviews.py}}. If that function sees any error from the shell command {{exit(1)}} is called which immediately terminates the Jenkins job.

As {{support/apply-reviews.py}} can fail if a patch does not apply cleanly anymore this means that any review requests which cannot be applied can largely disable reviewbot.

We should avoid calling {{exit}} in low-level functions in {{support/verify-reviews.py}} and instead bubble the error up to be handled at a larger scope. It looks like the script was alreadt designed to handle exceptions which might work much better here."	MESOS	Resolved	1	1	2531	integration, mesosphere
13028076	CniIsolatorTest.ROOT_EnvironmentLibprocessIP fails on systems using dash as sh	"On systems using {{dash}} as default shell (e.g., Debian, Ubuntu) {{CniIsolatorTest.ROOT_EnvironmentLibprocessIP}} often fails with

{noformat}
[ RUN      ] CniIsolatorTest.ROOT_EnvironmentLibprocessIP
I1214 05:04:11.653625 24102 cluster.cpp:160] Creating default 'local' authorizer
I1214 05:04:11.654268 24118 master.cpp:380] Master cf36e3a9-60fb-4885-8145-831ca6998263 (ip-172-16-10-182.mesosphere.io) started on 172.16.10.182:60883
I1214 05:04:11.654281 24118 master.cpp:382] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/mnt/teamcity/temp/buildTmp/qEaG50/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/mnt/teamcity/temp/buildTmp/qEaG50/master"" --zk_session_timeout=""10secs""
I1214 05:04:11.654422 24118 master.cpp:432] Master only allowing authenticated frameworks to register
I1214 05:04:11.654428 24118 master.cpp:446] Master only allowing authenticated agents to register
I1214 05:04:11.654431 24118 master.cpp:459] Master only allowing authenticated HTTP frameworks to register
I1214 05:04:11.654434 24118 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/qEaG50/credentials'
I1214 05:04:11.654512 24118 master.cpp:504] Using default 'crammd5' authenticator
I1214 05:04:11.654549 24118 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I1214 05:04:11.654598 24118 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I1214 05:04:11.654669 24118 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I1214 05:04:11.654706 24118 master.cpp:584] Authorization enabled
I1214 05:04:11.654785 24119 hierarchical.cpp:149] Initialized hierarchical allocator process
I1214 05:04:11.654793 24123 whitelist_watcher.cpp:77] No whitelist given
I1214 05:04:11.655488 24123 master.cpp:2045] Elected as the leading master!
I1214 05:04:11.655498 24123 master.cpp:1568] Recovering from registrar
I1214 05:04:11.655591 24116 registrar.cpp:329] Recovering registrar
I1214 05:04:11.655776 24122 registrar.cpp:362] Successfully fetched the registry (0B) in 163072ns
I1214 05:04:11.655800 24122 registrar.cpp:461] Applied 1 operations in 2760ns; attempting to update the registry
I1214 05:04:11.655972 24118 registrar.cpp:506] Successfully updated the registry in 156928ns
I1214 05:04:11.656020 24118 registrar.cpp:392] Successfully recovered registrar
I1214 05:04:11.656093 24118 master.cpp:1684] Recovered 0 agents from the registry (174B); allowing 10mins for agents to re-register
I1214 05:04:11.656112 24120 hierarchical.cpp:176] Skipping recovery of hierarchical allocator: nothing to recover
I1214 05:04:11.657594 24102 containerizer.cpp:220] Using isolation: network/cni,filesystem/posix
I1214 05:04:11.660497 24102 linux_launcher.cpp:150] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
I1214 05:04:11.661574 24102 cluster.cpp:446] Creating default 'local' authorizer
I1214 05:04:11.661927 24119 slave.cpp:209] Mesos agent started on (576)@172.16.10.182:60883
I1214 05:04:11.661937 24119 slave.cpp:210] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/mnt/teamcity/temp/buildTmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/mnt/teamcity/temp/buildTmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/http_credentials"" --http_heartbeat_interval=""30secs"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher=""linux"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --network_cni_config_dir=""/mnt/teamcity/temp/buildTmp/qEaG50/configs"" --network_cni_plugins_dir=""/mnt/teamcity/temp/buildTmp/qEaG50/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg""
I1214 05:04:11.662156 24119 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/credential'
I1214 05:04:11.662252 24119 slave.cpp:352] Agent using credential for: test-principal
I1214 05:04:11.662262 24119 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/http_credentials'
I1214 05:04:11.662319 24119 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I1214 05:04:11.662350 24119 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I1214 05:04:11.662586 24102 sched.cpp:232] Version: 1.2.0
I1214 05:04:11.662672 24119 slave.cpp:539] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1214 05:04:11.662709 24119 slave.cpp:547] Agent attributes: [  ]
I1214 05:04:11.662716 24121 sched.cpp:336] New master detected at master@172.16.10.182:60883
I1214 05:04:11.662717 24119 slave.cpp:552] Agent hostname: ip-172-16-10-182.mesosphere.io
I1214 05:04:11.662739 24121 sched.cpp:402] Authenticating with master master@172.16.10.182:60883
I1214 05:04:11.662744 24121 sched.cpp:409] Using default CRAM-MD5 authenticatee
I1214 05:04:11.662809 24120 authenticatee.cpp:121] Creating new client SASL connection
I1214 05:04:11.662976 24116 master.cpp:6748] Authenticating scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.663029 24120 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1172)@172.16.10.182:60883
I1214 05:04:11.663045 24121 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/meta'
I1214 05:04:11.663202 24119 status_update_manager.cpp:203] Recovering status update manager
I1214 05:04:11.663219 24120 authenticator.cpp:98] Creating new server SASL connection
I1214 05:04:11.663321 24119 containerizer.cpp:594] Recovering containerizer
I1214 05:04:11.663372 24119 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1214 05:04:11.663388 24119 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1214 05:04:11.663450 24120 authenticator.cpp:204] Received SASL authentication start
I1214 05:04:11.663487 24120 authenticator.cpp:326] Authentication requires more steps
I1214 05:04:11.663533 24120 authenticatee.cpp:259] Received SASL authentication step
I1214 05:04:11.663609 24122 authenticator.cpp:232] Received SASL authentication step
I1214 05:04:11.663625 24122 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1214 05:04:11.663630 24122 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1214 05:04:11.663635 24122 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1214 05:04:11.663642 24122 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1214 05:04:11.663646 24122 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.663650 24122 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.663657 24122 authenticator.cpp:318] Authentication success
I1214 05:04:11.663708 24122 authenticatee.cpp:299] Authentication success
I1214 05:04:11.663739 24120 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1172)@172.16.10.182:60883
I1214 05:04:11.663797 24118 master.cpp:6778] Successfully authenticated principal 'test-principal' at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.663831 24122 sched.cpp:508] Successfully authenticated with master master@172.16.10.182:60883
I1214 05:04:11.663841 24122 sched.cpp:826] Sending SUBSCRIBE call to master@172.16.10.182:60883
I1214 05:04:11.663913 24122 sched.cpp:859] Will retry registration in 1.01884683secs if necessary
I1214 05:04:11.663961 24116 master.cpp:2633] Received SUBSCRIBE call for framework 'default' at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.663985 24116 master.cpp:2081] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1214 05:04:11.664217 24116 master.cpp:2709] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1214 05:04:11.664366 24117 sched.cpp:749] Framework registered with cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.664367 24119 hierarchical.cpp:276] Added framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.664393 24117 sched.cpp:763] Scheduler::registered took 9855ns
I1214 05:04:11.664402 24119 hierarchical.cpp:1689] No allocations performed
I1214 05:04:11.664409 24119 hierarchical.cpp:1784] No inverse offers to send out!
I1214 05:04:11.664417 24119 hierarchical.cpp:1291] Performed allocation for 0 agents in 23024ns
I1214 05:04:11.664438 24116 provisioner.cpp:253] Provisioner recovery complete
I1214 05:04:11.664584 24121 slave.cpp:5420] Finished recovery
I1214 05:04:11.664744 24121 slave.cpp:5594] Querying resource estimator for oversubscribable resources
I1214 05:04:11.664862 24122 slave.cpp:924] New master detected at master@172.16.10.182:60883
I1214 05:04:11.664870 24118 status_update_manager.cpp:177] Pausing sending status updates
I1214 05:04:11.664877 24122 slave.cpp:983] Authenticating with master master@172.16.10.182:60883
I1214 05:04:11.664893 24122 slave.cpp:994] Using default CRAM-MD5 authenticatee
I1214 05:04:11.664923 24122 slave.cpp:956] Detecting new master
I1214 05:04:11.664952 24123 authenticatee.cpp:121] Creating new client SASL connection
I1214 05:04:11.664979 24122 slave.cpp:5608] Received oversubscribable resources {} from the resource estimator
I1214 05:04:11.665102 24123 master.cpp:6748] Authenticating slave(576)@172.16.10.182:60883
I1214 05:04:11.665148 24116 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1173)@172.16.10.182:60883
I1214 05:04:11.665217 24116 authenticator.cpp:98] Creating new server SASL connection
I1214 05:04:11.665410 24116 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1214 05:04:11.665427 24116 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1214 05:04:11.665475 24116 authenticator.cpp:204] Received SASL authentication start
I1214 05:04:11.665504 24116 authenticator.cpp:326] Authentication requires more steps
I1214 05:04:11.665547 24116 authenticatee.cpp:259] Received SASL authentication step
I1214 05:04:11.665601 24116 authenticator.cpp:232] Received SASL authentication step
I1214 05:04:11.665616 24116 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1214 05:04:11.665622 24116 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1214 05:04:11.665632 24116 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1214 05:04:11.665643 24116 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1214 05:04:11.665652 24116 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.665657 24116 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.665668 24116 authenticator.cpp:318] Authentication success
I1214 05:04:11.665733 24119 authenticatee.cpp:299] Authentication success
I1214 05:04:11.665763 24116 master.cpp:6778] Successfully authenticated principal 'test-principal' at slave(576)@172.16.10.182:60883
I1214 05:04:11.665784 24121 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1173)@172.16.10.182:60883
I1214 05:04:11.665923 24120 slave.cpp:1078] Successfully authenticated with master master@172.16.10.182:60883
I1214 05:04:11.665961 24120 slave.cpp:1492] Will retry registration in 14.874897ms if necessary
I1214 05:04:11.666002 24118 master.cpp:5161] Registering agent at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) with id cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.666106 24122 registrar.cpp:461] Applied 1 operations in 9638ns; attempting to update the registry
I1214 05:04:11.666337 24120 registrar.cpp:506] Successfully updated the registry in 217088ns
I1214 05:04:11.666514 24122 master.cpp:5232] Registered agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1214 05:04:11.666548 24119 slave.cpp:4272] Received ping from slave-observer(545)@172.16.10.182:60883
I1214 05:04:11.666596 24120 hierarchical.cpp:490] Added agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 (ip-172-16-10-182.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I1214 05:04:11.666616 24119 slave.cpp:1124] Registered with master master@172.16.10.182:60883; given agent ID cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.666630 24119 fetcher.cpp:90] Clearing fetcher cache
I1214 05:04:11.666707 24116 status_update_manager.cpp:184] Resuming sending status updates
I1214 05:04:11.666790 24119 slave.cpp:1147] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/meta/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/slave.info'
I1214 05:04:11.666831 24120 hierarchical.cpp:1784] No inverse offers to send out!
I1214 05:04:11.666857 24120 hierarchical.cpp:1314] Performed allocation for agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 in 234910ns
I1214 05:04:11.666913 24119 slave.cpp:1184] Forwarding total oversubscribed resources {}
I1214 05:04:11.666954 24122 master.cpp:6577] Sending 1 offers to framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.667026 24122 master.cpp:5633] Received update of agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) with total oversubscribed resources {}
I1214 05:04:11.667127 24120 sched.cpp:923] Scheduler::resourceOffers took 38599ns
I1214 05:04:11.667131 24118 hierarchical.cpp:560] Agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 (ip-172-16-10-182.mesosphere.io) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1214 05:04:11.667181 24118 hierarchical.cpp:1689] No allocations performed
I1214 05:04:11.667192 24118 hierarchical.cpp:1784] No inverse offers to send out!
I1214 05:04:11.667207 24118 hierarchical.cpp:1314] Performed allocation for agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 in 44745ns
I1214 05:04:11.667513 24118 master.cpp:3588] Processing ACCEPT call for offers: [ cf36e3a9-60fb-4885-8145-831ca6998263-O0 ] on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.667537 24118 master.cpp:3175] Authorizing framework principal 'test-principal' to launch task 80ddad17-5693-4883-a79d-1925a7b41661
I1214 05:04:11.667912 24119 master.cpp:8501] Adding task 80ddad17-5693-4883-a79d-1925a7b41661 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.667978 24119 master.cpp:4240] Launching task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.668177 24120 slave.cpp:1556] Got assigned task '80ddad17-5693-4883-a79d-1925a7b41661' for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.668344 24120 slave.cpp:1718] Launching task '80ddad17-5693-4883-a79d-1925a7b41661' for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.668531 24120 paths.cpp:530] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095' to user 'root'
I1214 05:04:11.672688 24120 slave.cpp:6347] Launching executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095'
I1214 05:04:11.672895 24123 containerizer.cpp:986] Starting container 696f16bc-efdc-4a36-b94c-b0739cc1c095 for executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.672901 24120 slave.cpp:2040] Queued task '80ddad17-5693-4883-a79d-1925a7b41661' for executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.672932 24120 slave.cpp:877] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095'
I1214 05:04:11.673694 24121 containerizer.cpp:1535] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072,67108864],""command"":{""arguments"":[""mesos-executor"",""--launcher_dir=\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src""],""shell"":false,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""value"":""172.16.10.182:60883""},{""name"":""MESOS_CHECKPOINT"",""value"":""0""},{""name"":""MESOS_DIRECTORY"",""value"":""\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg\/slaves\/cf36e3a9-60fb-4885-8145-831ca6998263-S0\/frameworks\/cf36e3a9-60fb-4885-8145-831ca6998263-0000\/executors\/80ddad17-5693-4883-a79d-1925a7b41661\/runs\/696f16bc-efdc-4a36-b94c-b0739cc1c095""},{""name"":""MESOS_EXECUTOR_ID"",""value"":""80ddad17-5693-4883-a79d-1925a7b41661""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""value"":""cf36e3a9-60fb-4885-8145-831ca6998263-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""value"":""0""},{""name"":""MESOS_SLAVE_ID"",""value"":""cf36e3a9-60fb-4885-8145-831ca6998263-S0""},{""name"":""MESOS_SLAVE_PID"",""value"":""slave(576)@172.16.10.182:60883""},{""name"":""MESOS_SANDBOX"",""value"":""\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg\/slaves\/cf36e3a9-60fb-4885-8145-831ca6998263-S0\/frameworks\/cf36e3a9-60fb-4885-8145-831ca6998263-0000\/executors\/80ddad17-5693-4883-a79d-1925a7b41661\/runs\/696f16bc-efdc-4a36-b94c-b0739cc1c095""},{""name"":""LIBPROCESS_IP"",""value"":""0.0.0.0""}]},""user"":""root"",""working_directory"":""\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg\/slaves\/cf36e3a9-60fb-4885-8145-831ca6998263-S0\/frameworks\/cf36e3a9-60fb-4885-8145-831ca6998263-0000\/executors\/80ddad17-5693-4883-a79d-1925a7b41661\/runs\/696f16bc-efdc-4a36-b94c-b0739cc1c095""}"" --pipe_read=""28"" --pipe_write=""32"" --runtime_directory=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/containers/696f16bc-efdc-4a36-b94c-b0739cc1c095"" --unshare_namespace_mnt=""false""'
I1214 05:04:11.674006 24117 linux_launcher.cpp:429] Launching container 696f16bc-efdc-4a36-b94c-b0739cc1c095 and cloning with namespaces CLONE_NEWNS | CLONE_NEWUTS
I1214 05:04:11.677374 24116 cni.cpp:844] Bind mounted '/proc/11118/ns/net' to '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095/ns' for container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.677500 24116 cni.cpp:1173] Invoking CNI plugin '/mnt/teamcity/temp/buildTmp/qEaG50/plugins/mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}' to attach container 696f16bc-efdc-4a36-b94c-b0739cc1c095 to network '__MESOS_TEST__'
I1214 05:04:11.742821 24122 cni.cpp:1260] Got assigned IPv4 address '172.17.0.1/16' from CNI network '__MESOS_TEST__' for container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.743064 24116 cni.cpp:969] DNS nameservers for container 696f16bc-efdc-4a36-b94c-b0739cc1c095 are:
nameserver 172.16.10.2
I1214 05:04:11.843673 24119 fetcher.cpp:349] Starting to fetch URIs for container: 696f16bc-efdc-4a36-b94c-b0739cc1c095, directory: /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.876816 11170 exec.cpp:162] Version: 1.2.0
I1214 05:04:11.879708 24118 slave.cpp:3314] Got registration for executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from executor(1)@172.17.0.1:34346
I1214 05:04:11.880328 11169 exec.cpp:237] Executor registered on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.880398 24121 slave.cpp:2256] Sending queued task '80ddad17-5693-4883-a79d-1925a7b41661' to executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 at executor(1)@172.17.0.1:34346
Received SUBSCRIBED event
Subscribed executor on ip-172-16-10-182.mesosphere.io
Received LAUNCH event
Starting task 80ddad17-5693-4883-a79d-1925a7b41661
/mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer launch --help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""\n      #!\/bin\/sh\n      if [ x\""$LIBPROCESS_IP\"" == x\""0.0.0.0\"" ]; then\n        exit 0\n      else\n        exit 1\n      fi""}}"" --unshare_namespace_mnt=""false""
Forked command at 11172
I1214 05:04:11.884364 24116 slave.cpp:3749] Handling status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from executor(1)@172.17.0.1:34346
I1214 05:04:11.884829 24121 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.884848 24121 status_update_manager.cpp:500] Creating StatusUpdate stream for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.884946 24121 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to the agent
I1214 05:04:11.885074 24120 slave.cpp:4190] Forwarding the update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to master@172.16.10.182:60883
I1214 05:04:11.885150 24120 slave.cpp:4084] Status update manager successfully handled status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.885172 24120 slave.cpp:4100] Sending acknowledgement for status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to executor(1)@172.17.0.1:34346
I1214 05:04:11.885200 24118 master.cpp:5769] Status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.885229 24118 master.cpp:5831] Forwarding status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.885284 24118 master.cpp:7867] Updating the state of task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1214 05:04:11.885409 24117 sched.cpp:1031] Scheduler::statusUpdate took 63040ns
I1214 05:04:11.885541 24122 master.cpp:4877] Processing ACKNOWLEDGE call 32ed996d-9fc5-4e82-afb1-d7936670af16 for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883 on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.885753 24119 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.885821 24119 slave.cpp:3031] Status update manager successfully handled status update acknowledgement (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
sh: 3: [: x0.0.0.0: unexpected operator
Command exited with status 1 (pid: 11172)
I1214 05:04:11.978447 24119 slave.cpp:3749] Handling status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from executor(1)@172.17.0.1:34346
I1214 05:04:11.979202 24122 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979249 24122 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to the agent
I1214 05:04:11.979321 24119 slave.cpp:4190] Forwarding the update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to master@172.16.10.182:60883
I1214 05:04:11.979404 24119 slave.cpp:4084] Status update manager successfully handled status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979425 24119 slave.cpp:4100] Sending acknowledgement for status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to executor(1)@172.17.0.1:34346
I1214 05:04:11.979462 24121 master.cpp:5769] Status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.979482 24121 master.cpp:5831] Forwarding status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979528 24121 master.cpp:7867] Updating the state of task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
I1214 05:04:11.979662 24120 sched.cpp:1031] Scheduler::statusUpdate took 66057ns
I1214 05:04:11.979784 24116 hierarchical.cpp:1023] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {}) on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 from framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
../../src/tests/containerizer/cni_isolator_tests.cpp:601: Failure
Value of: statusFinished->state()
  Actual: TASK_FAILED
Expected: TASK_FINISHED
I1214 05:04:11.979811 24123 master.cpp:4877] Processing ACKNOWLEDGE call 361b2ed5-7dee-4c96-854a-585c78d25532 for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883 on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.979845 24102 sched.cpp:2008] Asked to stop the driver
I1214 05:04:11.979832 24123 master.cpp:7963] Removing task 80ddad17-5693-4883-a79d-1925a7b41661 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.979892 24121 sched.cpp:1193] Stopping framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979966 24122 master.cpp:7287] Processing TEARDOWN call for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.979981 24122 master.cpp:7299] Removing framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.979990 24120 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980049 24120 status_update_manager.cpp:531] Cleaning up status update stream for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980075 24123 hierarchical.cpp:391] Deactivated framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980115 24121 slave.cpp:2584] Asked to shut down framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 by master@172.16.10.182:60883
I1214 05:04:11.980144 24121 slave.cpp:2609] Shutting down framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980160 24121 slave.cpp:4999] Shutting down executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 at executor(1)@172.17.0.1:34346
I1214 05:04:11.980175 24118 hierarchical.cpp:342] Removed framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980267 24121 slave.cpp:3031] Status update manager successfully handled status update acknowledgement (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980285 24121 slave.cpp:6719] Completing task 80ddad17-5693-4883-a79d-1925a7b41661
I1214 05:04:11.980413 11170 exec.cpp:414] Executor asked to shutdown
Received SHUTDOWN event
Shutting down
I1214 05:04:11.980662 24120 containerizer.cpp:2113] Destroying container 696f16bc-efdc-4a36-b94c-b0739cc1c095 in RUNNING state
I1214 05:04:11.980731 24120 linux_launcher.cpp:505] Asked to destroy container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.981130 24120 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.981762 24117 cgroups.cpp:2726] Freezing cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.982657 24123 cgroups.cpp:1439] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095 after 869120ns
I1214 05:04:11.983755 24117 cgroups.cpp:2744] Thawing cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.984719 24118 cgroups.cpp:1468] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095 after 944128ns
I1214 05:04:11.999498 24117 slave.cpp:4318] Got exited event for executor(1)@172.17.0.1:34346
I1214 05:04:12.043917 24120 containerizer.cpp:2476] Container 696f16bc-efdc-4a36-b94c-b0739cc1c095 has exited
I1214 05:04:12.044694 24119 cni.cpp:1511] Invoking CNI plugin '/mnt/teamcity/temp/buildTmp/qEaG50/plugins/mockPlugin' with network configuration '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095/__MESOS_TEST__/network.conf' to detach container 696f16bc-efdc-4a36-b94c-b0739cc1c095 from network '__MESOS_TEST__'
I1214 05:04:12.151413 24123 cni.cpp:1438] Unmounted the network namespace handle '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095/ns' for container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:12.151480 24123 cni.cpp:1449] Removed the container directory '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095'
I1214 05:04:12.151793 24116 provisioner.cpp:324] Ignoring destroy request for unknown container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:12.152027 24117 slave.cpp:4681] Executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 terminated with signal Killed
I1214 05:04:12.152050 24117 slave.cpp:4785] Cleaning up executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 at executor(1)@172.17.0.1:34346
I1214 05:04:12.152204 24121 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095' for gc 6.99999823931852days in the future
I1214 05:04:12.152215 24117 slave.cpp:4873] Cleaning up framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:12.152261 24121 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661' for gc 6.99999823841481days in the future
I1214 05:04:12.152300 24116 status_update_manager.cpp:285] Closing status update streams for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:12.152401 24117 slave.cpp:796] Agent terminating
I1214 05:04:12.152410 24122 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000' for gc 6.99999823711704days in the future
I1214 05:04:12.152472 24122 master.cpp:1258] Agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) disconnected
I1214 05:04:12.152488 24122 master.cpp:2977] Disconnecting agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:12.152509 24122 master.cpp:2996] Deactivating agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:12.152565 24122 hierarchical.cpp:589] Agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 deactivated
I1214 05:04:12.154162 24102 master.cpp:1097] Master terminating
I1214 05:04:12.154306 24118 hierarchical.cpp:522] Removed agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
[  FAILED  ] CniIsolatorTest.ROOT_EnvironmentLibprocessIP (503 ms)[05:04:11] :	 [Step 11/11] [ RUN      ] CniIsolatorTest.ROOT_EnvironmentLibprocessIP
I1214 05:04:11.653625 24102 cluster.cpp:160] Creating default 'local' authorizer
I1214 05:04:11.654268 24118 master.cpp:380] Master cf36e3a9-60fb-4885-8145-831ca6998263 (ip-172-16-10-182.mesosphere.io) started on 172.16.10.182:60883
I1214 05:04:11.654281 24118 master.cpp:382] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/mnt/teamcity/temp/buildTmp/qEaG50/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/mnt/teamcity/temp/buildTmp/qEaG50/master"" --zk_session_timeout=""10secs""
I1214 05:04:11.654422 24118 master.cpp:432] Master only allowing authenticated frameworks to register
I1214 05:04:11.654428 24118 master.cpp:446] Master only allowing authenticated agents to register
I1214 05:04:11.654431 24118 master.cpp:459] Master only allowing authenticated HTTP frameworks to register
I1214 05:04:11.654434 24118 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/qEaG50/credentials'
I1214 05:04:11.654512 24118 master.cpp:504] Using default 'crammd5' authenticator
I1214 05:04:11.654549 24118 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I1214 05:04:11.654598 24118 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I1214 05:04:11.654669 24118 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I1214 05:04:11.654706 24118 master.cpp:584] Authorization enabled
I1214 05:04:11.654785 24119 hierarchical.cpp:149] Initialized hierarchical allocator process
I1214 05:04:11.654793 24123 whitelist_watcher.cpp:77] No whitelist given
I1214 05:04:11.655488 24123 master.cpp:2045] Elected as the leading master!
I1214 05:04:11.655498 24123 master.cpp:1568] Recovering from registrar
I1214 05:04:11.655591 24116 registrar.cpp:329] Recovering registrar
I1214 05:04:11.655776 24122 registrar.cpp:362] Successfully fetched the registry (0B) in 163072ns
I1214 05:04:11.655800 24122 registrar.cpp:461] Applied 1 operations in 2760ns; attempting to update the registry
I1214 05:04:11.655972 24118 registrar.cpp:506] Successfully updated the registry in 156928ns
I1214 05:04:11.656020 24118 registrar.cpp:392] Successfully recovered registrar
I1214 05:04:11.656093 24118 master.cpp:1684] Recovered 0 agents from the registry (174B); allowing 10mins for agents to re-register
I1214 05:04:11.656112 24120 hierarchical.cpp:176] Skipping recovery of hierarchical allocator: nothing to recover
I1214 05:04:11.657594 24102 containerizer.cpp:220] Using isolation: network/cni,filesystem/posix
I1214 05:04:11.660497 24102 linux_launcher.cpp:150] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
I1214 05:04:11.661574 24102 cluster.cpp:446] Creating default 'local' authorizer
I1214 05:04:11.661927 24119 slave.cpp:209] Mesos agent started on (576)@172.16.10.182:60883
I1214 05:04:11.661937 24119 slave.cpp:210] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/mnt/teamcity/temp/buildTmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/mnt/teamcity/temp/buildTmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/http_credentials"" --http_heartbeat_interval=""30secs"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher=""linux"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --network_cni_config_dir=""/mnt/teamcity/temp/buildTmp/qEaG50/configs"" --network_cni_plugins_dir=""/mnt/teamcity/temp/buildTmp/qEaG50/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg""
I1214 05:04:11.662156 24119 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/credential'
I1214 05:04:11.662252 24119 slave.cpp:352] Agent using credential for: test-principal
I1214 05:04:11.662262 24119 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/http_credentials'
I1214 05:04:11.662319 24119 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I1214 05:04:11.662350 24119 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I1214 05:04:11.662586 24102 sched.cpp:232] Version: 1.2.0
I1214 05:04:11.662672 24119 slave.cpp:539] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1214 05:04:11.662709 24119 slave.cpp:547] Agent attributes: [  ]
I1214 05:04:11.662716 24121 sched.cpp:336] New master detected at master@172.16.10.182:60883
I1214 05:04:11.662717 24119 slave.cpp:552] Agent hostname: ip-172-16-10-182.mesosphere.io
I1214 05:04:11.662739 24121 sched.cpp:402] Authenticating with master master@172.16.10.182:60883
I1214 05:04:11.662744 24121 sched.cpp:409] Using default CRAM-MD5 authenticatee
I1214 05:04:11.662809 24120 authenticatee.cpp:121] Creating new client SASL connection
I1214 05:04:11.662976 24116 master.cpp:6748] Authenticating scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.663029 24120 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1172)@172.16.10.182:60883
I1214 05:04:11.663045 24121 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/meta'
I1214 05:04:11.663202 24119 status_update_manager.cpp:203] Recovering status update manager
I1214 05:04:11.663219 24120 authenticator.cpp:98] Creating new server SASL connection
I1214 05:04:11.663321 24119 containerizer.cpp:594] Recovering containerizer
I1214 05:04:11.663372 24119 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1214 05:04:11.663388 24119 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1214 05:04:11.663450 24120 authenticator.cpp:204] Received SASL authentication start
I1214 05:04:11.663487 24120 authenticator.cpp:326] Authentication requires more steps
I1214 05:04:11.663533 24120 authenticatee.cpp:259] Received SASL authentication step
I1214 05:04:11.663609 24122 authenticator.cpp:232] Received SASL authentication step
I1214 05:04:11.663625 24122 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1214 05:04:11.663630 24122 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1214 05:04:11.663635 24122 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1214 05:04:11.663642 24122 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1214 05:04:11.663646 24122 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.663650 24122 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.663657 24122 authenticator.cpp:318] Authentication success
I1214 05:04:11.663708 24122 authenticatee.cpp:299] Authentication success
I1214 05:04:11.663739 24120 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1172)@172.16.10.182:60883
I1214 05:04:11.663797 24118 master.cpp:6778] Successfully authenticated principal 'test-principal' at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.663831 24122 sched.cpp:508] Successfully authenticated with master master@172.16.10.182:60883
I1214 05:04:11.663841 24122 sched.cpp:826] Sending SUBSCRIBE call to master@172.16.10.182:60883
I1214 05:04:11.663913 24122 sched.cpp:859] Will retry registration in 1.01884683secs if necessary
I1214 05:04:11.663961 24116 master.cpp:2633] Received SUBSCRIBE call for framework 'default' at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.663985 24116 master.cpp:2081] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1214 05:04:11.664217 24116 master.cpp:2709] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1214 05:04:11.664366 24117 sched.cpp:749] Framework registered with cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.664367 24119 hierarchical.cpp:276] Added framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.664393 24117 sched.cpp:763] Scheduler::registered took 9855ns
I1214 05:04:11.664402 24119 hierarchical.cpp:1689] No allocations performed
I1214 05:04:11.664409 24119 hierarchical.cpp:1784] No inverse offers to send out!
I1214 05:04:11.664417 24119 hierarchical.cpp:1291] Performed allocation for 0 agents in 23024ns
I1214 05:04:11.664438 24116 provisioner.cpp:253] Provisioner recovery complete
I1214 05:04:11.664584 24121 slave.cpp:5420] Finished recovery
I1214 05:04:11.664744 24121 slave.cpp:5594] Querying resource estimator for oversubscribable resources
I1214 05:04:11.664862 24122 slave.cpp:924] New master detected at master@172.16.10.182:60883
I1214 05:04:11.664870 24118 status_update_manager.cpp:177] Pausing sending status updates
I1214 05:04:11.664877 24122 slave.cpp:983] Authenticating with master master@172.16.10.182:60883
I1214 05:04:11.664893 24122 slave.cpp:994] Using default CRAM-MD5 authenticatee
I1214 05:04:11.664923 24122 slave.cpp:956] Detecting new master
I1214 05:04:11.664952 24123 authenticatee.cpp:121] Creating new client SASL connection
I1214 05:04:11.664979 24122 slave.cpp:5608] Received oversubscribable resources {} from the resource estimator
I1214 05:04:11.665102 24123 master.cpp:6748] Authenticating slave(576)@172.16.10.182:60883
I1214 05:04:11.665148 24116 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1173)@172.16.10.182:60883
I1214 05:04:11.665217 24116 authenticator.cpp:98] Creating new server SASL connection
I1214 05:04:11.665410 24116 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1214 05:04:11.665427 24116 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1214 05:04:11.665475 24116 authenticator.cpp:204] Received SASL authentication start
I1214 05:04:11.665504 24116 authenticator.cpp:326] Authentication requires more steps
I1214 05:04:11.665547 24116 authenticatee.cpp:259] Received SASL authentication step
I1214 05:04:11.665601 24116 authenticator.cpp:232] Received SASL authentication step
I1214 05:04:11.665616 24116 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1214 05:04:11.665622 24116 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1214 05:04:11.665632 24116 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1214 05:04:11.665643 24116 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-182.mesosphere.io' server FQDN: 'ip-172-16-10-182.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1214 05:04:11.665652 24116 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.665657 24116 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1214 05:04:11.665668 24116 authenticator.cpp:318] Authentication success
I1214 05:04:11.665733 24119 authenticatee.cpp:299] Authentication success
I1214 05:04:11.665763 24116 master.cpp:6778] Successfully authenticated principal 'test-principal' at slave(576)@172.16.10.182:60883
I1214 05:04:11.665784 24121 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1173)@172.16.10.182:60883
I1214 05:04:11.665923 24120 slave.cpp:1078] Successfully authenticated with master master@172.16.10.182:60883
I1214 05:04:11.665961 24120 slave.cpp:1492] Will retry registration in 14.874897ms if necessary
I1214 05:04:11.666002 24118 master.cpp:5161] Registering agent at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) with id cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.666106 24122 registrar.cpp:461] Applied 1 operations in 9638ns; attempting to update the registry
I1214 05:04:11.666337 24120 registrar.cpp:506] Successfully updated the registry in 217088ns
I1214 05:04:11.666514 24122 master.cpp:5232] Registered agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1214 05:04:11.666548 24119 slave.cpp:4272] Received ping from slave-observer(545)@172.16.10.182:60883
I1214 05:04:11.666596 24120 hierarchical.cpp:490] Added agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 (ip-172-16-10-182.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I1214 05:04:11.666616 24119 slave.cpp:1124] Registered with master master@172.16.10.182:60883; given agent ID cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.666630 24119 fetcher.cpp:90] Clearing fetcher cache
I1214 05:04:11.666707 24116 status_update_manager.cpp:184] Resuming sending status updates
I1214 05:04:11.666790 24119 slave.cpp:1147] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/meta/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/slave.info'
I1214 05:04:11.666831 24120 hierarchical.cpp:1784] No inverse offers to send out!
I1214 05:04:11.666857 24120 hierarchical.cpp:1314] Performed allocation for agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 in 234910ns
I1214 05:04:11.666913 24119 slave.cpp:1184] Forwarding total oversubscribed resources {}
I1214 05:04:11.666954 24122 master.cpp:6577] Sending 1 offers to framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.667026 24122 master.cpp:5633] Received update of agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) with total oversubscribed resources {}
I1214 05:04:11.667127 24120 sched.cpp:923] Scheduler::resourceOffers took 38599ns
I1214 05:04:11.667131 24118 hierarchical.cpp:560] Agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 (ip-172-16-10-182.mesosphere.io) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1214 05:04:11.667181 24118 hierarchical.cpp:1689] No allocations performed
I1214 05:04:11.667192 24118 hierarchical.cpp:1784] No inverse offers to send out!
I1214 05:04:11.667207 24118 hierarchical.cpp:1314] Performed allocation for agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 in 44745ns
I1214 05:04:11.667513 24118 master.cpp:3588] Processing ACCEPT call for offers: [ cf36e3a9-60fb-4885-8145-831ca6998263-O0 ] on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.667537 24118 master.cpp:3175] Authorizing framework principal 'test-principal' to launch task 80ddad17-5693-4883-a79d-1925a7b41661
I1214 05:04:11.667912 24119 master.cpp:8501] Adding task 80ddad17-5693-4883-a79d-1925a7b41661 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.667978 24119 master.cpp:4240] Launching task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.668177 24120 slave.cpp:1556] Got assigned task '80ddad17-5693-4883-a79d-1925a7b41661' for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.668344 24120 slave.cpp:1718] Launching task '80ddad17-5693-4883-a79d-1925a7b41661' for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.668531 24120 paths.cpp:530] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095' to user 'root'
I1214 05:04:11.672688 24120 slave.cpp:6347] Launching executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095'
I1214 05:04:11.672895 24123 containerizer.cpp:986] Starting container 696f16bc-efdc-4a36-b94c-b0739cc1c095 for executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.672901 24120 slave.cpp:2040] Queued task '80ddad17-5693-4883-a79d-1925a7b41661' for executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.672932 24120 slave.cpp:877] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095'
I1214 05:04:11.673694 24121 containerizer.cpp:1535] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072,67108864],""command"":{""arguments"":[""mesos-executor"",""--launcher_dir=\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src""],""shell"":false,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""value"":""172.16.10.182:60883""},{""name"":""MESOS_CHECKPOINT"",""value"":""0""},{""name"":""MESOS_DIRECTORY"",""value"":""\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg\/slaves\/cf36e3a9-60fb-4885-8145-831ca6998263-S0\/frameworks\/cf36e3a9-60fb-4885-8145-831ca6998263-0000\/executors\/80ddad17-5693-4883-a79d-1925a7b41661\/runs\/696f16bc-efdc-4a36-b94c-b0739cc1c095""},{""name"":""MESOS_EXECUTOR_ID"",""value"":""80ddad17-5693-4883-a79d-1925a7b41661""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""value"":""cf36e3a9-60fb-4885-8145-831ca6998263-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""value"":""0""},{""name"":""MESOS_SLAVE_ID"",""value"":""cf36e3a9-60fb-4885-8145-831ca6998263-S0""},{""name"":""MESOS_SLAVE_PID"",""value"":""slave(576)@172.16.10.182:60883""},{""name"":""MESOS_SANDBOX"",""value"":""\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg\/slaves\/cf36e3a9-60fb-4885-8145-831ca6998263-S0\/frameworks\/cf36e3a9-60fb-4885-8145-831ca6998263-0000\/executors\/80ddad17-5693-4883-a79d-1925a7b41661\/runs\/696f16bc-efdc-4a36-b94c-b0739cc1c095""},{""name"":""LIBPROCESS_IP"",""value"":""0.0.0.0""}]},""user"":""root"",""working_directory"":""\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg\/slaves\/cf36e3a9-60fb-4885-8145-831ca6998263-S0\/frameworks\/cf36e3a9-60fb-4885-8145-831ca6998263-0000\/executors\/80ddad17-5693-4883-a79d-1925a7b41661\/runs\/696f16bc-efdc-4a36-b94c-b0739cc1c095""}"" --pipe_read=""28"" --pipe_write=""32"" --runtime_directory=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_kPcBAv/containers/696f16bc-efdc-4a36-b94c-b0739cc1c095"" --unshare_namespace_mnt=""false""'
I1214 05:04:11.674006 24117 linux_launcher.cpp:429] Launching container 696f16bc-efdc-4a36-b94c-b0739cc1c095 and cloning with namespaces CLONE_NEWNS | CLONE_NEWUTS
I1214 05:04:11.677374 24116 cni.cpp:844] Bind mounted '/proc/11118/ns/net' to '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095/ns' for container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.677500 24116 cni.cpp:1173] Invoking CNI plugin '/mnt/teamcity/temp/buildTmp/qEaG50/plugins/mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}' to attach container 696f16bc-efdc-4a36-b94c-b0739cc1c095 to network '__MESOS_TEST__'
I1214 05:04:11.742821 24122 cni.cpp:1260] Got assigned IPv4 address '172.17.0.1/16' from CNI network '__MESOS_TEST__' for container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.743064 24116 cni.cpp:969] DNS nameservers for container 696f16bc-efdc-4a36-b94c-b0739cc1c095 are:
nameserver 172.16.10.2
I1214 05:04:11.843673 24119 fetcher.cpp:349] Starting to fetch URIs for container: 696f16bc-efdc-4a36-b94c-b0739cc1c095, directory: /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.876816 11170 exec.cpp:162] Version: 1.2.0
I1214 05:04:11.879708 24118 slave.cpp:3314] Got registration for executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from executor(1)@172.17.0.1:34346
I1214 05:04:11.880328 11169 exec.cpp:237] Executor registered on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.880398 24121 slave.cpp:2256] Sending queued task '80ddad17-5693-4883-a79d-1925a7b41661' to executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 at executor(1)@172.17.0.1:34346
Received SUBSCRIBED event
Subscribed executor on ip-172-16-10-182.mesosphere.io
Received LAUNCH event
Starting task 80ddad17-5693-4883-a79d-1925a7b41661
/mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer launch --help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""\n      #!\/bin\/sh\n      if [ x\""$LIBPROCESS_IP\"" == x\""0.0.0.0\"" ]; then\n        exit 0\n      else\n        exit 1\n      fi""}}"" --unshare_namespace_mnt=""false""
Forked command at 11172
I1214 05:04:11.884364 24116 slave.cpp:3749] Handling status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from executor(1)@172.17.0.1:34346
I1214 05:04:11.884829 24121 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.884848 24121 status_update_manager.cpp:500] Creating StatusUpdate stream for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.884946 24121 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to the agent
I1214 05:04:11.885074 24120 slave.cpp:4190] Forwarding the update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to master@172.16.10.182:60883
I1214 05:04:11.885150 24120 slave.cpp:4084] Status update manager successfully handled status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.885172 24120 slave.cpp:4100] Sending acknowledgement for status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to executor(1)@172.17.0.1:34346
I1214 05:04:11.885200 24118 master.cpp:5769] Status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.885229 24118 master.cpp:5831] Forwarding status update TASK_RUNNING (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.885284 24118 master.cpp:7867] Updating the state of task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1214 05:04:11.885409 24117 sched.cpp:1031] Scheduler::statusUpdate took 63040ns
I1214 05:04:11.885541 24122 master.cpp:4877] Processing ACKNOWLEDGE call 32ed996d-9fc5-4e82-afb1-d7936670af16 for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883 on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.885753 24119 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.885821 24119 slave.cpp:3031] Status update manager successfully handled status update acknowledgement (UUID: 32ed996d-9fc5-4e82-afb1-d7936670af16) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
sh: 3: [: x0.0.0.0: unexpected operator
Command exited with status 1 (pid: 11172)
I1214 05:04:11.978447 24119 slave.cpp:3749] Handling status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from executor(1)@172.17.0.1:34346
I1214 05:04:11.979202 24122 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979249 24122 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to the agent
I1214 05:04:11.979321 24119 slave.cpp:4190] Forwarding the update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to master@172.16.10.182:60883
I1214 05:04:11.979404 24119 slave.cpp:4084] Status update manager successfully handled status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979425 24119 slave.cpp:4100] Sending acknowledgement for status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 to executor(1)@172.17.0.1:34346
I1214 05:04:11.979462 24121 master.cpp:5769] Status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 from agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.979482 24121 master.cpp:5831] Forwarding status update TASK_FAILED (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979528 24121 master.cpp:7867] Updating the state of task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
I1214 05:04:11.979662 24120 sched.cpp:1031] Scheduler::statusUpdate took 66057ns
I1214 05:04:11.979784 24116 hierarchical.cpp:1023] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {}) on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 from framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
../../src/tests/containerizer/cni_isolator_tests.cpp:601: Failure
Value of: statusFinished->state()
  Actual: TASK_FAILED
Expected: TASK_FINISHED
I1214 05:04:11.979811 24123 master.cpp:4877] Processing ACKNOWLEDGE call 361b2ed5-7dee-4c96-854a-585c78d25532 for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883 on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
I1214 05:04:11.979845 24102 sched.cpp:2008] Asked to stop the driver
I1214 05:04:11.979832 24123 master.cpp:7963] Removing task 80ddad17-5693-4883-a79d-1925a7b41661 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 on agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:11.979892 24121 sched.cpp:1193] Stopping framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.979966 24122 master.cpp:7287] Processing TEARDOWN call for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.979981 24122 master.cpp:7299] Removing framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 (default) at scheduler-dfbb45a6-1912-4912-acac-b4fb87c28181@172.16.10.182:60883
I1214 05:04:11.979990 24120 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980049 24120 status_update_manager.cpp:531] Cleaning up status update stream for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980075 24123 hierarchical.cpp:391] Deactivated framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980115 24121 slave.cpp:2584] Asked to shut down framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 by master@172.16.10.182:60883
I1214 05:04:11.980144 24121 slave.cpp:2609] Shutting down framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980160 24121 slave.cpp:4999] Shutting down executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 at executor(1)@172.17.0.1:34346
I1214 05:04:11.980175 24118 hierarchical.cpp:342] Removed framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980267 24121 slave.cpp:3031] Status update manager successfully handled status update acknowledgement (UUID: 361b2ed5-7dee-4c96-854a-585c78d25532) for task 80ddad17-5693-4883-a79d-1925a7b41661 of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:11.980285 24121 slave.cpp:6719] Completing task 80ddad17-5693-4883-a79d-1925a7b41661
I1214 05:04:11.980413 11170 exec.cpp:414] Executor asked to shutdown
Received SHUTDOWN event
Shutting down
I1214 05:04:11.980662 24120 containerizer.cpp:2113] Destroying container 696f16bc-efdc-4a36-b94c-b0739cc1c095 in RUNNING state
I1214 05:04:11.980731 24120 linux_launcher.cpp:505] Asked to destroy container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.981130 24120 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.981762 24117 cgroups.cpp:2726] Freezing cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.982657 24123 cgroups.cpp:1439] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095 after 869120ns
I1214 05:04:11.983755 24117 cgroups.cpp:2744] Thawing cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:11.984719 24118 cgroups.cpp:1468] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/696f16bc-efdc-4a36-b94c-b0739cc1c095 after 944128ns
I1214 05:04:11.999498 24117 slave.cpp:4318] Got exited event for executor(1)@172.17.0.1:34346
I1214 05:04:12.043917 24120 containerizer.cpp:2476] Container 696f16bc-efdc-4a36-b94c-b0739cc1c095 has exited
I1214 05:04:12.044694 24119 cni.cpp:1511] Invoking CNI plugin '/mnt/teamcity/temp/buildTmp/qEaG50/plugins/mockPlugin' with network configuration '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095/__MESOS_TEST__/network.conf' to detach container 696f16bc-efdc-4a36-b94c-b0739cc1c095 from network '__MESOS_TEST__'
I1214 05:04:12.151413 24123 cni.cpp:1438] Unmounted the network namespace handle '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095/ns' for container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:12.151480 24123 cni.cpp:1449] Removed the container directory '/run/mesos/isolators/network/cni/696f16bc-efdc-4a36-b94c-b0739cc1c095'
I1214 05:04:12.151793 24116 provisioner.cpp:324] Ignoring destroy request for unknown container 696f16bc-efdc-4a36-b94c-b0739cc1c095
I1214 05:04:12.152027 24117 slave.cpp:4681] Executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 terminated with signal Killed
I1214 05:04:12.152050 24117 slave.cpp:4785] Cleaning up executor '80ddad17-5693-4883-a79d-1925a7b41661' of framework cf36e3a9-60fb-4885-8145-831ca6998263-0000 at executor(1)@172.17.0.1:34346
I1214 05:04:12.152204 24121 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661/runs/696f16bc-efdc-4a36-b94c-b0739cc1c095' for gc 6.99999823931852days in the future
I1214 05:04:12.152215 24117 slave.cpp:4873] Cleaning up framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:12.152261 24121 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000/executors/80ddad17-5693-4883-a79d-1925a7b41661' for gc 6.99999823841481days in the future
I1214 05:04:12.152300 24116 status_update_manager.cpp:285] Closing status update streams for framework cf36e3a9-60fb-4885-8145-831ca6998263-0000
I1214 05:04:12.152401 24117 slave.cpp:796] Agent terminating
I1214 05:04:12.152410 24122 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_EnvironmentLibprocessIP_dqz8kg/slaves/cf36e3a9-60fb-4885-8145-831ca6998263-S0/frameworks/cf36e3a9-60fb-4885-8145-831ca6998263-0000' for gc 6.99999823711704days in the future
I1214 05:04:12.152472 24122 master.cpp:1258] Agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io) disconnected
I1214 05:04:12.152488 24122 master.cpp:2977] Disconnecting agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:12.152509 24122 master.cpp:2996] Deactivating agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 at slave(576)@172.16.10.182:60883 (ip-172-16-10-182.mesosphere.io)
I1214 05:04:12.152565 24122 hierarchical.cpp:589] Agent cf36e3a9-60fb-4885-8145-831ca6998263-S0 deactivated
I1214 05:04:12.154162 24102 master.cpp:1097] Master terminating
I1214 05:04:12.154306 24118 hierarchical.cpp:522] Removed agent cf36e3a9-60fb-4885-8145-831ca6998263-S0
[  FAILED  ] CniIsolatorTest.ROOT_EnvironmentLibprocessIP (503 ms)
{noformat}
"	MESOS	Resolved	3	1	2531	mesosphere
12901406	Replace use of strerror with thread-safe alternatives strerror_r / strerror_l.	"{{strerror()}} is not required to be thread safe by POSIX and is listed as unsafe on Linux:

http://pubs.opengroup.org/onlinepubs/9699919799/
http://man7.org/linux/man-pages/man3/strerror.3.html

I don't believe we've seen any issues reported due to this. We should replace occurrences of strerror accordingly, possibly offering a wrapper in stout to simplify callsites."	MESOS	Resolved	3	1	2531	mesosphere, newbie, tech-debt
13219080	Consider running tox as part of test suite, not as part of style checking	"Currently {{tox}} is being run as part of {{support/mesos-style.py}}. This is unusual as {{tox}} is a tool to run tests, extract code coverage metrics and similar tasks.

We should consider running it as part of the test suite instead of being part of {{mesos-style.py}}. This might also simplify some of the installation challenges to current style checking setup has."	MESOS	Resolved	3	4	2531	newbie++
12975603	Add support for movable types inside Option/Try	We should add support for {{Option}}/{{Try}} of movable {{T}}s.	MESOS	Resolved	3	4	2531	c++11, mesosphere
13109469	ResourceProviderRegistrarTest.AgentRegistrar is flaky.	Observed it in internal CI. Test log attached.	MESOS	Resolved	3	1	2531	flaky-test, mesosphere
13201768	Completed framework update streams may retry forever	"Since the agent/RP currently does not GC operation status update streams when frameworks are torn down, it's possible that active update streams associated with completed frameworks may remain and continue retrying forever. We should add a mechanism to complete these streams when the framework becomes completed.

A couple options which have come up during discussion:
* Have the master acknowledge updates associated with completed frameworks. Note that since completed frameworks are currently only tracked by the master in memory, a master failover could prevent this from working perfectly.
* Extend the RP API to allow the GC of particular update streams, and have the agent GC streams associated with a framework when it receives a {{ShutdownFrameworkMessage}}. This would also require the addition of a new method to the status update manager."	MESOS	Resolved	3	1	2531	mesosphere
13016610	Parallel test running does not respect GTEST_FILTER	Normally, you can use {{GTEST_FILTER}} to control which tests will be run by {{make check}}. However, this doesn't currently work if Mesos is configured with {{--enable-parallel-test-execution}}.	MESOS	Resolved	3	1	2531	mesosphere
12957624	Add authorization to agent's /monitor/statistics endpoint.	"Operators may want to enforce that only specific authorized users be able to view per-executor resource usage statistics. For 0.29 MVP, we can make this coarse-grained, and assume that only the operator or a operator-privileged monitoring service will be accessing the endpoint.
For a future release, we can consider fine-grained authz that filters statistics like we plan to do for /tasks."	MESOS	Resolved	3	3	2531	authorization, mesosphere, security
13004524	Resource leak in slave.cpp.	"Coverity detected the following resource leak:
{code}
1. Condition this->queuedTasks.contains(taskId), taking true branch.
6547  if (queuedTasks.contains(taskId)) {
    	2. Condition terminal, taking true branch.
6548    if (terminal) {
    	3. alloc_fn: Storage is returned from allocation function operator new. [Note: The source code implementation of the function has been overridden by a builtin model.]
    	4. var_assign: Assigning: task = storage returned from new mesos::Task(mesos::internal::protobuf::createTask(this->queuedTasks.at(taskId), mesos::TaskState const(status->state()), this->frameworkId)).
6549      Task* task = new Task(protobuf::createTask(
6550          queuedTasks.at(taskId),
6551          status.state(),
6552          frameworkId));
6553
6554      queuedTasks.erase(taskId);
6555
6556      // This might be a queued task belonging to a task group.
6557      // If so, we need to update the other tasks belonging to this task group.
6558      Option<TaskGroupInfo> taskGroup = getQueuedTaskGroup(taskId);
6559
    	5. Condition taskGroup.isSome(), taking true branch.
6560      if (taskGroup.isSome()) {
    	6. No elements left in taskGroup->tasks(), leaving loop.
6561        foreach (const TaskInfo& task_, taskGroup->tasks()) {
6562          Task* task = new Task(
6563              protobuf::createTask(task_, status.state(), frameworkId));
6564
6565          tasks.push_back(task);
6566        }
    	7. Falling through to end of if statement.
6567      } else {
6568        tasks.push_back(task);
6569      }
    	
CID 1372871 (#1 of 1): Resource leak (RESOURCE_LEAK)
8. leaked_storage: Variable task going out of scope leaks the storage it points to.
6570    } else {
6571      return Error(""Cannot send non-terminal update for queued task"");
{code}

https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881751&defectInstanceId=28450463"	MESOS	Resolved	3	1	2531	coverity, mesosphere
13209696	Add per-framework allocatable resources matcher/filter.	"Currently, Mesos has a single global flag `min_allocatable_resources` that provides some control over the shape of the offer. But, being a global flag, finding a one-size-fits-all shape is hard and less than ideal. It will be great if frameworks can specify different shapes based on their needs. 

In addition to extending this flag to be per-framework. It is also a good opportunity to see if it can be more than `min_alloctable` e.g. providing more predicates such as max,  (not) contain and etc. "	MESOS	Resolved	3	4	2531	mesosphere, storage
12959802	Add tests for Capability API.	Add basic tests for the capability API.	MESOS	Resolved	3	3	2531	mesosphere, unified-containerizer-mvp
13021707	Reject optimized builds with libcxx before 3.9	"Recent clang versions optimize more aggressively which leads to runtime errors using valid code, see e.g., MESOS-5745, due to code exposing undefined behavior in libcxx-3.8 and earlier. This was fixed with upstream libcxx-3.9. See https://reviews.llvm.org/D20786 for the patch and https://llvm.org/bugs/show_bug.cgi?id=28469 for the code example extracted from our code base.

We should consider rejecting builds if libcxx-3.8 or older is detected since not all users compiling Mesos might run the test suite. In our decision to reject we could possibly also take the used clang versions into account (which would just ensure we don't run into the known problems from the UB in libcxx)."	MESOS	Resolved	3	1	2531	newbie
12926919	Replace variadic List constructor with one taking a initializer_list	"{{List}} provides a variadic constructor currently implemented with some preprocessor magic. Given that we already require C++11 we can replace that one with a much simpler one just taking a {{std::initializer_list}}. This would change the invocations,
{code}
auto l1 = List<int>(1, 2, 3);    // now
auto l2 = List<int>({1, 2, 3});  // proposed
{code}

This addresses an existing {{TODO}}.
"	MESOS	Resolved	4	4	2531	mesosphere, tech-debt
12860537	FlagsBase copy-ctor leads to dangling pointer.	"Per [#3328], ubsan detects the following problem:

[ RUN ] FaultToleranceTest.ReregisterCompletedFrameworks
/mesos/3rdparty/libprocess/3rdparty/stout/include/stout/flags/flags.hpp:303:25: runtime error: load of value 33, which is not a valid value for type 'bool'

I believe what is going on here is the following:
* The test calls StartMaster(), which does MesosTest::CreateMasterFlags()
* MesosTest::CreateMasterFlags() allocates a new master::Flags on the stack, which is subsequently copy-constructed back to StartMaster()
* The FlagsBase constructor is:
bq. {{FlagsBase() { add(&help, ""help"", ""..."", false); }}}
where ""help"" is a member variable -- i.e., it is allocated on the stack in this case.
* {{FlagsBase()::add}} captures {{&help}}, e.g.:
{noformat}
flag.stringify = [t1](const FlagsBase&) -> Option<std::string> {
    return stringify(*t1);
  };}}
{noformat}
* The implicit copy constructor for FlagsBase is just going to copy the lambda above, i.e., the result of the copy constructor will have a lambda that points into MesosTest::CreateMasterFlags()'s stack frame, which is bad news.

Not sure the right fix -- comments welcome. You could define a copy-ctor for FlagsBase that does something gross (basically remove the old help flag and define a new one that points into the target of the copy), but that seems, well, gross.

Probably not a pressing-problem to fix -- AFAICS worst symptom is that we end up reading one byte from some random stack location when serving {{state.json}}, for example."	MESOS	Resolved	3	1	2531	mesosphere
13059312	Update Resource proto for storage resource providers.	"Storage resource provider support requires a number of changes to the {{Resource}} proto:

* support for {{RAW}} and {{BLOCK}} type {{Resource::DiskInfo::Source}}
* {{ResourceProviderID}} in Resource
* {{Resource::DiskInfo::Source::Path}} should be {{optional}}."	MESOS	Resolved	3	1	2531	storage
13128339	Master should bookkeep local resource providers.	This will simplify the handling of `UpdateSlaveMessage`. ALso, it'll simplify the endpoint serving.	MESOS	Resolved	3	3	2531	mesosphere, storage
13237761	Agent recovery code for task draining	In the case where the agent crashes while it's in the process of killing tasks due to agent draining, it must recover the checkpointed {{DrainInfo}} and kill any tasks which did not have KILL events sent to them already.	MESOS	Resolved	3	3	2531	foundations, mesosphere
13019152	The default stout stringify always copies its argument	"The default implementation of the template {{stringify}} in stout always copies its argument,
{code}
template <typename T> std::string stringify(T t)
{code}

For most types implementing a dedicated {{stringify}} we restrict {{T}} to some {{const}} ref with the exception of the specialization for {{bool}},
{code}
template <> std::string stringify(const bool& b)
{code}

Copying by default is bad since it requires {{T}} to be copyable without {{stringify}} actually requiring this. It also likely leads to bad performance.
It appears switching to e.g.,
{code}
template <typename T> std::string stringify(const T& t)
{code}
and adjusting the {{bool}} specialization would be a general improvement.

This issue was first detected by Coverity in CID 727974 way back on 2012-09-21."	MESOS	Resolved	3	1	2531	coverity, tech-debt
13128819	Resource provider manager should persist resource provider information	"Currently, the resource provider manager used to abstract away resource provider subscription and state does not persist resource provider information. It has no notion of e.g., disconnected or forcibly removed resource providers. This makes it hard to implement a number of features, e.g.,

* removal of a resource provider and make it possible to garbage collect its cached state (e.g., in the resource provider manager, agent, or master), or
* controlling resource provider resubscription, e.g., by observing and enforcing resubscription timeouts.

We should extend the resource provider manager to persist the state of each resource provider (e.g., {{CONNECTED}}, {{DISCONNECTED}}, its resources and other attributes). This information should also be exposed in resource provider reconciliation, and be reflected in master or agent endpoints."	MESOS	Resolved	3	4	2531	mesosphere
13026392	DefaultExecutorTest.KillTaskGroupOnTaskFailure is flaky	"This repros consistently for me (~10 test iterations or fewer). Test log:

{noformat}
[ RUN      ] DefaultExecutorTest.KillTaskGroupOnTaskFailure
I1208 03:26:47.461477 28632 cluster.cpp:160] Creating default 'local' authorizer
I1208 03:26:47.462673 28632 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1208 03:26:47.463248 28650 recover.cpp:451] Starting replica recovery
I1208 03:26:47.463537 28650 recover.cpp:477] Replica is in EMPTY status
I1208 03:26:47.476333 28651 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(64)@10.0.2.15:46643
I1208 03:26:47.476618 28650 recover.cpp:197] Received a recover response from a replica in EMPTY status
I1208 03:26:47.477242 28649 recover.cpp:568] Updating replica status to STARTING
I1208 03:26:47.477496 28649 replica.cpp:320] Persisted replica status to STARTING
I1208 03:26:47.477607 28649 recover.cpp:477] Replica is in STARTING status
I1208 03:26:47.478910 28653 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(65)@10.0.2.15:46643
I1208 03:26:47.479385 28651 recover.cpp:197] Received a recover response from a replica in STARTING status
I1208 03:26:47.479717 28647 recover.cpp:568] Updating replica status to VOTING
I1208 03:26:47.479996 28648 replica.cpp:320] Persisted replica status to VOTING
I1208 03:26:47.480077 28648 recover.cpp:582] Successfully joined the Paxos group
I1208 03:26:47.763380 28651 master.cpp:380] Master 0bcb0250-4cf5-4209-92fe-ce260518b50f (archlinux.vagrant.vm) started on 10.0.2.15:46643
I1208 03:26:47.763463 28651 master.cpp:382] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/7lpy50/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/7lpy50/master"" --zk_session_timeout=""10secs""
I1208 03:26:47.764010 28651 master.cpp:432] Master only allowing authenticated frameworks to register
I1208 03:26:47.764070 28651 master.cpp:446] Master only allowing authenticated agents to register
I1208 03:26:47.764076 28651 master.cpp:459] Master only allowing authenticated HTTP frameworks to register
I1208 03:26:47.764081 28651 credentials.hpp:37] Loading credentials for authentication from '/tmp/7lpy50/credentials'
I1208 03:26:47.764482 28651 master.cpp:504] Using default 'crammd5' authenticator
I1208 03:26:47.764659 28651 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I1208 03:26:47.764981 28651 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I1208 03:26:47.765136 28651 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I1208 03:26:47.765231 28651 master.cpp:584] Authorization enabled
I1208 03:26:47.768061 28651 master.cpp:2043] Elected as the leading master!
I1208 03:26:47.768097 28651 master.cpp:1566] Recovering from registrar
I1208 03:26:47.768766 28648 log.cpp:553] Attempting to start the writer
I1208 03:26:47.769899 28653 replica.cpp:493] Replica received implicit promise request from __req_res__(66)@10.0.2.15:46643 with proposal 1
I1208 03:26:47.769984 28653 replica.cpp:342] Persisted promised to 1
I1208 03:26:47.770534 28652 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1208 03:26:47.771479 28652 replica.cpp:388] Replica received explicit promise request from __req_res__(67)@10.0.2.15:46643 for position 0 with proposal 2
I1208 03:26:47.772897 28650 replica.cpp:537] Replica received write request for position 0 from __req_res__(68)@10.0.2.15:46643
I1208 03:26:47.773437 28650 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1208 03:26:47.774327 28650 log.cpp:569] Writer started with ending position 0
I1208 03:26:47.776505 28647 registrar.cpp:362] Successfully fetched the registry (0B) in 8.211712ms
I1208 03:26:47.776597 28647 registrar.cpp:461] Applied 1 operations in 11511ns; attempting to update the registry
I1208 03:26:47.777253 28653 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1208 03:26:47.778172 28648 replica.cpp:537] Replica received write request for position 1 from __req_res__(69)@10.0.2.15:46643
I1208 03:26:47.778695 28646 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1208 03:26:47.779631 28652 registrar.cpp:506] Successfully updated the registry in 2.979072ms
I1208 03:26:47.779736 28652 registrar.cpp:392] Successfully recovered registrar
I1208 03:26:47.779911 28652 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1208 03:26:47.780030 28653 master.cpp:1682] Recovered 0 agents from the registry (145B); allowing 10mins for agents to re-register
I1208 03:26:47.788097 28648 replica.cpp:537] Replica received write request for position 2 from __req_res__(70)@10.0.2.15:46643
I1208 03:26:47.788931 28651 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1208 03:26:47.844846 28632 containerizer.cpp:207] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W1208 03:26:47.845237 28632 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I1208 03:26:47.846787 28632 cluster.cpp:446] Creating default 'local' authorizer
I1208 03:26:47.848178 28647 slave.cpp:208] Mesos agent started on (8)@10.0.2.15:46643
I1208 03:26:47.848201 28647 slave.cpp:209] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --io_switchboard_enable_server=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/home/vagrant/build-mesos/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_M0FOoo""
I1208 03:26:47.848474 28647 credentials.hpp:86] Loading credential for authentication from '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL/credential'
I1208 03:26:47.848573 28647 slave.cpp:346] Agent using credential for: test-principal
I1208 03:26:47.848587 28647 credentials.hpp:37] Loading credentials for authentication from '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL/http_credentials'
I1208 03:26:47.848707 28647 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I1208 03:26:47.848764 28632 scheduler.cpp:182] Version: 1.2.0
I1208 03:26:47.921869 28647 slave.cpp:533] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1208 03:26:47.921993 28647 slave.cpp:541] Agent attributes: [  ]
I1208 03:26:47.922003 28647 slave.cpp:546] Agent hostname: archlinux.vagrant.vm
I1208 03:26:47.923415 28649 state.cpp:57] Recovering state from '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_M0FOoo/meta'
I1208 03:26:47.923770 28651 status_update_manager.cpp:203] Recovering status update manager
I1208 03:26:47.924353 28650 containerizer.cpp:581] Recovering containerizer
I1208 03:26:47.925879 28651 provisioner.cpp:253] Provisioner recovery complete
I1208 03:26:47.926267 28649 slave.cpp:5414] Finished recovery
I1208 03:26:47.926981 28646 slave.cpp:918] New master detected at master@10.0.2.15:46643
I1208 03:26:47.927004 28648 status_update_manager.cpp:177] Pausing sending status updates
I1208 03:26:47.927008 28646 slave.cpp:977] Authenticating with master master@10.0.2.15:46643
I1208 03:26:47.927127 28646 slave.cpp:988] Using default CRAM-MD5 authenticatee
I1208 03:26:47.927259 28646 slave.cpp:950] Detecting new master
I1208 03:26:47.927393 28651 authenticatee.cpp:121] Creating new client SASL connection
I1208 03:26:47.927543 28651 master.cpp:6793] Authenticating slave(8)@10.0.2.15:46643
I1208 03:26:47.927907 28649 authenticator.cpp:98] Creating new server SASL connection
I1208 03:26:47.928110 28648 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1208 03:26:47.928138 28648 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1208 03:26:47.928225 28648 authenticator.cpp:204] Received SASL authentication start
I1208 03:26:47.928256 28648 authenticator.cpp:326] Authentication requires more steps
I1208 03:26:47.928315 28648 authenticatee.cpp:259] Received SASL authentication step
I1208 03:26:47.928390 28648 authenticator.cpp:232] Received SASL authentication step
I1208 03:26:47.928442 28648 authenticator.cpp:318] Authentication success
I1208 03:26:47.928549 28651 authenticatee.cpp:299] Authentication success
I1208 03:26:47.928560 28648 master.cpp:6823] Successfully authenticated principal 'test-principal' at slave(8)@10.0.2.15:46643
I1208 03:26:47.928917 28648 slave.cpp:1072] Successfully authenticated with master master@10.0.2.15:46643
I1208 03:26:47.929293 28648 master.cpp:5202] Registering agent at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm) with id 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0
I1208 03:26:47.929668 28652 registrar.cpp:461] Applied 1 operations in 37939ns; attempting to update the registry
I1208 03:26:47.930258 28647 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1208 03:26:47.931262 28650 replica.cpp:537] Replica received write request for position 3 from __req_res__(71)@10.0.2.15:46643
I1208 03:26:47.931882 28650 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1208 03:26:47.932858 28650 registrar.cpp:506] Successfully updated the registry in 3.147008ms
I1208 03:26:47.933434 28650 master.cpp:5273] Registered agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1208 03:26:47.933506 28650 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1208 03:26:47.933869 28650 hierarchical.cpp:485] Added agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 (archlinux.vagrant.vm) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I1208 03:26:47.934068 28650 slave.cpp:1118] Registered with master master@10.0.2.15:46643; given agent ID 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0
I1208 03:26:47.934249 28650 slave.cpp:1178] Forwarding total oversubscribed resources {}
I1208 03:26:47.934497 28650 status_update_manager.cpp:184] Resuming sending status updates
I1208 03:26:47.934567 28650 master.cpp:5672] Received update of agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm) with total oversubscribed resources {}
I1208 03:26:47.934734 28650 hierarchical.cpp:555] Agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 (archlinux.vagrant.vm) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {})
I1208 03:26:47.935107 28650 replica.cpp:537] Replica received write request for position 4 from __req_res__(72)@10.0.2.15:46643
I1208 03:26:47.935642 28650 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1208 03:26:50.427475 28648 scheduler.cpp:475] New master detected at master@10.0.2.15:46643
I1208 03:26:50.435571 28648 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:50358
I1208 03:26:50.435753 28648 master.cpp:2340] Received subscription request for HTTP framework 'default'
I1208 03:26:50.435832 28648 master.cpp:2079] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1208 03:26:50.436213 28647 master.cpp:2454] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1208 03:26:50.436691 28651 hierarchical.cpp:275] Added framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:50.437602 28647 master.cpp:6622] Sending 1 offers to framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (default)
I1208 03:26:50.442335 28653 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:50356
I1208 03:26:50.442852 28653 master.cpp:3629] Processing ACCEPT call for offers: [ 0bcb0250-4cf5-4209-92fe-ce260518b50f-O0 ] on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm) for framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (default)
I1208 03:26:50.442912 28653 master.cpp:3216] Authorizing framework principal 'test-principal' to launch task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5
I1208 03:26:50.443050 28653 master.cpp:3216] Authorizing framework principal 'test-principal' to launch task bf21fae2-513e-4ea1-b85c-dfd2546e4249
I1208 03:26:50.445271 28653 master.cpp:8424] Adding task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 (archlinux.vagrant.vm)
I1208 03:26:50.445487 28653 master.cpp:8424] Adding task bf21fae2-513e-4ea1-b85c-dfd2546e4249 with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 (archlinux.vagrant.vm)
I1208 03:26:50.445565 28653 master.cpp:4486] Launching task group { 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, bf21fae2-513e-4ea1-b85c-dfd2546e4249 } of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (default) with resources cpus(*):0.2; mem(*):64; disk(*):64 on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm)
I1208 03:26:50.446413 28653 slave.cpp:1550] Got assigned task group containing tasks [ 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, bf21fae2-513e-4ea1-b85c-dfd2546e4249 ] for framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:50.446836 28653 slave.cpp:1712] Launching task group containing tasks [ 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, bf21fae2-513e-4ea1-b85c-dfd2546e4249 ] for framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:50.446993 28653 paths.cpp:530] Trying to chown '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_M0FOoo/slaves/0bcb0250-4cf5-4209-92fe-ce260518b50f-S0/frameworks/0bcb0250-4cf5-4209-92fe-ce260518b50f-0000/executors/default/runs/725fe374-0d1e-4d9f-b1b0-e5ffb16b1101' to user 'vagrant'
I1208 03:26:50.451642 28653 slave.cpp:6341] Launching executor 'default' of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 with resources cpus(*):0.1; mem(*):32; disk(*):32 in work directory '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_M0FOoo/slaves/0bcb0250-4cf5-4209-92fe-ce260518b50f-S0/frameworks/0bcb0250-4cf5-4209-92fe-ce260518b50f-0000/executors/default/runs/725fe374-0d1e-4d9f-b1b0-e5ffb16b1101'
I1208 03:26:50.452117 28652 containerizer.cpp:973] Starting container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101 for executor 'default' of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:50.452283 28653 slave.cpp:2034] Queued task group containing tasks [ 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, bf21fae2-513e-4ea1-b85c-dfd2546e4249 ] for executor 'default' of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:50.456531 28647 launcher.cpp:133] Forked child with pid '29159' for container '725fe374-0d1e-4d9f-b1b0-e5ffb16b1101'
I1208 03:26:53.105209 29173 executor.cpp:189] Version: 1.2.0
I1208 03:26:53.112563 28646 http.cpp:288] HTTP POST for /slave(8)/api/v1/executor from 10.0.2.15:50360
I1208 03:26:53.112725 28646 slave.cpp:3089] Received Subscribe request for HTTP executor 'default' of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.114614 28651 slave.cpp:2279] Sending queued task group task group containing tasks [ 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, bf21fae2-513e-4ea1-b85c-dfd2546e4249 ] to executor 'default' of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (via HTTP)
I1208 03:26:53.117642 29191 default_executor.cpp:131] Received SUBSCRIBED event
I1208 03:26:53.122820 29191 default_executor.cpp:135] Subscribed executor on archlinux.vagrant.vm
I1208 03:26:53.123080 29191 default_executor.cpp:131] Received LAUNCH_GROUP event
I1208 03:26:53.126833 28653 http.cpp:288] HTTP POST for /slave(8)/api/v1 from 10.0.2.15:50364
I1208 03:26:53.127091 28653 http.cpp:288] HTTP POST for /slave(8)/api/v1 from 10.0.2.15:50364
I1208 03:26:53.127477 28653 http.cpp:449] Processing call LAUNCH_NESTED_CONTAINER
I1208 03:26:53.127671 28653 http.cpp:449] Processing call LAUNCH_NESTED_CONTAINER
I1208 03:26:53.128360 28653 containerizer.cpp:1776] Starting nested container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8
I1208 03:26:53.128434 28653 containerizer.cpp:1800] Trying to chown '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_M0FOoo/slaves/0bcb0250-4cf5-4209-92fe-ce260518b50f-S0/frameworks/0bcb0250-4cf5-4209-92fe-ce260518b50f-0000/executors/default/runs/725fe374-0d1e-4d9f-b1b0-e5ffb16b1101/containers/a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8' to user 'vagrant'
I1208 03:26:53.134392 28653 containerizer.cpp:1776] Starting nested container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7
I1208 03:26:53.134567 28653 containerizer.cpp:1800] Trying to chown '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_M0FOoo/slaves/0bcb0250-4cf5-4209-92fe-ce260518b50f-S0/frameworks/0bcb0250-4cf5-4209-92fe-ce260518b50f-0000/executors/default/runs/725fe374-0d1e-4d9f-b1b0-e5ffb16b1101/containers/855fae95-810b-4e9d-8397-7138bdda91b7' to user 'vagrant'
I1208 03:26:53.142004 28653 launcher.cpp:133] Forked child with pid '29198' for container '725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8'
I1208 03:26:53.144634 28653 launcher.cpp:133] Forked child with pid '29199' for container '725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7'
I1208 03:26:53.152432 29187 default_executor.cpp:452] Successfully launched child containers [ 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8, 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7 ] for tasks [ 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, bf21fae2-513e-4ea1-b85c-dfd2546e4249 ]
I1208 03:26:53.154485 29189 default_executor.cpp:528] Waiting for child container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8 of task '3a8a0c1c-c386-409d-a21c-653dc2d3d7d5'
I1208 03:26:53.154712 29189 default_executor.cpp:528] Waiting for child container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7 of task 'bf21fae2-513e-4ea1-b85c-dfd2546e4249'
I1208 03:26:53.155655 28647 http.cpp:288] HTTP POST for /slave(8)/api/v1/executor from 10.0.2.15:50362
I1208 03:26:53.155807 28647 slave.cpp:3743] Handling status update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.156080 28647 http.cpp:288] HTTP POST for /slave(8)/api/v1/executor from 10.0.2.15:50362
I1208 03:26:53.156163 28647 slave.cpp:3743] Handling status update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.157243 28647 http.cpp:288] HTTP POST for /slave(8)/api/v1 from 10.0.2.15:50368
I1208 03:26:53.157467 28647 http.cpp:288] HTTP POST for /slave(8)/api/v1 from 10.0.2.15:50366
I1208 03:26:53.157699 28647 http.cpp:449] Processing call WAIT_NESTED_CONTAINER
I1208 03:26:53.157966 28647 http.cpp:449] Processing call WAIT_NESTED_CONTAINER
I1208 03:26:53.158440 28652 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.159534 28647 slave.cpp:4184] Forwarding the update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 to master@10.0.2.15:46643
I1208 03:26:53.159840 28651 master.cpp:5808] Status update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 from agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm)
I1208 03:26:53.159881 28651 master.cpp:5870] Forwarding status update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.159986 28647 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.160099 28651 master.cpp:7790] Updating the state of task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1208 03:26:53.160365 28647 slave.cpp:4184] Forwarding the update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 to master@10.0.2.15:46643
I1208 03:26:53.160670 28647 master.cpp:5808] Status update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 from agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm)
I1208 03:26:53.160711 28647 master.cpp:5870] Forwarding status update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.160899 28647 master.cpp:7790] Updating the state of task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1208 03:26:53.162302 29190 default_executor.cpp:131] Received ACKNOWLEDGED event
I1208 03:26:53.162602 29188 default_executor.cpp:131] Received ACKNOWLEDGED event
I1208 03:26:53.213343 28653 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:50356
I1208 03:26:53.213508 28653 master.cpp:4918] Processing ACKNOWLEDGE call bfb80b10-da9b-44d2-977a-61b88531e809 for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (default) on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0
I1208 03:26:53.213838 28648 status_update_manager.cpp:395] Received status update acknowledgement (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
W1208 03:26:53.213982 28648 status_update_manager.cpp:769] Unexpected status update acknowledgement (received bfb80b10-da9b-44d2-977a-61b88531e809, expecting dcdd2cb5-fdea-4556-94e9-ff6246132315) for update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.214042 28647 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:50356
E1208 03:26:53.214143 28648 slave.cpp:3018] Failed to handle status update acknowledgement (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000: Duplicate acknowledgement
I1208 03:26:53.214166 28647 master.cpp:4918] Processing ACKNOWLEDGE call dcdd2cb5-fdea-4556-94e9-ff6246132315 for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (default) on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0
I1208 03:26:53.214479 28653 status_update_manager.cpp:395] Received status update acknowledgement (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
W1208 03:26:53.214584 28653 status_update_manager.cpp:769] Unexpected status update acknowledgement (received dcdd2cb5-fdea-4556-94e9-ff6246132315, expecting bfb80b10-da9b-44d2-977a-61b88531e809) for update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
E1208 03:26:53.214701 28653 slave.cpp:3018] Failed to handle status update acknowledgement (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000: Duplicate acknowledgement
I1208 03:26:53.220249 28649 containerizer.cpp:2450] Container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8 has exited
I1208 03:26:53.220296 28649 containerizer.cpp:2087] Destroying container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8 in RUNNING state
I1208 03:26:53.220535 28649 launcher.cpp:149] Asked to destroy container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8
I1208 03:26:53.225530 28650 containerizer.cpp:2366] Checkpointing termination state to nested container's runtime directory '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL/containers/725fe374-0d1e-4d9f-b1b0-e5ffb16b1101/containers/a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8/termination'
I1208 03:26:53.228024 29188 default_executor.cpp:656] Successfully waited for child container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8 of task '3a8a0c1c-c386-409d-a21c-653dc2d3d7d5' in state TASK_FAILED
E1208 03:26:53.228068 29188 default_executor.cpp:667] Child container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.a532fb7f-fe4c-4588-b1c1-c45dee7fd9c8 terminated with status exited with status 1
I1208 03:26:53.228073 29188 default_executor.cpp:687] Shutting down
I1208 03:26:53.228792 29192 default_executor.cpp:781] Killing child container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7
I1208 03:26:53.230953 28653 http.cpp:288] HTTP POST for /slave(8)/api/v1 from 10.0.2.15:50370
I1208 03:26:53.231276 28653 http.cpp:449] Processing call KILL_NESTED_CONTAINER
I1208 03:26:53.231853 28652 containerizer.cpp:2087] Destroying container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7 in RUNNING state
I1208 03:26:53.232113 28652 launcher.cpp:149] Asked to destroy container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7
I1208 03:26:53.273080 28648 http.cpp:288] HTTP POST for /slave(8)/api/v1/executor from 10.0.2.15:50362
I1208 03:26:53.273331 28648 slave.cpp:3743] Handling status update TASK_FAILED (UUID: ff8338ce-58e5-4508-a2c1-0eb7580aa8f8) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.274930 28650 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: ff8338ce-58e5-4508-a2c1-0eb7580aa8f8) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.276623 29187 default_executor.cpp:131] Received ACKNOWLEDGED event
I1208 03:26:53.321367 28649 containerizer.cpp:2450] Container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7 has exited
I1208 03:26:53.322789 28649 containerizer.cpp:2366] Checkpointing termination state to nested container's runtime directory '/tmp/DefaultExecutorTest_KillTaskGroupOnTaskFailure_OQh5HL/containers/725fe374-0d1e-4d9f-b1b0-e5ffb16b1101/containers/855fae95-810b-4e9d-8397-7138bdda91b7/termination'
I1208 03:26:53.325847 29194 default_executor.cpp:656] Successfully waited for child container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101.855fae95-810b-4e9d-8397-7138bdda91b7 of task 'bf21fae2-513e-4ea1-b85c-dfd2546e4249' in state TASK_KILLED
I1208 03:26:53.325887 29194 default_executor.cpp:767] Terminating after 1secs
I1208 03:26:53.369621 28648 http.cpp:288] HTTP POST for /slave(8)/api/v1/executor from 10.0.2.15:50362
I1208 03:26:53.369870 28648 slave.cpp:3743] Handling status update TASK_KILLED (UUID: 07d27a3a-c58d-4c2e-8a8f-ee2e4900fb91) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:53.371409 28648 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: 07d27a3a-c58d-4c2e-8a8f-ee2e4900fb91) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:26:54.335306 28650 containerizer.cpp:2450] Container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101 has exited
I1208 03:26:54.335353 28650 containerizer.cpp:2087] Destroying container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101 in RUNNING state
I1208 03:26:54.335593 28650 launcher.cpp:149] Asked to destroy container 725fe374-0d1e-4d9f-b1b0-e5ffb16b1101
I1208 03:26:54.341533 28652 slave.cpp:4675] Executor 'default' of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 exited with status 0
I1208 03:26:54.341866 28651 master.cpp:5932] Executor 'default' of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm): exited with status 0
I1208 03:26:54.341914 28651 master.cpp:7915] Removing executor 'default' with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 on agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm)

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: failure(0x7ffec532a3f0, @0x7fe234016590 48-byte object <48-2F 78-64 E2-7F 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 C0-E5 00-34 E2-7F 00-00 90-D9 0A-34 E2-7F 00-00 00-00 00-00 00-00 00-00>)
Stack trace:
I1208 03:26:54.777720 28651 master.cpp:6622] Sending 1 offers to framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (default)
W1208 03:27:03.160513 28646 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
W1208 03:27:03.160785 28646 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:27:03.160995 28646 slave.cpp:4184] Forwarding the update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 to master@10.0.2.15:46643
I1208 03:27:03.161177 28646 slave.cpp:4184] Forwarding the update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 to master@10.0.2.15:46643
I1208 03:27:03.161424 28646 master.cpp:5808] Status update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 from agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm)
I1208 03:27:03.161469 28646 master.cpp:5870] Forwarding status update TASK_RUNNING (UUID: dcdd2cb5-fdea-4556-94e9-ff6246132315) for task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:27:03.161887 28646 master.cpp:7790] Updating the state of task 3a8a0c1c-c386-409d-a21c-653dc2d3d7d5 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (latest state: TASK_FAILED, status update state: TASK_RUNNING)
I1208 03:27:03.162178 28646 master.cpp:5808] Status update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 from agent 0bcb0250-4cf5-4209-92fe-ce260518b50f-S0 at slave(8)@10.0.2.15:46643 (archlinux.vagrant.vm)
I1208 03:27:03.162214 28646 master.cpp:5870] Forwarding status update TASK_RUNNING (UUID: bfb80b10-da9b-44d2-977a-61b88531e809) for task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000
I1208 03:27:03.162407 28646 master.cpp:7790] Updating the state of task bf21fae2-513e-4ea1-b85c-dfd2546e4249 of framework 0bcb0250-4cf5-4209-92fe-ce260518b50f-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING)
../../mesos/src/tests/default_executor_tests.cpp:610: Failure
Value of: taskStates
  Actual: { (3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, TASK_FAILED), (bf21fae2-513e-4ea1-b85c-dfd2546e4249, TASK_KILLED) }
Expected: expectedTaskStates
Which is: { (3a8a0c1c-c386-409d-a21c-653dc2d3d7d5, TASK_RUNNING), (bf21fae2-513e-4ea1-b85c-dfd2546e4249, TASK_RUNNING) }
*** Aborted at 1481128023 (unix time) try ""date -d @1481128023"" if you are using GNU date ***
PC: @          0x1bb3ed4 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 28632 (TID 0x7fe264b7ec40) from PID 0; stack trace: ***
    @     0x7fe25df89080 (unknown)
    @          0x1bb3ed4 testing::UnitTest::AddTestPartResult()
    @          0x1ba86d1 testing::internal::AssertHelper::operator=()
    @           0xe3889c mesos::internal::tests::DefaultExecutorTest_KillTaskGroupOnTaskFailure_Test::TestBody()
    @          0x1bd1df0 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1bcce74 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1badb08 testing::Test::Run()
    @          0x1bae2c0 testing::TestInfo::Run()
    @          0x1bae8fd testing::TestCase::Run()
    @          0x1bb53f1 testing::internal::UnitTestImpl::RunAllTests()
    @          0x1bd2ab7 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1bcd9b4 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1bb40f5 testing::UnitTest::Run()
    @          0x118c09e RUN_ALL_TESTS()
    @          0x118bc54 main
    @     0x7fe25bf16291 __libc_start_main
    @           0xa842fa _start
    @                0x0 (unknown)
Install 'notify-send' and try again
{noformat}"	MESOS	Resolved	3	1	2531	flaky-test, mesosphere
13118334	Add resource versions to RunTaskMessage	"To support speculative application of certain offer operations we have added resource versions to offer operation messages. This permits checking compatibility of master and agent state before applying operations.

Launch operations are not modelled with offer operation messages, but instead with {{RunTaskMessage}}. In order to provide the same consistency guarantees we need to add resource versions to {{RunTaskMessage}} as well. Otherwise we would only rely on resource containment checks in the agent to catch inconsistencies; these can be unreliable as there is no guarantee that the matched agent resource is unique (e.g., with two {{RESERVE}} operations on similar resorces triggered on the same agent and one of these failing, the other succeeding, we would end up potentially sending one framework a success status and the other a failed one, but would not do anything the make sure the speculative operation application matches the resources belonging to the sent offer operation status update)."	MESOS	Resolved	3	3	2531	mesosphere
13157768	MasterSlaveReconciliationTest.ReconcileDroppedOperation is flaky	"This was observed on a Debian 9 SSL/GRPC-enabled build. It appears that a poorly-timed {{UpdateSlaveMessage}} leads to the operation reconciliation occurring before the expectation for the {{ReconcileOperationsMessage}} is registered:
{code}
I0508 00:11:09.700815 22498 master.cpp:4362] Processing ACCEPT call for offers: [ f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-O0 ] on agent f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-S0 at slave(212)@127.0.0.1:36309 (localhost) for framework f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-0000 (default) at scheduler-b0f55e01-2f6f-42c8-8614-901036acfc31@127.0.0.1:36309
I0508 00:11:09.700870 22498 master.cpp:3602] Authorizing principal 'test-principal' to reserve resources 'cpus(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):2; mem(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):1024; disk(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):1024; ports(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):[31000-32000]'
I0508 00:11:09.701228 22493 master.cpp:4725] Applying RESERVE operation for resources [{""allocation_info"":{""role"":""default-role""},""name"":""cpus"",""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":2.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""default-role""},""name"":""mem"",""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""default-role""},""name"":""disk"",""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""default-role""},""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""type"":""RANGES""}] from framework f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-0000 (default) at scheduler-b0f55e01-2f6f-42c8-8614-901036acfc31@127.0.0.1:36309 to agent f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-S0 at slave(212)@127.0.0.1:36309 (localhost)
I0508 00:11:09.701498 22493 master.cpp:11265] Sending operation '' (uuid: 81dffb62-6e75-4c6c-a97b-41c92c58d6a7) to agent f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-S0 at slave(212)@127.0.0.1:36309 (localhost)
I0508 00:11:09.701627 22494 slave.cpp:1564] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""0HeA06ftS6m76SNoNZNPag==""},""slave_id"":{""value"":""f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-S0""},""update_oversubscribed_resources"":true}
I0508 00:11:09.701848 22494 master.cpp:7800] Received update of agent f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-S0 at slave(212)@127.0.0.1:36309 (localhost) with total oversubscribed resources {}
W0508 00:11:09.701905 22494 master.cpp:7974] Performing explicit reconciliation with agent for known operation 81dffb62-6e75-4c6c-a97b-41c92c58d6a7 since it was not present in original reconciliation message from agent
I0508 00:11:09.702085 22494 master.cpp:11015] Updating the state of operation '' (uuid: 81dffb62-6e75-4c6c-a97b-41c92c58d6a7) for framework f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-0000 (latest state: OPERATION_PENDING, status update state: OPERATION_DROPPED)
I0508 00:11:09.702239 22491 hierarchical.cpp:925] Updated allocation of framework f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-0000 on agent f850080d-9c7a-4ff7-8d4b-9e54aa0418cb-S0 from cpus(allocated: default-role):2; mem(allocated: default-role):1024; disk(allocated: default-role):1024; ports(allocated: default-role):[31000-32000] to disk(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):1024; cpus(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):2; mem(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):1024; ports(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)]):[31000-32000]
I0508 00:11:09.702267 22493 slave.cpp:1274] New master detected at master@127.0.0.1:36309
I0508 00:11:09.702306 22495 task_status_update_manager.cpp:181] Pausing sending task status updates
I0508 00:11:09.702337 22493 slave.cpp:1329] Detecting new master
{code}

Full log is attached as {{MasterSlaveReconciliationTest.ReconcileDroppedOperation.txt}}."	MESOS	Resolved	3	1	2531	mesosphere
13010183	Implement clang-tidy check to catch incorrect flags hierarchies	"Classes need to always use {{virtual}} inheritance when being derived from {{FlagsBase}}. Also, in order to compose such derived flags they should be inherited virtually again.

Some examples:
{code}
struct A : virtual FlagsBase {}; // OK
struct B : FlagsBase {}; // ERROR
struct C : A {}; // ERROR
{code}


We should implement a clang-tidy checker to catch such wrong inheritance issues."	MESOS	Resolved	3	1	2531	clang-tidy, mesosphere
13241297	Agent should erase DrainInfo when draining complete	When the agent is in the DRAINING state and it sees that all terminal acknowledgements for completed operations and tasks have been received, it should clear the checkpointed {{DrainInfo}} from disk and from memory so that it no longer believes it is DRAINING. It will then be ready to receive new tasks/operations if it is reactivated.	MESOS	Resolved	3	3	2531	foundations, mesosphere
13013842	Add rlimit support to Mesos containerizer	"Reviews:
https://reviews.apache.org/r/53061/
https://reviews.apache.org/r/53062/
https://reviews.apache.org/r/53063/
https://reviews.apache.org/r/53078/"	MESOS	Resolved	3	4	2531	mesosphere
13014707	PosixRLimitsIsolatorTest.TaskExceedingLimit fails on OS X	"This test consistently fails on OS X:

{code}
31-7e9c-4248-acfd-21634150a657@172.28.128.1:64864 on agent 52cc4957-1a39-4d66-ace6-5622fac3b85e-S0
../../src/tests/containerizer/posix_rlimits_isolator_tests.cpp:120: Failure
Value of: statusFailed->state()
  Actual: TASK_FINISHED
Expected: TASK_FAILED
{code}"	MESOS	Resolved	3	1	2531	mesosphere
13199190	PosixRLimitsIsolatorTest.UnsetLimits is broken on macOS 10.14.2 beta3.	"It appears that changes in macOS are now preventing the setup of rlimit for non root users.

{noformat}
3: [ RUN      ] PosixRLimitsIsolatorTest.UnsetLimits
3: Failed to set RLMT_CORE limit: Failed to set rlimit: Operation not permitted
3: ../src/tests/containerizer/posix_rlimits_isolator_tests.cpp:194: Failure
3:       Expected: TASK_STARTING
3: To be equal to: statusStarting->state()
3:       Which is: TASK_FAILED
3: ../src/tests/containerizer/posix_rlimits_isolator_tests.cpp:196: Failure
3: Failed to wait 15secs for statusRunning
3: ../src/tests/containerizer/posix_rlimits_isolator_tests.cpp:185: Failure
3: Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
3:          Expected: to be called 3 times
3:            Actual: called once - unsatisfied and active
3: [  FAILED  ] PosixRLimitsIsolatorTest.UnsetLimits (15209 ms)
{noformat}

It is still unclear if this was just a beta-quirk or intended behaviour in upcoming macOS versions."	MESOS	Resolved	3	1	2531	test
13029100	IOSwitchboardServerTest.SendHeartbeat and IOSwitchboardServerTest.ReceiveHeartbeat broken on OS X	"The tests IOSwitchboardServerTest.SendHeartbeat and IOSwitchboardServerTest.ReceiveHeartbeat are broken on OS X.
{noformat}
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from IOSwitchboardServerTest
[ RUN      ] IOSwitchboardServerTest.SendHeartbeat
../../src/tests/containerizer/io_switchboard_tests.cpp:392: Failure
server: Failed to build address from '/var/folders/6t/yp_xgc8d6k32rpp0bsbfqm9m0000gp/T/04ioBQ/mesos-io-switchboard-0ce96b84-fc47-4a21-b7bc-71eddd8b0f13': Path too long, must be less than 104 bytes
[  FAILED  ] IOSwitchboardServerTest.SendHeartbeat (5 ms)
[ RUN      ] IOSwitchboardServerTest.ReceiveHeartbeat
../../src/tests/containerizer/io_switchboard_tests.cpp:630: Failure
server: Failed to build address from '/var/folders/6t/yp_xgc8d6k32rpp0bsbfqm9m0000gp/T/FryfgE/mesos-io-switchboard-df1d7004-7ea1-43f3-bec9-bf0c2663b260': Path too long, must be less than 104 bytes
[  FAILED  ] IOSwitchboardServerTest.ReceiveHeartbeat (0 ms)
[----------] 2 tests from IOSwitchboardServerTest (5 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (29 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] IOSwitchboardServerTest.SendHeartbeat
[  FAILED  ] IOSwitchboardServerTest.ReceiveHeartbeat
{noformat}

The issue is caused by the way the socket paths are constructed in the tests,
{code}
string socketPath = path::join(
    sandbox.get(),
    ""mesos-io-switchboard-"" + UUID::random().toString());
{code}

The lengths of the components are

* sandbox path: 55 characters (including directory delimiters),
* {{mesos-io-switchboard}}: 20 characters,
* UUID: 36 characters

which amounts to a total of 113 non-zero characters.

Since the socket is already created in the test's sandbox and only a single socket is created in the test, it appears that it might be possible to strip e.g., the UUID from the path to make the path fit."	MESOS	Resolved	3	1	2531	mesosphere
13029800	mesos-this-capture clang-tidy check has false positives	"The {{mesos-this-capture}} clang-tidy checks incorrectly triggers on the code here,

  https://github.com/apache/mesos/blob/d2117362349ab4c383045720f77d42b2d9fd6871/src/slave/containerizer/mesos/io/switchboard.cpp#L1487

We should tighten the matcher to avoid triggering on such constructs."	MESOS	Resolved	3	1	2531	clang-tidy
13237763	Agent should modify status updates while draining	While it's draining, the agent should decorate TASK_KILLING and TASK_KILLED status updates with REASON_AGENT_DRAINING. It should also convert TASK_KILLED to TASK_GONE_BY_OPERATOR in the {{mark_gone}} case, ensuring that TASK_GONE_BY_OPERATOR is the state checkpointed to disk.	MESOS	Resolved	3	3	2531	foundations, mesosphere
13204850	Resource provider manager can crash on invalid data from resource providers	"The resource provider manager code currently contains a number of assertions which will crash the manager (and its agent) if some forms of invalid data are received from a resource provider. This is dangerous since resource providers are not necessarily part of Mesos-controlled code (they talk to the manager over an HTTP API and could even be in external processes).

Instead of crashing, the resource provider manager should disconnect the resource providers in such scenarios."	MESOS	Resolved	3	1	2531	mesosphere, mesosphere-dss-post-ga, storage
13214316	ContentType/AgentAPITest.MarkResourceProviderGone/1 is flaky	"We observed a segfault in {{ContentType/AgentAPITest.MarkResourceProviderGone/1}} on test teardown.
{noformat}
I0131 23:55:59.378453  6798 slave.cpp:923] Agent terminating
I0131 23:55:59.378813 31143 master.cpp:1269] Agent a27bcaba-70cc-4ec3-9786-38f9512c61fd-S0 at slave(1112)@172.16.10.236:43229 (ip-172-16-10-236.ec2.internal) disconnected
I0131 23:55:59.378831 31143 master.cpp:3272] Disconnecting agent a27bcaba-70cc-4ec3-9786-38f9512c61fd-S0 at slave(1112)@172.16.10.236:43229 (ip-172-16-10-236.ec2.internal)
I0131 23:55:59.378846 31143 master.cpp:3291] Deactivating agent a27bcaba-70cc-4ec3-9786-38f9512c61fd-S0 at slave(1112)@172.16.10.236:43229 (ip-172-16-10-236.ec2.internal)
I0131 23:55:59.378891 31143 hierarchical.cpp:793] Agent a27bcaba-70cc-4ec3-9786-38f9512c61fd-S0 deactivated
F0131 23:55:59.378891 31149 logging.cpp:67] RAW: Pure virtual method called
    @     0x7f633aaaebdd  google::LogMessage::Fail()
    @     0x7f633aab6281  google::RawLog__()
    @     0x7f6339821262  __cxa_pure_virtual
    @     0x55671cacc113  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @     0x55671b532e78  mesos::internal::tests::resource_provider::MockResourceProvider<>::disconnected()
    @     0x7f633978f6b0  process::AsyncExecutorProcess::execute<>()
    @     0x7f633979f218  _ZN5cpp176invokeIZN7process8dispatchI7NothingNS1_20AsyncExecutorProcessERKSt8functionIFvvEES9_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMSE_FSB_T1_EOT2_EUlSt10unique_ptrINS1_7PromiseIS3_EESt14default_deleteISP_EEOS7_PNS1_11ProcessBaseEE_JSS_S7_SV_EEEDTclcl7forwardISB_Efp_Espcl7forwardIT0_Efp0_EEEOSB_DpOSX_
    @     0x7f633a9f5d01  process::ProcessBase::consume()
    @     0x7f633aa1a08a  process::ProcessManager::resume()
    @     0x7f633aa1db06  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
    @     0x7f633acc9f80  execute_native_thread_routine
    @     0x7f6337142e25  start_thread
    @     0x7f6336241bad  __clone
{noformat}"	MESOS	Resolved	2	1	2531	flaky, flaky-test, mesosphere, storage, test
13126291	When a resource provider driver is disconnected, it fails to reconnect.	"If the resource provider manager closes the HTTP connection of a resource provider, the resource provider should reconnect itself. For that, the resource provider driver will change its state to ""DISCONNECTED"", call a {{disconnected}} callback and use its endpoint detector to reconnect.
This doesn't work in a testing environment where a {{ConstantEndpointDetector}} is used. While the resource provider is notified of the closed HTTP connection (and logs {{End-Of-File received}}), it never disconnects itself and calls the {{disconnected}} callback. Discarding {{HttpConnectionProcess::detection}} in {{HttpConnectionProcess::disconnected}} doesn't trigger the {{onAny}} callback of that future. This might not be a problem in {{HttpConnectionProcess}} but could be related to the test case using a {{ConstantEndpointDetector}}."	MESOS	Resolved	3	1	2531	mesosphere
12921820	GMock warning in SlaveTest.ContainerizerUsageFailure	"{noformat}
[ RUN      ] SlaveTest.ContainerizerUsageFailure

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7f920271dfd0)
Stack trace:
[       OK ] SlaveTest.ContainerizerUsageFailure (94 ms)
[----------] 1 test from SlaveTest (95 ms total)
{noformat}

Occurs deterministically for me on OSX 10.10"	MESOS	Resolved	3	1	2531	mesosphere, tech-debt
12958086	SSLTest.ProtocolMismatch is slow	For me {{SSLTest.ProtocolMismatch}} currently takes more than 8 seconds for an unoptimized build under OS X.	MESOS	Resolved	4	1	2531	mesosphere, tech-debt
12957159	Add authentication to agent's /monitor/statistics endpoint	"Operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per-executor resource usage statistics.
Since this endpoint is handled by the ResourceMonitorProcess, I would expect the work necessary to be similar to what was done for /files or /registry endpoint authn."	MESOS	Resolved	3	3	2531	authentication, mesosphere, security
13123669	Add authorization to display of resource provider information in API calls and endpoints	The {{GET_RESOURCE_PROVIDERS}} call is used to list all resource providers known to a Mesos agent. We akso display resource provider infos for the master's {{GET_AGENTS}} call. These call needs to be authorized.	MESOS	Resolved	3	3	2531	csi-post-mvp
12963682	Add capabilities support for mesos execute cli.	Add support for `user` and `capabilities` to execute cli. This will help in testing the `capabilities` feature for unified containerizer.	MESOS	Resolved	3	1	2531	mesosphere
12930128	Add tracking of the role a Resource was offered for	If a framework can have multiple roles, we need a way to identify for which of the framework's role a resource was offered for (e.g., for resource recovery and reconciliation).	MESOS	Resolved	3	4	2531	mesosphere
13264018	Duplicate tasks if agent partitioned during maintenance down	"When the master starts maintenance for a node it

(1) sends a {{ShutdownMessage}} message to agent, and
(2) removes the slave which transitions all tasks to {{TASK_LOST}} and moves them
to the completed task set.

If the {{ShutdownMessage}} isn't fully processed on the agent (e.g., message dropped between (1) and (2), or agent process killed before the executor has shut down), the agent could come back with the lost task running. It would report the task on registration with the master, which would add it to the list of active tasks. With that the same task could be both completed and active.
"	MESOS	Resolved	3	1	2531	foundations
12952419	MasterTest.MasterLost is flaky	"The test {{MasterTest.MasterLost}} and {{ExceptionTest.DisallowSchedulerActionsOnAbort}} fail at least half the time under OS X (clang, not optimized, {{30efac7}}), e.g.,
{code}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from MasterTest
[ RUN      ] MasterTest.MasterLost
*** Aborted at 1458650698 (unix time) try ""date -d @1458650698"" if you are using GNU date ***
PC: @        0x109685fcc mesos::internal::state::State::store()
*** SIGSEGV (@0x0) received by PID 18620 (TID 0x111259000) stack trace: ***
    @     0x7fff850e1f1a _sigtramp
    @        0x108c74eaf boost::uuids::detail::sha1::process_byte_impl()
    @        0x1095fd723 mesos::internal::state::protobuf::State::store<>()
    @        0x1095fbd3e mesos::internal::master::RegistrarProcess::update()
    @        0x1095fcf6c mesos::internal::master::RegistrarProcess::_apply()
    @        0x1096797a0 _ZZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS_5OwnedINS3_9OperationEEES7_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSC_FSA_T1_ET2_ENKUlPNS_11ProcessBaseEE_clESL_
    @        0x1096795f0 _ZNSt3__128__invoke_void_return_wrapperIvE6__callIJRZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS3_5OwnedINS7_9OperationEEESB_EENS3_6FutureIT_EERKNS3_3PIDIT0_EEMSG_FSE_T1_ET2_EUlPNS3_11ProcessBaseEE_SP_EEEvDpOT_
    @        0x1096792d9 _ZNSt3__110__function6__funcIZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS2_5OwnedINS6_9OperationEEESA_EENS2_6FutureIT_EERKNS2_3PIDIT0_EEMSF_FSD_T1_ET2_EUlPNS2_11ProcessBaseEE_NS_9allocatorISP_EEFvSO_EEclEOSO_
    @        0x10b2e9e4c std::__1::function<>::operator()()
    @        0x10b2e9d9c process::ProcessBase::visit()
    @        0x10b31d26e process::DispatchEvent::visit()
    @        0x108ad7d81 process::ProcessBase::serve()
    @        0x10b2e3cb4 process::ProcessManager::resume()
    @        0x10b36c479 process::ProcessManager::init_threads()::$_1::operator()()
    @        0x10b36c0a2 _ZNSt3__114__thread_proxyINS_5tupleIJNS_6__bindIZN7process14ProcessManager12init_threadsEvE3$_1JNS_17reference_wrapperIKNS_6atomicIbEEEEEEEEEEEEPvSD_
    @     0x7fff90eca05a _pthread_body
    @     0x7fff90ec9fd7 _pthread_start
    @     0x7fff90ec73ed thread_start
{code}

Sometimes also {{FaultToleranceTest.SchedulerFailover}} fails with the same stack trace.

I could trace this to the recent refactoring of the test helpers (MESOS-4633, MESOS-4634),
{code}
There are only 'skip'ped commits left to test.
The first bad commit could be any of:
75ca1e6c9fde655c41fdf835aa20c47570d21f10
56e9406763e8514a7557ab3862d2f352a61425d5
b377557c2bfc35c894e87becb47122955540f133
7bf6e4f70131175edd4d6d77ea0dc7692b3e72ae
c7df1d7bcb1604c95800871cc0473c946e5b5d16
951539317525f3afe9490ed098617e5d4563a80a
We cannot bisect more!
{code}

It appears the lifetimes of some objects are still not ordered correctly.
"	MESOS	Resolved	3	1	2531	flaky, flaky-test, mesosphere
12959908	Add capability information to ContainerInfo protobuf message.	To enable support for capability as first class framework entity, we need to add capabilities related information to the ContainerInfo protobuf.	MESOS	Resolved	3	3	2531	mesosphere
12940481	Add allocator metric for number of active offer filters	To diagnose scenarios where frameworks unexpectedly do not receive offers information on currently active filters are needed.	MESOS	Resolved	3	4	2531	mesosphere
12973031	Update RUN_TASK_WITH_USER to use additional metadata	"Currently, the `authorization::Action` `RUN_TASK_WITH_USER` will pass the user as its `Object.value` string, but some authorizers may want to make authorization decisions based on additional task attributes, like role, resources, labels, container type, etc.

We should create a new Action `RUN_TASK` that passes FrameworkInfo and TaskInfo in its Object, and the LocalAuthorizer's RunTaskWithUser ACL can be implemented using the user found in TaskInfo/FrameworkInfo.
We may need to leave the old _WITH_USER action around, but it's arguable whether we should call the authorizer once for RUN_TASK and once for RUN_TASK_WITH_USER, or only use the new action and deprecate the old one?"	MESOS	Resolved	1	4	2531	mesosphere, security
13084798	Website ruby deps do not bundle on macOS	"When trying to bundle the ruby dependencies of the website on macOS-10.12.5 I get

{code}
$ cd site/
$ bundle install
Fetching gem metadata from https://rubygems.org/............
Fetching version metadata from https://rubygems.org/..
Fetching dependency metadata from https://rubygems.org/.
Using coffee-script-source 1.6.3
Using multi_json 1.8.2
Using chunky_png 1.2.9
Using fssm 0.2.10
Using sass 3.2.12
Using tilt 1.3.7
Using kramdown 1.2.0
Using i18n 0.6.5
Using rb-fsevent 0.9.3
Using ffi 1.9.3
Using rack 1.5.2
Using thor 0.18.1
Using bundler 1.15.1
Using hike 1.2.3
Fetching eventmachine 1.0.3
Installing eventmachine 1.0.3 with native extensions
Using http_parser.rb 0.5.3
Using addressable 2.3.5
Using atomic 1.1.14
Using rdiscount 2.1.7
Using htmlentities 4.3.2
Fetching libv8 3.16.14.15
Installing libv8 3.16.14.15 with native extensions
Using ref 2.0.0
Using execjs 1.4.0
Using compass 0.12.2
Using haml 4.0.4
Using activesupport 3.2.15
Using rb-inotify 0.9.2
Using rb-kqueue 0.2.0
Using rack-test 0.6.2
Using rack-livereload 0.3.15
Using rouge 0.3.10
Using sprockets 2.10.0
Gem::Ext::BuildError: ERROR: Failed to build gem native extension.

    current directory: /Users/bbannier/src/mesos/site/vendor/ruby/2.4.0/gems/eventmachine-1.0.3/ext
/Users/bbannier/src/homebrew/opt/ruby/bin/ruby -r ./siteconf20170705-46478-cy91ue.rb extconf.rb
checking for rb_trap_immediate in ruby.h,rubysig.h... no
checking for rb_thread_blocking_region()... no
checking for inotify_init() in sys/inotify.h... no
checking for __NR_inotify_init in sys/syscall.h... no
checking for writev() in sys/uio.h... yes
checking for rb_wait_for_single_fd()... yes
checking for rb_enable_interrupt()... no
checking for rb_time_new()... yes
checking for sys/event.h... yes
checking for sys/queue.h... yes
creating Makefile

current directory: /Users/bbannier/src/mesos/site/vendor/ruby/2.4.0/gems/eventmachine-1.0.3/ext
make ""DESTDIR="" clean

current directory: /Users/bbannier/src/mesos/site/vendor/ruby/2.4.0/gems/eventmachine-1.0.3/ext
make ""DESTDIR=""
compiling binder.cpp
compiling cmain.cpp
compiling ed.cpp
compiling em.cpp
compiling kb.cpp
compiling page.cpp
compiling pipe.cpp
compiling rubymain.cpp
compiling ssl.cpp
In file included from pipe.cpp:20:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from page.cpp:21:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from kb.cpp:20:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from em.cpp:23:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from binder.cpp:20:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from rubymain.cpp:20:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from ed.cpp:20:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from cmain.cpp:20:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from ssl.cpp:23:
In file included from ./project.h:149:
./binder.h:35:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long GetBinding() {return Binding;}
                ^~~~~~
In file included from pipe.cpp:20:
In file included from ./project.h:150:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
./em.h:85:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToServer (const char *, int, const char *, int);
                ^~~~~~
./em.h:86:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~
./em.h:88:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
./em.h:89:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateUnixDomainServer (const char*);
                ^~~~~~
./em.h:91:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenKeyboard();
                ^~~~~~
./em.h:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long Socketpair (char* const*);
                ^~~~~~
./em.h:99:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:116:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchPid (int);
                ^~~~~~
In file included from binder.cpp:20:
In file included from ./project.h:150:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
./em.h:In file included from kb.cpp:20:
In file included from ./project.h:150:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
85:3:                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
 warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToServer (const char *, int, const char *, int);./em.h:
85:                ^~~~~~
./em.h:86:3: 3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToServer (const char *, int, const char *, int);
                ^~~~~~
./em.h:86:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
In file included from                 const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~
warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:88em.cpp:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
:23:
In file included from ./project.h                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
:150:
./em.h:84:3: warning: ./em.h                const unsigned long ConnectToUnixServer (const char *);'const' type qualifier on return type has no effect [-Wignored-qualifiers]

                ^~~~~~
                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
./em.h:./em.h:85:3::88:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
89: 3:warning:                 const unsigned long CreateTcpServer (const char *, int);'const' type qualifier on return type has no effect [-Wignored-qualifiers] warning: 
                ^~~~~~
'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
./em.h:89:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]

                const unsigned long CreateUnixDomainServer (const char*);
                ^~~~~~
                const unsigned long ConnectToServer (const char *, int, const char *, int);
                ^~~~~~
./em.h:91:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenKeyboard();
                ^~~~~~
./em.h:./em.h:93                const unsigned long OpenDatagramSocket (const char *, int);
:                ^~~~~~
3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long Socketpair (char* const*);
                ^~~~~~
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:99:3                const unsigned long CreateUnixDomainServer (const char*);86: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]

                ^~~~~~
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:91:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenKeyboard();
                ^~~~~~
./em.h:116:3:: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h3:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]                const unsigned long Socketpair (char* const*);
                ^~~~~~

./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~                const unsigned long WatchPid (int);
                ^~~~~~

./em.h:99:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:88:3:                const unsigned long AttachFD (int, bool);
                ^~~~~~
 warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
./em.h:89:3: ./em.h:warning116:3: : warning'const' type qualifier on return type has no effect [-Wignored-qualifiers]
: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateUnixDomainServer (const char*);
                ^~~~~~
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:91:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchPid (int);                const unsigned long OpenKeyboard();
                ^~~~~~

                ^~~~~~
./em.h:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long Socketpair (char* const*);
                ^~~~~~
./em.h:99:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:116:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchPid (int);
                ^~~~~~
In file included from rubymain.cpp:20:
In file included from ./project.h:150:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
In file included from page.cpp:21:
In file included from ./project.h:150                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
./em.h:85:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:85                const unsigned long ConnectToServer (const char *, int, const char *, int);:
3                ^~~~~~
: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToServer (const char *, int, const char *, int);
                ^~~~~~
./em.h:86:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:86:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~
                const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~
./em.h:88:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]./em.h:88:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]

                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
./em.h:89:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
./em.h:89:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
                const unsigned long CreateUnixDomainServer (const char*);
                ^~~~~~
./em.h:90:3: warning: ./em.h:91:3'const' type qualifier on return type has no effect [-Wignored-qualifiers]
: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateUnixDomainServer (const char*);                const unsigned long OpenKeyboard();

                ^~~~~~                ^~~~~~

./em.h:91:./em.h:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long Socketpair (char* const*);
                ^~~~~~
                const unsigned long OpenKeyboard();
                ^~~~~~
./em.h:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:99                const unsigned long Socketpair (char* const*);:
3                ^~~~~~:
 warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:99:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:116:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h:116:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
                const unsigned long WatchPid (int);
                ^~~~~~
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchPid (int);
                ^~~~~~
In file included from cmain.cpp:20:
In file included from ./project.h:150:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
./em.h:85:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToServer (const char *, int, const char *, int);
                ^~~~~~
./em.h:86:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~
./em.h:88:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
./em.h:89:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateUnixDomainServer (const char*);
                ^~~~~~
./em.h:91:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenKeyboard();
                ^~~~~~
./em.h:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long Socketpair (char* const*);
                ^~~~~~
./em.h:99:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:116:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchPid (int);
                ^~~~~~
In file included from ed.cpp:20:
In file included from ./project.h:150:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
./em.h:85:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToServer (const char *, int, const char *, int);
                ^~~~~~
./em.h:86:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~
./em.h:88:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
./em.h:89:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateUnixDomainServer (const char*);
                ^~~~~~
./em.h:91:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenKeyboard();
                ^~~~~~
./em.h:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long Socketpair (char* const*);
                ^~~~~~
./em.h:99:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:116:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchPid (int);
                ^~~~~~
In file included from ssl.cpp:23:
In file included from ./project.h:150:
./em.h:84:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long InstallOneshotTimer (int);
                ^~~~~~
./em.h:85:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToServer (const char *, int, const char *, int);
                ^~~~~~
./em.h:86:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long ConnectToUnixServer (const char *);
                ^~~~~~
./em.h:88:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateTcpServer (const char *, int);
                ^~~~~~
./em.h:89:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenDatagramSocket (const char *, int);
                ^~~~~~
./em.h:90:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long CreateUnixDomainServer (const char*);
                ^~~~~~
./em.h:91:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long OpenKeyboard();
                ^~~~~~
./em.h:93:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long Socketpair (char* const*);
                ^~~~~~
./em.h:99:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long AttachFD (int, bool);
                ^~~~~~
./em.h:116:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchFile (const char*);
                ^~~~~~
./em.h:125:3: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
                const unsigned long WatchPid (int);
                ^~~~~~
In file included from pipe.cpp:20:
In file included from ./project.h:154:
./eventmachine.h:46:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
./eventmachine.h:47:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
./eventmachine.h:48:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
./eventmachine.h:50:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
./eventmachine.h:65:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
./eventmachine.h:66:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
./eventmachine.h:67:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
./eventmachine.h:68:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_keyboard();
        ^~~~~~
./eventmachine.h:103:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_popen (char * const*cmd_strings);
        ^~~~~~
./eventmachine.h:105:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
./eventmachine.h:108:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_pid (int);
        ^~~~~~
In file included from binder.cpp:20:
In file included from ./project.h:154:
./eventmachine.h:46:2: warningIn file included from em.cpp: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
:23:
In file included from ./project.h        const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
:154:
./eventmachine.h:46:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:47:2:         const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~warning: 
'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
./eventmachine.h:47:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
:48:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:48:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
./eventmachine.h:50:./eventmachine.h:50:22: warning: warning: : 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
./eventmachine.h:65:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:65:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~

        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
./eventmachine.h:66:2: ./eventmachine.h:66warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
./eventmachine.h:67:2: warning: ./eventmachine.h:'const' type qualifier on return type has no effect [-Wignored-qualifiers]67
:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
./eventmachine.h./eventmachine.h::6868::22::  warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_keyboard();
        ^~~~~~
        const unsigned long evma_open_keyboard();
        ^~~~~~
./eventmachine.h:103:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_popen (char * const*cmd_strings);
        ^~~~~~
./eventmachine.h:103:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:105:2        const unsigned long evma_popen (char * const*cmd_strings);:
         ^~~~~~warning
: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
./eventmachine.h:105:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:108:2: warning:         const unsigned long evma_watch_filename (const char *fname);
'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        ^~~~~~
        const unsigned long evma_watch_pid (int);
        ^~~~~~
In file included from page.cpp:21:
In file included from ./project.h:154:
./eventmachine.h:46:2./eventmachine.h:108: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
:2:         const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_pid (int);
        ^~~~~~
./eventmachine.h:47:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
./eventmachine.h:48:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
./eventmachine.h:50:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
./eventmachine.h:65:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
./eventmachine.h:66:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
./eventmachine.h:67:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
./eventmachine.h:68:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_keyboard();
        ^~~~~~
In file included from kb.cppIn file included from rubymain.cpp:20:
:In file included from 20:
In file included from ./project.h:154./project.h:154:
./eventmachine.h:46:2:
./eventmachine.h: warning: :46'const' type qualifier on return type has no effect [-Wignored-qualifiers]:2
: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
        const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
./eventmachine.h:47:2:./eventmachine.h:47 :2: warning: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]'const' type qualifier on return type has no effect
[-Wignored-qualifiers]

        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
./eventmachine.h:103:2: warning: ./eventmachine.h:'const' type qualifier on return type has no effect [-Wignored-qualifiers]
48:2./eventmachine.h:48:: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_popen (char * const*cmd_strings);
2        ^~~~~~
: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
./eventmachine.h:105:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
./eventmachine.h:50:2:./eventmachine.h:50:2:  warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
./eventmachine.h        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);:
108        ^~~~~~:
2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_pid (int);
        ^~~~~~
./eventmachine.h:65:2: ./eventmachine.h:65:2: warning: warning'const' type qualifier on return type has no effect [-Wignored-qualifiers]
: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
./eventmachine.h:66:./eventmachine.h2: warning: :'const' type qualifier on return type has no effect [-Wignored-qualifiers]
66:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
./eventmachine.h:67:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:67:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
./eventmachine.h:68:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:68:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_keyboard();
        ^~~~~~
        const unsigned long evma_open_keyboard();
        ^~~~~~
./eventmachine.h:103:2: warning./eventmachine.h:103:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_popen (char * const*cmd_strings);
        ^~~~~~
        const unsigned long evma_popen (char * const*cmd_strings);
        ^~~~~~
./eventmachine.h./eventmachine.h::105105::2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
./eventmachine.h:108:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
./eventmachine.h:108:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_pid (int);
        ^~~~~~
        const unsigned long evma_watch_pid (int);
        ^~~~~~
page.cpp:55:15: warning: implicit conversion loses integer precision: 'size_t' (aka 'unsigned long') to 'int' [-Wshorten-64-to-32]
                *length = p.Size;
                        ~ ~~^~~~
In file included from cmain.cpp:20:
In file included from ./project.h:154:
./eventmachine.h:46:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
./eventmachine.h:47:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
./eventmachine.h:48:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
./eventmachine.h:50:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
./eventmachine.h:65:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
./eventmachine.h:66:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
./eventmachine.h:67:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
./eventmachine.h:68:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_keyboard();
        ^~~~~~
./eventmachine.h:103:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_popen (char * const*cmd_strings);
        ^~~~~~
./eventmachine.h:105:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
./eventmachine.h:108:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_pid (int);
        ^~~~~~
cmain.cpp:96:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_install_oneshot_timer (int seconds)
           ^~~~~~
cmain.cpp:107:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port)
           ^~~~~~
cmain.cpp:117:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_connect_to_unix_server (const char *server)
           ^~~~~~
In file included from ed.cpp:20:
In file included from ./project.h:154:
cmain.cpp:127:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]./eventmachine.h
:46:2: extern ""C"" const unsigned long evma_attach_fd (int file_descriptor, int watch_mode)
           ^~~~~~
warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
./eventmachine.h:47:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
./eventmachine.h:48:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
./eventmachine.h:50:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
./eventmachine.h:65:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
./eventmachine.h:66:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
./eventmachine.h:67:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
./eventmachine.h:68:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_keyboard();
        ^~~~~~
./eventmachine.h:103:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_popen (char * const*cmd_strings);
        ^~~~~~
./eventmachine.h:105:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
rubymain.cpp:803:13: warning: implicit conversion loses integer precision: 'long' to 'int' [-Wshorten-64-to-32]
                int len = RARRAY_LEN(cmd);
                    ~~~   ^~~~~~~~~~~~~~~
./eventmachine.h:108:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
/Users/bbannier/src/homebrew/Cellar/ruby/2.4.1_1/include/ruby-2.4.0/ruby/ruby.h:1026:23:        const unsigned long evma_watch_pid (int);
        ^~~~~~
 note: expanded from macro 'RARRAY_LEN'
#define RARRAY_LEN(a) rb_array_len(a)
                      ^~~~~~~~~~~~~~~
rubymain.cpp:1020:42: warning: implicit conversion loses integer precision: 'VALUE' (aka 'unsigned long') to 'int' [-Wshorten-64-to-32]
        return INT2NUM (evma_set_rlimit_nofile (arg));
                        ~~~~~~~~~~~~~~~~~~~~~~  ^~~
/Users/bbannier/src/homebrew/Cellar/ruby/2.4.1_1/include/ruby-2.4.0/ruby/ruby.h:1538:31: note: expanded from macro 'INT2NUM'
#define INT2NUM(x) RB_INT2NUM(x)
                              ^
/Users/bbannier/src/homebrew/Cellar/ruby/2.4.1_1/include/ruby-2.4.0/ruby/ruby.h:1515:41: note: expanded from macro 'RB_INT2NUM'
# define RB_INT2NUM(v) RB_INT2FIX((int)(v))
                                        ^
/Users/bbannier/src/homebrew/Cellar/ruby/2.4.1_1/include/ruby-2.4.0/ruby/ruby.h:231:33: note: expanded from macro 'RB_INT2FIX'
#define RB_INT2FIX(i) (((VALUE)(i))<<1 | RUBY_FIXNUM_FLAG)
                                ^
em.cpp:75:2:pipe.cpp:160:11: warning: implicit conversion loses integer precision: 'ssize_t' (aka 'long') to 'int' [-Wshorten-64-to-32]
                int r = read (sd, readbuffer, sizeof(readbuffer) - 1);
                    ~   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pipe.cpp:217:36: warning: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Wshorten-64-to-32]
                        int len = sizeof(output_buffer) - nbytes;
                            ~~~   ~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~
pipe.cpp:231:22: warning: implicit conversion loses integer precision: 'ssize_t' (aka 'long') to 'int' [-Wshorten-64-to-32]
        int bytes_written = write (GetSocket(), output_buffer, nbytes);
            ~~~~~~~~~~~~~   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pipe.cpp:236:21: warning: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Wshorten-64-to-32]
                        int len = nbytes - bytes_written;
                            ~~~   ~~~~~~~^~~~~~~~~~~~~~~
 warning: field 'LoopBreakerWriter' will be initialized after field 'NumCloseScheduled' [-Wreorder]
        LoopBreakerWriter (-1),
        ^
In file included from ssl.cpp:23:
In file included from ./project.h:154:
./eventmachine.h:46:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_install_oneshot_timer (int seconds);
        ^~~~~~
./eventmachine.h:47:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
em.cpp:265:14: warning: implicit conversion loses integer precision: 'rlim_t' (aka 'unsigned long long') to 'int' [-Wshorten-64-to-32]
        const unsigned long evma_connect_to_server (const char *bind_addr, int bind_port, const char *server, int port);
        ^~~~~~
        return rlim.rlim_cur;
        ~~~~~~ ~~~~~^~~~~~~~
./eventmachine.h:48:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_connect_to_unix_server (const char *server);
        ^~~~~~
./eventmachine.h:50:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_attach_fd (int file_descriptor, int watch_mode);
        ^~~~~~
./eventmachine.h:65:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_tcp_server (const char *address, int port);
        ^~~~~~
./eventmachine.h:66:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_create_unix_domain_server (const char *filename);
        ^~~~~~
./eventmachine.h:67:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_datagram_socket (const char *server, int port);
        ^~~~~~
./eventmachine.h:68:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_open_keyboard();
        ^~~~~~
./eventmachine.h:103:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_popen (char * const*cmd_strings);
        ^~~~~~
./eventmachine.h:105:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_filename (const char *fname);
        ^~~~~~
./eventmachine.h:108:2: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
        const unsigned long evma_watch_pid (int);
        ^~~~~~
ed.cpp:297:39: warning: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Wshorten-64-to-32]
                        ProxyTarget->SendOutboundData(buf, proxied);
                                     ~~~~~~~~~~~~~~~~      ^~~~~~~
ed.cpp:303:17: warning: comparison of integers of different signs: 'unsigned long' and 'int' [-Wsign-compare]
                                if (proxied < size) {
                                    ~~~~~~~ ^ ~~~~
em.cpp:736:29: warning: cmain.cpp:269:12: warning: implicit conversion loses integer precision: 'std::__1::vector<EventableDescriptor *, std::__1::allocator<EventableDescriptor *> >::size_type' (aka
'unsigned long') to 'int' [-Wshorten-64-to-32]
        int nSockets = Descriptors.size();
            ~~~~~~~~   ~~~~~~~~~~~~^~~~~~
'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_create_tcp_server (const char *address, int port)
           ^~~~~~
cmain.cpp:279:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_create_unix_domain_server (const char *filename)
           ^~~~~~
cmain.cpp:289:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_open_datagram_socket (const char *address, int port)
           ^~~~~~
cmain.cpp:299:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_open_keyboard()
           ^~~~~~
cmain.cpp:309:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_watch_filename (const char *fname)
           ^~~~~~
cmain.cpp:329:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_watch_pid (int pid)
           ^~~~~~
em.cpp:827:9: error: use of undeclared identifier 'rb_thread_select'; did you mean 'rb_thread_fd_select'?
        return EmSelect (maxsocket+1, &fdreads, &fdwrites, &fderrors, &tv);
               ^~~~~~~~
               rb_thread_fd_select
./em.h:25:20: note: expanded from macro 'EmSelect'
  #define EmSelect rb_thread_select
                   ^
/Users/bbannier/src/homebrew/Cellar/ruby/2.4.1_1/include/ruby-2.4.0/ruby/intern.h:456:5: note: 'rb_thread_fd_select' declared here
int rb_thread_fd_select(int, rb_fdset_t *, rb_fdset_t *, rb_fdset_t *, struct timeval *);
    ^
em.cpp:827:32: error: cannot initialize a parameter of type 'rb_fdset_t *' with an rvalue of type 'fd_set *'
        return EmSelect (maxsocket+1, &fdreads, &fdwrites, &fderrors, &tv);
                                      ^~~~~~~~
/Users/bbannier/src/homebrew/Cellar/ruby/2.4.1_1/include/ruby-2.4.0/ruby/intern.h:456:42: note: passing argument to parameter here
int rb_thread_fd_select(int, rb_fdset_t *, rb_fdset_t *, rb_fdset_t *, struct timeval *);
                                         ^
ed.cpp:767:11: warning: implicit conversion loses integer precision: 'ssize_t' (aka 'long') to 'int' [-Wshorten-64-to-32]
                int r = read (sd, readbuffer, sizeof(readbuffer) - 1);
                    ~   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cmain.cpp:678:12: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
extern ""C"" const unsigned long evma_popen (char * const*cmd_strings)
           ^~~~~~
cmain.cpp:778:6: warning: implicit conversion loses integer precision: 'ssize_t' (aka 'long') to 'int' [-Wshorten-64-to-32]
        r = read (Fd, data, filesize);
          ~ ^~~~~~~~~~~~~~~~~~~~~~~~~
ed.cpp:980:29: warning: implicit conversion loses integer precision: 'std::__1::deque<ConnectionDescriptor::OutboundPage, std::__1::allocator<ConnectionDescriptor::OutboundPage> >::size_type' (aka
'unsigned long') to 'int' [-Wshorten-64-to-32]
        int iovcnt = OutboundPages.size();
            ~~~~~~   ~~~~~~~~~~~~~~^~~~~~
ed.cpp:1029:22: warning: implicit conversion loses integer precision: 'ssize_t' (aka 'long') to 'int' [-Wshorten-64-to-32]
        int bytes_written = writev (GetSocket(), iov, iovcnt);
            ~~~~~~~~~~~~~   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
em.cpp:946:6: error: use of undeclared identifier 'rb_thread_select'; did you mean 'rb_thread_fd_select'?
                                        EmSelect (0, NULL, NULL, NULL, &tv);
                                        ^~~~~~~~
                                        rb_thread_fd_select
./em.h:25:20: note: expanded from macro 'EmSelect'
  #define EmSelect rb_thread_select
                   ^
/Users/bbannier/src/homebrew/Cellar/ruby/2.4.1_1/include/ruby-2.4.0/ruby/intern.h:456:5: note: 'rb_thread_fd_select' declared here
int rb_thread_fd_select(int, rb_fdset_t *, rb_fdset_t *, rb_fdset_t *, struct timeval *);
    ^
em.cpp:1027:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::InstallOneshotTimer (int milliseconds)
^~~~~~
em.cpp:1049:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::ConnectToServer (const char *bind_addr, int bind_port, const char *server, int port)
^~~~~~
em.cpp:1235:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::ConnectToUnixServer (const char *server)
^~~~~~
em.cpp:1308:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::AttachFD (int fd, bool watch_mode)
^~~~~~
23 warnings generated.
em.cpp:1480:1: warning: ed.cpp:1595:11: warning: implicit conversion loses integer precision: 'ssize_t' (aka 'long') to 'int' [-Wshorten-64-to-32]
                int r = recvfrom (sd, readbuffer, sizeof(readbuffer) - 1, 0, (struct sockaddr*)&sin, &slen);
                    ~   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::CreateTcpServer (const char *server, int port)
^~~~~~
em.cpp:1563:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::OpenDatagramSocket (const char *address, int port)
^~~~~~
ed.cpp:1665:11: warning: implicit conversion loses integer precision: 'ssize_t' (aka 'long') to 'int' [-Wshorten-64-to-32]
                int s = sendto (sd, (char*)op->Buffer, op->Length, 0, (struct sockaddr*)&(op->From), sizeof(op->From));
                    ~   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
em.cpp:1826:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::CreateUnixDomainServer (const char *filename)
^~~~~~
ed.cpp:1782:24: warning: implicit conversion loses integer precision: 'unsigned long' to 'in_addr_t' (aka 'unsigned int') [-Wshorten-64-to-32]
        pin.sin_addr.s_addr = HostAddr;
                            ~ ^~~~~~~~
em.cpp:1952:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::Socketpair (char * const*cmd_strings)
^~~~~~
em.cpp:2016:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::OpenKeyboard()
^~~~~~
em.cpp:2032:28: warning: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Wshorten-64-to-32]
        return Descriptors.size() + NewDescriptors.size();
        ~~~~~~ ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~
em.cpp:2040:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::WatchPid (int pid)
^~~~~~
em.cpp:2112:1: warning: 'const' type qualifier on return type has no effect [-Wignored-qualifiers]
const unsigned long EventMachine_t::WatchFile (const char *fpath)
^~~~~~
In file included from em.cpp:23:
In file included from ./project.h:150:
./em.h:189:12: warning: private field 'NextHeartbeatTime' is not used [-Wunused-private-field]
                uint64_t NextHeartbeatTime;
                         ^
./em.h:221:22: warning: private field 'inotify' is not used [-Wunused-private-field]
                InotifyDescriptor *inotify; // pollable descriptor for our inotify instance
                                   ^
40 warnings and 3 errors generated.
make: *** [em.o] Error 1
make: *** Waiting for unfinished jobs....
23 warnings generated.
35 warnings generated.
23 warnings generated.
24 warnings generated.
25 warnings generated.
27 warnings generated.
31 warnings generated.

make failed, exit code 2

Gem files will remain installed in /Users/bbannier/src/mesos/site/vendor/ruby/2.4.0/gems/eventmachine-1.0.3 for inspection.
Results logged to /Users/bbannier/src/mesos/site/vendor/ruby/2.4.0/extensions/x86_64-darwin-16/2.4.0/eventmachine-1.0.3/gem_make.out

An error occurred while installing eventmachine (1.0.3), and Bundler cannot continue.
Make sure that `gem install eventmachine -v '1.0.3'` succeeds before bundling.

In Gemfile:
  middleman-livereload was resolved to 3.1.0, which depends on
    em-websocket was resolved to 0.5.0, which depends on
      eventmachine
{code}

It seems eventmachine-1.0.3 has known and fixed issues on macOS-10.10.1 already. I suspect there might be a similar issue for the macOS-10.12.5 I am using."	MESOS	Resolved	3	1	2531	mesosphere
13148243	OversubscriptionTest.ForwardUpdateSlaveMessage is flaky	"Observed this failure in CI,
{noformat}
[ RUN ] OversubscriptionTest.ForwardUpdateSlaveMessage
3: I0327 10:12:04.032042 18320 cluster.cpp:172] Creating default 'local' authorizer
3: I0327 10:12:04.035696 18321 master.cpp:463] Master b5c97327-11cc-4183-82ed-75e62b71cc58 (1931c74e0c4c) started on 172.17.0.2:35020
3: I0327 10:12:04.035732 18321 master.cpp:466] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/4j65Va/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/4j65Va/master"" --zk_session_timeout=""10secs""
3: I0327 10:12:04.036129 18321 master.cpp:515] Master only allowing authenticated frameworks to register
3: I0327 10:12:04.036140 18321 master.cpp:521] Master only allowing authenticated agents to register
3: I0327 10:12:04.036147 18321 master.cpp:527] Master only allowing authenticated HTTP frameworks to register
3: I0327 10:12:04.036156 18321 credentials.hpp:37] Loading credentials for authentication from '/tmp/4j65Va/credentials'
3: I0327 10:12:04.036468 18321 master.cpp:571] Using default 'crammd5' authenticator
3: I0327 10:12:04.036643 18321 http.cpp:959] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
3: I0327 10:12:04.036834 18321 http.cpp:959] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
3: I0327 10:12:04.037005 18321 http.cpp:959] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
3: I0327 10:12:04.037170 18321 master.cpp:652] Authorization enabled
3: I0327 10:12:04.037370 18338 whitelist_watcher.cpp:77] No whitelist given
3: I0327 10:12:04.037374 18322 hierarchical.cpp:175] Initialized hierarchical allocator process
3: I0327 10:12:04.040787 18321 master.cpp:2126] Elected as the leading master!
3: I0327 10:12:04.040812 18321 master.cpp:1682] Recovering from registrar
3: I0327 10:12:04.040966 18342 registrar.cpp:347] Recovering registrar
3: I0327 10:12:04.041606 18330 registrar.cpp:391] Successfully fetched the registry (0B) in 590848ns
3: I0327 10:12:04.041764 18330 registrar.cpp:495] Applied 1 operations in 57052ns; attempting to update the registry
3: I0327 10:12:04.042466 18330 registrar.cpp:552] Successfully updated the registry in 638976ns
3: I0327 10:12:04.042615 18330 registrar.cpp:424] Successfully recovered registrar
3: I0327 10:12:04.043128 18339 master.cpp:1796] Recovered 0 agents from the registry (135B); allowing 10mins for agents to reregister
3: I0327 10:12:04.043151 18326 hierarchical.cpp:213] Skipping recovery of hierarchical allocator: nothing to recover
3: W0327 10:12:04.048898 18320 process.cpp:2805] Attempted to spawn already running process files@172.17.0.2:35020
3: I0327 10:12:04.050076 18320 containerizer.cpp:304] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }
3: W0327 10:12:04.050720 18320 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges
3: W0327 10:12:04.050746 18320 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
3: I0327 10:12:04.050791 18320 provisioner.cpp:299] Using default backend 'copy'
3: I0327 10:12:04.053491 18320 cluster.cpp:460] Creating default 'local' authorizer
3: I0327 10:12:04.056531 18326 slave.cpp:261] Mesos agent started on (546)@172.17.0.2:35020
3: I0327 10:12:04.056571 18326 slave.cpp:262] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/jwt_secret_key"" --launcher=""posix"" --launcher_dir=""/tmp/SRC/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_8qkWeD"" --zk_session_timeout=""10secs""
3: I0327 10:12:04.057035 18326 credentials.hpp:86] Loading credential for authentication from '/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/credential'
3: I0327 10:12:04.057212 18326 slave.cpp:294] Agent using credential for: test-principal
3: I0327 10:12:04.057235 18326 credentials.hpp:37] Loading credentials for authentication from '/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_YeoNx5/http_credentials'
3: I0327 10:12:04.057521 18326 http.cpp:959] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'
3: I0327 10:12:04.057674 18326 http.cpp:980] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'
3: I0327 10:12:04.057922 18326 http.cpp:959] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
3: I0327 10:12:04.058051 18326 http.cpp:980] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'
3: I0327 10:12:04.058272 18326 http.cpp:959] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
3: I0327 10:12:04.058408 18326 http.cpp:980] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'
3: I0327 10:12:04.058784 18326 disk_profile_adaptor.cpp:80] Creating default disk profile adaptor module
3: I0327 10:12:04.060353 18326 slave.cpp:609] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
3: I0327 10:12:04.060569 18326 slave.cpp:617] Agent attributes: [ ]
3: I0327 10:12:04.060583 18326 slave.cpp:626] Agent hostname: 1931c74e0c4c
3: I0327 10:12:04.060739 18330 task_status_update_manager.cpp:181] Pausing sending task status updates
3: I0327 10:12:04.062536 18331 state.cpp:66] Recovering state from '/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_8qkWeD/meta'
3: I0327 10:12:04.062916 18322 task_status_update_manager.cpp:207] Recovering task status update manager
3: I0327 10:12:04.063143 18323 containerizer.cpp:674] Recovering containerizer
3: I0327 10:12:04.064961 18330 provisioner.cpp:495] Provisioner recovery complete
3: I0327 10:12:04.065325 18336 slave.cpp:7212] Finished recovery
3: I0327 10:12:04.066190 18331 task_status_update_manager.cpp:181] Pausing sending task status updates
3: I0327 10:12:04.066213 18336 slave.cpp:1260] New master detected at master@172.17.0.2:35020
3: I0327 10:12:04.066336 18336 slave.cpp:1315] Detecting new master
3: I0327 10:12:04.067641 18338 slave.cpp:1342] Authenticating with master master@172.17.0.2:35020
3: I0327 10:12:04.067776 18338 slave.cpp:1351] Using default CRAM-MD5 authenticatee
3: I0327 10:12:04.068178 18322 authenticatee.cpp:121] Creating new client SASL connection
3: I0327 10:12:04.068650 18324 master.cpp:9206] Authenticating slave(546)@172.17.0.2:35020
3: I0327 10:12:04.068862 18321 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1085)@172.17.0.2:35020
3: I0327 10:12:04.069332 18327 authenticator.cpp:98] Creating new server SASL connection
3: I0327 10:12:04.069733 18335 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
3: I0327 10:12:04.069778 18335 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
3: I0327 10:12:04.070008 18332 authenticator.cpp:204] Received SASL authentication start
3: I0327 10:12:04.070113 18332 authenticator.cpp:326] Authentication requires more steps
3: I0327 10:12:04.070336 18323 authenticatee.cpp:259] Received SASL authentication step
3: I0327 10:12:04.070583 18342 authenticator.cpp:232] Received SASL authentication step
3: I0327 10:12:04.070636 18342 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '1931c74e0c4c' server FQDN: '1931c74e0c4c' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
3: I0327 10:12:04.070659 18342 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
3: I0327 10:12:04.070724 18342 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
3: I0327 10:12:04.070760 18342 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '1931c74e0c4c' server FQDN: '1931c74e0c4c' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
3: I0327 10:12:04.070824 18342 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
3: I0327 10:12:04.070832 18342 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
3: I0327 10:12:04.070847 18342 authenticator.cpp:318] Authentication success
3: I0327 10:12:04.070940 18334 authenticatee.cpp:299] Authentication success
3: I0327 10:12:04.071063 18333 master.cpp:9236] Successfully authenticated principal 'test-principal' at slave(546)@172.17.0.2:35020
3: I0327 10:12:04.071118 18337 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1085)@172.17.0.2:35020
3: I0327 10:12:04.071286 18328 slave.cpp:1434] Successfully authenticated with master master@172.17.0.2:35020
3: I0327 10:12:04.071718 18328 slave.cpp:1877] Will retry registration in 383294ns if necessary
3: I0327 10:12:04.071923 18330 master.cpp:6326] Received register agent message from slave(546)@172.17.0.2:35020 (1931c74e0c4c)
3: I0327 10:12:04.072154 18330 master.cpp:3802] Authorizing agent providing resources 'cpus:2; mem:1024; disk:1024; ports:[31000-32000]' with principal 'test-principal'
3: I0327 10:12:04.072834 18331 master.cpp:6397] Authorized registration of agent at slave(546)@172.17.0.2:35020 (1931c74e0c4c)
3: I0327 10:12:04.072928 18331 master.cpp:6509] Registering agent at slave(546)@172.17.0.2:35020 (1931c74e0c4c) with id b5c97327-11cc-4183-82ed-75e62b71cc58-S0
3: I0327 10:12:04.073508 18329 registrar.cpp:495] Applied 1 operations in 237308ns; attempting to update the registry
3: I0327 10:12:04.074270 18321 registrar.cpp:552] Successfully updated the registry in 675072ns
3: I0327 10:12:04.074518 18335 master.cpp:6557] Admitted agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c)
3: I0327 10:12:04.075176 18335 master.cpp:6602] Registered agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c) with cpus:2; mem:1024; disk:1024; ports:[31000-32000]
3: I0327 10:12:04.075368 18323 slave.cpp:1877] Will retry registration in 26.831215ms if necessary
3: I0327 10:12:04.075518 18342 master.cpp:6326] Received register agent message from slave(546)@172.17.0.2:35020 (1931c74e0c4c)
3: I0327 10:12:04.075597 18323 slave.cpp:1481] Registered with master master@172.17.0.2:35020; given agent ID b5c97327-11cc-4183-82ed-75e62b71cc58-S0
3: I0327 10:12:04.075626 18334 hierarchical.cpp:574] Added agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 (1931c74e0c4c) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})
3: I0327 10:12:04.075739 18341 task_status_update_manager.cpp:188] Resuming sending task status updates
3: I0327 10:12:04.075709 18342 master.cpp:3802] Authorizing agent providing resources 'cpus:2; mem:1024; disk:1024; ports:[31000-32000]' with principal 'test-principal'
3: I0327 10:12:04.075896 18323 slave.cpp:1501] Checkpointing SlaveInfo to '/tmp/OversubscriptionTest_ForwardUpdateSlaveMessage_8qkWeD/meta/slaves/b5c97327-11cc-4183-82ed-75e62b71cc58-S0/slave.info'
3: I0327 10:12:04.075943 18334 hierarchical.cpp:1517] Performed allocation for 1 agents in 169342ns
3: I0327 10:12:04.076222 18339 master.cpp:6397] Authorized registration of agent at slave(546)@172.17.0.2:35020 (1931c74e0c4c)
3: I0327 10:12:04.076292 18339 master.cpp:6488] Agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c) already registered, resending acknowledgement
3: I0327 10:12:04.076493 18323 slave.cpp:1548] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""rd+fCEbpQsWYa07c\/1tXpw==""},""slave_id"":{""value"":""b5c97327-11cc-4183-82ed-75e62b71cc58-S0""},""update_oversubscribed_resources"":false}
3: W0327 10:12:04.076702 18323 slave.cpp:1530] Already registered with master master@172.17.0.2:35020
3: I0327 10:12:04.076735 18323 slave.cpp:1548] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""rd+fCEbpQsWYa07c\/1tXpw==""},""slave_id"":{""value"":""b5c97327-11cc-4183-82ed-75e62b71cc58-S0""},""update_oversubscribed_resources"":false}
3: I0327 10:12:04.077424 18343 master.cpp:7639] Ignoring update on agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c) as it reports no changes
3: I0327 10:12:04.078074 18343 master.cpp:7639] Ignoring update on agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c) as it reports no changes
3: I0327 10:12:04.080782 18341 hierarchical.cpp:1517] Performed allocation for 1 agents in 140840ns
3: /tmp/SRC/src/tests/oversubscription_tests.cpp:319: Failure
3: Value of: update.isReady()
3: Actual: true
3: Expected: false
3: I0327 10:12:04.082888 18321 slave.cpp:919] Agent terminating
3: I0327 10:12:04.083225 18335 master.cpp:1295] Agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c) disconnected
3: I0327 10:12:04.083271 18335 master.cpp:3283] Disconnecting agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c)
3: I0327 10:12:04.083369 18335 master.cpp:3302] Deactivating agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 at slave(546)@172.17.0.2:35020 (1931c74e0c4c)
3: I0327 10:12:04.083616 18341 hierarchical.cpp:766] Agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0 deactivated
3: I0327 10:12:04.092846 18320 master.cpp:1137] Master terminating
3: I0327 10:12:04.093572 18323 hierarchical.cpp:609] Removed agent b5c97327-11cc-4183-82ed-75e62b71cc58-S0
3: [ FAILED ] OversubscriptionTest.ForwardUpdateSlaveMessage (68 ms){noformat}"	MESOS	Resolved	3	1	2531	flaky-test, mesosphere
13156736	ResourceProviderManagerHttpApiTest.ResubscribeResourceProvider is flaky.	"This test is flaky on CI:
{noformat}
../../src/tests/resource_provider_manager_tests.cpp:1114: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:2972:
    Function call: subscribed(@0x7f881c00aff0 32-byte object <58-04 98-43 88-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 E0-01 01-1C 88-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{noformat}

This is different from https://issues.apache.org/jira/browse/MESOS-8315."	MESOS	Resolved	3	1	2531	flaky-test, mesosphere, storage
13039543	Update agent for hierarchical roles.	"Agents use the role name in the file system path for persistent volumes: a persistent volume is written to {{work_dir/volumes/roles/<role-name>/<persistence-id>}}. When using hierarchical roles, {{role-name}} might contain slashes. It seems like there are three options here:

# When converting the role name into the file system path, escape any slashes that appear.
# Hash the role name before using it in the file system path.
# Create a directory hierarchy that corresponds to the nesting in the role name. So a volume for role {{a/b/c/d}} would be stored in {{roles/a/b/c/d/<persistence-id>}}.

If we adopt #3, we'd probably also want to cleanup the filesystem when a volume is removed."	MESOS	Resolved	3	3	2531	mesosphere
13060632	Add resource provider to offer	"In order to introduce external resource providers we need to add an {{optional}} resource provider field to the {{Offer}} message which can be used to unambiguously identify the provider. In addition, the existing {{slave_id}} will become {{optional}} with the requirement that either {{slave_id}} or {{resource_provider_id}} is set,
{code}
message Offer {
  // ..
  optional SlaveID slave_id = 3;
  optional ResourceProviderID resource_provider_id = 11;
  // ..
}
{code}"	MESOS	Resolved	4	4	2531	csi-post-mvp, external-resources, mesosphere, storage
13141963	The 'allocatable' check in the allocator is problematic with multi-role frameworks	"The [allocatable|https://github.com/apache/mesos/blob/1.5.x/src/master/allocator/mesos/hierarchical.cpp#L2471-L2479] check in the allocator (shown below) was originally introduced to help alleviate the situation where a framework receives only disk, but not cpu/memory, thus cannot launch a task.

{code}
bool HierarchicalAllocatorProcess::allocatable(
    const Resources& resources)
{
  Option<double> cpus = resources.cpus();
  Option<Bytes> mem = resources.mem();

  return (cpus.isSome() && cpus.get() >= MIN_CPUS) ||
         (mem.isSome() && mem.get() >= MIN_MEM);
}
{code}

When we introduce multi-role capability to the frameworks, this check makes less sense now. For instance, consider the following case:
1) There is a single agent and a single framework in the cluster
2) The agent has cpu/memory reserved to role A, and disk reserved to B
3) The framework subscribes to both role A and role B
4) The framework expects that it'll receive an offer containing the resources on the agent
5) However, the framework receives no disk resources due to the following [code|https://github.com/apache/mesos/blob/1.5.x/src/master/allocator/mesos/hierarchical.cpp#L2078-L2100]. This is counter intuitive.

{code}
void HierarchicalAllocatorProcess::__allocate()
{
  ...
  Resources resources = available.allocatableTo(role);
  if (!allocatable(resources)) {
    break;
  }
  ...
}

bool Resources::isAllocatableTo(
    const Resource& resource,
    const std::string& role)
{
  CHECK(!resource.has_role()) << resource;
  CHECK(!resource.has_reservation()) << resource;

  return isUnreserved(resource) ||
         role == reservationRole(resource) ||
         roles::isStrictSubroleOf(role, reservationRole(resource));
}
{code}

Two comments:
1) If `allocatable` check is still necessary (see MESOS-7398)?
2) If we want to keep `allocatable` check for the original purpose, we should do that based on framework not role, given that a framework can subscribe to multiple roles now?

Some related JIRAs:
MESOS-1688
MESOS-7398
"	MESOS	Resolved	3	1	2531	mesosphere, multitenancy
13226978	StorageLocalResourceProviderTest.CsiPluginRpcMetrics is flaky.	"From an internal CI run:
{noformat}
[ RUN      ] StorageLocalResourceProviderTest.CsiPluginRpcMetrics
06:56:26 I0409 06:56:26.350445 23181 cluster.cpp:176] Creating default 'local' authorizer
06:56:26 malloc_consolidate(): invalid chunk size
06:56:26 *** Aborted at 1554792986 (unix time) try ""date -d @1554792986"" if you are using GNU date ***
06:56:26 PC: @     0x7f1cf4481f3b (unknown)
06:56:26 *** SIGABRT (@0x5a8d) received by PID 23181 (TID 0x7f1ce9be8700) from PID 23181; stack trace: ***
06:56:26     @     0x7f1cf461b8e0 __GI___pthread_rwlock_rdlock
06:56:26     @     0x7f1cf4481f3b (unknown)
06:56:26     @     0x7f1cf44832f1 (unknown)
06:56:26     @     0x7f1cf44c4867 (unknown)
06:56:26     @     0x7f1cf44cae0a (unknown)
06:56:26     @     0x7f1cf44cb10e (unknown)
06:56:26     @     0x7f1cf44cddad (unknown)
06:56:26     @     0x7f1cf44cf7dd (unknown)
06:56:26     @     0x7f1cf4a647a8 (unknown)
06:56:26     @     0x7f1cf88d0805 google::LogMessage::Init()
06:56:26     @     0x7f1cf88d10ac google::LogMessage::LogMessage()
06:56:26     @     0x7f1cf752a46a mesos::internal::master::Master::initialize()
06:56:26     @     0x7f1cf882bd72 process::ProcessManager::resume()
06:56:26     @     0x7f1cf88303c6 _ZNSt6thread11_State_implISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
06:56:26     @     0x7f1cf4a8ee6f (unknown)
06:56:26     @     0x7f1cf4610f2a (unknown)
06:56:26     @     0x7f1cf4543edf (unknown)
{noformat}"	MESOS	Resolved	2	1	2531	storage
13075533	Specifying an unbundled dependency can cause build to pick up wrong Boost version	"Specifying an unbundled dependency can cause the build to pick up a wrong Boost version. Assuming we have e.g., both protobuf and Boost installed in {{PREFIX}}, configuring with {{--with-protobuf=PREFIX}} causes the build to pick up the Boost version from {{PREFIX}} instead of using the bundled one.

This appears to be due to how we specify Boost include paths. Boost paths are added with {{-isystem}} to suppress warnings; the protobuf include path, on the other hand, would be added with {{-I}}. GCC and for compatibility clang first search all paths specified with {{-I}} left-to-right before looking at paths given with {{-isystem}}, see [the GCC documenation|https://gcc.gnu.org/onlinedocs/gcc/Directory-Options.html]."	MESOS	Resolved	3	1	2531	autotools, mesosphere
13252173	master::Slave::hasExecutor occupies 37% of a 150 second perf sample.	"If you drop the attached perf stacks into flamescope, you can see that mesos::internal::master::Slave::hasExecutor occupies 37% of the overall samples!

This function does 3 hashmap lookups, 1 can be eliminated for a quick win. However, the larger improvement here will come from eliminating many of the calls to this function.

This was reported by [~carlone]."	MESOS	Resolved	3	4	2531	foundations, performance
12926937	Provide constexpr Duration::min() and max()	"{{Duration}} could be implemented so that it can provide {{constexpr}} {{min}} and {{max}} functions.

This addresses an existing {{TODO}}."	MESOS	Resolved	4	4	2531	tech-debt
12980118	Implement clang-tidy check for incorrect use of capturing lambdas with Futures	"When one enqueues capturing lambdas to a {{Future}} with {{then}} or the {{onXXX}} variations, in general any actor might execute that callback (no constraints imposed per se).

This can lead to hard to understand dependencies or bugs if the lambda needs to access external state (i.e. anything it captures by references/pointer to instead of by value); instead such callbacks should always be constraint to a specific actor with {{dispatch}}/{{defer}} to ensure the pointed to data isn't modified in a concurrent thread."	MESOS	Resolved	3	4	2531	mesosphere
13013785	Possible nullptr dereference in flag loading	"Coverity reports the following:
{code}
/3rdparty/stout/include/stout/flags/flags.hpp: 375 in flags::FlagsBase::add<mesos::internal::logger::rotate::Flags, std::basic_string<char, std::char_traits<char>, std::allocator<char>>, char [10], mesos::internal::logger::rotate::Flags::Flags()::[lambda(const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&) (instance 1)]>(T2 T1::*, const flags::Name &, const Option<flags::Name> &, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&, const T3 *, T4)::[lambda(flags::FlagsBase*, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&) (instance 1)]::operator ()(flags::FlagsBase*, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&) const()
369         Flags* flags = dynamic_cast<Flags*>(base);
370         if (base != nullptr) {
371           // NOTE: 'fetch' ""retrieves"" the value if necessary and then
372           // invokes 'parse'. See 'fetch' for more details.
373           Try<T1> t = fetch<T1>(value);
374           if (t.isSome()) {
   CID 1374083:    (FORWARD_NULL)
   Dereferencing null pointer ""flags"".
375             flags->*t1 = t.get();
376           } else {
377             return Error(""Failed to load value '"" + value + ""': "" + t.error());
378           }
379         }
380     

** CID 1374082:  Null pointer dereferences  (FORWARD_NULL)
/3rdparty/stout/include/stout/flags/flags.hpp: 375 in flags::FlagsBase::add<mesos::internal::logger::LoggerFlags, Bytes, Megabytes, Option<Error> (*)(const Bytes &)>(T2 T1::*, const flags::Name &, Option<flags::Name>&, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&, const T3 *, T4)::[lambda(flags::FlagsBase*, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&) (instance 1)]::operator ()(flags::FlagsBase*, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&) const()


________________________________________________________________________________________________________
*** CID 1374082:  Null pointer dereferences  (FORWARD_NULL)
/3rdparty/stout/include/stout/flags/flags.hpp: 375 in flags::FlagsBase::add<mesos::internal::logger::LoggerFlags, Bytes, Megabytes, Option<Error> (*)(const Bytes &)>(T2 T1::*, const flags::Name &, Option<flags::Name>&, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&, const T3 *, T4)::[lambda(flags::FlagsBase*, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&) (instance 1)]::operator ()(flags::FlagsBase*, const std::basic_string<char, std::char_traits<char>, std::allocator<char>>&) const()
369         Flags* flags = dynamic_cast<Flags*>(base);
370         if (base != nullptr) {
371           // NOTE: 'fetch' ""retrieves"" the value if necessary and then
372           // invokes 'parse'. See 'fetch' for more details.
373           Try<T1> t = fetch<T1>(value);
374           if (t.isSome()) {
   CID 1374082:  Null pointer dereferences  (FORWARD_NULL)
   Dereferencing null pointer ""flags"".
375             flags->*t1 = t.get();
376           } else {
377             return Error(""Failed to load value '"" + value + ""': "" + t.error());
378           }
379         }
380     
{code}

The {{dynamic_cast}} is needed here if the derived {{Flags}} class got intentionally sliced (e.g., to a {{FlagsBase}}). Since the base class of the hierarchy ({{FlagsBase}}) stores the flags they would not be sliced away; the {{dynamic_cast}} here effectively filters out all flags still valid for the {{Flags}} used when the {{Flag}} was {{add}}'ed.

It seems the intention here was to confirm that the {{dynamic_cast}} to {{Flags*}} succeeded like is done e.g., in {{flags.stringify}} and {{flags.validate}} just below.


AFAICT this code has existed since 2013, but was only reported by coverity recently."	MESOS	Resolved	3	1	2531	coverity
12945377	LevelDBStateTests write to the current directory	"All {{LevelDBStateTest}} tests write to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing tests from the same suite in parallel (e.g., with {{gtest-parallel}} would race on the existence of the created files, and show bogus behavior.

The tests should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture."	MESOS	Resolved	3	1	2531	newbie, parallel-tests
13003510	Add a parallel test runner	In order to allow parallelization of the test execution we should add a parallel test executor to Mesos, and subsequently activate it in the build setup.	MESOS	Resolved	3	4	2531	mesosphere
12940482	Add allocator metric for currently satisfied quotas	We currently expose information on set quotas via dedicated quota endpoints. To diagnose allocator problems one additionally needs information about used quotas.	MESOS	Resolved	3	4	2531	mesosphere
13187160	Make SLRP be able to update its volumes and storage pools.	"We should consider making SLRP update its resources periodically, or adding an endpoint to trigger that, for the following reasons:

1. Mesos currently assumes all profiles have disjoint storage pools. This is because Mesos models each resource independently. However, in practice an operatorcan set up, say two profiles, one for linear volumes and one for raid volumes,and an ""LVM"" resource provider that can provision both linear and raid volumes. The correlation between the storage pools of the linear and raid profiles would reduce one's pool capacity when a volume of the other type is provisioned. To reflect the actual sizes of correlated storage pools, we need a way to make SLRP update its resources.

2. The SLRP now only queries the CSI plugin to report a list of volumes during startup, so if a new device is added, the operator will have to restart the agent to trigger another SLRP startup, which is inconvenient."	MESOS	Resolved	2	4	2531	mesosphere, mesosphere-dss-post-ga, storage
13102338	reviewboard's GUESS_FIELDS setting leads to redundant information in commit messages	"Reviewboard can be set up to automatically guess a patch's summary and description when {{GUESS_FIELDS}} is set. For commits that have no dedicated description, it uses the commit summary as description as well. This leads to commits with redundant commit messages, e.g.,

{code}
Frobnicated the foobarizer.

Frobnicated the foobarizer.

Review: https://reviews.apache.org/r/1234567890
{code}

When applying this commit with e.g., {{apply_reviews.py}} the redundant body is faithfully copied, but we should consider updating it to instead remove the redundant information automatically leading to e.g.,

{code}
Frobnicated the foobarizer.

Review: https://reviews.apache.org/r/1234567890
{code}"	MESOS	Resolved	4	1	2531	mesosphere, newbie, python
13129722	Master's UpdateSlave handler not correctly updating terminated operations	"I created a test that verifies that operation status updates are resent to the master after being dropped en route to it (MESOS-8420).

The test does the following:

# Creates a volume from a RAW disk resource.
# Drops the first `UpdateOperationStatusMessage` message from the agent to the master, so that it isn't acknowledged by the master.
# Restarts the agent.
# Verifies that the agent resends the operation status update.

The good news are that the agent is resending the operation status update, the bad news are that it triggers a CHECK failure that crashes the master.

Here are the relevant sections of the log produced by the test:

{noformat}
[ RUN      ] StorageLocalResourceProviderTest.ROOT_RetryOperationStatusUpdateAfterRecovery
[...]
I0109 16:36:08.515882 24106 master.cpp:4284] Processing ACCEPT call for offers: [ 046b3f21-6e97-4a56-9a13-773f7d481efd-O0 ] on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (default) at scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1@10.0.49.2:40681
I0109 16:36:08.516487 24106 master.cpp:5260] Processing CREATE_VOLUME operation with source disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096 from framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (default) at scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1@10.0.49.2:40681 to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev)
I0109 16:36:08.518704 24106 master.cpp:10622] Sending operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev)
I0109 16:36:08.521210 24130 provider.cpp:504] Received APPLY_OPERATION event
I0109 16:36:08.521276 24130 provider.cpp:1368] Received CREATE_VOLUME operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408)
I0109 16:36:08.523131 24432 test_csi_plugin.cpp:305] CreateVolumeRequest '{""version"":{""minor"":1},""name"":""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408"",""capacityRange"":{""requiredBytes"":""4294967296"",""limitBytes"":""4294967296""},""volumeCapabilities"":[{""mount"":{},""accessMode"":{""mode"":""SINGLE_NODE_WRITER""}}]}'
I0109 16:36:08.525806 24152 provider.cpp:2635] Applying conversion from 'disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096' to 'disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096' for operation (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408)
I0109 16:36:08.528725 24134 status_update_manager_process.hpp:152] Received operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.529207 24134 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.573177 24150 http.cpp:1185] HTTP POST for /slave(2)/api/v1/resource_provider from 10.0.49.2:53598
I0109 16:36:08.573974 24139 slave.cpp:7065] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)'
I0109 16:36:08.574154 24139 slave.cpp:7409] Updating the state of operation ' with no ID (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.574785 24139 slave.cpp:7249] Forwarding status update of operation with no ID (operation_uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000
I0109 16:36:08.583748 24084 slave.cpp:931] Agent terminating
I0109 16:36:08.584115 24144 master.cpp:1305] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev) disconnected
[...]
I0109 16:36:08.655766 24140 slave.cpp:1378] Re-registered with master master@10.0.49.2:40681
I0109 16:36:08.655936 24117 task_status_update_manager.cpp:188] Resuming sending task status updates
I0109 16:36:08.655995 24149 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
I0109 16:36:08.656008 24140 slave.cpp:1423] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""icuAKyO6TymMt2Y9vyF6Jg==""},""slave_id"":{""value"":""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""},""update_oversubscribed_resources"":true}
I0109 16:36:08.656121 24149 hierarchical.cpp:754] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 reactivated
W0109 16:36:08.656481 24113 master.cpp:7277] !!!! update slave message: slave_id {
  value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
}
update_oversubscribed_resources: true
operations {
}
resource_version_uuid {
  value: ""\211\313\200+#\272O)\214\267f=\277!z&""
}
I0109 16:36:08.656637 24113 master.cpp:7320] Received update of agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(3)@10.0.49.2:40681 (core-dev) with total oversubscribed resources {}
W0109 16:36:08.657387 24113 master.cpp:7704] Performing explicit reconciliation with agent for known operation 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 since it was not present in original reconciliation message from agent
I0109 16:36:08.657917 24133 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
W0109 16:36:08.658048 24125 manager.cpp:472] Dropping operation reconciliation message with operation_uuid 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 because resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01 is not subscribed
I0109 16:36:08.658609 24143 container_daemon.cpp:119] Launching container 'org-apache-mesos-rp-local-storage-test--org-apache-mesos-csi-test-slrp_test--CONTROLLER_SERVICE-NODE_SERVICE'
[...]
I0109 16:36:08.689859 24130 provider.cpp:3066] Sending UPDATE_STATE call with resources 'disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096' and 1 operations to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.690449 24130 provider.cpp:1042] Resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01 is in READY state
I0109 16:36:08.690491 24105 status_update_manager_process.hpp:385] Resuming operation status update manager
I0109 16:36:08.690640 24105 status_update_manager_process.hpp:394] Sending operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.693244 24131 http.cpp:1185] HTTP POST for /slave(3)/api/v1/resource_provider from 10.0.49.2:53606
I0109 16:36:08.693912 24140 http.cpp:1185] HTTP POST for /slave(3)/api/v1/resource_provider from 10.0.49.2:53606
I0109 16:36:08.693974 24115 manager.cpp:677] Received UPDATE_STATE call with resources '[{""disk"":{""source"":{""id"":""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408"",""metadata"":{""labels"":[{""key"":""path"",""value"":""\/tmp\/n5thZ3\/test\/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""}]},""mount"":{""root"":"".\/csi\/org.apache.mesos.csi.test\/slrp_test\/mounts\/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""},""profile"":""volume-default"",""type"":""MOUNT""}},""name"":""disk"",""provider_id"":{""value"":""605b22f5-e39d-4d9f-950a-e7f44d202c01""},""reservations"":[{""role"":""storage"",""type"":""DYNAMIC""}],""scalar"":{""value"":4096.0},""type"":""SCALAR""}]' and 1 operations from resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01
I0109 16:36:08.694897 24144 slave.cpp:7065] Handling resource provider message 'UPDATE_STATE: 605b22f5-e39d-4d9f-950a-e7f44d202c01 disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096'
I0109 16:36:08.695184 24144 slave.cpp:7182] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096
I0109 16:36:08.696467 24144 slave.cpp:7065] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)'
I0109 16:36:08.696594 24144 slave.cpp:7409] Updating the state of operation ' with no ID (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.696666 24144 slave.cpp:7249] Forwarding status update of operation with no ID (operation_uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000
W0109 16:36:08.697093 24142 master.cpp:7277] !!!! update slave message: slave_id {
  value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
}
update_oversubscribed_resources: false
operations {
}
resource_version_uuid {
  value: ""\211\313\200+#\272O)\214\267f=\277!z&""
}
resource_providers {
  providers {
    info {
      id {
        value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
      }
      type: ""org.apache.mesos.rp.local.storage""
      name: ""test""
      default_reservations {
        role: ""storage""
        type: DYNAMIC
      }
      storage {
        plugin {
          type: ""org.apache.mesos.csi.test""
          name: ""slrp_test""
          containers {
            [...]
          }
        }
      }
    }
    total_resources {
      name: ""disk""
      type: SCALAR
      scalar {
        value: 4096
      }
      disk {
        source {
          type: MOUNT
          mount {
            root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
          }
          id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
          metadata {
            labels {
              key: ""path""
              value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
            }
          }
          profile: ""volume-default""
        }
      }
      provider_id {
        value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
      }
      reservations {
        role: ""storage""
        type: DYNAMIC
      }
    }
    operations {
      operations {
        framework_id {
          value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-0000""
        }
        slave_id {
          value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
        }
        info {
          type: CREATE_VOLUME
          create_volume {
            source {
              name: ""disk""
              type: SCALAR
              scalar {
                value: 4096
              }
              disk {
                source {
                  type: RAW
                  profile: ""volume-default""
                }
              }
              allocation_info {
                role: ""storage""
              }
              provider_id {
                value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
              }
              reservations {
                role: ""storage""
                type: DYNAMIC
              }
            }
            target_type: MOUNT
          }
        }
        latest_status {
          state: OPERATION_FINISHED
          converted_resources {
            name: ""disk""
            type: SCALAR
            scalar {
              value: 4096
            }
            disk {
              source {
                type: MOUNT
                mount {
                  root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                }
                id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                metadata {
                  labels {
                    key: ""path""
                    value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                  }
                }
                profile: ""volume-default""
              }
            }
            allocation_info {
              role: ""storage""
            }
            provider_id {
              value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
            }
            reservations {
              role: ""storage""
              type: DYNAMIC
            }
          }
          uuid {
            value: ""\014y\315\362\270\235E;\273bWvn\226\215\320""
          }
        }
        statuses {
          state: OPERATION_FINISHED
          converted_resources {
            name: ""disk""
            type: SCALAR
            scalar {
              value: 4096
            }
            disk {
              source {
                type: MOUNT
                mount {
                  root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                }
                id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                metadata {
                  labels {
                    key: ""path""
                    value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                  }
                }
                profile: ""volume-default""
              }
            }
            allocation_info {
              role: ""storage""
            }
            provider_id {
              value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
            }
            reservations {
              role: ""storage""
              type: DYNAMIC
            }
          }
          uuid {
            value: ""\014y\315\362\270\235E;\273bWvn\226\215\320""
          }
        }
        uuid {
          value: ""\030\264\304\245\321bM\317\273!\241<n\340\364\010""
        }
      }
    }
    resource_version_uuid {
      value: ""M\250\313j\320\301IG\262\0164e\004\367\304\333""
    }
  }
}
I0109 16:36:08.700137 24142 master.cpp:10411] Updating the state of operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) of framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.700417 24146 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000]
F0109 16:36:08.700610 24142 master.cpp:11687] CHECK_SOME(resources): disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000] does not contain disk(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096
*** Check failure stack trace: ***
F0109 16:36:08.700896 24146 hierarchical.cpp:908] CHECK_SOME(updatedTotal): disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000] does not contain disk(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096
*** Check failure stack trace: ***
    @     0x7ff06d3bbe7e  (unknown)
    @     0x7ff06d3bbe7e  (unknown)
    @     0x7ff06d3bbddd  (unknown)
    @     0x7ff06d3bbddd  (unknown)
    @     0x7ff06d3bb7ee  (unknown)
    @     0x7ff06d3bb7ee  (unknown)
    @     0x7ff06d3be522  (unknown)
    @     0x55c1c6c2be77  _ZTSN6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureISt4listIN5mesos5slave13QoSCorrectionESaISF_EEEEEclINS0_IFSI_vEEEEESI_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISH_EESt14default_deleteISU_EEOSM_S3_E_ISX_SM_St12_PlaceholderILi1EEEEEEE
    @     0x7ff06d3be522  (unknown)
    @     0x55c1c6c2be77  _ZTSN6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureISt4listIN5mesos5slave13QoSCorrectionESaISF_EEEEEclINS0_IFSI_vEEEEESI_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISH_EESt14default_deleteISU_EEOSM_S3_E_ISX_SM_St12_PlaceholderILi1EEEEEEE
    @     0x7ff06b729277  (unknown)
    @     0x55c1c6f3be8a  _ZTSN6lambda12CallableOnceIFvRK6ResultIN5mesos2v117resource_provider5EventEEEE10CallableFnINS_8internal7PartialIZNK7process6FutureIS6_E7onReadyISt5_BindIFSt7_Mem_fnIMSG_FbS8_EESG_St12_PlaceholderILi1EEEEbEERKSG_OT_NSG_6PreferEEUlOSQ_S8_E_ISQ_SO_EEEEE
{noformat}

We can see that once the SLRP reregisters with the agent, the following happens:

# The agent will send an {{UpdateSlave}} message to the master including the converted resources and the {{CREATE_VOLUME}} operation with the status {{OPERATION_FINISHED}}.
# The master will update the agent's resources, including the volume created by the operation.
# The agent will resend the operation status update.
# The master will try to apply the operation and crash, because it already updated the agent's resources on step #2."	MESOS	Resolved	1	1	2531	mesosphere
13040969	mesos-execute rejects all offers	"Mesos now includes {{Resource.AllocationInfo}} in the resources sent in an offer.

An {{Resources}} instance without {{Resource.AllocationInfo}} will not be contained in one with it set. The subtraction operator will also treat those instances different.

This makes {{mesos-execute}} reject all offers.

We need to update {{mesos-execute}} and probably other C++ frameworks in our repo that use the Resources class."	MESOS	Resolved	1	1	2732	resources
12831562	Add -> operator for Option<T>, Try<T>, Result<T>, Future<T>.	"Let's add operator overloads to Option<T> to allow access to the underlying T using the `->` operator.
"	MESOS	Resolved	3	4	2732	c++11, option, stout, twitter
12613220	Allow new masters to have better understanding of cluster state	"If a new master becomes elected, it will only have knowledge of the current state of the cluster. This can lead to a situation where tasks become lost but aren't properly killed. For instance:

1) A set of machines (perhaps a datacenter rack) lose network connectivity and their tasks are marked LOST by the master. However, they're still running.
2) Through a potentially unrelated situation, there is a master failover to a new master
3) The network connection to the machines come back up
4) These slaves never killed their tasks (and they shouldn't if they can't talk to a master)
5) Tasks stay running and aren't killed, taking up resources and running outside the scope of the new master"	MESOS	Resolved	2	4	2732	twitter
13102085	v1/scheduler: Revive/Suppress role field should be marked experimental for 1.2.x branch	"The role field of the v1 scheduler API's Revive and Suppress call should have been marked as experimental since it was part of the experimental MULTI-ROLE feature. The field has replaced in 1.3.x by a ""repeated string roles"" field."	MESOS	Open	3	1	2732	mesosphere
13244139	RoleTest.RolesEndpointContainsConsumedQuota is flaky.	"{noformat}
[ RUN      ] RoleTest.RolesEndpointContainsConsumedQuota
I0710 07:05:42.670790  9995 cluster.cpp:176] Creating default 'local' authorizer
I0710 07:05:42.672238  9999 master.cpp:440] Master 8db40cec-43ef-41a1-89a4-4f7b877d8f13 (ip-172-16-10-69.ec2.internal) started on 172.16.10.69:37082
I0710 07:05:42.672256  9999 master.cpp:443] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""hierarchical"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authentication_v0_timeout=""15secs"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/1d0m6o/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_operator_event_stream_subscribers=""1000"" --max_unreachable_tasks_per_framework=""1000"" --memory_profiling=""false"" --min_allocatable_resources=""cpus:0.01|mem:32"" --port=""5050"" --publish_per_framework_metrics=""true"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --role_sorter=""drf"" --root_submissions=""true"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/1d0m6o/master"" --zk_session_timeout=""10secs""
I0710 07:05:42.672351  9999 master.cpp:492] Master only allowing authenticated frameworks to register
I0710 07:05:42.672356  9999 master.cpp:498] Master only allowing authenticated agents to register
I0710 07:05:42.672360  9999 master.cpp:504] Master only allowing authenticated HTTP frameworks to register
I0710 07:05:42.672364  9999 credentials.hpp:37] Loading credentials for authentication from '/tmp/1d0m6o/credentials'
I0710 07:05:42.672430  9999 master.cpp:548] Using default 'crammd5' authenticator
I0710 07:05:42.672466  9999 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0710 07:05:42.672508  9999 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0710 07:05:42.672538  9999 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0710 07:05:42.672569  9999 master.cpp:629] Authorization enabled
I0710 07:05:42.672658 10001 hierarchical.cpp:241] Initialized hierarchical allocator process
I0710 07:05:42.672685 10001 whitelist_watcher.cpp:77] No whitelist given
I0710 07:05:42.673316 10001 master.cpp:2150] Elected as the leading master!
I0710 07:05:42.673331 10001 master.cpp:1664] Recovering from registrar
I0710 07:05:42.673616 10001 registrar.cpp:339] Recovering registrar
I0710 07:05:42.673874 10001 registrar.cpp:383] Successfully fetched the registry (0B) in 239104ns
I0710 07:05:42.673923 10001 registrar.cpp:487] Applied 1 operations in 7745ns; attempting to update the registry
I0710 07:05:42.674052  9999 registrar.cpp:544] Successfully updated the registry in 108032ns
I0710 07:05:42.674082  9999 registrar.cpp:416] Successfully recovered registrar
I0710 07:05:42.674152  9999 master.cpp:1799] Recovered 0 agents from the registry (180B); allowing 10mins for agents to reregister
I0710 07:05:42.674185  9996 hierarchical.cpp:280] Skipping recovery of hierarchical allocator: nothing to recover
W0710 07:05:42.676100  9995 process.cpp:2877] Attempted to spawn already running process files@172.16.10.69:37082
I0710 07:05:42.676537  9995 containerizer.cpp:314] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }
I0710 07:05:42.678514  9995 linux_launcher.cpp:144] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
I0710 07:05:42.678980  9995 provisioner.cpp:298] Using default backend 'copy'
I0710 07:05:42.680043  9995 cluster.cpp:510] Creating default 'local' authorizer
I0710 07:05:42.680832  9998 slave.cpp:265] Mesos agent started on (522)@172.16.10.69:37082
I0710 07:05:42.680850  9998 slave.cpp:266] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/1d0m6o/qvvVks/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/1d0m6o/qvvVks/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/1d0m6o/qvvVks/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/1d0m6o/qvvVks/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/1d0m6o/qvvVks/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/1d0m6o/qvvVks/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/1d0m6o/qvvVks/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-centos-6/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus(role):1;mem(role):10;disk:0;ports:[]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1IIJA6"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK"" --zk_session_timeout=""10secs""
I0710 07:05:42.681233  9998 credentials.hpp:86] Loading credential for authentication from '/tmp/1d0m6o/qvvVks/credential'
I0710 07:05:42.681301  9998 slave.cpp:298] Agent using credential for: test-principal
I0710 07:05:42.681313  9998 credentials.hpp:37] Loading credentials for authentication from '/tmp/1d0m6o/qvvVks/http_credentials'
I0710 07:05:42.681396  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'
I0710 07:05:42.681429  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'
I0710 07:05:42.681473  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0710 07:05:42.681493  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'
I0710 07:05:42.681519  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0710 07:05:42.681535  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'
W0710 07:05:42.681565  9995 process.cpp:2877] Attempted to spawn already running process version@172.16.10.69:37082
I0710 07:05:42.681998  9998 disk_profile_adaptor.cpp:78] Creating default disk profile adaptor module
I0710 07:05:42.682571  9998 slave.cpp:613] Agent resources: [{""name"":""cpus"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":10.0},""type"":""SCALAR""}]
I0710 07:05:42.682636  9998 slave.cpp:621] Agent attributes: [  ]
I0710 07:05:42.682643  9998 slave.cpp:630] Agent hostname: ip-172-16-10-69.ec2.internal
I0710 07:05:42.682718  9996 task_status_update_manager.cpp:181] Pausing sending task status updates
I0710 07:05:42.682737  9996 status_update_manager_process.hpp:379] Pausing operation status update manager
I0710 07:05:42.682947  9998 state.cpp:67] Recovering state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/meta'
I0710 07:05:42.683017  9998 slave.cpp:7246] Finished recovering checkpointed state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/meta', beginning agent recovery
I0710 07:05:42.683105  9998 task_status_update_manager.cpp:207] Recovering task status update manager
I0710 07:05:42.683320 10002 containerizer.cpp:796] Recovering Mesos containers
I0710 07:05:42.683378 10002 linux_launcher.cpp:286] Recovering Linux launcher
I0710 07:05:42.683485 10002 containerizer.cpp:1122] Recovering isolators
I0710 07:05:42.683931 10000 containerizer.cpp:1161] Recovering provisioner
I0710 07:05:42.684257 10001 provisioner.cpp:498] Provisioner recovery complete
I0710 07:05:42.684689  9997 composing.cpp:339] Finished recovering all containerizers
I0710 07:05:42.684762  9997 slave.cpp:7708] Recovering executors
I0710 07:05:42.684785  9997 slave.cpp:7861] Finished recovery
I0710 07:05:42.685237  9996 status_update_manager_process.hpp:379] Pausing operation status update manager
I0710 07:05:42.685223 10003 task_status_update_manager.cpp:181] Pausing sending task status updates
I0710 07:05:42.685498  9995 sched.cpp:239] Version: 1.9.0
I0710 07:05:42.685214  9997 slave.cpp:1258] New master detected at master@172.16.10.69:37082
I0710 07:05:42.685556  9997 slave.cpp:1323] Detecting new master
I0710 07:05:42.685786  9997 sched.cpp:343] New master detected at master@172.16.10.69:37082
I0710 07:05:42.686059  9997 sched.cpp:408] Authenticating with master master@172.16.10.69:37082
I0710 07:05:42.686069  9997 sched.cpp:415] Using default CRAM-MD5 authenticatee
I0710 07:05:42.686162  9997 authenticatee.cpp:121] Creating new client SASL connection
I0710 07:05:42.686285  9999 master.cpp:10380] Authenticating scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.686347  9999 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1050)@172.16.10.69:37082
I0710 07:05:42.686412  9999 authenticator.cpp:98] Creating new server SASL connection
I0710 07:05:42.686487  9999 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0710 07:05:42.686499  9999 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0710 07:05:42.686535  9999 authenticator.cpp:204] Received SASL authentication start
I0710 07:05:42.686586  9999 authenticator.cpp:326] Authentication requires more steps
I0710 07:05:42.686622  9999 authenticatee.cpp:259] Received SASL authentication step
I0710 07:05:42.686669  9999 authenticator.cpp:232] Received SASL authentication step
I0710 07:05:42.686686  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0710 07:05:42.686715  9999 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0710 07:05:42.686728  9999 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0710 07:05:42.686738  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0710 07:05:42.686743  9999 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.686749  9999 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.686763  9999 authenticator.cpp:318] Authentication success
I0710 07:05:42.686810  9999 authenticatee.cpp:299] Authentication success
I0710 07:05:42.686854  9999 master.cpp:10412] Successfully authenticated principal 'test-principal' at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.686887 10000 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1050)@172.16.10.69:37082
I0710 07:05:42.687379  9999 sched.cpp:520] Successfully authenticated with master master@172.16.10.69:37082
I0710 07:05:42.687400  9999 sched.cpp:835] Sending SUBSCRIBE call to master@172.16.10.69:37082
I0710 07:05:42.687448  9999 sched.cpp:870] Will retry registration in 667.029736ms if necessary
I0710 07:05:42.687536 10001 master.cpp:2890] Received SUBSCRIBE call for framework 'default' at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.687556 10001 master.cpp:2222] Authorizing framework principal 'test-principal' to receive offers for roles '{ role }'
I0710 07:05:42.687672  9998 master.cpp:2977] Subscribing framework default with checkpointing disabled and capabilities [ MULTI_ROLE, RESERVATION_REFINEMENT ]
I0710 07:05:42.688201  9998 master.cpp:10610] Adding framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 with roles {  } suppressed
I0710 07:05:42.688334 10000 sched.cpp:751] Framework registered with 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.688364 10000 sched.cpp:770] Scheduler::registered took 14639ns
I0710 07:05:42.689081  9998 hierarchical.cpp:368] Added framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.689308  9998 hierarchical.cpp:1508] Performed allocation for 0 agents in 29346ns
I0710 07:05:42.693774  9997 slave.cpp:1350] Authenticating with master master@172.16.10.69:37082
I0710 07:05:42.693804  9997 slave.cpp:1359] Using default CRAM-MD5 authenticatee
I0710 07:05:42.693876  9997 authenticatee.cpp:121] Creating new client SASL connection
I0710 07:05:42.693958  9997 master.cpp:10380] Authenticating slave(522)@172.16.10.69:37082
I0710 07:05:42.693998  9997 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1051)@172.16.10.69:37082
I0710 07:05:42.694056  9997 authenticator.cpp:98] Creating new server SASL connection
I0710 07:05:42.694115  9997 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0710 07:05:42.694125  9997 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0710 07:05:42.694160  9999 authenticator.cpp:204] Received SASL authentication start
I0710 07:05:42.694198  9999 authenticator.cpp:326] Authentication requires more steps
I0710 07:05:42.694236  9999 authenticatee.cpp:259] Received SASL authentication step
I0710 07:05:42.694277  9999 authenticator.cpp:232] Received SASL authentication step
I0710 07:05:42.694290  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0710 07:05:42.694296  9999 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0710 07:05:42.694304  9999 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0710 07:05:42.694312  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0710 07:05:42.694319  9999 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.694336  9999 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.694345  9999 authenticator.cpp:318] Authentication success
I0710 07:05:42.694392 10001 authenticatee.cpp:299] Authentication success
I0710 07:05:42.694452  9999 master.cpp:10412] Successfully authenticated principal 'test-principal' at slave(522)@172.16.10.69:37082
I0710 07:05:42.694469 10001 slave.cpp:1450] Successfully authenticated with master master@172.16.10.69:37082
I0710 07:05:42.694512 10003 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1051)@172.16.10.69:37082
I0710 07:05:42.694573 10001 slave.cpp:1900] Will retry registration in 15.73611ms if necessary
I0710 07:05:42.694640 10001 master.cpp:6900] Received register agent message from slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.694720 10001 master.cpp:4099] Authorizing agent providing resources 'cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10' with principal 'test-principal'
I0710 07:05:42.694792 10001 master.cpp:3721] Authorizing principal 'test-principal' to reserve resources 'cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10'
I0710 07:05:42.694949 10001 master.cpp:6967] Authorized registration of agent at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.694988 10001 master.cpp:7082] Registering agent at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with id 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0
I0710 07:05:42.695119 10001 registrar.cpp:487] Applied 1 operations in 52120ns; attempting to update the registry
I0710 07:05:42.695276 10001 registrar.cpp:544] Successfully updated the registry in 137216ns
I0710 07:05:42.695327 10001 master.cpp:7130] Admitted agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.695451 10001 master.cpp:7175] Registered agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10
I0710 07:05:42.695500 10000 hierarchical.cpp:617] Added agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 (ip-172-16-10-69.ec2.internal) with cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10 (allocated: {})
I0710 07:05:42.695525 10001 slave.cpp:1483] Registered with master master@172.16.10.69:37082; given agent ID 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0
I0710 07:05:42.695639 10000 hierarchical.cpp:1508] Performed allocation for 1 agents in 92565ns
I0710 07:05:42.695673 10000 task_status_update_manager.cpp:188] Resuming sending task status updates
I0710 07:05:42.695745 10001 slave.cpp:1518] Checkpointing SlaveInfo to '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/meta/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/slave.info'
I0710 07:05:42.695773 10000 master.cpp:10195] Sending offers [ 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O0 ] to framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.695832 10000 status_update_manager_process.hpp:385] Resuming operation status update manager
I0710 07:05:42.696385 10001 slave.cpp:1570] Forwarding agent update {""operations"":{},""resource_providers"":{},""resource_version_uuid"":{""value"":""RXSv7a6tQBuTqO8uJ7NIJw==""},""slave_id"":{""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0""},""update_oversubscribed_resources"":false}
I0710 07:05:42.696590 10001 master.cpp:8261] Ignoring update on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) as it reports no changes
I0710 07:05:42.696941 10000 sched.cpp:934] Scheduler::resourceOffers took 1.002341ms
I0710 07:05:42.697464 10000 master.cpp:12470] Removing offer 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O0
I0710 07:05:42.697743 10000 master.cpp:4636] Processing ACCEPT call for offers: [ 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O0 ] on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.697794 10000 master.cpp:3653] Authorizing framework principal 'test-principal' to launch task 26c92e4e-ef13-4984-9b06-b18f880decf3
W0710 07:05:42.698462 10003 validation.cpp:1640] Executor 'dummy' for task '26c92e4e-ef13-4984-9b06-b18f880decf3' uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0710 07:05:42.698487 10003 validation.cpp:1652] Executor 'dummy' for task '26c92e4e-ef13-4984-9b06-b18f880decf3' uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0710 07:05:42.698549 10003 master.cpp:4171] Adding executor 'dummy' with resources {} of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.698590 10003 master.cpp:4197] Adding task 26c92e4e-ef13-4984-9b06-b18f880decf3 with resources cpus(allocated: role)(reservations: [(STATIC,role)]):1; mem(allocated: role)(reservations: [(STATIC,role)]):10 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.698686 10003 master.cpp:5615] Launching task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 with resources [{""allocation_info"":{""role"":""role""},""name"":""cpus"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""role""},""name"":""mem"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":10.0},""type"":""SCALAR""}] on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) on  new executor
I0710 07:05:42.698873 10000 hierarchical.cpp:1432] Allocation paused
I0710 07:05:42.698909 10000 hierarchical.cpp:1442] Allocation resumed
I0710 07:05:42.698961 10003 slave.cpp:2037] Got assigned task '26c92e4e-ef13-4984-9b06-b18f880decf3' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.699254 10003 slave.cpp:2411] Authorizing task '26c92e4e-ef13-4984-9b06-b18f880decf3' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.699282 10003 slave.cpp:9198] Authorizing framework principal 'test-principal' to launch task 26c92e4e-ef13-4984-9b06-b18f880decf3
I0710 07:05:42.699653 10003 slave.cpp:2854] Launching task '26c92e4e-ef13-4984-9b06-b18f880decf3' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.699702 10003 paths.cpp:801] Creating sandbox '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' for user 'root'
W0710 07:05:42.701002  9995 process.cpp:2877] Attempted to spawn already running process files@172.16.10.69:37082
I0710 07:05:42.701611  9995 containerizer.cpp:314] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }
I0710 07:05:42.702396 10003 slave.cpp:9708] Launching executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 with resources [] in work directory '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166'
I0710 07:05:42.702580 10003 slave.cpp:3080] Queued task '26c92e4e-ef13-4984-9b06-b18f880decf3' for executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.702809 10003 slave.cpp:3528] Launching container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 for executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.703053 10003 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' to virtual path '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/latest'
I0710 07:05:42.703063 10000 containerizer.cpp:1357] Starting container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.703073 10003 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' to virtual path '/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/latest'
I0710 07:05:42.703085 10003 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' to virtual path '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166'
I0710 07:05:42.703593 10000 containerizer.cpp:1529] Checkpointed ContainerConfig at '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1IIJA6/containers/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166/config'
I0710 07:05:42.703614 10000 containerizer.cpp:3277] Transitioning the state of container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 from PROVISIONING to PREPARING
I0710 07:05:42.705749  9998 containerizer.cpp:2055] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""sleep 3600""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.16.10.69:37082""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166""},{""name"":""MESOS_EXECUTOR_AUTHENTICATION_TOKEN"",""type"":""VALUE"",""value"":""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjaWQiOiI4YTdkOWIwYS1lYmNmLTRmOGEtYWE3Yy0xYmJjYWRiMGMxNjYiLCJlaWQiOiJkdW1teSIsImZpZCI6IjhkYjQwY2VjLTQzZWYtNDFhMS04OWE0LTRmN2I4NzdkOGYxMy0wMDAwIn0.EfKHriG000A1P2MwWmmXnaLmTpTVdnl5-ZER_MFOkzQ""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""dummy""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(522)@172.16.10.69:37082""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166""}]},""task_environment"":{},""user"":""root"",""working_directory"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166""}"" --pipe_read=""22"" --pipe_write=""24"" --runtime_directory=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1IIJA6/containers/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166"" --unshare_namespace_mnt=""false""'
I0710 07:05:42.706059 10002 linux_launcher.cpp:492] Launching container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 and cloning with namespaces 
I0710 07:05:42.706065  9995 linux_launcher.cpp:144] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
I0710 07:05:42.710623  9995 provisioner.cpp:298] Using default backend 'copy'
I0710 07:05:42.712615  9995 cluster.cpp:510] Creating default 'local' authorizer
I0710 07:05:42.714180 10001 slave.cpp:265] Mesos agent started on (523)@172.16.10.69:37082
I0710 07:05:42.714211 10001 slave.cpp:266] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/1d0m6o/9m8Ujd/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/1d0m6o/9m8Ujd/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/1d0m6o/9m8Ujd/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/1d0m6o/9m8Ujd/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/1d0m6o/9m8Ujd/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/1d0m6o/9m8Ujd/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/1d0m6o/9m8Ujd/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-centos-6/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:10;mem:100;disk:0;ports:[]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_vKyXvR"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv"" --zk_session_timeout=""10secs""
I0710 07:05:42.714774 10001 credentials.hpp:86] Loading credential for authentication from '/tmp/1d0m6o/9m8Ujd/credential'
I0710 07:05:42.715389 10001 slave.cpp:298] Agent using credential for: test-principal
I0710 07:05:42.715405 10001 credentials.hpp:37] Loading credentials for authentication from '/tmp/1d0m6o/9m8Ujd/http_credentials'
I0710 07:05:42.715535 10001 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'
I0710 07:05:42.715889 10001 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'
I0710 07:05:42.716038 10001 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0710 07:05:42.716095 10001 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'
I0710 07:05:42.716150 10001 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0710 07:05:42.716186 10001 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'
I0710 07:05:42.716320 10001 disk_profile_adaptor.cpp:78] Creating default disk profile adaptor module
I0710 07:05:42.717517  9998 containerizer.cpp:3277] Transitioning the state of container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 from PREPARING to ISOLATING
I0710 07:05:42.717968  9998 containerizer.cpp:3277] Transitioning the state of container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 from ISOLATING to FETCHING
I0710 07:05:42.718013  9998 fetcher.cpp:369] Starting to fetch URIs for container: 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166, directory: /tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.718289  9998 containerizer.cpp:3277] Transitioning the state of container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 from FETCHING to RUNNING
I0710 07:05:42.718997 10001 slave.cpp:613] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":10.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":100.0},""type"":""SCALAR""}]
I0710 07:05:42.719040 10001 slave.cpp:621] Agent attributes: [  ]
I0710 07:05:42.719048 10001 slave.cpp:630] Agent hostname: ip-172-16-10-69.ec2.internal
I0710 07:05:42.719467 10000 status_update_manager_process.hpp:379] Pausing operation status update manager
I0710 07:05:42.719682  9999 state.cpp:67] Recovering state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/meta'
I0710 07:05:42.719738  9999 slave.cpp:7246] Finished recovering checkpointed state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/meta', beginning agent recovery
I0710 07:05:42.719815 10001 task_status_update_manager.cpp:181] Pausing sending task status updates
I0710 07:05:42.719830 10001 task_status_update_manager.cpp:207] Recovering task status update manager
I0710 07:05:42.719913 10000 containerizer.cpp:796] Recovering Mesos containers
I0710 07:05:42.719964 10000 linux_launcher.cpp:286] Recovering Linux launcher
I0710 07:05:42.720106 10000 linux_launcher.cpp:343] Recovered container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.720118 10000 linux_launcher.cpp:437] 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 is a known orphaned container
I0710 07:05:42.720191 10000 containerizer.cpp:1122] Recovering isolators
I0710 07:05:42.720398 10000 containerizer.cpp:1161] Recovering provisioner
I0710 07:05:42.720507 10000 provisioner.cpp:498] Provisioner recovery complete
I0710 07:05:42.720561 10000 containerizer.cpp:1233] Cleaning up orphan container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.720571 10000 containerizer.cpp:2575] Destroying container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 in RUNNING state
I0710 07:05:42.720577 10000 containerizer.cpp:3277] Transitioning the state of container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 from RUNNING to DESTROYING
I0710 07:05:42.720619 10000 containerizer.cpp:3116] Container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 has exited
I0710 07:05:42.720666 10000 linux_launcher.cpp:576] Asked to destroy container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.720688 10000 linux_launcher.cpp:618] Destroying cgroup '/cgroup/freezer/mesos/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166'
I0710 07:05:42.720826 10000 cgroups.cpp:2854] Freezing cgroup /cgroup/freezer/mesos/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.720971 10000 cgroups.cpp:1242] Successfully froze cgroup /cgroup/freezer/mesos/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 after 123904ns
I0710 07:05:42.721382  9996 cgroups.cpp:2872] Thawing cgroup /cgroup/freezer/mesos/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.721483  9996 cgroups.cpp:1271] Successfully thawed cgroup /cgroup/freezer/mesos/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 after 79872ns
I0710 07:05:42.721833  9997 composing.cpp:339] Finished recovering all containerizers
I0710 07:05:42.721921  9997 slave.cpp:7708] Recovering executors
I0710 07:05:42.721946  9997 slave.cpp:7861] Finished recovery
I0710 07:05:42.722301  9999 task_status_update_manager.cpp:181] Pausing sending task status updates
I0710 07:05:42.722313 10002 slave.cpp:1258] New master detected at master@172.16.10.69:37082
I0710 07:05:42.722318  9999 status_update_manager_process.hpp:379] Pausing operation status update manager
I0710 07:05:42.722340 10002 slave.cpp:1323] Detecting new master
I0710 07:05:42.726434  9997 containerizer.cpp:3116] Container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 has exited
I0710 07:05:42.726450  9997 containerizer.cpp:2575] Destroying container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 in RUNNING state
I0710 07:05:42.726457  9997 containerizer.cpp:3277] Transitioning the state of container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 from RUNNING to DESTROYING
I0710 07:05:42.727011 10000 posix.hpp:119] Ignoring cleanup request for unknown container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.727073  9997 posix.hpp:119] Ignoring cleanup request for unknown container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.727084  9999 linux_launcher.cpp:576] Asked to destroy container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
W0710 07:05:42.727108  9999 linux_launcher.cpp:612] Couldn't find freezer cgroup for container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 so assuming partially destroyed
I0710 07:05:42.727242  9999 provisioner.cpp:609] Ignoring destroy request for unknown container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.727774 10002 provisioner.cpp:609] Ignoring destroy request for unknown container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.728112 10002 slave.cpp:6613] Executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 terminated with signal Killed
I0710 07:05:42.728158 10002 slave.cpp:5545] Handling status update TASK_FAILED (Status UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 from @0.0.0.0:0
I0710 07:05:42.728368 10002 master.cpp:9087] Executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal): terminated with signal Killed
E0710 07:05:42.728380  9998 slave.cpp:5876] Failed to update resources for container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 of executor 'dummy' running task 26c92e4e-ef13-4984-9b06-b18f880decf3 on status update for terminal task, destroying container: Container not found
I0710 07:05:42.728389 10002 master.cpp:11957] Removing executor 'dummy' with resources {} of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
W0710 07:05:42.728435  9998 composing.cpp:609] Attempted to destroy unknown container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166
I0710 07:05:42.728485  9999 sched.cpp:1144] Executor dummy on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 exited with status 9

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: executorLost(0x7ffdf32e2180, @0x7fa4b40560d0 dummy, @0x7fa4b4042230 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0, 9)
NOTE: You can safely ignore the above warning unless this call should not happen.  Do not suppress it by blindly adding an EXPECT_CALL() if you don't mean to enforce the call.  See https://github.com/google/googletest/blob/master/googlemock/docs/CookBook.md#knowing-when-to-expect for details.
I0710 07:05:42.728511  9999 sched.cpp:1155] Scheduler::executorLost took 17655ns
I0710 07:05:42.728528  9998 task_status_update_manager.cpp:328] Received task status update TASK_FAILED (Status UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.728538  9998 task_status_update_manager.cpp:507] Creating StatusUpdate stream for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.728665  9998 task_status_update_manager.cpp:383] Forwarding task status update TASK_FAILED (Status UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 to the agent
I0710 07:05:42.728718 10002 slave.cpp:6037] Forwarding the update TASK_FAILED (Status UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 to master@172.16.10.69:37082
I0710 07:05:42.728778 10002 slave.cpp:5930] Task status update manager successfully handled status update TASK_FAILED (Status UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.728833 10002 master.cpp:8750] Status update TASK_FAILED (Status UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 from agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.728853 10002 master.cpp:8807] Forwarding status update TASK_FAILED (Status UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.728927 10002 master.cpp:11819] Updating the state of task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
I0710 07:05:42.729089 10002 sched.cpp:1042] Scheduler::statusUpdate took 13020ns
I0710 07:05:42.729220  9999 master.cpp:6511] Processing ACKNOWLEDGE call for status ad5b35b9-3e9c-49ca-9e26-766a18bacf27 for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0
I0710 07:05:42.729223 10002 hierarchical.cpp:1218] Recovered cpus(allocated: role)(reservations: [(STATIC,role)]):1; mem(allocated: role)(reservations: [(STATIC,role)]):10 (total: cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10, allocated: {}) on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 from framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.729259  9999 master.cpp:11917] Removing task 26c92e4e-ef13-4984-9b06-b18f880decf3 with resources cpus(allocated: role)(reservations: [(STATIC,role)]):1; mem(allocated: role)(reservations: [(STATIC,role)]):10 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.729396  9999 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.729426  9999 task_status_update_manager.cpp:538] Cleaning up status update stream for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.729501  9999 slave.cpp:4684] Task status update manager successfully handled status update acknowledgement (UUID: ad5b35b9-3e9c-49ca-9e26-766a18bacf27) for task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.729516  9999 slave.cpp:10377] Completing task 26c92e4e-ef13-4984-9b06-b18f880decf3
I0710 07:05:42.729526  9999 slave.cpp:6724] Cleaning up executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.729727  9999 slave.cpp:6853] Cleaning up framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.729831  9996 gc.cpp:95] Scheduling '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' for gc 6.99999155561482days in the future
I0710 07:05:42.729882  9996 gc.cpp:95] Scheduling '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy' for gc 6.99999155469926days in the future
I0710 07:05:42.729931  9996 gc.cpp:95] Scheduling '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000' for gc 6.99999155361185days in the future
I0710 07:05:42.729969  9998 task_status_update_manager.cpp:289] Closing task status update streams for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.732446 10002 slave.cpp:1350] Authenticating with master master@172.16.10.69:37082
I0710 07:05:42.732475 10002 slave.cpp:1359] Using default CRAM-MD5 authenticatee
I0710 07:05:42.732589 10002 authenticatee.cpp:121] Creating new client SASL connection
I0710 07:05:42.732667 10002 master.cpp:10380] Authenticating slave(523)@172.16.10.69:37082
I0710 07:05:42.732733  9996 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1052)@172.16.10.69:37082
I0710 07:05:42.732798  9998 authenticator.cpp:98] Creating new server SASL connection
I0710 07:05:42.732856  9998 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0710 07:05:42.732868  9998 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0710 07:05:42.732946  9998 authenticator.cpp:204] Received SASL authentication start
I0710 07:05:42.732995  9998 authenticator.cpp:326] Authentication requires more steps
I0710 07:05:42.733026  9998 authenticatee.cpp:259] Received SASL authentication step
I0710 07:05:42.733062  9998 authenticator.cpp:232] Received SASL authentication step
I0710 07:05:42.733076  9998 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0710 07:05:42.733084  9998 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0710 07:05:42.733101  9998 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0710 07:05:42.733110  9998 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0710 07:05:42.733117  9998 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.733124  9998 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.733155  9998 authenticator.cpp:318] Authentication success
I0710 07:05:42.733201  9998 authenticatee.cpp:299] Authentication success
I0710 07:05:42.733222  9996 master.cpp:10412] Successfully authenticated principal 'test-principal' at slave(523)@172.16.10.69:37082
I0710 07:05:42.733235  9998 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1052)@172.16.10.69:37082
I0710 07:05:42.733302  9996 slave.cpp:1450] Successfully authenticated with master master@172.16.10.69:37082
I0710 07:05:42.733389  9996 slave.cpp:1900] Will retry registration in 7.605677ms if necessary
I0710 07:05:42.733439 10000 master.cpp:6900] Received register agent message from slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.733523 10000 master.cpp:4099] Authorizing agent providing resources 'cpus:10; mem:100' with principal 'test-principal'
I0710 07:05:42.733680 10000 master.cpp:6967] Authorized registration of agent at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.733718 10000 master.cpp:7082] Registering agent at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with id 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1
I0710 07:05:42.733886 10001 registrar.cpp:487] Applied 1 operations in 44086ns; attempting to update the registry
I0710 07:05:42.734061 10001 registrar.cpp:544] Successfully updated the registry in 140032ns
I0710 07:05:42.734213  9996 master.cpp:7130] Admitted agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.734323  9996 master.cpp:7175] Registered agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with cpus:10; mem:100
I0710 07:05:42.734393  9996 slave.cpp:1483] Registered with master master@172.16.10.69:37082; given agent ID 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1
I0710 07:05:42.734426  9999 task_status_update_manager.cpp:188] Resuming sending task status updates
I0710 07:05:42.734390 10002 hierarchical.cpp:617] Added agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 (ip-172-16-10-69.ec2.internal) with cpus:10; mem:100 (allocated: {})
I0710 07:05:42.734593 10002 hierarchical.cpp:1508] Performed allocation for 1 agents in 117981ns
I0710 07:05:42.734598  9996 slave.cpp:1518] Checkpointing SlaveInfo to '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/meta/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/slave.info'
I0710 07:05:42.734688  9997 master.cpp:10195] Sending offers [ 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O1 ] to framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.734988  9997 sched.cpp:934] Scheduler::resourceOffers took 183157ns
I0710 07:05:42.735193 10002 status_update_manager_process.hpp:385] Resuming operation status update manager
I0710 07:05:42.735244 10000 master.cpp:12470] Removing offer 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O1
I0710 07:05:42.735322 10000 master.cpp:4636] Processing ACCEPT call for offers: [ 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O1 ] on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.735352 10000 master.cpp:3653] Authorizing framework principal 'test-principal' to launch task 569d27cb-b353-42d0-8bd6-54c09f89f89d
I0710 07:05:42.735504  9996 slave.cpp:1570] Forwarding agent update {""operations"":{},""resource_providers"":{},""resource_version_uuid"":{""value"":""FsYsMjR8RZaQN9nUJYAd2A==""},""slave_id"":{""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1""},""update_oversubscribed_resources"":false}
W0710 07:05:42.735776  9996 validation.cpp:1640] Executor 'dummy' for task '569d27cb-b353-42d0-8bd6-54c09f89f89d' uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0710 07:05:42.735791  9996 validation.cpp:1652] Executor 'dummy' for task '569d27cb-b353-42d0-8bd6-54c09f89f89d' uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0710 07:05:42.735822  9996 master.cpp:4171] Adding executor 'dummy' with resources {} of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.735844  9996 master.cpp:4197] Adding task 569d27cb-b353-42d0-8bd6-54c09f89f89d with resources cpus(allocated: role):10; mem(allocated: role):100 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.735924  9996 master.cpp:5615] Launching task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 with resources [{""allocation_info"":{""role"":""role""},""name"":""cpus"",""scalar"":{""value"":10.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""role""},""name"":""mem"",""scalar"":{""value"":100.0},""type"":""SCALAR""}] on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) on  new executor
I0710 07:05:42.736100  9996 master.cpp:8261] Ignoring update on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) as it reports no changes
I0710 07:05:42.736245  9996 slave.cpp:2037] Got assigned task '569d27cb-b353-42d0-8bd6-54c09f89f89d' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.736505 10000 hierarchical.cpp:1432] Allocation paused
I0710 07:05:42.736522 10000 hierarchical.cpp:1442] Allocation resumed
I0710 07:05:42.736714  9996 slave.cpp:2411] Authorizing task '569d27cb-b353-42d0-8bd6-54c09f89f89d' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.736740  9996 slave.cpp:9198] Authorizing framework principal 'test-principal' to launch task 569d27cb-b353-42d0-8bd6-54c09f89f89d
I0710 07:05:42.737113  9996 slave.cpp:2854] Launching task '569d27cb-b353-42d0-8bd6-54c09f89f89d' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.737164  9996 paths.cpp:801] Creating sandbox '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5' for user 'root'
I0710 07:05:42.738385  9996 slave.cpp:9708] Launching executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 with resources [] in work directory '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5'
I0710 07:05:42.738695  9996 slave.cpp:3080] Queued task '569d27cb-b353-42d0-8bd6-54c09f89f89d' for executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.739418  9996 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5' to virtual path '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/latest'
I0710 07:05:42.739511  9996 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5' to virtual path '/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/latest'
I0710 07:05:42.739528  9996 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5' to virtual path '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5'
I0710 07:05:42.739715  9996 slave.cpp:3528] Launching container e65bd434-1435-4c4c-a1cc-c0924710bcd5 for executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.739837 10002 containerizer.cpp:1357] Starting container e65bd434-1435-4c4c-a1cc-c0924710bcd5
W0710 07:05:42.740772  9995 process.cpp:2877] Attempted to spawn already running process files@172.16.10.69:37082
I0710 07:05:42.741189  9995 containerizer.cpp:314] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }
I0710 07:05:42.741294 10002 containerizer.cpp:1529] Checkpointed ContainerConfig at '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_vKyXvR/containers/e65bd434-1435-4c4c-a1cc-c0924710bcd5/config'
I0710 07:05:42.741308 10002 containerizer.cpp:3277] Transitioning the state of container e65bd434-1435-4c4c-a1cc-c0924710bcd5 from PROVISIONING to PREPARING
I0710 07:05:42.742360  9997 containerizer.cpp:2055] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""sleep 3600""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.16.10.69:37082""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5""},{""name"":""MESOS_EXECUTOR_AUTHENTICATION_TOKEN"",""type"":""VALUE"",""value"":""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjaWQiOiJlNjViZDQzNC0xNDM1LTRjNGMtYTFjYy1jMDkyNDcxMGJjZDUiLCJlaWQiOiJkdW1teSIsImZpZCI6IjhkYjQwY2VjLTQzZWYtNDFhMS04OWE0LTRmN2I4NzdkOGYxMy0wMDAwIn0.QI3SX1Bm7RFY5Z9unqU5UG8U2NTHMNFxdKrqWq87IIY""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""dummy""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(523)@172.16.10.69:37082""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5""}]},""task_environment"":{},""user"":""root"",""working_directory"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5""}"" --pipe_read=""28"" --pipe_write=""41"" --runtime_directory=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_vKyXvR/containers/e65bd434-1435-4c4c-a1cc-c0924710bcd5"" --unshare_namespace_mnt=""false""'
I0710 07:05:42.742801 10000 linux_launcher.cpp:492] Launching container e65bd434-1435-4c4c-a1cc-c0924710bcd5 and cloning with namespaces 
I0710 07:05:42.751811  9997 containerizer.cpp:3277] Transitioning the state of container e65bd434-1435-4c4c-a1cc-c0924710bcd5 from PREPARING to ISOLATING
I0710 07:05:42.753885  9997 containerizer.cpp:3277] Transitioning the state of container e65bd434-1435-4c4c-a1cc-c0924710bcd5 from ISOLATING to FETCHING
I0710 07:05:42.753968  9997 fetcher.cpp:369] Starting to fetch URIs for container: e65bd434-1435-4c4c-a1cc-c0924710bcd5, directory: /tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.754302  9997 containerizer.cpp:3277] Transitioning the state of container e65bd434-1435-4c4c-a1cc-c0924710bcd5 from FETCHING to RUNNING
I0710 07:05:42.757205  9995 linux_launcher.cpp:144] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
I0710 07:05:42.757709  9995 provisioner.cpp:298] Using default backend 'copy'
I0710 07:05:42.758428  9995 cluster.cpp:510] Creating default 'local' authorizer
I0710 07:05:42.760093  9998 slave.cpp:265] Mesos agent started on (524)@172.16.10.69:37082
I0710 07:05:42.760107  9998 slave.cpp:266] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/1d0m6o/nhq4g2/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/1d0m6o/nhq4g2/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/1d0m6o/nhq4g2/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/1d0m6o/nhq4g2/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/1d0m6o/nhq4g2/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/1d0m6o/nhq4g2/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/1d0m6o/nhq4g2/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-centos-6/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus(role):100;mem(role):1000;cpus:1000;mem:10000;;disk:0;ports:[]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_y8F3mG"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1vh3sk"" --zk_session_timeout=""10secs""
I0710 07:05:42.760334  9998 credentials.hpp:86] Loading credential for authentication from '/tmp/1d0m6o/nhq4g2/credential'
I0710 07:05:42.760402  9998 slave.cpp:298] Agent using credential for: test-principal
I0710 07:05:42.760412  9998 credentials.hpp:37] Loading credentials for authentication from '/tmp/1d0m6o/nhq4g2/http_credentials'
I0710 07:05:42.760509  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'
I0710 07:05:42.760550  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'
I0710 07:05:42.760598  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0710 07:05:42.760618  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'
I0710 07:05:42.760644  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0710 07:05:42.760660  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'
I0710 07:05:42.760718  9998 disk_profile_adaptor.cpp:78] Creating default disk profile adaptor module
I0710 07:05:42.761492  9998 slave.cpp:613] Agent resources: [{""name"":""cpus"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":100.0},""type"":""SCALAR""},{""name"":""mem"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":1000.0},""type"":""SCALAR""},{""name"":""cpus"",""scalar"":{""value"":1000.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":10000.0},""type"":""SCALAR""}]
I0710 07:05:42.761574  9998 slave.cpp:621] Agent attributes: [  ]
I0710 07:05:42.761582  9998 slave.cpp:630] Agent hostname: ip-172-16-10-69.ec2.internal
I0710 07:05:42.761935 10002 task_status_update_manager.cpp:181] Pausing sending task status updates
I0710 07:05:42.761960 10002 status_update_manager_process.hpp:379] Pausing operation status update manager
I0710 07:05:42.762143  9998 state.cpp:67] Recovering state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1vh3sk/meta'
I0710 07:05:42.762202  9998 slave.cpp:7246] Finished recovering checkpointed state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1vh3sk/meta', beginning agent recovery
I0710 07:05:42.762287  9998 task_status_update_manager.cpp:207] Recovering task status update manager
I0710 07:05:42.762373 10001 containerizer.cpp:796] Recovering Mesos containers
I0710 07:05:42.762420  9998 linux_launcher.cpp:286] Recovering Linux launcher
I0710 07:05:42.762591  9998 linux_launcher.cpp:343] Recovered container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.762603  9998 linux_launcher.cpp:437] e65bd434-1435-4c4c-a1cc-c0924710bcd5 is a known orphaned container
I0710 07:05:42.762668  9998 containerizer.cpp:1122] Recovering isolators
I0710 07:05:42.762888  9998 containerizer.cpp:1161] Recovering provisioner
I0710 07:05:42.763167 10003 provisioner.cpp:498] Provisioner recovery complete
I0710 07:05:42.763443  9998 containerizer.cpp:1233] Cleaning up orphan container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.763458  9998 containerizer.cpp:2575] Destroying container e65bd434-1435-4c4c-a1cc-c0924710bcd5 in RUNNING state
I0710 07:05:42.763484  9998 containerizer.cpp:3277] Transitioning the state of container e65bd434-1435-4c4c-a1cc-c0924710bcd5 from RUNNING to DESTROYING
I0710 07:05:42.763531  9998 containerizer.cpp:3116] Container e65bd434-1435-4c4c-a1cc-c0924710bcd5 has exited
I0710 07:05:42.763595 10003 linux_launcher.cpp:576] Asked to destroy container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.763646 10003 linux_launcher.cpp:618] Destroying cgroup '/cgroup/freezer/mesos/e65bd434-1435-4c4c-a1cc-c0924710bcd5'
I0710 07:05:42.763885 10003 cgroups.cpp:2854] Freezing cgroup /cgroup/freezer/mesos/e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.764073 10003 cgroups.cpp:1242] Successfully froze cgroup /cgroup/freezer/mesos/e65bd434-1435-4c4c-a1cc-c0924710bcd5 after 141056ns
I0710 07:05:42.764142 10003 composing.cpp:339] Finished recovering all containerizers
I0710 07:05:42.764317 10003 cgroups.cpp:2872] Thawing cgroup /cgroup/freezer/mesos/e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.764420 10003 slave.cpp:7708] Recovering executors
I0710 07:05:42.764467 10003 slave.cpp:7861] Finished recovery
I0710 07:05:42.765122  9996 slave.cpp:1258] New master detected at master@172.16.10.69:37082
I0710 07:05:42.765321  9997 task_status_update_manager.cpp:181] Pausing sending task status updates
I0710 07:05:42.765341  9997 status_update_manager_process.hpp:379] Pausing operation status update manager
I0710 07:05:42.765812 10003 cgroups.cpp:1271] Successfully thawed cgroup /cgroup/freezer/mesos/e65bd434-1435-4c4c-a1cc-c0924710bcd5 after 1.467904ms
I0710 07:05:42.766065  9996 slave.cpp:1323] Detecting new master
I0710 07:05:42.766970 10002 slave.cpp:1350] Authenticating with master master@172.16.10.69:37082
I0710 07:05:42.767000 10002 slave.cpp:1359] Using default CRAM-MD5 authenticatee
I0710 07:05:42.767086 10002 authenticatee.cpp:121] Creating new client SASL connection
I0710 07:05:42.767170 10002 master.cpp:10380] Authenticating slave(524)@172.16.10.69:37082
I0710 07:05:42.767230 10002 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1053)@172.16.10.69:37082
I0710 07:05:42.767287 10002 authenticator.cpp:98] Creating new server SASL connection
I0710 07:05:42.767347 10002 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0710 07:05:42.767360 10002 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0710 07:05:42.767390 10002 authenticator.cpp:204] Received SASL authentication start
I0710 07:05:42.767428 10002 authenticator.cpp:326] Authentication requires more steps
I0710 07:05:42.767462 10002 authenticatee.cpp:259] Received SASL authentication step
I0710 07:05:42.767513 10001 authenticator.cpp:232] Received SASL authentication step
I0710 07:05:42.767534 10001 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0710 07:05:42.767544 10001 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0710 07:05:42.767558 10001 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0710 07:05:42.767567 10001 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0710 07:05:42.767573 10001 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.767578 10001 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0710 07:05:42.767591 10001 authenticator.cpp:318] Authentication success
I0710 07:05:42.767629 10003 authenticatee.cpp:299] Authentication success
I0710 07:05:42.767637  9999 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1053)@172.16.10.69:37082
I0710 07:05:42.767684 10003 slave.cpp:1450] Successfully authenticated with master master@172.16.10.69:37082
I0710 07:05:42.767706 10001 master.cpp:10412] Successfully authenticated principal 'test-principal' at slave(524)@172.16.10.69:37082
I0710 07:05:42.767781 10003 slave.cpp:1900] Will retry registration in 13.7987ms if necessary
I0710 07:05:42.767853 10003 master.cpp:6900] Received register agent message from slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.767944 10003 master.cpp:4099] Authorizing agent providing resources 'cpus(reservations: [(STATIC,role)]):100; mem(reservations: [(STATIC,role)]):1000; cpus:1000; mem:10000' with principal 'test-principal'
I0710 07:05:42.768015 10003 master.cpp:3721] Authorizing principal 'test-principal' to reserve resources 'cpus(reservations: [(STATIC,role)]):100; mem(reservations: [(STATIC,role)]):1000; cpus:1000; mem:10000'
I0710 07:05:42.768442 10003 master.cpp:6967] Authorized registration of agent at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.768492 10003 master.cpp:7082] Registering agent at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with id 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2
I0710 07:05:42.768646 10003 registrar.cpp:487] Applied 1 operations in 54580ns; attempting to update the registry
I0710 07:05:42.768805 10003 registrar.cpp:544] Successfully updated the registry in 134144ns
I0710 07:05:42.768859 10003 master.cpp:7130] Admitted agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.769022 10003 master.cpp:7175] Registered agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with cpus(reservations: [(STATIC,role)]):100; mem(reservations: [(STATIC,role)]):1000; cpus:1000; mem:10000
I0710 07:05:42.769095 10003 slave.cpp:1483] Registered with master master@172.16.10.69:37082; given agent ID 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2
I0710 07:05:42.769098  9998 hierarchical.cpp:617] Added agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 (ip-172-16-10-69.ec2.internal) with cpus(reservations: [(STATIC,role)]):100; mem(reservations: [(STATIC,role)]):1000; cpus:1000; mem:10000 (allocated: {})
I0710 07:05:42.769269  9998 hierarchical.cpp:1508] Performed allocation for 1 agents in 117869ns
I0710 07:05:42.769294  9998 task_status_update_manager.cpp:188] Resuming sending task status updates
I0710 07:05:42.769302 10003 slave.cpp:1518] Checkpointing SlaveInfo to '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1vh3sk/meta/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2/slave.info'
I0710 07:05:42.769402  9998 master.cpp:10195] Sending offers [ 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O2 ] to framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.769469  9998 status_update_manager_process.hpp:385] Resuming operation status update manager
I0710 07:05:42.769570  9998 sched.cpp:934] Scheduler::resourceOffers took 16191ns
I0710 07:05:42.769608 10003 slave.cpp:1570] Forwarding agent update {""operations"":{},""resource_providers"":{},""resource_version_uuid"":{""value"":""NxZjtIFYTJmNFPUgPJ4Odw==""},""slave_id"":{""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2""},""update_oversubscribed_resources"":false}
I0710 07:05:42.769840 10003 master.cpp:8261] Ignoring update on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) as it reports no changes
I0710 07:05:42.770727 10001 process.cpp:3671] Handling HTTP event for process 'master' with path: '/master/roles'
I0710 07:05:42.771086 10001 http.cpp:1115] HTTP GET for /master/roles from 172.16.10.69:47106
../../src/tests/role_tests.cpp:615: Failure
      Expected: *expected
      Which is: 24-byte object <03-00 00-00 57-55 00-00 A0-DD DE-4C 57-55 00-00 01-00 00-00 00-00 00-00>
To be equal to: *parse
      Which is: 24-byte object <03-00 00-00 00-00 00-00 50-D8 DE-4C 57-55 00-00 00-00 00-00 00-00 00-00>
expected {""roles"":[{""frameworks"":[""8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000""],""name"":""role"",""quota"":{""consumed"":{""cpus"":111.0,""mem"":1110.0},""guarantee"":{},""limit"":{},""role"":""role""},""resources"":{""cpus"":1111.0,""disk"":0,""gpus"":0,""mem"":11110.0},""weight"":1.0}]} vs actual {""roles"":[{""frameworks"":[""8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000""],""name"":""role"",""quota"":{""consumed"":{""cpus"":111.0,""mem"":1110.0},""guarantee"":{},""limit"":{},""role"":""role""},""resources"":{""cpus"":1110.0,""disk"":0.0,""gpus"":0.0,""mem"":11100.0},""weight"":1.0}]}
I0710 07:05:42.772022  9995 slave.cpp:912] Agent terminating
I0710 07:05:42.782775  9997 master.cpp:1295] Agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) disconnected
I0710 07:05:42.782796  9997 master.cpp:3379] Disconnecting agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.782812  9997 master.cpp:3398] Deactivating agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 at slave(524)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.782881 10000 hierarchical.cpp:799] Agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 deactivated
I0710 07:05:42.782927  9997 master.cpp:12470] Removing offer 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O2
I0710 07:05:42.782994  9997 sched.cpp:960] Rescinded offer 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O2
I0710 07:05:42.782990 10000 hierarchical.cpp:1218] Recovered cpus(allocated: role)(reservations: [(STATIC,role)]):100; mem(allocated: role)(reservations: [(STATIC,role)]):1000; cpus(allocated: role):1000; mem(allocated: role):10000 (total: cpus(reservations: [(STATIC,role)]):100; mem(reservations: [(STATIC,role)]):1000; cpus:1000; mem:10000, allocated: {}) on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2 from framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7ffdf32e2180, @0x7fa4ac05e8c8 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O2)
NOTE: You can safely ignore the above warning unless this call should not happen.  Do not suppress it by blindly adding an EXPECT_CALL() if you don't mean to enforce the call.  See https://github.com/google/googletest/blob/master/googlemock/docs/CookBook.md#knowing-when-to-expect for details.
I0710 07:05:42.783027  9997 sched.cpp:971] Scheduler::offerRescinded took 19723ns
I0710 07:05:42.827190  9999 containerizer.cpp:3116] Container e65bd434-1435-4c4c-a1cc-c0924710bcd5 has exited
I0710 07:05:42.827210  9999 containerizer.cpp:2575] Destroying container e65bd434-1435-4c4c-a1cc-c0924710bcd5 in RUNNING state
I0710 07:05:42.827219  9999 containerizer.cpp:3277] Transitioning the state of container e65bd434-1435-4c4c-a1cc-c0924710bcd5 from RUNNING to DESTROYING
I0710 07:05:42.827332  9999 linux_launcher.cpp:576] Asked to destroy container e65bd434-1435-4c4c-a1cc-c0924710bcd5
W0710 07:05:42.827359  9999 linux_launcher.cpp:612] Couldn't find freezer cgroup for container e65bd434-1435-4c4c-a1cc-c0924710bcd5 so assuming partially destroyed
I0710 07:05:42.827975 10002 posix.hpp:119] Ignoring cleanup request for unknown container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.828035  9997 posix.hpp:119] Ignoring cleanup request for unknown container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.828187 10001 provisioner.cpp:609] Ignoring destroy request for unknown container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.828363  9999 provisioner.cpp:609] Ignoring destroy request for unknown container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.828725  9999 slave.cpp:6613] Executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 terminated with signal Killed
I0710 07:05:42.828778  9999 slave.cpp:5545] Handling status update TASK_FAILED (Status UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 from @0.0.0.0:0
I0710 07:05:42.829021  9999 master.cpp:9087] Executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal): terminated with signal Killed
E0710 07:05:42.829030 10000 slave.cpp:5876] Failed to update resources for container e65bd434-1435-4c4c-a1cc-c0924710bcd5 of executor 'dummy' running task 569d27cb-b353-42d0-8bd6-54c09f89f89d on status update for terminal task, destroying container: Container not found
I0710 07:05:42.829043  9999 master.cpp:11957] Removing executor 'dummy' with resources {} of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
W0710 07:05:42.829088 10000 composing.cpp:609] Attempted to destroy unknown container e65bd434-1435-4c4c-a1cc-c0924710bcd5
I0710 07:05:42.829113  9999 task_status_update_manager.cpp:328] Received task status update TASK_FAILED (Status UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.829131  9999 task_status_update_manager.cpp:507] Creating StatusUpdate stream for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.829141 10000 sched.cpp:1144] Executor dummy on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 exited with status 9

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: executorLost(0x7ffdf32e2180, @0x7fa4a4013590 dummy, @0x7fa4a40ace00 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1, 9)
NOTE: You can safely ignore the above warning unless this call should not happen.  Do not suppress it by blindly adding an EXPECT_CALL() if you don't mean to enforce the call.  See https://github.com/google/googletest/blob/master/googlemock/docs/CookBook.md#knowing-when-to-expect for details.
I0710 07:05:42.829171 10000 sched.cpp:1155] Scheduler::executorLost took 19974ns
I0710 07:05:42.829339  9999 task_status_update_manager.cpp:383] Forwarding task status update TASK_FAILED (Status UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 to the agent
I0710 07:05:42.829401  9999 slave.cpp:6037] Forwarding the update TASK_FAILED (Status UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 to master@172.16.10.69:37082
I0710 07:05:42.829454  9999 slave.cpp:5930] Task status update manager successfully handled status update TASK_FAILED (Status UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.829504  9999 master.cpp:8750] Status update TASK_FAILED (Status UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 from agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.829519  9999 master.cpp:8807] Forwarding status update TASK_FAILED (Status UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.829567  9999 master.cpp:11819] Updating the state of task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
I0710 07:05:42.829679  9999 sched.cpp:1042] Scheduler::statusUpdate took 10656ns
I0710 07:05:42.829782  9999 hierarchical.cpp:1218] Recovered cpus(allocated: role):10; mem(allocated: role):100 (total: cpus:10; mem:100, allocated: {}) on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 from framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.829838  9999 master.cpp:6511] Processing ACKNOWLEDGE call for status 68222006-6847-43f1-a92a-00c530a4b600 for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1
I0710 07:05:42.829860  9999 master.cpp:11917] Removing task 569d27cb-b353-42d0-8bd6-54c09f89f89d with resources cpus(allocated: role):10; mem(allocated: role):100 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.830682  9999 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.830718  9999 task_status_update_manager.cpp:538] Cleaning up status update stream for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.830792  9999 slave.cpp:4684] Task status update manager successfully handled status update acknowledgement (UUID: 68222006-6847-43f1-a92a-00c530a4b600) for task 569d27cb-b353-42d0-8bd6-54c09f89f89d of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.830806  9999 slave.cpp:10377] Completing task 569d27cb-b353-42d0-8bd6-54c09f89f89d
I0710 07:05:42.830814  9999 slave.cpp:6724] Cleaning up executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.831112 10000 gc.cpp:95] Scheduling '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/e65bd434-1435-4c4c-a1cc-c0924710bcd5' for gc 6.99999041237333days in the future
I0710 07:05:42.831207  9999 slave.cpp:6853] Cleaning up framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.831210  9998 gc.cpp:95] Scheduling '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy' for gc 6.99999041237333days in the future
I0710 07:05:42.831255  9998 task_status_update_manager.cpp:289] Closing task status update streams for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.831547 10001 gc.cpp:95] Scheduling '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000' for gc 6.99999041237333days in the future
I0710 07:05:42.835628 10000 slave.cpp:912] Agent terminating
I0710 07:05:42.835824  9996 master.cpp:1295] Agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) disconnected
I0710 07:05:42.835842  9996 master.cpp:3379] Disconnecting agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.835861  9996 master.cpp:3398] Deactivating agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 at slave(523)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.835888 10000 hierarchical.cpp:799] Agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1 deactivated
I0710 07:05:42.842595  9998 master.cpp:1410] Framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 disconnected
I0710 07:05:42.842618  9998 master.cpp:3342] Deactivating framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.842640  9998 master.cpp:3319] Disconnecting framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.842655  9998 master.cpp:1425] Giving framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 0ns to failover
I0710 07:05:42.842986  9997 hierarchical.cpp:475] Deactivated framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.843145  9996 master.cpp:9987] Framework failover timeout, removing framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.843192  9996 master.cpp:10979] Removing framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082
I0710 07:05:42.843421  9999 hierarchical.cpp:1432] Allocation paused
I0710 07:05:42.843487  9999 hierarchical.cpp:417] Removed framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.843492 10000 slave.cpp:3908] Asked to shut down framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 by master@172.16.10.69:37082
I0710 07:05:42.843508 10000 slave.cpp:3923] Cannot shut down unknown framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000
I0710 07:05:42.843531  9999 hierarchical.cpp:1442] Allocation resumed
I0710 07:05:42.851032  9995 slave.cpp:912] Agent terminating
I0710 07:05:42.851421 10001 master.cpp:1295] Agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) disconnected
I0710 07:05:42.851441 10001 master.cpp:3379] Disconnecting agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.851459 10001 master.cpp:3398] Deactivating agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)
I0710 07:05:42.851487  9997 hierarchical.cpp:799] Agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 deactivated
I0710 07:05:42.853623  9997 master.cpp:1135] Master terminating
I0710 07:05:42.853726 10000 hierarchical.cpp:775] Removed all filters for agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2
I0710 07:05:42.853737 10000 hierarchical.cpp:650] Removed agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S2
I0710 07:05:42.853775 10000 hierarchical.cpp:775] Removed all filters for agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1
I0710 07:05:42.853780 10000 hierarchical.cpp:650] Removed agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S1
I0710 07:05:42.854012 10003 hierarchical.cpp:775] Removed all filters for agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0
I0710 07:05:42.854027 10003 hierarchical.cpp:650] Removed agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0
[  FAILED  ] RoleTest.RolesEndpointContainsConsumedQuota (199 ms)
{noformat}"	MESOS	Resolved	3	1	2732	resource-management
13056032	Add a Windows segfault handler for stacktraces	"Current in the Windows builds, if we segfault, the program will exit immediately with no other output.  

For example, when you add a failing test to {{stout-tests}} and run {{3rdparty/stout/tests/Debug/stout-tests --gtest_break_on_failure}}, gtest simulates a segfault to exit immediately.  On Posix, this also prints a stacktrace.

We may be able to re-use the code added to glog for print stacktraces (MESOS-6815)."	MESOS	Resolved	1	4	2732	mesosphere, windows
12987239	"docker::inspect() may get wrong output when a docker container is not in ""running"" state"	"I (klueska) am copying the text from an email I got about a bug report from Yubo Li at IBM.

docker::inspect() may get wrong output when the docker container is not in ""running"" state. In this case, the ""docker inspect"" will failed to parse data, and system can not enter TASK:RUNNING status.

I attached related logs in stderr, I printed the docker inspect output. The inspected output shows that the docker is in ""created"" status, not ""running"", so that many of inspect fields are invalid. 

Possible Fix: detect the ""State->Running"" field, and get success return when ""State->Running"" is true.

{noformat}
I0706 09:01:05.342895  2975 docker.cpp:780] Running docker -H unix:///var/run/docker.sock run --cpu-shares 512 --memory 536870912 -e MARATHON_APP_VERSION=2016-07-06T08:15:02.610Z -e HOST=9.186.57.67 -e MARATHON_APP_RESOURCE_CPUS=0.5 -e MARATHON_APP_RESOURCE_GPUS=1 -e MARATHON_APP_DOCKER_IMAGE=cuda_test_v0.1 -e PORT_10000=31435 -e MESOS_TASK_ID=ubuntu-gpu-32520.29f083bf-4358-11e6-b886-2ee1446b5607 -e PORT=31435 -e MARATHON_APP_RESOURCE_MEM=512.0 -e PORTS=31435 -e MARATHON_APP_RESOURCE_DISK=0.0 -e MARATHON_APP_LABELS= -e MARATHON_APP_ID=/ubuntu-gpu-32520 -e PORT0=31435 -e MESOS_SANDBOX=/mnt/mesos/sandbox -e MESOS_CONTAINER_NAME=mesos-1875c0d3-9712-43c3-9d58-572c89fac50b-S1.cfe287a0-8a37-4a0f-8ffb-55eb0e6e4439 -v /var/run/mesos/slaves/1875c0d3-9712-43c3-9d58-572c89fac50b-S1/frameworks/aee07017-f8e6-4ed5-8008-b4ea3a090282-0000/executors/ubuntu-gpu-32520.29f083bf-4358-11e6-b886-2ee1446b5607/runs/cfe287a0-8a37-4a0f-8ffb-55eb0e6e4439:/mnt/mesos/sandbox --net host --device=/dev/nvidiactl:/dev/nvidiactl:rwm --device=/dev/nvidia-uvm:/dev/nvidia-uvm:rwm --device=/dev/nvidia0:/dev/nvidia0:rwm --entrypoint /bin/sh --name mesos-1875c0d3-9712-43c3-9d58-572c89fac50b-S1.cfe287a0-8a37-4a0f-8ffb-55eb0e6e4439 cuda_test_v0.1 -c nvidia-smi && sleep 60s
I0706 09:01:05.345935  2975 docker.cpp:943] Running docker -H unix:///var/run/docker.sock inspect mesos-1875c0d3-9712-43c3-9d58-572c89fac50b-S1.cfe287a0-8a37-4a0f-8ffb-55eb0e6e4439
I0706 09:01:05.548992  2976 docker.cpp:249] Docker inspect: [
{
    ""Id"": ""5a4dc17e739b60593c04abf310f2485dddea832476e83007387b612839933f5a"",
    ""Created"": ""2016-07-06T09:01:05.531216924Z"",
    ""Path"": ""/bin/sh"",
    ""Args"": [
        ""-c"",
        ""nvidia-smi \u0026\u0026 sleep 60s""
    ],
    ""State"": {
        ""Status"": ""created"",
        ""Running"": false,
        ""Paused"": false,
        ""Restarting"": false,
        ""OOMKilled"": false,
        ""Dead"": false,
        ""Pid"": 0,
        ""ExitCode"": 0,
        ""Error"": """",
        ""StartedAt"": ""0001-01-01T00:00:00Z"",
        ""FinishedAt"": ""0001-01-01T00:00:00Z""
    },
    ""Image"": ""8cf6c8da7045ec24b1e561906dfa54ab0276753ec617e139a7b2da3ef72d245e"",
    ""ResolvConfPath"": """",
    ""HostnamePath"": """",
    ""HostsPath"": """",
    ""LogPath"": """",
    ""Name"": ""/mesos-1875c0d3-9712-43c3-9d58-572c89fac50b-S1.cfe287a0-8a37-4a0f-8ffb-55eb0e6e4439"",
    ""RestartCount"": 0,
    ""Driver"": ""aufs"",
    ""ExecDriver"": ""native-0.2"",
    ""MountLabel"": """",
    ""ProcessLabel"": """",
    ""AppArmorProfile"": """",
    ""ExecIDs"": null,
    ""HostConfig"": {
        ""Binds"": null,
        ""ContainerIDFile"": """",
        ""LxcConf"": null,
        ""Memory"": 0,
        ""MemoryReservation"": 0,
        ""MemorySwap"": 0,
        ""KernelMemory"": 0,
        ""CpuShares"": 0,
        ""CpuPeriod"": 0,
        ""CpusetCpus"": """",
        ""CpusetMems"": """",
        ""CpuQuota"": 0,
        ""BlkioWeight"": 0,
        ""OomKillDisable"": false,
        ""MemorySwappiness"": null,
        ""Privileged"": false,
        ""PortBindings"": null,
        ""Links"": null,
        ""PublishAllPorts"": false,
        ""Dns"": null,
        ""DnsOptions"": null,
        ""DnsSearch"": null,
        ""ExtraHosts"": null,
        ""VolumesFrom"": null,
        ""Devices"": null,
        ""NetworkMode"": """",
        ""IpcMode"": """",
        ""PidMode"": """",
        ""UTSMode"": """",
        ""CapAdd"": null,
        ""CapDrop"": null,
        ""GroupAdd"": null,
        ""RestartPolicy"": {
            ""Name"": """",
            ""MaximumRetryCount"": 0
        },
        ""SecurityOpt"": null,
        ""ReadonlyRootfs"": false,
        ""Ulimits"": null,
        ""LogConfig"": {
            ""Type"": ""json-file"",
            ""Config"": {}
        },
        ""CgroupParent"": """",
        ""ConsoleSize"": [
            0,
            0
        ],
        ""VolumeDriver"": """"
    },
    ""GraphDriver"": {
        ""Name"": ""aufs"",
        ""Data"": null
    },
    ""Mounts"": [],
    ""Config"": {
        ""Hostname"": ""5a4dc17e739b"",
        ""Domainname"": """",
        ""User"": """",
        ""AttachStdin"": false,
        ""AttachStdout"": true,
        ""AttachStderr"": true,
        ""Tty"": false,
        ""OpenStdin"": false,
        ""StdinOnce"": false,
        ""Env"": [
            ""MARATHON_APP_VERSION=2016-07-06T08:15:02.610Z"",
            ""HOST=9.186.57.67"",
            ""MARATHON_APP_RESOURCE_CPUS=0.5"",
            ""MARATHON_APP_RESOURCE_GPUS=1"",
            ""MARATHON_APP_DOCKER_IMAGE=cuda_test_v0.1"",
            ""PORT_10000=31435"",
            ""MESOS_TASK_ID=ubuntu-gpu-32520.29f083bf-4358-11e6-b886-2ee1446b5607"",
            ""PORT=31435"",
            ""MARATHON_APP_RESOURCE_MEM=512.0"",
            ""PORTS=31435"",
            ""MARATHON_APP_RESOURCE_DISK=0.0"",
            ""MARATHON_APP_LABELS="",
            ""MARATHON_APP_ID=/ubuntu-gpu-32520"",
            ""PORT0=31435"",
            ""MESOS_SANDBOX=/mnt/mesos/sandbox"",
            ""MESOS_CONTAINER_NAME=mesos-1875c0d3-9712-43c3-9d58-572c89fac50b-S1.cfe287a0-8a37-4a0f-8ffb-55eb0e6e4439"",
            ""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin""
        ],
        ""Cmd"": [
            ""-c"",
            ""nvidia-smi \u0026\u0026 sleep 60s""
        ],
        ""Image"": ""cuda_test_v0.1"",
        ""Volumes"": null,
        ""WorkingDir"": """",
        ""Entrypoint"": [
            ""/bin/sh""
        ],
        ""OnBuild"": null,
        ""Labels"": {},
        ""StopSignal"": ""SIGTERM""
    },
    ""NetworkSettings"": {
        ""Bridge"": """",
        ""SandboxID"": """",
        ""HairpinMode"": false,
        ""LinkLocalIPv6Address"": """",
        ""LinkLocalIPv6PrefixLen"": 0,
        ""Ports"": null,
        ""SandboxKey"": """",
        ""SecondaryIPAddresses"": null,
        ""SecondaryIPv6Addresses"": null,
        ""EndpointID"": """",
        ""Gateway"": """",
        ""GlobalIPv6Address"": """",
        ""GlobalIPv6PrefixLen"": 0,
        ""IPAddress"": """",
        ""IPPrefixLen"": 0,
        ""IPv6Gateway"": """",
        ""MacAddress"": """",
        ""Networks"": null
    }
}
]
I0706 09:01:05.549659  2976 docker.cpp:335] Unable to detect IP Address at 'NetworkSettings.Networks..IPAddress', attempting deprecated field
WARNING: Your kernel does not support swap limit capabilities, memory limited without swap.
I0706 09:01:52.983609  2973 exec.cpp:486] Agent exited, but framework has checkpointing enabled. Waiting 15mins to reconnect with agent 1875c0d3-9712-43c3-9d58-572c89fac50b-S1
I0706 09:02:06.057607  2978 exec.cpp:549] Executor sending status update TASK_FINISHED (UUID: 2cff35f2-9512-4120-b912-74a82c197696) for task ubuntu-gpu-32520.29f083bf-4358-11e6-b886-2ee1446b5607 of framework aee07017-f8e6-4ed5-8008-b4ea3a090282-0000
I0706 09:02:06.058717  2980 poll_socket.cpp:131] Socket error while connecting
I0706 09:02:06.058815  2980 process.cpp:1799] Failed to send 'mesos.internal.StatusUpdateMessage' to '127.0.1.1:5051', connect: Socket error while connecting
E0706 09:02:06.058931  2980 process.cpp:2104] Failed to shutdown socket with fd 6: Transport endpoint is not connected
{noformat}"	MESOS	Resolved	3	1	2732	containerizer, docker
13231126	Take ports out of the GET_ROLES endpoints.	It does not make sense to combine ports across agents.	MESOS	Resolved	3	4	2732	resource-management
12771164	MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky.	"Good run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD'
I0122 19:23:06.481690 17483 leveldb.cpp:176] Opened db in 21.058723ms
I0122 19:23:06.488590 17483 leveldb.cpp:183] Compacted db in 6.6715ms
I0122 19:23:06.488816 17483 leveldb.cpp:198] Created db iterator in 30034ns
I0122 19:23:06.489053 17483 leveldb.cpp:204] Seeked to beginning of db in 2908ns
I0122 19:23:06.489073 17483 leveldb.cpp:273] Iterated through 0 keys in the db in 492ns
I0122 19:23:06.489148 17483 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0122 19:23:06.490272 17504 recover.cpp:449] Starting replica recovery
I0122 19:23:06.490900 17504 recover.cpp:475] Replica is in EMPTY status
I0122 19:23:06.492422 17504 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0122 19:23:06.492694 17504 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0122 19:23:06.493185 17504 recover.cpp:566] Updating replica status to STARTING
I0122 19:23:06.514881 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 21.459963ms
I0122 19:23:06.514920 17504 replica.cpp:323] Persisted replica status to STARTING
I0122 19:23:06.515861 17501 master.cpp:262] Master 20150122-192306-16842879-46283-17483 (lucid) started on 127.0.1.1:46283
I0122 19:23:06.515910 17501 master.cpp:308] Master only allowing authenticated frameworks to register
I0122 19:23:06.515923 17501 master.cpp:313] Master only allowing authenticated slaves to register
I0122 19:23:06.515946 17501 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD/credentials'
I0122 19:23:06.516150 17501 master.cpp:357] Authorization enabled
I0122 19:23:06.517511 17501 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0122 19:23:06.517607 17501 whitelist_watcher.cpp:65] No whitelist given
I0122 19:23:06.518066 17498 master.cpp:1219] The newly elected leader is master@127.0.1.1:46283 with id 20150122-192306-16842879-46283-17483
I0122 19:23:06.518095 17498 master.cpp:1232] Elected as the leading master!
I0122 19:23:06.518121 17498 master.cpp:1050] Recovering from registrar
I0122 19:23:06.518333 17498 registrar.cpp:313] Recovering registrar
I0122 19:23:06.523987 17504 recover.cpp:475] Replica is in STARTING status
I0122 19:23:06.525090 17504 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0122 19:23:06.525337 17504 recover.cpp:195] Received a recover response from a replica in STARTING status
I0122 19:23:06.525693 17504 recover.cpp:566] Updating replica status to VOTING
I0122 19:23:06.532680 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 6.810884ms
I0122 19:23:06.532714 17504 replica.cpp:323] Persisted replica status to VOTING
I0122 19:23:06.532835 17504 recover.cpp:580] Successfully joined the Paxos group
I0122 19:23:06.533004 17504 recover.cpp:464] Recover process terminated
I0122 19:23:06.533833 17500 log.cpp:660] Attempting to start the writer
I0122 19:23:06.535225 17500 replica.cpp:477] Replica received implicit promise request with proposal 1
I0122 19:23:06.540340 17500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.086139ms
I0122 19:23:06.540371 17500 replica.cpp:345] Persisted promised to 1
I0122 19:23:06.541502 17504 coordinator.cpp:230] Coordinator attemping to fill missing position
I0122 19:23:06.543021 17504 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0122 19:23:06.548140 17504 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.083443ms
I0122 19:23:06.548171 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.549746 17500 replica.cpp:511] Replica received write request for position 0
I0122 19:23:06.549926 17500 leveldb.cpp:438] Reading position from leveldb took 31962ns
I0122 19:23:06.555033 17500 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.065823ms
I0122 19:23:06.555064 17500 replica.cpp:679] Persisted action at 0
I0122 19:23:06.556094 17504 replica.cpp:658] Replica received learned notice for position 0
I0122 19:23:06.558815 17504 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.688382ms
I0122 19:23:06.558847 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.558868 17504 replica.cpp:664] Replica learned NOP action at position 0
I0122 19:23:06.559917 17500 log.cpp:676] Writer started with ending position 0
I0122 19:23:06.560995 17500 leveldb.cpp:438] Reading position from leveldb took 27742ns
I0122 19:23:06.563467 17500 registrar.cpp:346] Successfully fetched the registry (0B) in 45.095936ms
I0122 19:23:06.563551 17500 registrar.cpp:445] Applied 1 operations in 19686ns; attempting to update the 'registry'
I0122 19:23:06.566107 17500 log.cpp:684] Attempting to append 118 bytes to the log
I0122 19:23:06.566267 17500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0122 19:23:06.567126 17500 replica.cpp:511] Replica received write request for position 1
I0122 19:23:06.582588 17500 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 15.425511ms
I0122 19:23:06.582631 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.583425 17500 replica.cpp:658] Replica received learned notice for position 1
I0122 19:23:06.589001 17500 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.549486ms
I0122 19:23:06.589200 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.589416 17500 replica.cpp:664] Replica learned APPEND action at position 1
I0122 19:23:06.596420 17500 registrar.cpp:490] Successfully updated the 'registry' in 32.815104ms
I0122 19:23:06.596551 17500 registrar.cpp:376] Successfully recovered registrar
I0122 19:23:06.596923 17500 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0122 19:23:06.597007 17500 log.cpp:703] Attempting to truncate the log to 1
I0122 19:23:06.597239 17500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0122 19:23:06.598464 17501 replica.cpp:511] Replica received write request for position 2
I0122 19:23:06.604038 17501 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.536264ms
I0122 19:23:06.604084 17501 replica.cpp:679] Persisted action at 2
I0122 19:23:06.608747 17503 replica.cpp:658] Replica received learned notice for position 2
I0122 19:23:06.614094 17503 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.315347ms
I0122 19:23:06.614171 17503 leveldb.cpp:401] Deleting ~1 keys from leveldb took 33021ns
I0122 19:23:06.614188 17503 replica.cpp:679] Persisted action at 2
I0122 19:23:06.614208 17503 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0122 19:23:06.628820 17483 sched.cpp:151] Version: 0.22.0
I0122 19:23:06.629879 17505 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.629973 17505 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.629995 17505 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.630314 17505 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.630722 17505 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.630750 17505 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.631115 17505 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.631423 17505 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.631459 17505 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.631563 17505 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.631605 17505 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.631671 17505 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.631748 17505 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.631774 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.631784 17505 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.631822 17505 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.631856 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.631870 17505 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631877 17505 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631892 17505 authenticator.hpp:390] Authentication success
I0122 19:23:06.631988 17505 authenticatee.hpp:315] Authentication success
I0122 19:23:06.632066 17505 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632359 17505 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.632382 17505 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.632432 17505 sched.cpp:548] Will retry registration in 598.155756ms if necessary
I0122 19:23:06.632575 17505 master.cpp:1420] Received registration request for framework 'default' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632639 17505 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.632912 17505 master.cpp:1484] Registering framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.633421 17505 hierarchical_allocator_process.hpp:319] Added framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633448 17505 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0122 19:23:06.633458 17505 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 17704ns
I0122 19:23:06.633919 17505 sched.cpp:442] Framework registered with 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633980 17505 sched.cpp:456] Scheduler::registered took 37063ns
I0122 19:23:06.636554 17500 sched.cpp:242] Scheduler::disconnected took 14843ns
I0122 19:23:06.636579 17500 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.636625 17500 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.636641 17500 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.636914 17500 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.637313 17500 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.637341 17500 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.637675 17500 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.638056 17501 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.638083 17501 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.638182 17501 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.638221 17501 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.638286 17501 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.638360 17501 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.638383 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.638393 17501 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.638422 17501 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.638447 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.638458 17501 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638464 17501 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638478 17501 authenticator.hpp:390] Authentication success
I0122 19:23:06.638566 17501 authenticatee.hpp:315] Authentication success
I0122 19:23:06.638643 17501 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.638919 17501 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.638942 17501 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.638994 17501 sched.cpp:548] Will retry registration in 489.304713ms if necessary
I0122 19:23:06.639169 17501 master.cpp:1557] Received re-registration request from framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.639242 17501 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.639839 17483 sched.cpp:1471] Asked to stop the driver
I0122 19:23:06.640379 17499 sched.cpp:808] Stopping framework '20150122-192306-16842879-46283-17483-0000'
I0122 19:23:06.640697 17499 master.cpp:745] Framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 disconnected
I0122 19:23:06.640723 17499 master.cpp:1789] Disconnecting framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640744 17499 master.cpp:1805] Deactivating framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640806 17499 master.cpp:767] Giving framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 0ns to failover
I0122 19:23:06.640951 17499 hierarchical_allocator_process.hpp:398] Deactivated framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.646342 17498 master.cpp:1604] Dropping re-registration request of framework 20150122-192306-16842879-46283-17483-0000 (default)  at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 because it is not authenticated
I0122 19:23:06.648844 17498 master.cpp:3941] Framework failover timeout, removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.648871 17498 master.cpp:4499] Removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.649624 17498 hierarchical_allocator_process.hpp:352] Removed framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.656532 17483 master.cpp:654] Master terminating
[       OK ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration (216 ms)
{noformat}

Bad run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm'
I0126 19:19:55.517570  2381 leveldb.cpp:176] Opened db in 34.341401ms
I0126 19:19:55.529630  2381 leveldb.cpp:183] Compacted db in 11.824435ms
I0126 19:19:55.529878  2381 leveldb.cpp:198] Created db iterator in 26176ns
I0126 19:19:55.530200  2381 leveldb.cpp:204] Seeked to beginning of db in 3457ns
I0126 19:19:55.530455  2381 leveldb.cpp:273] Iterated through 0 keys in the db in 902ns
I0126 19:19:55.530658  2381 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0126 19:19:55.531492  2397 recover.cpp:449] Starting replica recovery
I0126 19:19:55.531793  2397 recover.cpp:475] Replica is in EMPTY status
I0126 19:19:55.533327  2397 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0126 19:19:55.533608  2397 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0126 19:19:55.534101  2397 recover.cpp:566] Updating replica status to STARTING
I0126 19:19:55.550417  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.106821ms
I0126 19:19:55.550472  2397 replica.cpp:323] Persisted replica status to STARTING
I0126 19:19:55.551434  2397 recover.cpp:475] Replica is in STARTING status
I0126 19:19:55.552846  2397 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0126 19:19:55.553099  2397 recover.cpp:195] Received a recover response from a replica in STARTING status
I0126 19:19:55.553565  2397 recover.cpp:566] Updating replica status to VOTING
I0126 19:19:55.564590  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 10.719218ms
I0126 19:19:55.564919  2397 replica.cpp:323] Persisted replica status to VOTING
I0126 19:19:55.565982  2397 recover.cpp:580] Successfully joined the Paxos group
I0126 19:19:55.566231  2397 recover.cpp:464] Recover process terminated
I0126 19:19:55.567878  2401 master.cpp:262] Master 20150126-191955-16842879-51862-2381 (lucid) started on 127.0.1.1:51862
I0126 19:19:55.567927  2401 master.cpp:308] Master only allowing authenticated frameworks to register
I0126 19:19:55.567950  2401 master.cpp:313] Master only allowing authenticated slaves to register
I0126 19:19:55.567978  2401 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm/credentials'
I0126 19:19:55.568220  2401 master.cpp:357] Authorization enabled
I0126 19:19:55.569890  2401 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0126 19:19:55.569999  2401 whitelist_watcher.cpp:65] No whitelist given
I0126 19:19:55.570694  2401 master.cpp:1219] The newly elected leader is master@127.0.1.1:51862 with id 20150126-191955-16842879-51862-2381
I0126 19:19:55.570721  2401 master.cpp:1232] Elected as the leading master!
I0126 19:19:55.570742  2401 master.cpp:1050] Recovering from registrar
I0126 19:19:55.570977  2401 registrar.cpp:313] Recovering registrar
I0126 19:19:55.571959  2401 log.cpp:660] Attempting to start the writer
I0126 19:19:55.573441  2401 replica.cpp:477] Replica received implicit promise request with proposal 1
I0126 19:19:55.590724  2401 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.243964ms
I0126 19:19:55.590785  2401 replica.cpp:345] Persisted promised to 1
I0126 19:19:55.592140  2396 coordinator.cpp:230] Coordinator attemping to fill missing position
I0126 19:19:55.593834  2396 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0126 19:19:55.603837  2396 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 9.955824ms
I0126 19:19:55.603902  2396 replica.cpp:679] Persisted action at 0
I0126 19:19:55.606082  2401 replica.cpp:511] Replica received write request for position 0
I0126 19:19:55.606331  2401 leveldb.cpp:438] Reading position from leveldb took 44524ns
I0126 19:19:55.612546  2401 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.870411ms
I0126 19:19:55.612597  2401 replica.cpp:679] Persisted action at 0
I0126 19:19:55.613416  2401 replica.cpp:658] Replica received learned notice for position 0
I0126 19:19:55.616269  2401 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.82145ms
I0126 19:19:55.616305  2401 replica.cpp:679] Persisted action at 0
I0126 19:19:55.616328  2401 replica.cpp:664] Replica learned NOP action at position 0
I0126 19:19:55.628062  2399 log.cpp:676] Writer started with ending position 0
I0126 19:19:55.629328  2399 leveldb.cpp:438] Reading position from leveldb took 57003ns
I0126 19:19:55.631995  2399 registrar.cpp:346] Successfully fetched the registry (0B) in 60.973824ms
I0126 19:19:55.632109  2399 registrar.cpp:445] Applied 1 operations in 35531ns; attempting to update the 'registry'
I0126 19:19:55.634799  2399 log.cpp:684] Attempting to append 117 bytes to the log
I0126 19:19:55.634996  2399 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0126 19:19:55.636651  2397 replica.cpp:511] Replica received write request for position 1
I0126 19:19:55.642165  2397 leveldb.cpp:343] Persisting action (134 bytes) to leveldb took 5.474306ms
I0126 19:19:55.642215  2397 replica.cpp:679] Persisted action at 1
I0126 19:19:55.643226  2397 replica.cpp:658] Replica received learned notice for position 1
I0126 19:19:55.648574  2397 leveldb.cpp:343] Persisting action (136 bytes) to leveldb took 5.317891ms
I0126 19:19:55.648808  2397 replica.cpp:679] Persisted action at 1
I0126 19:19:55.649158  2397 replica.cpp:664] Replica learned APPEND action at position 1
I0126 19:19:55.663101  2397 registrar.cpp:490] Successfully updated the 'registry' in 30.918144ms
I0126 19:19:55.663267  2397 registrar.cpp:376] Successfully recovered registrar
I0126 19:19:55.663699  2397 master.cpp:1077] Recovered 0 slaves from the Registry (81B) ; allowing 10mins for slaves to re-register
I0126 19:19:55.663795  2397 log.cpp:703] Attempting to truncate the log to 1
I0126 19:19:55.664083  2397 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0126 19:19:55.665573  2403 replica.cpp:511] Replica received write request for position 2
I0126 19:19:55.671500  2403 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.883759ms
I0126 19:19:55.671547  2403 replica.cpp:679] Persisted action at 2
I0126 19:19:55.672780  2403 replica.cpp:658] Replica received learned notice for position 2
I0126 19:19:55.685999  2403 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 12.808643ms
I0126 19:19:55.686099  2403 leveldb.cpp:401] Deleting ~1 keys from leveldb took 49867ns
I0126 19:19:55.686121  2403 replica.cpp:679] Persisted action at 2
I0126 19:19:55.686149  2403 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0126 19:19:55.722545  2381 sched.cpp:151] Version: 0.22.0
I0126 19:19:55.723795  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862
I0126 19:19:55.723891  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862
I0126 19:19:55.723914  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:19:55.724244  2401 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:19:55.724694  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.724725  2401 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:19:55.725108  2401 authenticator.hpp:170] Creating new server SASL connection
I0126 19:19:55.725390  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:19:55.725415  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:19:55.725515  2401 authenticator.hpp:276] Received SASL authentication start
I0126 19:19:55.725566  2401 authenticator.hpp:398] Authentication requires more steps
I0126 19:19:55.725632  2401 authenticatee.hpp:275] Received SASL authentication step
I0126 19:19:55.725710  2401 authenticator.hpp:304] Received SASL authentication step
I0126 19:19:55.725744  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:19:55.725757  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:19:55.725808  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:19:55.725834  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:19:55.725847  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.725853  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.725867  2401 authenticator.hpp:390] Authentication success
I0126 19:19:55.728629  2399 authenticatee.hpp:315] Authentication success
I0126 19:19:55.729228  2399 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862
I0126 19:19:55.729277  2399 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.729365  2399 sched.cpp:548] Will retry registration in 3.855403ms if necessary
I0126 19:19:55.729671  2399 master.cpp:1411] Queuing up registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because authentication is still in progress
I0126 19:19:55.733487  2400 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.734094  2400 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.734177  2400 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.734724  2400 master.cpp:1484] Registering framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.735335  2402 hierarchical_allocator_process.hpp:319] Added framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.735376  2402 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:19:55.735389  2402 hierarchical_allocator_process.hpp:738] Performed allocation for 0 slaves in 22978ns
I0126 19:19:55.741891  2398 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.744575  2398 sched.cpp:548] Will retry registration in 3.86742709secs if necessary
I0126 19:19:55.744742  2398 sched.cpp:442] Framework registered with 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.744827  2398 sched.cpp:456] Scheduler::registered took 60111ns
I0126 19:19:55.744956  2398 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.745020  2398 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.749315  2401 sched.cpp:242] Scheduler::disconnected took 19450ns
I0126 19:19:55.749343  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862
I0126 19:19:55.749394  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862
I0126 19:19:55.749411  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:19:55.749743  2401 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:19:55.750208  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.750238  2401 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:19:55.750629  2401 authenticator.hpp:170] Creating new server SASL connection
I0126 19:19:55.750938  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:19:55.750963  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:19:55.751063  2401 authenticator.hpp:276] Received SASL authentication start
I0126 19:19:55.751109  2401 authenticator.hpp:398] Authentication requires more steps
I0126 19:19:55.751175  2401 authenticatee.hpp:275] Received SASL authentication step
I0126 19:19:55.751269  2401 authenticator.hpp:304] Received SASL authentication step
I0126 19:19:55.751296  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:19:55.751307  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:19:55.751358  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:19:55.751392  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:19:55.751405  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.751413  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.751427  2401 authenticator.hpp:390] Authentication success
I0126 19:19:55.751524  2401 authenticatee.hpp:315] Authentication success
I0126 19:19:55.751605  2401 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.751898  2401 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862
I0126 19:19:55.751922  2401 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.751996  2401 sched.cpp:548] Will retry registration in 1.511226315secs if necessary
I0126 19:19:55.752174  2401 master.cpp:1557] Received re-registration request from framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.752256  2401 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.752485  2401 master.cpp:1610] Re-registering framework 20150126-191955-16842879-51862-2381-0000 (default)  at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.752527  2401 master.cpp:1650] Allowing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 to re-register with an already used id
I0126 19:19:55.752689  2401 sched.cpp:484] Framework re-registered with 20150126-191955-16842879-51862-2381-0000
tests/master_authorization_tests.cpp:980: Failure
Mock function called more times than expected - returning directly.
    Function call: reregistered(0x7fff5cef57e0, @0x56077d0 id: ""20150126-191955-16842879-51862-2381""
ip: 16842879
port: 51862
pid: ""master@127.0.1.1:51862""
hostname: ""lucid""
)
         Expected: to be never called
           Actual: called once - over-saturated and active
I0126 19:19:55.753191  2401 sched.cpp:498] Scheduler::reregistered took 478798ns
I0126 19:19:55.753600  2381 sched.cpp:1471] Asked to stop the driver
I0126 19:19:55.754518  2402 sched.cpp:808] Stopping framework '20150126-191955-16842879-51862-2381-0000'
I0126 19:19:55.755089  2402 master.cpp:1744] Asked to unregister framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.755302  2402 master.cpp:4499] Removing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.759419  2402 hierarchical_allocator_process.hpp:398] Deactivated framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.759850  2402 hierarchical_allocator_process.hpp:352] Removed framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.761160  2400 master.cpp:1462] Dropping registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because it is not authenticated
I0126 19:19:55.771309  2381 master.cpp:654] Master terminating
[  FAILED  ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration	 (312 ms)
{noformat}"	MESOS	Resolved	3	1	2732	flaky-test, twitter
12662470	Master improperly prints the exit status of the executor	"Instead of extracting the status via WEXITSTATUS() or checking WEXITED(), it just prints the 'int' value. This is erroneous.


slave
-------
I0807 18:39:55.782146 41771 slave.cpp:2074] Executor 'default' of framework 201308071828-2081170186-5050-41032-0000 has exited with status 127


master
----------
I0807 18:39:55.785429 41058 master.cpp:1260] Executor default of framework 201308071828-2081170186-5050-41032-0000 on slave 201308071828-2081170186-5050-41032-0 (***) exited with status 32512
"	MESOS	Resolved	4	1	2732	newbie
13129473	mesos-agent high cpu usage because of numerous /proc/mounts reads	"/proc/mounts is read many, many times from src/(linux/fs|linux/cgroups|slave/slave).cpp.

When using overlayfs, the /proc/mounts contents can become quite large. 
As an example, one of our Q/A single node running ~150 tasks,  have a 361 lines/ 201299 chars  /proc/mounts file.

This 200kB file is read on this node about 25 to 150 times per second. This is a (huge) waste of cpu and I/O time.

Most of these calls are related to cgroups.

Please consider these proposals :

1/ Is /proc/mounts mandatory for cgroups ? 
We already have cgroup subsystems list from /proc/cgroups.
The only compelling information from /proc/mounts seems to be the root mount point, 
/sys/fs/cgroup/, which could be obtained by a unique read on agent start.

2/ use /proc/self/mountstats

{noformat}
wc /proc/self/mounts /proc/self/mountstats
361 2166 201299 /proc/self/mounts
361 2888 50200 /proc/self/mountstats
{noformat}

{noformat}
grep cgroup /proc/self/mounts
cgroup /sys/fs/cgroup tmpfs rw,relatime,mode=755 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0
cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0
cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0
cgroup /sys/fs/cgroup/net_cls cgroup rw,relatime,net_cls 0 0
cgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0
cgroup /sys/fs/cgroup/net_prio cgroup rw,relatime,net_prio 0 0
cgroup /sys/fs/cgroup/pids cgroup rw,relatime,pids 0 0
{noformat}

{noformat}
grep cgroup /proc/self/mountstats
device cgroup mounted on /sys/fs/cgroup with fstype tmpfs
device cgroup mounted on /sys/fs/cgroup/cpuset with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/cpu with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/cpuacct with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/blkio with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/memory with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/devices with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/freezer with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/net_cls with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/perf_event with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/net_prio with fstype cgroup
device cgroup mounted on /sys/fs/cgroup/pids with fstype cgroup
{noformat}

This file contains all the required information, and is 4x smaller

3/ microcaching
Caching cgroups data for just 1 second would be a huge perfomance improvement, but i'm not aware of the possible side effects.





"	MESOS	Resolved	2	4	2732	containerizer, performance
13161898	process::await/collect n^2 performance issue	"Due to the use of std::list::size (which appears to be linear in complexity even with g++ and c++11), process::await and process::collect suffer from n^2 complexity. A minimal patch to switch to std::vector shows the following improvement:

{noformat: Title=Before}
Registered 2000 frameworks
Finished launching the tasks; Sleep 10 seconds ...
Start collecting metrics ...
v0 '/metrics/snapshot' response took 17.751689014secs
v1 'master::call::GetMetrics' application/x-protobuf response took 17.523928635secs
v1 'master::call::GetMetrics' application/json response took 18.111901732secs
{noformat}

{noformat: Title=After}
Registered 2000 frameworks
Finished launching the tasks; Sleep 10 seconds ...
Start collecting metrics ...
v0 '/metrics/snapshot' response took 1.730948431secs
v1 'master::call::GetMetrics' application/x-protobuf response took 1.697177667secs
v1 'master::call::GetMetrics' application/json response took 2.160314525secs
{noformat}

A follow up to switch the interface to std::vector would be beneficial since we don't need any of the std::list benefits."	MESOS	Resolved	3	1	2732	performance
13249961	Removal of a role from the suppression list should be equivalent to REVIVE.	"[~timcharper] and [~zen-dog] pointed out that removal of a role from the suppression list (e.g. via UPDATE_FRAMEWORK) does not clear filters. This means that schedulers have to issue a separate explicit REVIVE for the roles they want to remove.

It seems like these are not the semantics we want, and we should instead be clearing filters upon removing a role from the suppression list."	MESOS	Resolved	3	4	2732	resource-management
13215987	Document per framework minimal allocatable resources in framework development guides	With MESOS-9523 we introduced fields into {{FrameworkInfo}} to give frameworks a way to express their resource requirements. We should document this feature in the framework development guide(s).	MESOS	Resolved	1	3	2732	resource-management
12666008	GroupTest.GroupJoinWithDisconnect fails on master.	"[ RUN      ] GroupTest.GroupJoinWithDisconnect
2013-08-28 14:15:21,348:40067(0x11c447000):ZOO_ERROR@handle_socket_error_msg@1579: Socket [127.0.0.1:64547] zk retcode=-4, errno=61(Connection refused): server refused to accept the client
Exception in thread ""AWT-AppKit"" java.lang.IllegalThreadStateException
	at java.lang.Thread.start(Thread.java:656)
	at org.apache.zookeeper.server.ZooKeeperServer.startSessionTracker(ZooKeeperServer.java:402)
	at org.apache.zookeeper.server.ZooKeeperServer.startup(ZooKeeperServer.java:376)
	at org.apache.zookeeper.server.NIOServerCnxn$Factory.startup(NIOServerCnxn.java:161)
Caught a JVM exception, not propagating

I committed this patch from Vinson Lee:
https://reviews.apache.org/r/13598/

It appears this has possibly affected the ZK tests.

There appears to be a code change between 3.3.4 and 3.3.6 relevant to this issue:

http://grepcode.com/file/repo1.maven.org/maven2/org.apache.zookeeper/zookeeper/3.3.4/org/apache/zookeeper/server/ZooKeeperServer.java#370

vs

http://grepcode.com/file/repo1.maven.org/maven2/org.apache.zookeeper/zookeeper/3.3.6/org/apache/zookeeper/server/ZooKeeperServer.java#372

I'll dig a little further, hopefully I can avoid needing to revert this commit."	MESOS	Resolved	3	1	2732	twitter
13122169	SlaveTest.IgnoreV0ExecutorIfItReregistersWithoutReconnect is flaky.	"{noformat}
../../src/tests/slave_tests.cpp:7888
Actual function call count doesn't match EXPECT_CALL(exec, shutdown(_))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
{noformat}
Full log attached."	MESOS	Resolved	3	1	2732	flaky-test
13017880	TestContainerizer is not thread-safe.	"The TestContainerizer is currently not backed by a Process and does not do any explicit synchronization and so is not thread safe.

Most tests currently cannot trip the concurrency issues, but this surfaced recently in MESOS-6544."	MESOS	Resolved	3	1	2732	tech-debt
12902286	OversubscriptionTest.UpdateAllocatorOnSchedulerFailover is flaky	"Showed up on ASF CI

https://builds.apache.org/job/Mesos/890/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull

{code}
[ RUN      ] OversubscriptionTest.UpdateAllocatorOnSchedulerFailover
Using temporary directory '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_y5LK6v'
I1003 20:29:03.367100 31549 leveldb.cpp:176] Opened db in 2.322276ms
I1003 20:29:03.368028 31549 leveldb.cpp:183] Compacted db in 888247ns
I1003 20:29:03.368093 31549 leveldb.cpp:198] Created db iterator in 22626ns
I1003 20:29:03.368108 31549 leveldb.cpp:204] Seeked to beginning of db in 1842ns
I1003 20:29:03.368115 31549 leveldb.cpp:273] Iterated through 0 keys in the db in 395ns
I1003 20:29:03.368165 31549 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1003 20:29:03.368722 31575 recover.cpp:449] Starting replica recovery
I1003 20:29:03.369118 31575 recover.cpp:475] Replica is in EMPTY status
I1003 20:29:03.370707 31572 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I1003 20:29:03.371100 31572 master.cpp:376] Master d4ff5e08-2202-4f3b-8fb2-5515adf9a97e (9efc27440ed0) started on 172.17.5.73:38504
I1003 20:29:03.371124 31572 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_y5LK6v/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_y5LK6v/master"" --zk_session_timeout=""10secs""
I1003 20:29:03.371477 31572 master.cpp:423] Master only allowing authenticated frameworks to register
I1003 20:29:03.371496 31572 master.cpp:428] Master only allowing authenticated slaves to register
I1003 20:29:03.371510 31572 credentials.hpp:37] Loading credentials for authentication from '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_y5LK6v/credentials'
I1003 20:29:03.371841 31572 master.cpp:467] Using default 'crammd5' authenticator
I1003 20:29:03.371989 31572 master.cpp:504] Authorization enabled
I1003 20:29:03.372009 31580 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1003 20:29:03.372231 31568 hierarchical.hpp:468] Initialized hierarchical allocator process
I1003 20:29:03.372349 31579 whitelist_watcher.cpp:79] No whitelist given
I1003 20:29:03.373409 31572 recover.cpp:566] Updating replica status to STARTING
I1003 20:29:03.373558 31576 master.cpp:1603] The newly elected leader is master@172.17.5.73:38504 with id d4ff5e08-2202-4f3b-8fb2-5515adf9a97e
I1003 20:29:03.373670 31576 master.cpp:1616] Elected as the leading master!
I1003 20:29:03.373775 31576 master.cpp:1376] Recovering from registrar
I1003 20:29:03.374174 31579 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 611973ns
I1003 20:29:03.374233 31581 registrar.cpp:309] Recovering registrar
I1003 20:29:03.374248 31579 replica.cpp:323] Persisted replica status to STARTING
I1003 20:29:03.374455 31579 recover.cpp:475] Replica is in STARTING status
I1003 20:29:03.375416 31576 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I1003 20:29:03.375880 31575 recover.cpp:195] Received a recover response from a replica in STARTING status
I1003 20:29:03.376230 31576 recover.cpp:566] Updating replica status to VOTING
I1003 20:29:03.376729 31580 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 370830ns
I1003 20:29:03.376752 31580 replica.cpp:323] Persisted replica status to VOTING
I1003 20:29:03.376893 31580 recover.cpp:580] Successfully joined the Paxos group
I1003 20:29:03.377115 31580 recover.cpp:464] Recover process terminated
I1003 20:29:03.377531 31569 log.cpp:661] Attempting to start the writer
I1003 20:29:03.378665 31583 replica.cpp:477] Replica received implicit promise request with proposal 1
I1003 20:29:03.379005 31583 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 316870ns
I1003 20:29:03.379026 31583 replica.cpp:345] Persisted promised to 1
I1003 20:29:03.379591 31568 coordinator.cpp:231] Coordinator attemping to fill missing position
I1003 20:29:03.380627 31581 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I1003 20:29:03.380982 31581 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 320435ns
I1003 20:29:03.381006 31581 replica.cpp:679] Persisted action at 0
I1003 20:29:03.381953 31570 replica.cpp:511] Replica received write request for position 0
I1003 20:29:03.382004 31570 leveldb.cpp:438] Reading position from leveldb took 27214ns
I1003 20:29:03.382390 31570 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 346853ns
I1003 20:29:03.382414 31570 replica.cpp:679] Persisted action at 0
I1003 20:29:03.382910 31572 replica.cpp:658] Replica received learned notice for position 0
I1003 20:29:03.383209 31572 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 272453ns
I1003 20:29:03.383231 31572 replica.cpp:679] Persisted action at 0
I1003 20:29:03.383245 31572 replica.cpp:664] Replica learned NOP action at position 0
I1003 20:29:03.383776 31583 log.cpp:677] Writer started with ending position 0
I1003 20:29:03.384682 31573 leveldb.cpp:438] Reading position from leveldb took 25930ns
I1003 20:29:03.385607 31573 registrar.cpp:342] Successfully fetched the registry (0B) in 11.33184ms
I1003 20:29:03.385702 31573 registrar.cpp:441] Applied 1 operations in 20556ns; attempting to update the 'registry'
I1003 20:29:03.386354 31575 log.cpp:685] Attempting to append 174 bytes to the log
I1003 20:29:03.386463 31570 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I1003 20:29:03.387131 31577 replica.cpp:511] Replica received write request for position 1
I1003 20:29:03.387465 31577 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 304517ns
I1003 20:29:03.387487 31577 replica.cpp:679] Persisted action at 1
I1003 20:29:03.388046 31577 replica.cpp:658] Replica received learned notice for position 1
I1003 20:29:03.388494 31577 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 422691ns
I1003 20:29:03.388515 31577 replica.cpp:679] Persisted action at 1
I1003 20:29:03.388530 31577 replica.cpp:664] Replica learned APPEND action at position 1
I1003 20:29:03.389806 31581 log.cpp:704] Attempting to truncate the log to 1
I1003 20:29:03.389914 31573 registrar.cpp:486] Successfully updated the 'registry' in 4.150016ms
I1003 20:29:03.389978 31583 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I1003 20:29:03.390210 31573 registrar.cpp:372] Successfully recovered registrar
I1003 20:29:03.390650 31583 master.cpp:1413] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I1003 20:29:03.391228 31575 replica.cpp:511] Replica received write request for position 2
I1003 20:29:03.391638 31575 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 371414ns
I1003 20:29:03.391661 31575 replica.cpp:679] Persisted action at 2
I1003 20:29:03.392222 31570 replica.cpp:658] Replica received learned notice for position 2
I1003 20:29:03.392685 31570 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 362209ns
I1003 20:29:03.392747 31570 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35127ns
I1003 20:29:03.392772 31570 replica.cpp:679] Persisted action at 2
I1003 20:29:03.392799 31570 replica.cpp:664] Replica learned TRUNCATE action at position 2
I1003 20:29:03.404861 31575 slave.cpp:190] Slave started on 181)@172.17.5.73:38504
I1003 20:29:03.405005 31575 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_zWCYr1/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_zWCYr1/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_zWCYr1""
I1003 20:29:03.405575 31575 credentials.hpp:85] Loading credential for authentication from '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_zWCYr1/credential'
I1003 20:29:03.405788 31575 slave.cpp:321] Slave using credential for: test-principal
I1003 20:29:03.406411 31575 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1003 20:29:03.406519 31575 slave.cpp:390] Slave hostname: 9efc27440ed0
I1003 20:29:03.406545 31575 slave.cpp:395] Slave checkpoint: true
I1003 20:29:03.406986 31549 sched.cpp:164] Version: 0.26.0
I1003 20:29:03.407536 31571 state.cpp:54] Recovering state from '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_zWCYr1/meta'
I1003 20:29:03.407918 31575 status_update_manager.cpp:202] Recovering status update manager
I1003 20:29:03.408010 31568 sched.cpp:262] New master detected at master@172.17.5.73:38504
I1003 20:29:03.408089 31568 sched.cpp:318] Authenticating with master master@172.17.5.73:38504
I1003 20:29:03.408109 31568 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1003 20:29:03.408393 31580 authenticatee.cpp:115] Creating new client SASL connection
I1003 20:29:03.408444 31575 slave.cpp:4110] Finished recovery
I1003 20:29:03.408839 31568 master.cpp:5138] Authenticating scheduler-fa60968c-5249-4c99-8628-d2115aef22e4@172.17.5.73:38504
I1003 20:29:03.408946 31580 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(427)@172.17.5.73:38504
I1003 20:29:03.408967 31575 slave.cpp:4267] Querying resource estimator for oversubscribable resources
I1003 20:29:03.409216 31575 authenticator.cpp:92] Creating new server SASL connection
I1003 20:29:03.409247 31568 slave.cpp:705] New master detected at master@172.17.5.73:38504
I1003 20:29:03.409253 31570 status_update_manager.cpp:176] Pausing sending status updates
I1003 20:29:03.409327 31568 slave.cpp:768] Authenticating with master master@172.17.5.73:38504
I1003 20:29:03.409368 31568 slave.cpp:773] Using default CRAM-MD5 authenticatee
I1003 20:29:03.409461 31575 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I1003 20:29:03.409498 31575 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I1003 20:29:03.409525 31568 slave.cpp:741] Detecting new master
I1003 20:29:03.409628 31575 authenticatee.cpp:115] Creating new client SASL connection
I1003 20:29:03.409632 31581 authenticator.cpp:197] Received SASL authentication start
I1003 20:29:03.409901 31581 authenticator.cpp:319] Authentication requires more steps
I1003 20:29:03.409970 31575 master.cpp:5138] Authenticating slave(181)@172.17.5.73:38504
I1003 20:29:03.410159 31569 authenticatee.cpp:252] Received SASL authentication step
I1003 20:29:03.410233 31575 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(428)@172.17.5.73:38504
I1003 20:29:03.410276 31569 authenticator.cpp:225] Received SASL authentication step
I1003 20:29:03.410311 31569 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '9efc27440ed0' server FQDN: '9efc27440ed0' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1003 20:29:03.410331 31569 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I1003 20:29:03.410387 31569 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1003 20:29:03.410429 31569 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '9efc27440ed0' server FQDN: '9efc27440ed0' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1003 20:29:03.410454 31569 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1003 20:29:03.410467 31569 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1003 20:29:03.410490 31569 authenticator.cpp:311] Authentication success
I1003 20:29:03.410523 31575 authenticator.cpp:92] Creating new server SASL connection
I1003 20:29:03.410677 31569 authenticatee.cpp:292] Authentication success
I1003 20:29:03.410703 31575 master.cpp:5168] Successfully authenticated principal 'test-principal' at scheduler-fa60968c-5249-4c99-8628-d2115aef22e4@172.17.5.73:38504
I1003 20:29:03.410791 31569 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(427)@172.17.5.73:38504
I1003 20:29:03.410836 31575 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I1003 20:29:03.410961 31575 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I1003 20:29:03.411046 31571 sched.cpp:407] Successfully authenticated with master master@172.17.5.73:38504
I1003 20:29:03.411075 31571 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.5.73:38504
I1003 20:29:03.411090 31582 authenticator.cpp:197] Received SASL authentication start
I1003 20:29:03.411159 31582 authenticator.cpp:319] Authentication requires more steps
I1003 20:29:03.411180 31571 sched.cpp:747] Will retry registration in 1.121725197secs if necessary
I1003 20:29:03.411272 31582 authenticatee.cpp:252] Received SASL authentication step
I1003 20:29:03.411303 31581 master.cpp:2179] Received SUBSCRIBE call for framework 'default' at scheduler-fa60968c-5249-4c99-8628-d2115aef22e4@172.17.5.73:38504
I1003 20:29:03.411492 31581 master.cpp:1642] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1003 20:29:03.411509 31575 authenticator.cpp:225] Received SASL authentication step
I1003 20:29:03.411540 31575 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '9efc27440ed0' server FQDN: '9efc27440ed0' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1003 20:29:03.411555 31575 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I1003 20:29:03.411581 31575 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1003 20:29:03.411599 31575 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '9efc27440ed0' server FQDN: '9efc27440ed0' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1003 20:29:03.411608 31575 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1003 20:29:03.411619 31575 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1003 20:29:03.411635 31575 authenticator.cpp:311] Authentication success
I1003 20:29:03.411770 31576 authenticatee.cpp:292] Authentication success
I1003 20:29:03.411768 31575 master.cpp:5168] Successfully authenticated principal 'test-principal' at slave(181)@172.17.5.73:38504
I1003 20:29:03.411994 31575 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(428)@172.17.5.73:38504
I1003 20:29:03.412241 31576 slave.cpp:836] Successfully authenticated with master master@172.17.5.73:38504
I1003 20:29:03.412360 31576 slave.cpp:1230] Will retry registration in 10.113977ms if necessary
I1003 20:29:03.412411 31581 master.cpp:2250] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1003 20:29:03.412767 31576 hierarchical.hpp:515] Added framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:03.412798 31576 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:03.412837 31576 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:03.412858 31576 hierarchical.hpp:1223] Performed allocation for 0 slaves in 70147ns
I1003 20:29:03.413169 31571 sched.cpp:641] Framework registered with d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:03.413228 31571 sched.cpp:655] Scheduler::registered took 31641ns
I1003 20:29:03.413275 31581 master.cpp:3862] Registering slave at slave(181)@172.17.5.73:38504 (9efc27440ed0) with id d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0
I1003 20:29:03.413812 31574 registrar.cpp:441] Applied 1 operations in 58662ns; attempting to update the 'registry'
I1003 20:29:03.414679 31574 log.cpp:685] Attempting to append 343 bytes to the log
I1003 20:29:03.414809 31572 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I1003 20:29:03.415621 31576 replica.cpp:511] Replica received write request for position 3
I1003 20:29:03.415855 31576 leveldb.cpp:343] Persisting action (362 bytes) to leveldb took 196430ns
I1003 20:29:03.415885 31576 replica.cpp:679] Persisted action at 3
I1003 20:29:03.416429 31575 replica.cpp:658] Replica received learned notice for position 3
I1003 20:29:03.416859 31575 leveldb.cpp:343] Persisting action (364 bytes) to leveldb took 402997ns
I1003 20:29:03.416882 31575 replica.cpp:679] Persisted action at 3
I1003 20:29:03.416903 31575 replica.cpp:664] Replica learned APPEND action at position 3
I1003 20:29:03.418669 31577 registrar.cpp:486] Successfully updated the 'registry' in 4.766976ms
I1003 20:29:03.418918 31580 log.cpp:704] Attempting to truncate the log to 3
I1003 20:29:03.419070 31576 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I1003 20:29:03.419718 31580 slave.cpp:3138] Received ping from slave-observer(181)@172.17.5.73:38504
I1003 20:29:03.419813 31575 master.cpp:3930] Registered slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 at slave(181)@172.17.5.73:38504 (9efc27440ed0) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1003 20:29:03.419993 31568 replica.cpp:511] Replica received write request for position 4
I1003 20:29:03.420011 31570 hierarchical.hpp:675] Added slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 (9efc27440ed0) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1003 20:29:03.420229 31580 slave.cpp:880] Registered with master master@172.17.5.73:38504; given slave ID d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0
I1003 20:29:03.420261 31580 fetcher.cpp:77] Clearing fetcher cache
I1003 20:29:03.420368 31568 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 340897ns
I1003 20:29:03.420497 31568 replica.cpp:679] Persisted action at 4
I1003 20:29:03.420442 31575 status_update_manager.cpp:183] Resuming sending status updates
I1003 20:29:03.420678 31580 slave.cpp:903] Checkpointing SlaveInfo to '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_zWCYr1/meta/slaves/d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0/slave.info'
I1003 20:29:03.420812 31570 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:03.420948 31570 hierarchical.hpp:1241] Performed allocation for slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 in 893988ns
I1003 20:29:03.421216 31579 master.cpp:4967] Sending 1 offers to framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-fa60968c-5249-4c99-8628-d2115aef22e4@172.17.5.73:38504
I1003 20:29:03.421609 31578 replica.cpp:658] Replica received learned notice for position 4
I1003 20:29:03.421767 31573 sched.cpp:811] Scheduler::resourceOffers took 170881ns
I1003 20:29:03.422123 31578 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 410124ns
I1003 20:29:03.422200 31578 leveldb.cpp:401] Deleting ~2 keys from leveldb took 48863ns
I1003 20:29:03.422225 31578 replica.cpp:679] Persisted action at 4
I1003 20:29:03.422255 31578 replica.cpp:664] Replica learned TRUNCATE action at position 4
I1003 20:29:03.423061 31549 sched.cpp:164] Version: 0.26.0
I1003 20:29:03.423636 31576 sched.cpp:262] New master detected at master@172.17.5.73:38504
I1003 20:29:03.423720 31576 sched.cpp:318] Authenticating with master master@172.17.5.73:38504
I1003 20:29:03.423743 31576 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1003 20:29:03.424024 31581 authenticatee.cpp:115] Creating new client SASL connection
I1003 20:29:03.424335 31582 master.cpp:5138] Authenticating scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:03.424444 31578 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(429)@172.17.5.73:38504
I1003 20:29:03.424655 31568 authenticator.cpp:92] Creating new server SASL connection
I1003 20:29:03.424839 31576 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I1003 20:29:03.424865 31576 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I1003 20:29:03.424943 31576 authenticator.cpp:197] Received SASL authentication start
I1003 20:29:03.424993 31576 authenticator.cpp:319] Authentication requires more steps
I1003 20:29:03.425070 31576 authenticatee.cpp:252] Received SASL authentication step
I1003 20:29:03.425150 31576 authenticator.cpp:225] Received SASL authentication step
I1003 20:29:03.425184 31576 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '9efc27440ed0' server FQDN: '9efc27440ed0' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1003 20:29:03.425200 31576 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I1003 20:29:03.425254 31576 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1003 20:29:03.425292 31576 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '9efc27440ed0' server FQDN: '9efc27440ed0' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1003 20:29:03.425310 31576 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1003 20:29:03.425323 31576 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1003 20:29:03.425344 31576 authenticator.cpp:311] Authentication success
I1003 20:29:03.425436 31570 authenticatee.cpp:292] Authentication success
I1003 20:29:03.425534 31574 master.cpp:5168] Successfully authenticated principal 'test-principal' at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:03.425539 31578 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(429)@172.17.5.73:38504
I1003 20:29:03.425843 31569 sched.cpp:407] Successfully authenticated with master master@172.17.5.73:38504
I1003 20:29:03.425865 31569 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.5.73:38504
I1003 20:29:03.425961 31569 sched.cpp:747] Will retry registration in 1.710335618secs if necessary
I1003 20:29:03.426127 31576 master.cpp:2179] Received SUBSCRIBE call for framework 'default' at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:03.426195 31576 master.cpp:1642] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1003 20:29:03.426439 31570 master.cpp:2250] Subscribing framework default with checkpointing disabled and capabilities [ REVOCABLE_RESOURCES ]
I1003 20:29:03.426496 31570 master.cpp:2314] Updating info for framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:03.426590 31570 master.cpp:2327] Framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-fa60968c-5249-4c99-8628-d2115aef22e4@172.17.5.73:38504 failed over
I1003 20:29:03.426753 31579 sched.cpp:1024] Got error 'Framework failed over'
I1003 20:29:03.426776 31579 sched.cpp:1805] Asked to abort the driver
I1003 20:29:03.426867 31579 sched.cpp:1035] Scheduler::error took 52362ns
I1003 20:29:03.427084 31579 sched.cpp:1070] Aborting framework 'd4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000'
I1003 20:29:03.427255 31578 hierarchical.hpp:1103] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 from framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:03.427348 31574 sched.cpp:641] Framework registered with d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:03.427399 31574 sched.cpp:655] Scheduler::registered took 24077ns
W1003 20:29:03.427441 31578 slave.cpp:2141] Ignoring updating pid for framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 because it does not exist
W1003 20:29:03.427455 31570 master.cpp:2461] Ignoring deactivate framework message for framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504 because it is not expected from scheduler-fa60968c-5249-4c99-8628-d2115aef22e4@172.17.5.73:38504
I1003 20:29:04.374158 31577 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:04.374223 31577 hierarchical.hpp:1223] Performed allocation for 1 slaves in 634304ns
I1003 20:29:04.374565 31571 master.cpp:4967] Sending 1 offers to framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:04.375128 31576 sched.cpp:811] Scheduler::resourceOffers took 113401ns
I1003 20:29:04.375712 31568 slave.cpp:4281] Received oversubscribable resources cpus(*){REV}:2 from the resource estimator
I1003 20:29:04.375774 31568 slave.cpp:4304] Forwarding total oversubscribed resources cpus(*){REV}:2
I1003 20:29:04.376061 31568 master.cpp:4272] Received update of slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 at slave(181)@172.17.5.73:38504 (9efc27440ed0) with total oversubscribed resources cpus(*){REV}:2
I1003 20:29:04.376384 31568 hierarchical.hpp:735] Slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 (9efc27440ed0) updated with oversubscribed resources cpus(*){REV}:2 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]; cpus(*){REV}:2, allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1003 20:29:04.376866 31568 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:04.376895 31568 hierarchical.hpp:1241] Performed allocation for slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 in 470420ns
I1003 20:29:04.377118 31570 master.cpp:4967] Sending 1 offers to framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
../../src/tests/oversubscription_tests.cpp:973: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7fff2c3f5b00, @0x7f36ef2a88f0 { 144-byte object <B0-C7 06-FD 36-7F 00-00 00-00 00-00 00-00 00-00 20-65 02-D0 36-7F 00-00 C0-CD 03-D0 36-7F 00-00 60-BA 03-D0 36-7F 00-00 10-CE 03-D0 36-7F 00-00 00-D5 02-D0 36-7F 00-00 60-B9 03-D0 36-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 36-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I1003 20:29:04.377485 31570 sched.cpp:811] Scheduler::resourceOffers took 146114ns
I1003 20:29:05.375358 31580 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:05.375414 31580 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:05.375428 31580 hierarchical.hpp:1223] Performed allocation for 1 slaves in 542475ns
2015-10-03 20:29:06,037:31549(0x7f367ffff700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33701] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1003 20:29:06.377113 31572 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:06.377166 31572 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:06.377181 31572 hierarchical.hpp:1223] Performed allocation for 1 slaves in 565826ns
I1003 20:29:07.378443 31580 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:07.378502 31580 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:07.378520 31580 hierarchical.hpp:1223] Performed allocation for 1 slaves in 567289ns
I1003 20:29:08.379870 31575 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:08.379927 31575 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:08.379940 31575 hierarchical.hpp:1223] Performed allocation for 1 slaves in 572649ns
I1003 20:29:08.408592 31578 sched.cpp:419] Ignoring authentication timeout because the driver is not running!
2015-10-03 20:29:09,374:31549(0x7f367ffff700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33701] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1003 20:29:09.381644 31578 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:09.381691 31578 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:09.381705 31578 hierarchical.hpp:1223] Performed allocation for 1 slaves in 540955ns
I1003 20:29:10.383394 31580 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:10.383447 31580 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:10.383460 31580 hierarchical.hpp:1223] Performed allocation for 1 slaves in 500886ns
I1003 20:29:11.384301 31578 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:11.384354 31578 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:11.384367 31578 hierarchical.hpp:1223] Performed allocation for 1 slaves in 524827ns
I1003 20:29:12.386200 31577 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:12.386268 31577 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:12.386294 31577 hierarchical.hpp:1223] Performed allocation for 1 slaves in 805961ns
2015-10-03 20:29:12,710:31549(0x7f367ffff700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33701] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1003 20:29:13.387652 31574 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:13.387707 31574 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:13.387720 31574 hierarchical.hpp:1223] Performed allocation for 1 slaves in 519943ns
I1003 20:29:14.389140 31571 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:14.389194 31571 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:14.389206 31571 hierarchical.hpp:1223] Performed allocation for 1 slaves in 512587ns
I1003 20:29:15.390326 31574 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:15.390383 31574 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:15.390396 31574 hierarchical.hpp:1223] Performed allocation for 1 slaves in 527415ns
2015-10-03 20:29:16,047:31549(0x7f367ffff700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33701] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1003 20:29:16.391540 31575 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:16.391598 31575 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:16.391618 31575 hierarchical.hpp:1223] Performed allocation for 1 slaves in 556749ns
I1003 20:29:17.392669 31578 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:17.392730 31578 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:17.392745 31578 hierarchical.hpp:1223] Performed allocation for 1 slaves in 534985ns
I1003 20:29:18.394364 31571 hierarchical.hpp:1328] No resources available to allocate!
I1003 20:29:18.394421 31571 hierarchical.hpp:1423] No inverse offers to send out!
I1003 20:29:18.394435 31571 hierarchical.hpp:1223] Performed allocation for 1 slaves in 560401ns
I1003 20:29:18.420331 31580 slave.cpp:3138] Received ping from slave-observer(181)@172.17.5.73:38504
I1003 20:29:19.376513 31578 slave.cpp:4267] Querying resource estimator for oversubscribable resources
../../src/tests/oversubscription_tests.cpp:910: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:798:
    Function call: oversubscribable()
          Returns: 16-byte object <E0-AA 45-02 00-00 00-00 80-6D 44-02 00-00 00-00>
         Expected: to be called once
           Actual: called twice - over-saturated and active
../../src/tests/oversubscription_tests.cpp:999: Failure
Failed to wait 15secs for offers2
I1003 20:29:19.380501 31572 master.cpp:1119] Framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504 disconnected
I1003 20:29:19.380607 31572 master.cpp:2475] Disconnecting framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:19.380681 31572 master.cpp:2499] Deactivating framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
../../src/tests/oversubscription_tests.cpp:995: Failure
Actual function call count doesn't match EXPECT_CALL(sched2, resourceOffers(&driver2, _))...
         Expected: to be called at least once
           Actual: never called - unsatisfied and active
W1003 20:29:19.381388 31572 master.hpp:1532] Master attempted to send message to disconnected framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:19.381559 31578 hierarchical.hpp:599] Deactivated framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
2015-10-03 20:29:19,381:31549(0x7f367ffff700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33701] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W1003 20:29:19.382132 31572 master.hpp:1532] Master attempted to send message to disconnected framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:19.382289 31572 master.cpp:1143] Giving framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504 0ns to failover
I1003 20:29:19.382238 31578 hierarchical.hpp:1103] Recovered cpus(*){REV}:2 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]; cpus(*){REV}:2, allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 from framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:19.382781 31583 master.cpp:4815] Framework failover timeout, removing framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:19.382840 31583 master.cpp:5571] Removing framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 (default) at scheduler-7b5f398c-d9a8-4a15-a9b3-afe5d2d0ce18@172.17.5.73:38504
I1003 20:29:19.382984 31572 slave.cpp:1980] Asked to shut down framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000 by master@172.17.5.73:38504
W1003 20:29:19.383014 31572 slave.cpp:1995] Cannot shut down unknown framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:19.383008 31578 hierarchical.hpp:1103] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]; cpus(*){REV}:2, allocated: ) on slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0 from framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:19.383318 31573 hierarchical.hpp:552] Removed framework d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-0000
I1003 20:29:19.383466 31569 master.cpp:919] Master terminating
I1003 20:29:19.383764 31573 hierarchical.hpp:706] Removed slave d4ff5e08-2202-4f3b-8fb2-5515adf9a97e-S0
I1003 20:29:19.384182 31572 slave.cpp:3184] master@172.17.5.73:38504 exited
W1003 20:29:19.384207 31572 slave.cpp:3187] Master disconnected! Waiting for a new master to be elected
I1003 20:29:19.388171 31579 slave.cpp:585] Slave terminating
[  FAILED  ] OversubscriptionTest.UpdateAllocatorOnSchedulerFailover (16026 ms)
{code}

Output from a good run:
{code}
[ RUN      ] OversubscriptionTest.UpdateAllocatorOnSchedulerFailover
Using temporary directory '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_ORy7d7'
I1005 09:29:15.486059 31547 leveldb.cpp:176] Opened db in 3.969392ms
I1005 09:29:15.487290 31547 leveldb.cpp:183] Compacted db in 1.178129ms
I1005 09:29:15.487352 31547 leveldb.cpp:198] Created db iterator in 20562ns
I1005 09:29:15.487375 31547 leveldb.cpp:204] Seeked to beginning of db in 2007ns
I1005 09:29:15.487386 31547 leveldb.cpp:273] Iterated through 0 keys in the db in 407ns
I1005 09:29:15.487438 31547 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1005 09:29:15.487932 31576 recover.cpp:449] Starting replica recovery
I1005 09:29:15.488252 31576 recover.cpp:475] Replica is in EMPTY status
I1005 09:29:15.489248 31574 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I1005 09:29:15.489642 31572 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1005 09:29:15.490124 31570 recover.cpp:566] Updating replica status to STARTING
I1005 09:29:15.490792 31581 master.cpp:376] Master 77693c8c-91ee-4765-97c4-366aadefcd7d (fc9c3c7e4c75) started on 172.17.1.255:41982
I1005 09:29:15.490964 31576 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 733433ns
I1005 09:29:15.490993 31576 replica.cpp:323] Persisted replica status to STARTING
I1005 09:29:15.490811 31581 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_ORy7d7/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_ORy7d7/master"" --zk_session_timeout=""10secs""
I1005 09:29:15.491197 31581 master.cpp:423] Master only allowing authenticated frameworks to register
I1005 09:29:15.491209 31581 master.cpp:428] Master only allowing authenticated slaves to register
I1005 09:29:15.491216 31581 credentials.hpp:37] Loading credentials for authentication from '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_ORy7d7/credentials'
I1005 09:29:15.491217 31572 recover.cpp:475] Replica is in STARTING status
I1005 09:29:15.491473 31581 master.cpp:467] Using default 'crammd5' authenticator
I1005 09:29:15.491590 31581 master.cpp:504] Authorization enabled
I1005 09:29:15.491776 31572 whitelist_watcher.cpp:79] No whitelist given
I1005 09:29:15.491816 31574 hierarchical.hpp:468] Initialized hierarchical allocator process
I1005 09:29:15.492147 31580 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I1005 09:29:15.492511 31566 recover.cpp:195] Received a recover response from a replica in STARTING status
I1005 09:29:15.492986 31572 recover.cpp:566] Updating replica status to VOTING
I1005 09:29:15.493208 31577 master.cpp:1603] The newly elected leader is master@172.17.1.255:41982 with id 77693c8c-91ee-4765-97c4-366aadefcd7d
I1005 09:29:15.493252 31577 master.cpp:1616] Elected as the leading master!
I1005 09:29:15.493283 31577 master.cpp:1376] Recovering from registrar
I1005 09:29:15.493474 31569 registrar.cpp:309] Recovering registrar
I1005 09:29:15.493788 31575 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 508438ns
I1005 09:29:15.493865 31575 replica.cpp:323] Persisted replica status to VOTING
I1005 09:29:15.494056 31572 recover.cpp:580] Successfully joined the Paxos group
I1005 09:29:15.494407 31572 recover.cpp:464] Recover process terminated
I1005 09:29:15.494915 31578 log.cpp:661] Attempting to start the writer
I1005 09:29:15.496220 31570 replica.cpp:477] Replica received implicit promise request with proposal 1
I1005 09:29:15.496793 31570 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 538438ns
I1005 09:29:15.496822 31570 replica.cpp:345] Persisted promised to 1
I1005 09:29:15.497493 31578 coordinator.cpp:231] Coordinator attemping to fill missing position
I1005 09:29:15.498869 31576 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I1005 09:29:15.499552 31576 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 636037ns
I1005 09:29:15.499584 31576 replica.cpp:679] Persisted action at 0
I1005 09:29:15.501091 31580 replica.cpp:511] Replica received write request for position 0
I1005 09:29:15.501165 31580 leveldb.cpp:438] Reading position from leveldb took 36474ns
I1005 09:29:15.501670 31580 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 451907ns
I1005 09:29:15.501703 31580 replica.cpp:679] Persisted action at 0
I1005 09:29:15.502441 31574 replica.cpp:658] Replica received learned notice for position 0
I1005 09:29:15.502915 31574 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 438249ns
I1005 09:29:15.502945 31574 replica.cpp:679] Persisted action at 0
I1005 09:29:15.502969 31574 replica.cpp:664] Replica learned NOP action at position 0
I1005 09:29:15.503648 31581 log.cpp:677] Writer started with ending position 0
I1005 09:29:15.504997 31573 leveldb.cpp:438] Reading position from leveldb took 32637ns
I1005 09:29:15.506117 31580 registrar.cpp:342] Successfully fetched the registry (0B) in 12.594176ms
I1005 09:29:15.506266 31580 registrar.cpp:441] Applied 1 operations in 36719ns; attempting to update the 'registry'
I1005 09:29:15.507222 31568 log.cpp:685] Attempting to append 176 bytes to the log
I1005 09:29:15.507370 31577 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I1005 09:29:15.508220 31567 replica.cpp:511] Replica received write request for position 1
I1005 09:29:15.508644 31567 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 376171ns
I1005 09:29:15.508678 31567 replica.cpp:679] Persisted action at 1
I1005 09:29:15.509405 31579 replica.cpp:658] Replica received learned notice for position 1
I1005 09:29:15.509903 31579 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 461124ns
I1005 09:29:15.509939 31579 replica.cpp:679] Persisted action at 1
I1005 09:29:15.509975 31579 replica.cpp:664] Replica learned APPEND action at position 1
I1005 09:29:15.511237 31579 registrar.cpp:486] Successfully updated the 'registry' in 4.901888ms
I1005 09:29:15.511389 31579 registrar.cpp:372] Successfully recovered registrar
I1005 09:29:15.511615 31573 log.cpp:704] Attempting to truncate the log to 1
I1005 09:29:15.511760 31581 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I1005 09:29:15.511976 31574 master.cpp:1413] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register
I1005 09:29:15.512859 31577 replica.cpp:511] Replica received write request for position 2
I1005 09:29:15.513334 31577 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 435280ns
I1005 09:29:15.513366 31577 replica.cpp:679] Persisted action at 2
I1005 09:29:15.514147 31574 replica.cpp:658] Replica received learned notice for position 2
I1005 09:29:15.514621 31574 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 439979ns
I1005 09:29:15.514685 31574 leveldb.cpp:401] Deleting ~1 keys from leveldb took 34532ns
I1005 09:29:15.514711 31574 replica.cpp:679] Persisted action at 2
I1005 09:29:15.514734 31574 replica.cpp:664] Replica learned TRUNCATE action at position 2
I1005 09:29:15.527739 31572 slave.cpp:190] Slave started on 181)@172.17.1.255:41982
I1005 09:29:15.527870 31572 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_xpOl7A/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_xpOl7A/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_xpOl7A""
I1005 09:29:15.528455 31572 credentials.hpp:85] Loading credential for authentication from '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_xpOl7A/credential'
I1005 09:29:15.528669 31572 slave.cpp:321] Slave using credential for: test-principal
I1005 09:29:15.529208 31572 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1005 09:29:15.529386 31572 slave.cpp:390] Slave hostname: fc9c3c7e4c75
I1005 09:29:15.529412 31572 slave.cpp:395] Slave checkpoint: true
I1005 09:29:15.530339 31566 state.cpp:54] Recovering state from '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_xpOl7A/meta'
I1005 09:29:15.530728 31577 status_update_manager.cpp:202] Recovering status update manager
I1005 09:29:15.530925 31547 sched.cpp:164] Version: 0.26.0
I1005 09:29:15.531236 31572 slave.cpp:4110] Finished recovery
I1005 09:29:15.531599 31570 sched.cpp:262] New master detected at master@172.17.1.255:41982
I1005 09:29:15.531695 31570 sched.cpp:318] Authenticating with master master@172.17.1.255:41982
I1005 09:29:15.531723 31570 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1005 09:29:15.531801 31572 slave.cpp:4267] Querying resource estimator for oversubscribable resources
I1005 09:29:15.532099 31572 slave.cpp:705] New master detected at master@172.17.1.255:41982
I1005 09:29:15.532135 31574 authenticatee.cpp:115] Creating new client SASL connection
I1005 09:29:15.532161 31572 slave.cpp:768] Authenticating with master master@172.17.1.255:41982
I1005 09:29:15.532176 31572 slave.cpp:773] Using default CRAM-MD5 authenticatee
I1005 09:29:15.532184 31571 status_update_manager.cpp:176] Pausing sending status updates
I1005 09:29:15.532299 31572 slave.cpp:741] Detecting new master
I1005 09:29:15.532346 31573 master.cpp:5138] Authenticating scheduler-7ad52576-7d7c-447a-862c-b2923d33c49e@172.17.1.255:41982
I1005 09:29:15.532351 31578 authenticatee.cpp:115] Creating new client SASL connection
I1005 09:29:15.532460 31572 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(427)@172.17.1.255:41982
I1005 09:29:15.532598 31573 master.cpp:5138] Authenticating slave(181)@172.17.1.255:41982
I1005 09:29:15.532649 31578 authenticator.cpp:92] Creating new server SASL connection
I1005 09:29:15.532697 31572 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(428)@172.17.1.255:41982
I1005 09:29:15.532927 31566 authenticator.cpp:92] Creating new server SASL connection
I1005 09:29:15.532928 31581 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I1005 09:29:15.532976 31581 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I1005 09:29:15.533072 31581 authenticator.cpp:197] Received SASL authentication start
I1005 09:29:15.533124 31581 authenticator.cpp:319] Authentication requires more steps
I1005 09:29:15.533208 31578 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I1005 09:29:15.533251 31578 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I1005 09:29:15.533247 31571 authenticatee.cpp:252] Received SASL authentication step
I1005 09:29:15.533381 31578 authenticator.cpp:197] Received SASL authentication start
I1005 09:29:15.533437 31580 authenticator.cpp:225] Received SASL authentication step
I1005 09:29:15.533479 31578 authenticator.cpp:319] Authentication requires more steps
I1005 09:29:15.533484 31580 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'fc9c3c7e4c75' server FQDN: 'fc9c3c7e4c75' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1005 09:29:15.533529 31580 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I1005 09:29:15.533586 31580 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1005 09:29:15.533634 31580 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'fc9c3c7e4c75' server FQDN: 'fc9c3c7e4c75' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1005 09:29:15.533670 31580 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1005 09:29:15.533689 31580 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1005 09:29:15.533713 31580 authenticator.cpp:311] Authentication success
I1005 09:29:15.533812 31566 authenticatee.cpp:292] Authentication success
I1005 09:29:15.533857 31572 master.cpp:5168] Successfully authenticated principal 'test-principal' at scheduler-7ad52576-7d7c-447a-862c-b2923d33c49e@172.17.1.255:41982
I1005 09:29:15.533634 31575 authenticatee.cpp:252] Received SASL authentication step
I1005 09:29:15.533913 31569 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(427)@172.17.1.255:41982
I1005 09:29:15.534070 31575 authenticator.cpp:225] Received SASL authentication step
I1005 09:29:15.534109 31575 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'fc9c3c7e4c75' server FQDN: 'fc9c3c7e4c75' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1005 09:29:15.534122 31575 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I1005 09:29:15.534155 31575 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1005 09:29:15.534188 31575 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'fc9c3c7e4c75' server FQDN: 'fc9c3c7e4c75' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1005 09:29:15.534207 31575 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1005 09:29:15.534224 31575 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1005 09:29:15.534251 31575 authenticator.cpp:311] Authentication success
I1005 09:29:15.534281 31574 sched.cpp:407] Successfully authenticated with master master@172.17.1.255:41982
I1005 09:29:15.534308 31574 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.1.255:41982
I1005 09:29:15.534370 31570 authenticatee.cpp:292] Authentication success
I1005 09:29:15.534587 31574 sched.cpp:747] Will retry registration in 1.149375537secs if necessary
I1005 09:29:15.534603 31580 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(428)@172.17.1.255:41982
I1005 09:29:15.534629 31576 master.cpp:5168] Successfully authenticated principal 'test-principal' at slave(181)@172.17.1.255:41982
I1005 09:29:15.534770 31581 slave.cpp:836] Successfully authenticated with master master@172.17.1.255:41982
I1005 09:29:15.534812 31576 master.cpp:2179] Received SUBSCRIBE call for framework 'default' at scheduler-7ad52576-7d7c-447a-862c-b2923d33c49e@172.17.1.255:41982
I1005 09:29:15.534884 31576 master.cpp:1642] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1005 09:29:15.534929 31581 slave.cpp:1230] Will retry registration in 4.891155ms if necessary
I1005 09:29:15.535244 31576 master.cpp:3862] Registering slave at slave(181)@172.17.1.255:41982 (fc9c3c7e4c75) with id 77693c8c-91ee-4765-97c4-366aadefcd7d-S0
I1005 09:29:15.535533 31576 master.cpp:2250] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1005 09:29:15.535811 31577 registrar.cpp:441] Applied 1 operations in 67842ns; attempting to update the 'registry'
I1005 09:29:15.536068 31570 hierarchical.hpp:515] Added framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000
I1005 09:29:15.536083 31573 sched.cpp:641] Framework registered with 77693c8c-91ee-4765-97c4-366aadefcd7d-0000
I1005 09:29:15.536111 31570 hierarchical.hpp:1328] No resources available to allocate!
I1005 09:29:15.536136 31570 hierarchical.hpp:1423] No inverse offers to send out!
I1005 09:29:15.536151 31573 sched.cpp:655] Scheduler::registered took 40549ns
I1005 09:29:15.536156 31570 hierarchical.hpp:1223] Performed allocation for 0 slaves in 55637ns
I1005 09:29:15.536798 31572 log.cpp:685] Attempting to append 345 bytes to the log
I1005 09:29:15.536967 31568 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I1005 09:29:15.537840 31566 replica.cpp:511] Replica received write request for position 3
I1005 09:29:15.538086 31566 leveldb.cpp:343] Persisting action (364 bytes) to leveldb took 205701ns
I1005 09:29:15.538120 31566 replica.cpp:679] Persisted action at 3
I1005 09:29:15.538823 31581 replica.cpp:658] Replica received learned notice for position 3
I1005 09:29:15.539402 31581 leveldb.cpp:343] Persisting action (366 bytes) to leveldb took 547136ns
I1005 09:29:15.539434 31581 replica.cpp:679] Persisted action at 3
I1005 09:29:15.539456 31581 replica.cpp:664] Replica learned APPEND action at position 3
I1005 09:29:15.540318 31572 slave.cpp:1230] Will retry registration in 18.909777ms if necessary
I1005 09:29:15.540449 31570 master.cpp:3850] Ignoring register slave message from slave(181)@172.17.1.255:41982 (fc9c3c7e4c75) as admission is already in progress
I1005 09:29:15.541157 31581 registrar.cpp:486] Successfully updated the 'registry' in 5.255936ms
I1005 09:29:15.541429 31568 log.cpp:704] Attempting to truncate the log to 3
I1005 09:29:15.541666 31570 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I1005 09:29:15.542251 31580 slave.cpp:3138] Received ping from slave-observer(180)@172.17.1.255:41982
I1005 09:29:15.542318 31572 master.cpp:3930] Registered slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0 at slave(181)@172.17.1.255:41982 (fc9c3c7e4c75) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1005 09:29:15.542503 31580 slave.cpp:880] Registered with master master@172.17.1.255:41982; given slave ID 77693c8c-91ee-4765-97c4-366aadefcd7d-S0
I1005 09:29:15.542490 31573 hierarchical.hpp:675] Added slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0 (fc9c3c7e4c75) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1005 09:29:15.542543 31580 fetcher.cpp:77] Clearing fetcher cache
I1005 09:29:15.542626 31568 replica.cpp:511] Replica received write request for position 4
I1005 09:29:15.542714 31576 status_update_manager.cpp:183] Resuming sending status updates
I1005 09:29:15.543210 31573 hierarchical.hpp:1423] No inverse offers to send out!
I1005 09:29:15.543246 31573 hierarchical.hpp:1241] Performed allocation for slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0 in 689651ns
I1005 09:29:15.543361 31580 slave.cpp:903] Checkpointing SlaveInfo to '/tmp/OversubscriptionTest_UpdateAllocatorOnSchedulerFailover_xpOl7A/meta/slaves/77693c8c-91ee-4765-97c4-366aadefcd7d-S0/slave.info'
I1005 09:29:15.543413 31568 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 745851ns
I1005 09:29:15.543452 31568 replica.cpp:679] Persisted action at 4
I1005 09:29:15.543723 31572 master.cpp:4967] Sending 1 offers to framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000 (default) at scheduler-7ad52576-7d7c-447a-862c-b2923d33c49e@172.17.1.255:41982
I1005 09:29:15.544183 31577 replica.cpp:658] Replica received learned notice for position 4
I1005 09:29:15.544330 31575 sched.cpp:811] Scheduler::resourceOffers took 174769ns
I1005 09:29:15.544636 31577 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 420950ns
I1005 09:29:15.544729 31577 leveldb.cpp:401] Deleting ~2 keys from leveldb took 53144ns
I1005 09:29:15.544764 31577 replica.cpp:679] Persisted action at 4
I1005 09:29:15.544796 31577 replica.cpp:664] Replica learned TRUNCATE action at position 4
I1005 09:29:15.545887 31547 sched.cpp:164] Version: 0.26.0
I1005 09:29:15.546413 31579 sched.cpp:262] New master detected at master@172.17.1.255:41982
I1005 09:29:15.546521 31579 sched.cpp:318] Authenticating with master master@172.17.1.255:41982
I1005 09:29:15.546556 31579 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1005 09:29:15.546779 31568 authenticatee.cpp:115] Creating new client SASL connection
I1005 09:29:15.547103 31575 master.cpp:5138] Authenticating scheduler-20982337-435e-4c5b-bd1f-f3a6523cfb9c@172.17.1.255:41982
I1005 09:29:15.547219 31579 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(429)@172.17.1.255:41982
I1005 09:29:15.547490 31566 authenticator.cpp:92] Creating new server SASL connection
I1005 09:29:15.547732 31568 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I1005 09:29:15.547768 31568 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I1005 09:29:15.547878 31568 authenticator.cpp:197] Received SASL authentication start
I1005 09:29:15.547951 31568 authenticator.cpp:319] Authentication requires more steps
I1005 09:29:15.548102 31581 authenticatee.cpp:252] Received SASL authentication step
I1005 09:29:15.548276 31566 authenticator.cpp:225] Received SASL authentication step
I1005 09:29:15.548317 31566 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'fc9c3c7e4c75' server FQDN: 'fc9c3c7e4c75' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1005 09:29:15.548332 31566 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I1005 09:29:15.548377 31566 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1005 09:29:15.548403 31566 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'fc9c3c7e4c75' server FQDN: 'fc9c3c7e4c75' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1005 09:29:15.548418 31566 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1005 09:29:15.548426 31566 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1005 09:29:15.548444 31566 authenticator.cpp:311] Authentication success
I1005 09:29:15.548580 31581 authenticatee.cpp:292] Authentication success
I1005 09:29:15.548638 31580 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(429)@172.17.1.255:41982
I1005 09:29:15.548813 31572 master.cpp:5168] Successfully authenticated principal 'test-principal' at scheduler-20982337-435e-4c5b-bd1f-f3a6523cfb9c@172.17.1.255:41982
I1005 09:29:15.549000 31578 sched.cpp:407] Successfully authenticated with master master@172.17.1.255:41982
I1005 09:29:15.549049 31578 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.1.255:41982
I1005 09:29:15.549180 31578 sched.cpp:747] Will retry registration in 254.102834ms if necessary
I1005 09:29:15.549331 31580 master.cpp:2179] Received SUBSCRIBE call for framework 'default' at scheduler-20982337-435e-4c5b-bd1f-f3a6523cfb9c@172.17.1.255:41982
I1005 09:29:15.549422 31580 master.cpp:1642] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1005 09:29:15.549756 31579 master.cpp:2250] Subscribing framework default with checkpointing disabled and capabilities [ REVOCABLE_RESOURCES ]
I1005 09:29:15.549816 31579 master.cpp:2314] Updating info for framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000
I1005 09:29:15.549921 31579 master.cpp:2327] Framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000 (default) at scheduler-7ad52576-7d7c-447a-862c-b2923d33c49e@172.17.1.255:41982 failed over
I1005 09:29:15.550123 31574 sched.cpp:1024] Got error 'Framework failed over'
I1005 09:29:15.550158 31574 sched.cpp:1805] Asked to abort the driver
I1005 09:29:15.550243 31574 sched.cpp:1035] Scheduler::error took 34304ns
I1005 09:29:15.550317 31574 sched.cpp:1070] Aborting framework '77693c8c-91ee-4765-97c4-366aadefcd7d-0000'
I1005 09:29:15.550771 31568 sched.cpp:641] Framework registered with 77693c8c-91ee-4765-97c4-366aadefcd7d-0000
W1005 09:29:15.550870 31575 slave.cpp:2141] Ignoring updating pid for framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000 because it does not exist
I1005 09:29:15.550832 31576 hierarchical.hpp:1103] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0 from framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000
I1005 09:29:15.550905 31568 sched.cpp:655] Scheduler::registered took 35612ns
W1005 09:29:15.550879 31579 master.cpp:2461] Ignoring deactivate framework message for framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000 (default) at scheduler-20982337-435e-4c5b-bd1f-f3a6523cfb9c@172.17.1.255:41982 because it is not expected from scheduler-7ad52576-7d7c-447a-862c-b2923d33c49e@172.17.1.255:41982
I1005 09:29:16.493671 31569 hierarchical.hpp:1423] No inverse offers to send out!
I1005 09:29:16.493721 31569 hierarchical.hpp:1223] Performed allocation for 1 slaves in 590us
I1005 09:29:16.494014 31572 master.cpp:4967] Sending 1 offers to framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000 (default) at scheduler-20982337-435e-4c5b-bd1f-f3a6523cfb9c@172.17.1.255:41982
I1005 09:29:16.494505 31581 sched.cpp:811] Scheduler::resourceOffers took 124660ns
I1005 09:29:16.495869 31577 slave.cpp:4281] Received oversubscribable resources cpus(*){REV}:2 from the resource estimator
I1005 09:29:16.496151 31577 slave.cpp:4304] Forwarding total oversubscribed resources cpus(*){REV}:2
I1005 09:29:16.496521 31577 master.cpp:4272] Received update of slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0 at slave(181)@172.17.1.255:41982 (fc9c3c7e4c75) with total oversubscribed resources cpus(*){REV}:2
I1005 09:29:16.497033 31578 hierarchical.hpp:735] Slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0 (fc9c3c7e4c75) updated with oversubscribed resources cpus(*){REV}:2 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]; cpus(*){REV}:2, allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1005 09:29:16.497767 31578 hierarchical.hpp:1423] No inverse offers to send out!
I1005 09:29:16.497807 31578 hierarchical.hpp:1241] Performed allocation for slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0 in 705507ns
I1005 09:29:16.498114 31577 master.cpp:4967] Sending 1 offers to framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000 (default) at scheduler-20982337-435e-4c5b-bd1f-f3a6523cfb9c@172.17.1.255:41982
I1005 09:29:16.498600 31581 sched.cpp:811] Scheduler::resourceOffers took 111653ns
I1005 09:29:16.499043 31547 sched.cpp:1771] Asked to stop the driver
I1005 09:29:16.499110 31547 sched.cpp:1771] Asked to stop the driver
I1005 09:29:16.499146 31578 sched.cpp:1040] Stopping framework '77693c8c-91ee-4765-97c4-366aadefcd7d-0000'
I1005 09:29:16.499191 31577 sched.cpp:1040] Stopping framework '77693c8c-91ee-4765-97c4-366aadefcd7d-0000'
I1005 09:29:16.499234 31570 master.cpp:919] Master terminating
I1005 09:29:16.499647 31576 hierarchical.hpp:706] Removed slave 77693c8c-91ee-4765-97c4-366aadefcd7d-S0
I1005 09:29:16.500577 31571 hierarchical.hpp:552] Removed framework 77693c8c-91ee-4765-97c4-366aadefcd7d-0000
I1005 09:29:16.500599 31570 slave.cpp:3184] master@172.17.1.255:41982 exited
W1005 09:29:16.500634 31570 slave.cpp:3187] Master disconnected! Waiting for a new master to be elected
I1005 09:29:16.505249 31579 slave.cpp:585] Slave terminating
[       OK ] OversubscriptionTest.UpdateAllocatorOnSchedulerFailover (1029 ms)
{code}"	MESOS	Resolved	3	1	2732	flaky-test
12773576	MasterSlaveReconciliationTest.ReconcileRace is flaky	"{noformat:title=}
[ RUN      ] MasterSlaveReconciliationTest.ReconcileRace
Using temporary directory '/tmp/MasterSlaveReconciliationTest_ReconcileRace_NE9nhV'
I0206 19:09:44.196542 32362 leveldb.cpp:175] Opened db in 38.230192ms
I0206 19:09:44.206826 32362 leveldb.cpp:182] Compacted db in 9.988493ms
I0206 19:09:44.207164 32362 leveldb.cpp:197] Created db iterator in 29979ns
I0206 19:09:44.207641 32362 leveldb.cpp:203] Seeked to beginning of db in 4478ns
I0206 19:09:44.207929 32362 leveldb.cpp:272] Iterated through 0 keys in the db in 737ns
I0206 19:09:44.208222 32362 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 19:09:44.209132 32384 recover.cpp:448] Starting replica recovery
I0206 19:09:44.209524 32384 recover.cpp:474] Replica is in EMPTY status
I0206 19:09:44.211094 32384 replica.cpp:640] Replica in EMPTY status received a broadcasted recover request
I0206 19:09:44.211385 32384 recover.cpp:194] Received a recover response from a replica in EMPTY status
I0206 19:09:44.211902 32384 recover.cpp:565] Updating replica status to STARTING
I0206 19:09:44.236177 32381 master.cpp:344] Master 20150206-190944-16842879-36452-32362 (lucid) started on 127.0.1.1:36452
I0206 19:09:44.236291 32381 master.cpp:390] Master only allowing authenticated frameworks to register
I0206 19:09:44.236305 32381 master.cpp:395] Master only allowing authenticated slaves to register
I0206 19:09:44.236327 32381 credentials.hpp:35] Loading credentials for authentication from '/tmp/MasterSlaveReconciliationTest_ReconcileRace_NE9nhV/credentials'
I0206 19:09:44.236601 32381 master.cpp:439] Authorization enabled
I0206 19:09:44.238539 32381 hierarchical_allocator_process.hpp:284] Initialized hierarchical allocator process
I0206 19:09:44.238662 32381 whitelist_watcher.cpp:64] No whitelist given
I0206 19:09:44.239364 32381 master.cpp:1350] The newly elected leader is master@127.0.1.1:36452 with id 20150206-190944-16842879-36452-32362
I0206 19:09:44.239392 32381 master.cpp:1363] Elected as the leading master!
I0206 19:09:44.239413 32381 master.cpp:1181] Recovering from registrar
I0206 19:09:44.239645 32381 registrar.cpp:312] Recovering registrar
I0206 19:09:44.241142 32384 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 29.029117ms
I0206 19:09:44.241189 32384 replica.cpp:322] Persisted replica status to STARTING
I0206 19:09:44.241478 32384 recover.cpp:474] Replica is in STARTING status
I0206 19:09:44.243075 32384 replica.cpp:640] Replica in STARTING status received a broadcasted recover request
I0206 19:09:44.243398 32384 recover.cpp:194] Received a recover response from a replica in STARTING status
I0206 19:09:44.243964 32384 recover.cpp:565] Updating replica status to VOTING
I0206 19:09:44.255692 32384 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 11.502759ms
I0206 19:09:44.255765 32384 replica.cpp:322] Persisted replica status to VOTING
I0206 19:09:44.256009 32384 recover.cpp:579] Successfully joined the Paxos group
I0206 19:09:44.256253 32384 recover.cpp:463] Recover process terminated
I0206 19:09:44.257669 32384 log.cpp:659] Attempting to start the writer
I0206 19:09:44.259944 32377 replica.cpp:476] Replica received implicit promise request with proposal 1
I0206 19:09:44.268805 32377 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 8.45858ms
I0206 19:09:44.269067 32377 replica.cpp:344] Persisted promised to 1
I0206 19:09:44.277974 32383 coordinator.cpp:229] Coordinator attemping to fill missing position
I0206 19:09:44.279767 32383 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2
I0206 19:09:44.288940 32383 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 9.128603ms
I0206 19:09:44.289294 32383 replica.cpp:678] Persisted action at 0
I0206 19:09:44.296417 32377 replica.cpp:510] Replica received write request for position 0
I0206 19:09:44.296944 32377 leveldb.cpp:437] Reading position from leveldb took 48457ns
I0206 19:09:44.305337 32377 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 8.141689ms
I0206 19:09:44.305662 32377 replica.cpp:678] Persisted action at 0
I0206 19:09:44.318168 32378 replica.cpp:657] Replica received learned notice for position 0
I0206 19:09:44.326036 32378 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 6.910907ms
I0206 19:09:44.326654 32378 replica.cpp:678] Persisted action at 0
I0206 19:09:44.326942 32378 replica.cpp:663] Replica learned NOP action at position 0
I0206 19:09:44.338552 32377 log.cpp:675] Writer started with ending position 0
I0206 19:09:44.340262 32377 leveldb.cpp:437] Reading position from leveldb took 58420ns
I0206 19:09:44.342963 32377 registrar.cpp:345] Successfully fetched the registry (0B) in 103.275776ms
I0206 19:09:44.343070 32377 registrar.cpp:444] Applied 1 operations in 26702ns; attempting to update the 'registry'
I0206 19:09:44.345824 32377 log.cpp:683] Attempting to append 118 bytes to the log
I0206 19:09:44.346019 32377 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 1
I0206 19:09:44.347017 32382 replica.cpp:510] Replica received write request for position 1
I0206 19:09:44.354070 32382 leveldb.cpp:342] Persisting action (135 bytes) to leveldb took 7.011319ms
I0206 19:09:44.354118 32382 replica.cpp:678] Persisted action at 1
I0206 19:09:44.355094 32382 replica.cpp:657] Replica received learned notice for position 1
I0206 19:09:44.358392 32382 leveldb.cpp:342] Persisting action (137 bytes) to leveldb took 3.267785ms
I0206 19:09:44.358621 32382 replica.cpp:678] Persisted action at 1
I0206 19:09:44.358949 32382 replica.cpp:663] Replica learned APPEND action at position 1
I0206 19:09:44.367099 32382 registrar.cpp:489] Successfully updated the 'registry' in 23.9488ms
I0206 19:09:44.367259 32382 registrar.cpp:375] Successfully recovered registrar
I0206 19:09:44.367698 32382 master.cpp:1208] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0206 19:09:44.367790 32382 log.cpp:702] Attempting to truncate the log to 1
I0206 19:09:44.368067 32382 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 2
I0206 19:09:44.369544 32381 replica.cpp:510] Replica received write request for position 2
I0206 19:09:44.378540 32381 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 8.940568ms
I0206 19:09:44.378811 32381 replica.cpp:678] Persisted action at 2
I0206 19:09:44.387471 32377 replica.cpp:657] Replica received learned notice for position 2
I0206 19:09:44.394693 32377 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 6.892787ms
I0206 19:09:44.394815 32377 leveldb.cpp:400] Deleting ~1 keys from leveldb took 57353ns
I0206 19:09:44.394840 32377 replica.cpp:678] Persisted action at 2
I0206 19:09:44.394875 32377 replica.cpp:663] Replica learned TRUNCATE action at position 2
I0206 19:09:44.423720 32380 slave.cpp:172] Slave started on 14)@127.0.1.1:36452
I0206 19:09:44.426247 32380 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterSlaveReconciliationTest_ReconcileRace_nT15cx/credential'
I0206 19:09:44.426745 32380 slave.cpp:281] Slave using credential for: test-principal
I0206 19:09:44.427234 32380 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 19:09:44.427616 32380 slave.cpp:328] Slave hostname: lucid
I0206 19:09:44.427829 32380 slave.cpp:329] Slave checkpoint: false
W0206 19:09:44.428047 32380 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0206 19:09:44.429821 32379 state.cpp:32] Recovering state from '/tmp/MasterSlaveReconciliationTest_ReconcileRace_nT15cx/meta'
I0206 19:09:44.438045 32380 status_update_manager.cpp:196] Recovering status update manager
I0206 19:09:44.438377 32380 slave.cpp:3526] Finished recovery
I0206 19:09:44.439144 32380 slave.cpp:620] New master detected at master@127.0.1.1:36452
I0206 19:09:44.439275 32380 slave.cpp:683] Authenticating with master master@127.0.1.1:36452
I0206 19:09:44.439297 32380 slave.cpp:688] Using default CRAM-MD5 authenticatee
I0206 19:09:44.439472 32380 slave.cpp:656] Detecting new master
I0206 19:09:44.439589 32380 status_update_manager.cpp:170] Pausing sending status updates
I0206 19:09:44.439757 32380 authenticatee.hpp:137] Creating new client SASL connection
I0206 19:09:44.440255 32380 master.cpp:3786] Authenticating slave(14)@127.0.1.1:36452
I0206 19:09:44.440287 32380 master.cpp:3797] Using default CRAM-MD5 authenticator
I0206 19:09:44.440712 32380 authenticator.hpp:169] Creating new server SASL connection
I0206 19:09:44.441010 32380 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 19:09:44.441036 32380 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 19:09:44.441136 32380 authenticator.hpp:275] Received SASL authentication start
I0206 19:09:44.441216 32380 authenticator.hpp:397] Authentication requires more steps
I0206 19:09:44.441303 32380 authenticatee.hpp:274] Received SASL authentication step
I0206 19:09:44.441395 32380 authenticator.hpp:303] Received SASL authentication step
I0206 19:09:44.441431 32380 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 19:09:44.441442 32380 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0206 19:09:44.441493 32380 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 19:09:44.441526 32380 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 19:09:44.441540 32380 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 19:09:44.441546 32380 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 19:09:44.441561 32380 authenticator.hpp:389] Authentication success
I0206 19:09:44.441668 32380 authenticatee.hpp:314] Authentication success
I0206 19:09:44.441756 32380 master.cpp:3844] Successfully authenticated principal 'test-principal' at slave(14)@127.0.1.1:36452
I0206 19:09:44.442070 32380 slave.cpp:754] Successfully authenticated with master master@127.0.1.1:36452
I0206 19:09:44.442211 32380 slave.cpp:1082] Will retry registration in 19.488037ms if necessary
I0206 19:09:44.442474 32380 master.cpp:2911] Registering slave at slave(14)@127.0.1.1:36452 (lucid) with id 20150206-190944-16842879-36452-32362-S0
I0206 19:09:44.442922 32380 registrar.cpp:444] Applied 1 operations in 72989ns; attempting to update the 'registry'
I0206 19:09:44.445914 32380 log.cpp:683] Attempting to append 283 bytes to the log
I0206 19:09:44.446125 32380 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 3
I0206 19:09:44.448060 32384 replica.cpp:510] Replica received write request for position 3
I0206 19:09:44.450963 32384 leveldb.cpp:342] Persisting action (302 bytes) to leveldb took 2.859704ms
I0206 19:09:44.451252 32384 replica.cpp:678] Persisted action at 3
I0206 19:09:44.452453 32382 replica.cpp:657] Replica received learned notice for position 3
I0206 19:09:44.463752 32382 leveldb.cpp:342] Persisting action (304 bytes) to leveldb took 11.254934ms
I0206 19:09:44.463817 32382 replica.cpp:678] Persisted action at 3
I0206 19:09:44.463848 32382 replica.cpp:663] Replica learned APPEND action at position 3
I0206 19:09:44.465450 32382 registrar.cpp:489] Successfully updated the 'registry' in 22.45888ms
I0206 19:09:44.465725 32382 log.cpp:702] Attempting to truncate the log to 3
I0206 19:09:44.466614 32379 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 4
I0206 19:09:44.466958 32379 slave.cpp:1082] Will retry registration in 34.738916ms if necessary
I0206 19:09:44.467807 32379 replica.cpp:510] Replica received write request for position 4
I0206 19:09:44.469007 32381 slave.cpp:2595] Received ping from slave-observer(14)@127.0.1.1:36452
I0206 19:09:44.469740 32377 hierarchical_allocator_process.hpp:450] Added slave 20150206-190944-16842879-36452-32362-S0 (lucid) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0206 19:09:44.469871 32377 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:44.469892 32377 hierarchical_allocator_process.hpp:753] Performed allocation for slave 20150206-190944-16842879-36452-32362-S0 in 107661ns
I0206 19:09:44.470643 32378 slave.cpp:788] Registered with master master@127.0.1.1:36452; given slave ID 20150206-190944-16842879-36452-32362-S0
I0206 19:09:44.473881 32383 status_update_manager.cpp:177] Resuming sending status updates
I0206 19:09:44.476253 32379 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 8.4136ms
I0206 19:09:44.476297 32379 replica.cpp:678] Persisted action at 4
I0206 19:09:44.488441 32382 master.cpp:2968] Registered slave 20150206-190944-16842879-36452-32362-S0 at slave(14)@127.0.1.1:36452 (lucid) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 19:09:44.489220 32382 master.cpp:2887] Slave 20150206-190944-16842879-36452-32362-S0 at slave(14)@127.0.1.1:36452 (lucid) already registered, resending acknowledgement
I0206 19:09:44.490684 32379 replica.cpp:657] Replica received learned notice for position 4
I0206 19:09:44.491462 32362 sched.cpp:149] Version: 0.22.0
I0206 19:09:44.494812 32378 slave.cpp:620] New master detected at master@127.0.1.1:36452
I0206 19:09:44.494870 32378 slave.cpp:683] Authenticating with master master@127.0.1.1:36452
I0206 19:09:44.494889 32378 slave.cpp:688] Using default CRAM-MD5 authenticatee
I0206 19:09:44.495014 32378 slave.cpp:656] Detecting new master
I0206 19:09:44.495160 32378 sched.cpp:246] New master detected at master@127.0.1.1:36452
I0206 19:09:44.495232 32378 sched.cpp:302] Authenticating with master master@127.0.1.1:36452
I0206 19:09:44.495252 32378 sched.cpp:309] Using default CRAM-MD5 authenticatee
I0206 19:09:44.495458 32378 status_update_manager.cpp:170] Pausing sending status updates
I0206 19:09:44.495589 32378 authenticatee.hpp:137] Creating new client SASL connection
I0206 19:09:44.495846 32378 authenticatee.hpp:137] Creating new client SASL connection
I0206 19:09:44.498770 32382 master.cpp:3786] Authenticating slave(14)@127.0.1.1:36452
I0206 19:09:44.498812 32382 master.cpp:3797] Using default CRAM-MD5 authenticator
I0206 19:09:44.499124 32382 master.cpp:3786] Authenticating scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452
I0206 19:09:44.499151 32382 master.cpp:3797] Using default CRAM-MD5 authenticator
I0206 19:09:44.499363 32382 slave.cpp:788] Registered with master master@127.0.1.1:36452; given slave ID 20150206-190944-16842879-36452-32362-S0
I0206 19:09:44.499675 32382 authenticator.hpp:169] Creating new server SASL connection
I0206 19:09:44.499925 32382 authenticator.hpp:169] Creating new server SASL connection
I0206 19:09:44.500035 32382 status_update_manager.cpp:177] Resuming sending status updates
I0206 19:09:44.500330 32382 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 19:09:44.500356 32382 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 19:09:44.500501 32382 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 19:09:44.500526 32382 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 19:09:44.500691 32382 authenticator.hpp:275] Received SASL authentication start
I0206 19:09:44.500758 32382 authenticator.hpp:397] Authentication requires more steps
I0206 19:09:44.500836 32382 authenticator.hpp:275] Received SASL authentication start
I0206 19:09:44.500874 32382 authenticator.hpp:397] Authentication requires more steps
I0206 19:09:44.500943 32382 authenticatee.hpp:274] Received SASL authentication step
I0206 19:09:44.501026 32382 authenticatee.hpp:274] Received SASL authentication step
I0206 19:09:44.501104 32382 authenticator.hpp:303] Received SASL authentication step
I0206 19:09:44.501130 32382 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 19:09:44.501142 32382 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0206 19:09:44.501188 32382 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 19:09:44.501212 32382 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 19:09:44.501224 32382 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 19:09:44.501230 32382 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 19:09:44.501245 32382 authenticator.hpp:389] Authentication success
I0206 19:09:44.501368 32382 authenticator.hpp:303] Received SASL authentication step
I0206 19:09:44.501394 32382 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 19:09:44.501405 32382 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0206 19:09:44.501435 32382 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 19:09:44.501458 32382 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 19:09:44.501469 32382 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 19:09:44.501477 32382 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 19:09:44.501489 32382 authenticator.hpp:389] Authentication success
I0206 19:09:44.501582 32382 authenticatee.hpp:314] Authentication success
I0206 19:09:44.501672 32382 master.cpp:3844] Successfully authenticated principal 'test-principal' at slave(14)@127.0.1.1:36452
I0206 19:09:44.501898 32382 master.cpp:3844] Successfully authenticated principal 'test-principal' at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452
I0206 19:09:44.502104 32382 authenticatee.hpp:314] Authentication success
I0206 19:09:44.502272 32382 slave.cpp:754] Successfully authenticated with master master@127.0.1.1:36452
I0206 19:09:44.502421 32382 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:36452
I0206 19:09:44.502447 32382 sched.cpp:513] Sending registration request to master@127.0.1.1:36452
I0206 19:09:44.502512 32382 sched.cpp:546] Will retry registration in 1.990458655secs if necessary
I0206 19:09:44.502679 32382 master.cpp:1568] Received registration request for framework 'default' at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452
I0206 19:09:44.502760 32382 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 19:09:44.503207 32379 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 12.488335ms
I0206 19:09:44.503340 32379 leveldb.cpp:400] Deleting ~2 keys from leveldb took 95587ns
I0206 19:09:44.503365 32379 replica.cpp:678] Persisted action at 4
I0206 19:09:44.503394 32379 replica.cpp:663] Replica learned TRUNCATE action at position 4
I0206 19:09:44.504719 32382 master.cpp:1632] Registering framework 20150206-190944-16842879-36452-32362-0000 (default) at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452
I0206 19:09:44.505327 32380 hierarchical_allocator_process.hpp:318] Added framework 20150206-190944-16842879-36452-32362-0000
I0206 19:09:44.505712 32380 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 353338ns
I0206 19:09:44.517038 32382 master.cpp:3728] Sending 1 offers to framework 20150206-190944-16842879-36452-32362-0000 (default) at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452
I0206 19:09:44.517523 32382 sched.cpp:440] Framework registered with 20150206-190944-16842879-36452-32362-0000
I0206 19:09:44.517575 32382 sched.cpp:454] Scheduler::registered took 26978ns
I0206 19:09:44.517899 32382 sched.cpp:603] Scheduler::resourceOffers took 48926ns
I0206 19:09:45.246789 32381 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:45.246841 32381 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 239355ns
I0206 19:09:46.257019 32383 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:46.257325 32383 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 642985ns
I0206 19:09:47.259109 32382 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:47.259160 32382 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 247172ns
I0206 19:09:48.267207 32378 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:48.267590 32378 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 737879ns
I0206 19:09:49.276871 32379 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:49.276924 32379 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 243974ns
I0206 19:09:50.287142 32382 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:50.287504 32382 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 784304ns
I0206 19:09:51.296839 32379 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:51.296893 32379 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 244814ns
I0206 19:09:52.297719 32378 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:52.299626 32378 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 2.277105ms
I0206 19:09:53.306870 32384 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:53.306924 32384 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 262416ns
I0206 19:09:54.317059 32382 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:54.320051 32382 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 3.270301ms
I0206 19:09:55.326977 32379 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:55.330344 32379 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 3.569401ms
I0206 19:09:56.339260 32381 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:56.343905 32381 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 5.144928ms
I0206 19:09:57.346943 32377 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:57.347012 32377 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 326655ns
I0206 19:09:58.348335 32380 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:58.351338 32380 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 3.622285ms
I0206 19:09:59.356886 32383 hierarchical_allocator_process.hpp:828] No resources available to allocate!
I0206 19:09:59.356941 32383 hierarchical_allocator_process.hpp:735] Performed allocation for 1 slaves in 271092ns
I0206 19:09:59.477521 32381 slave.cpp:2595] Received ping from slave-observer(14)@127.0.1.1:36452
tests/master_slave_reconciliation_tests.cpp:283: Failure
Failed to wait 15secs for reregisterSlaveMessage
I0206 19:09:59.510076 32377 master.cpp:872] Framework 20150206-190944-16842879-36452-32362-0000 (default) at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452 disconnected
I0206 19:09:59.510154 32377 master.cpp:1937] Disconnecting framework 20150206-190944-16842879-36452-32362-0000 (default) at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452
I0206 19:09:59.510216 32377 master.cpp:1953] Deactivating framework 20150206-190944-16842879-36452-32362-0000 (default) at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452
I0206 19:09:59.511065 32377 master.cpp:894] Giving framework 20150206-190944-16842879-36452-32362-0000 (default) at scheduler-167ce2f8-8268-41f7-9c72-a4e29dc3316c@127.0.1.1:36452 0ns to failover
I0206 19:09:59.512804 32383 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-190944-16842879-36452-32362-0000
I0206 19:09:59.512990 32383 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150206-190944-16842879-36452-32362-S0 from framework 20150206-190944-16842879-36452-32362-0000
I0206 19:09:59.513653 32382 slave.cpp:609] Re-detecting master
I0206 19:09:59.513687 32382 slave.cpp:656] Detecting new master
*** Aborted at 1423278599 (unix time) try ""date -d @1423278599"" if you are using GNU date ***
I0206 19:09:59.514680 32380 status_update_manager.cpp:170] Pausing sending status updates
I0206 19:09:59.516027 32379 master.cpp:781] Master terminating
PC: @     0x2b2b91358b79 mesos::slave::Slave::detected()
*** SIGSEGV (@0x10) received by PID 32362 (TID 0x2b2b98dad700) from PID 16; stack trace: ***
    @     0x2b2b9b6ff3b4 os::Linux::chained_handler()
    @     0x2b2b9b70459a JVM_handle_linux_signal
    @     0x2b2b8f0a98f0 (unknown)
    @     0x2b2b91358b79 mesos::slave::Slave::detected()
    @     0x2b2b913d31b4 std::tr1::_Mem_fn<>::operator()()
    @     0x2b2b913cd54f _ZNSt3tr15_BindIFNS_7_Mem_fnIMN5mesos5slave5SlaveEFvRKN7process6FutureI6OptionINS2_10MasterInfoEEEEEEENS_12_PlaceholderILi1EEESA_EE6__callIIRPS4_EILi0ELi1EEEENS_9result_ofIFSF_NSN_IFNS_3_MuISH_Lb0ELb1EEESH_NS_5tupleIIDpT_EEEEE4typeENSN_IFNSO_ISA_Lb0ELb0EEESA_ST_EE4typeEEE4typeERKST_NS_12_Index_tupleIIXspT0_EEEE
    @     0x2b2b913c3216 std::tr1::_Bind<>::operator()<>()
    @     0x2b2b913b6a7e std::tr1::_Function_handler<>::_M_invoke()
    @           0x7f6b9a std::tr1::function<>::operator()()
    @           0x7e765c process::internal::vdispatcher<>()
    @           0x80e823 _ZNSt3tr15_BindIFPFvPN7process11ProcessBaseENS_10shared_ptrINS_8functionIFvPN5mesos5slave5SlaveEEEEEEENS_12_PlaceholderILi1EEESC_EE6__callIIRS3_EILi0ELi1EEEENS_9result_ofIFSE_NSL_IFNS_3_MuISG_Lb0ELb1EEESG_NS_5tupleIIDpT_EEEEE4typeENSL_IFNSM_ISC_Lb0ELb0EEESC_SR_EE4typeEEE4typeERKSR_NS_12_Index_tupleIIXspT0_EEEE
    @           0x803b6c std::tr1::_Bind<>::operator()<>()
    @           0x7f6c86 std::tr1::_Function_handler<>::_M_invoke()
    @     0x2b2b91a25714 std::tr1::function<>::operator()()
    @     0x2b2b91a0ae4f process::ProcessBase::visit()
    @     0x2b2b91a0f3a0 process::DispatchEvent::visit()
    @           0x8a56f2 process::ProcessBase::serve()
    @     0x2b2b91a06fc2 process::ProcessManager::resume()
    @     0x2b2b919fabfc process::schedule()
    @     0x2b2b8f0a09ca start_thread
    @     0x2b2b94ab745d (unknown)
make[3]: *** [check-local] Segmentation fault
{noformat}"	MESOS	Resolved	3	1	2732	flaky
13094154	Quota heuristic check not accounting for mount volumes	"This may be expected but came as a surprise to us. We are unable to create a quota bigger than the root disk space on slaves.

Given two clusters with the same number of slaves and root disk size, but one that also has mount volumes, is what the disk resources look like:

{noformat}
[root@fin-fang-foom-master-1 ~]# curl -s master.mesos:5050/state | jq '.slaves[] .resources .disk'
28698
28699
28698
28698
28697
{noformat}

{noformat}
[root@hydra-master-1 ~]# curl -s master.mesos:5050/state | jq '.slaves[] .resources .disk'
50817
50817
50814
50819
50817
{noformat}

In {{fin-fang-foom}}, I was able to create a quota for {{143490mb}} which is the total of available disk resources, root in this case, as reported by Mesos. For {{hydra}}, I am only able to create a quota for {{143489mb}}. This is equivalent to the total of root disks available in {{hydra}} rather than the total available disks reported by Mesos resources which is {{254084mb}}.

With a modified Mesos that adds logging to {{quota_handler}}, we can see that only the {{disk(*)}} number increases in {{nonStaticClusterResources}} after every iteration. The final iteration is {{disk(*):143489}} which is the maximum quota I was able to create on {{hydra}}. We expected that quota heuristic check would also include resources such as {{disk(*)[MOUNT:/dcos/volume2]:7373}}

{noformat}
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763764 24902 quota_handler.cpp:71] Performing capacity heuristic check for a set quota request
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763783 24902 quota_handler.cpp:87] heuristic: total quota 'disk(*):143489'

Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763870 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763923 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'


Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763989 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764022 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):57396; cpus(*):8; mem(*):30046; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'

Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764077 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28695; cpus(*):4; mem(*):15023'
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764119 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):86091; cpus(*):12; mem(*):45069; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'

Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764225 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28700; cpus(*):4; mem(*):15023'
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764307 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):114791; cpus(*):16; mem(*):60092; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'

Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764434 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764492 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):143489; cpus(*):20; mem(*):75115; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'

Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764562 24902 quota_handler.cpp:118] heuristic: nonStaticClusterResources.contains(totalQuota)
{noformat}
"	MESOS	Resolved	3	1	2732	resource-management
12863971	Perf event isolator stops performing sampling if a single timeout occurs.	"Currently the perf event isolator times out a sample after a fixed extra time of 2 seconds on top of the sample time elapses:

{code}
    Duration timeout = flags.perf_duration + Seconds(2);
{code}

This should be based on the reap interval maximum.

Also, the code stops sampling altogether when a single timeout occurs. We've observed time outs during normal operation, so it would be better for the isolator to continue performing perf sampling in the case of timeouts. It may also make sense to continue sampling in the case of errors, since these may be transient."	MESOS	Resolved	3	1	2732	twitter
12783342	Log IP addresses from HTTP requests	"Querying /master/state.json is an expensive operation when a cluster is large, and it's possible to DOS the master via frequent and repeated queries (which is a separate problem). Querying the endpoint results in a log entry being written, but the entry lacks useful information, such as an IP address, response code and response size. These details are useful for tracking down who/what is querying the endpoint. Consider adding these details to the log entry, or even writing a separate [access|https://httpd.apache.org/docs/trunk/logs.html#accesslog] [log|https://httpd.apache.org/docs/trunk/logs.html#common]. Also consider writing log entries for _all_ HTTP requests (/metrics/snapshot produces no log entries).

{noformat:title=sample log entry}
I0319 18:06:18.824846 10521 http.cpp:478] HTTP request for '/master/state.json'
{noformat}"	MESOS	Resolved	4	4	2732	twitter
13231515	/__processes__ endpoint can hang.	"A user reported that the {{/\_\_processes\_\_}} endpoint occasionally hangs.

Stack traces provided by [~alexr] revealed that all the threads appeared to be idle waiting for events. After investigating the code, the issue was found to be possible when a process gets terminated after the {{/\_\_processes\_\_}} route handler dispatches to it, thus dropping the dispatch and abandoning the future."	MESOS	Resolved	3	1	2732	foundations
12838954	Add queue size metrics for the allocator.	"In light of the performance regression in MESOS-2891, we'd like to have visibility into the queue size of the allocator. This will enable alerting on performance problems.

We currently have no metrics in the allocator.

I will also look into MESOS-1286 now that we have gcc 4.8, current queue size gauges require a trip through the Process' queue."	MESOS	Resolved	2	3	2732	twitter
13308291	Mesos failed to build due to error C2668 on windows with MSVC	"Hi All,

I tried to build Mesos on Windows with VS2019.It failed to build due to error C2668: 'os::spawn': ambiguous call to overloaded function on Windows using MSVC. It can be reproduced on latest reversiond4634f4 on master branch. Could you please take a look at this isssue? Thanks a lot!



Reproduce steps:

1. git clone -c core.autocrlf=true[https://github.com/apache/mesos] F:\gitP\apache\mesos
 2. Open a VS 2019 x64 command prompt as admin and browse to F:\gitP\apache\mesos
 3. mkdir build_amd64 && pushd build_amd64

4.cmake -G ""Visual Studio 16 2019"" -A x64 -DCMAKE_SYSTEM_VERSION=10.0.18362.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=""F:\tools\gnuwin32\bin"" -T host=x64 ..

5.set _CL_=/D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING %_CL_%

6. msbuild /maxcpucount:4 /p:Platform=x64 /p:Configuration=Debug Mesos.sln /t:Rebuild



ErrorMessage:

F:\gitP\apache\mesos\3rdparty\stout\include\stout/os/windows/shell.hpp(168,68): error C2668: 'os::spawn': ambiguous call to overloaded function (compiling source file F:\gitP\apache\mesos\3rdparty\libprocess\src\authenticator.cpp) [F:\gitP\apache\mesos\build_amd64\3rdparty\libprocess\src\process.vcxproj]



"	MESOS	Resolved	3	1	2732	windows
13300033	Windows overlapped IO discard handling can drop data.	"When getting a discard request for an io operation on windows, a cancellation is requested [1] and when the io operation completes we check whether the future had a discard request to decide whether to discard it [2]:

{code}
template <typename T>
static void set_io_promise(Promise<T>* promise, const T& data, DWORD error)
{
  if (promise->future().hasDiscard()) {
    promise->discard();
  } else if (error == ERROR_SUCCESS) {
    promise->set(data);
  } else {
    promise->fail(""IO failed with error code: "" + WindowsError(error).message);
  }
}
{code}

However, it's possible the operation completed successfully, in which case we did not succeed at canceling it. We need to check for {{ERROR_OPERATION_ABORTED}} [3]:

{code}
template <typename T>
static void set_io_promise(Promise<T>* promise, const T& data, DWORD error)
{
  if (promise->future().hasDiscard() && error == ERROR_OPERATION_ABORTED) {
    promise->discard();
  } else if (error == ERROR_SUCCESS) {
    promise->set(data);
  } else {
    promise->fail(""IO failed with error code: "" + WindowsError(error).message);
  }
}
{code}

I don't think there are currently any major consequences to this issue, since most callers tend to be discarding only when they're essentially abandoning the entire process of reading or writing.

[1] https://github.com/apache/mesos/blob/1.9.0/3rdparty/libprocess/src/windows/libwinio.cpp#L448
[2] https://github.com/apache/mesos/blob/1.9.0/3rdparty/libprocess/src/windows/libwinio.cpp#L141-L151
[3] https://docs.microsoft.com/en-us/windows/win32/fileio/cancelioex-func"	MESOS	Resolved	4	1	2732	windows
13086569	Framework might not receive status update when a just launched task is killed immediately	"Our Marathon team are seeing issues in their integration test suite when Marathon gets stuck in an infinite loop trying to kill a just launched task. In their test a task launched which is immediately followed by killing the task -- the framework does e.g., not wait for any task status update.

In this case the launch and kill messages arrive at the agent in the correct order, but both the launch and kill paths in the agent do not reach the point where a status update is sent to the framework. Since the framework has seen no status update on the task it re-triggers a kill, causing an infinite loop."	MESOS	Resolved	2	1	2732	reliability
13101736	SlaveTest.HTTPSchedulerSlaveRestart test is flaky.	"Saw this on ASF CI when testing 1.4.0-rc5

{code}
[ RUN      ] SlaveTest.HTTPSchedulerSlaveRestart
I0912 05:40:15.280185 32547 cluster.cpp:162] Creating default 'local' authorizer
I0912 05:40:15.282783 32554 master.cpp:442] Master c23ff8cf-cb2f-40d0-8f18-871a41f128cf (b909d5e22907) started on 172.17.0.2:58922
I0912 05:40:15.282804 32554 master.cpp:444] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/he1E9j/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/he1E9j/master"" --zk_session_timeout=""10secs""
I0912 05:40:15.283092 32554 master.cpp:494] Master only allowing authenticated frameworks to register
I0912 05:40:15.283110 32554 master.cpp:508] Master only allowing authenticated agents to register
I0912 05:40:15.283118 32554 master.cpp:521] Master only allowing authenticated HTTP frameworks to register
I0912 05:40:15.283123 32554 credentials.hpp:37] Loading credentials for authentication from '/tmp/he1E9j/credentials'
I0912 05:40:15.283394 32554 master.cpp:566] Using default 'crammd5' authenticator
I0912 05:40:15.283543 32554 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0912 05:40:15.283731 32554 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0912 05:40:15.283887 32554 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0912 05:40:15.284021 32554 master.cpp:646] Authorization enabled
I0912 05:40:15.284293 32552 whitelist_watcher.cpp:77] No whitelist given
I0912 05:40:15.284335 32550 hierarchical.cpp:171] Initialized hierarchical allocator process
I0912 05:40:15.287078 32561 master.cpp:2163] Elected as the leading master!
I0912 05:40:15.287103 32561 master.cpp:1702] Recovering from registrar
I0912 05:40:15.287214 32557 registrar.cpp:347] Recovering registrar
I0912 05:40:15.287703 32557 registrar.cpp:391] Successfully fetched the registry (0B) in 455936ns
I0912 05:40:15.287791 32557 registrar.cpp:495] Applied 1 operations in 24179ns; attempting to update the registry
I0912 05:40:15.288317 32557 registrar.cpp:552] Successfully updated the registry in 473088ns
I0912 05:40:15.288435 32557 registrar.cpp:424] Successfully recovered registrar
I0912 05:40:15.288789 32548 master.cpp:1801] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I0912 05:40:15.288822 32559 hierarchical.cpp:209] Skipping recovery of hierarchical allocator: nothing to recover
I0912 05:40:15.292457 32547 containerizer.cpp:246] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni,environment_secret
W0912 05:40:15.293053 32547 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges
W0912 05:40:15.293184 32547 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I0912 05:40:15.293220 32547 provisioner.cpp:255] Using default backend 'copy'
W0912 05:40:15.297993 32547 process.cpp:3196] Attempted to spawn already running process files@172.17.0.2:58922
I0912 05:40:15.298338 32547 cluster.cpp:448] Creating default 'local' authorizer
I0912 05:40:15.300554 32551 slave.cpp:250] Mesos agent started on (198)@172.17.0.2:58922
I0912 05:40:15.300576 32551 slave.cpp:251] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2:
secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V""
I0912 05:40:15.301059 32551 credentials.hpp:86] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/credential'
W0912 05:40:15.301174 32547 process.cpp:3196] Attempted to spawn already running process version@172.17.0.2:58922
I0912 05:40:15.301239 32551 slave.cpp:283] Agent using credential for: test-principal
I0912 05:40:15.301256 32551 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/http_credentials'
I0912 05:40:15.301512 32551 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0912 05:40:15.301681 32551 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0912 05:40:15.301935 32547 sched.cpp:232] Version: 1.4.0
I0912 05:40:15.302479 32557 sched.cpp:336] New master detected at master@172.17.0.2:58922
I0912 05:40:15.302592 32557 sched.cpp:407] Authenticating with master master@172.17.0.2:58922
I0912 05:40:15.302614 32557 sched.cpp:414] Using default CRAM-MD5 authenticatee
I0912 05:40:15.302922 32553 authenticatee.cpp:121] Creating new client SASL connection
I0912 05:40:15.303220 32562 master.cpp:7832] Authenticating scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:15.303400 32556 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(406)@172.17.0.2:58922
I0912 05:40:15.303673 32554 authenticator.cpp:98] Creating new server SASL connection
I0912 05:40:15.303473 32551 slave.cpp:565] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
I0912 05:40:15.303707 32551 slave.cpp:573] Agent attributes: [  ]
I0912 05:40:15.303717 32551 slave.cpp:582] Agent hostname: b909d5e22907
I0912 05:40:15.303900 32559 status_update_manager.cpp:177] Pausing sending status updates
I0912 05:40:15.304033 32548 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0912 05:40:15.304070 32548 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0912 05:40:15.304189 32548 authenticator.cpp:204] Received SASL authentication start
I0912 05:40:15.304265 32548 authenticator.cpp:326] Authentication requires more steps
I0912 05:40:15.304404 32561 authenticatee.cpp:259] Received SASL authentication step
I0912 05:40:15.304566 32549 authenticator.cpp:232] Received SASL authentication step
I0912 05:40:15.304603 32549 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0912 05:40:15.304615 32549 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0912 05:40:15.304647 32549 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0912 05:40:15.304671 32549 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0912 05:40:15.304682 32549 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0912 05:40:15.304697 32549 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0912 05:40:15.304715 32549 authenticator.cpp:318] Authentication success
I0912 05:40:15.304852 32563 authenticatee.cpp:299] Authentication success
I0912 05:40:15.304916 32552 master.cpp:7862] Successfully authenticated principal 'test-principal' at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:15.305004 32557 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(406)@172.17.0.2:58922
I0912 05:40:15.305253 32549 sched.cpp:513] Successfully authenticated with master master@172.17.0.2:58922
I0912 05:40:15.305269 32549 sched.cpp:836] Sending SUBSCRIBE call to master@172.17.0.2:58922
I0912 05:40:15.305433 32549 sched.cpp:869] Will retry registration in 237.896638ms if necessary
I0912 05:40:15.305629 32555 state.cpp:64] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta'
I0912 05:40:15.305652 32559 master.cpp:2894] Received SUBSCRIBE call for framework 'default' at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:15.305742 32559 master.cpp:2228] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'
I0912 05:40:15.305963 32560 status_update_manager.cpp:203] Recovering status update manager
I0912 05:40:15.306152 32550 containerizer.cpp:609] Recovering containerizer
I0912 05:40:15.306252 32553 master.cpp:2974] Subscribing framework default with checkpointing enabled and capabilities [ RESERVATION_REFINEMENT ]
I0912 05:40:15.306928 32559 sched.cpp:759] Framework registered with c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.307013 32559 sched.cpp:773] Scheduler::registered took 58136ns
I0912 05:40:15.307162 32552 hierarchical.cpp:303] Added framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.307384 32552 hierarchical.cpp:1925] No allocations performed
I0912 05:40:15.307423 32552 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:15.307464 32552 hierarchical.cpp:1468] Performed allocation for 0 agents in 124365ns
I0912 05:40:15.308010 32557 provisioner.cpp:416] Provisioner recovery complete
I0912 05:40:15.308349 32556 slave.cpp:6295] Finished recovery
I0912 05:40:15.308863 32556 slave.cpp:6477] Querying resource estimator for oversubscribable resources
I0912 05:40:15.309139 32562 slave.cpp:6491] Received oversubscribable resources {} from the resource estimator
I0912 05:40:15.309347 32562 slave.cpp:971] New master detected at master@172.17.0.2:58922
I0912 05:40:15.309409 32550 status_update_manager.cpp:177] Pausing sending status updates
I0912 05:40:15.309500 32562 slave.cpp:1006] Detecting new master
I0912 05:40:15.311897 32559 slave.cpp:1033] Authenticating with master master@172.17.0.2:58922
I0912 05:40:15.311975 32559 slave.cpp:1044] Using default CRAM-MD5 authenticatee
I0912 05:40:15.312253 32560 authenticatee.cpp:121] Creating new client SASL connection
I0912 05:40:15.312513 32560 master.cpp:7832] Authenticating slave(198)@172.17.0.2:58922
I0912 05:40:15.312654 32548 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(407)@172.17.0.2:58922
I0912 05:40:15.312940 32558 authenticator.cpp:98] Creating new server SASL connection
I0912 05:40:15.313187 32552 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0912 05:40:15.313213 32552 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0912 05:40:15.313313 32552 authenticator.cpp:204] Received SASL authentication start
I0912 05:40:15.313364 32552 authenticator.cpp:326] Authentication requires more steps
I0912 05:40:15.313478 32551 authenticatee.cpp:259] Received SASL authentication step
I0912 05:40:15.313613 32553 authenticator.cpp:232] Received SASL authentication step
I0912 05:40:15.313649 32553 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0912 05:40:15.313673 32553 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0912 05:40:15.313743 32553 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0912 05:40:15.313788 32553 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0912 05:40:15.313808 32553 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0912 05:40:15.313817 32553 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0912 05:40:15.313833 32553 authenticator.cpp:318] Authentication success
I0912 05:40:15.313931 32557 authenticatee.cpp:299] Authentication success
I0912 05:40:15.314019 32554 master.cpp:7862] Successfully authenticated principal 'test-principal' at slave(198)@172.17.0.2:58922
I0912 05:40:15.314079 32553 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(407)@172.17.0.2:58922
I0912 05:40:15.314239 32555 slave.cpp:1128] Successfully authenticated with master master@172.17.0.2:58922
I0912 05:40:15.314457 32555 slave.cpp:1607] Will retry registration in 9.221574ms if necessary
I0912 05:40:15.314672 32561 master.cpp:5714] Received register agent message from slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.314810 32561 master.cpp:3803] Authorizing agent with principal 'test-principal'
I0912 05:40:15.315261 32548 master.cpp:5774] Authorized registration of agent at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.315383 32548 master.cpp:5867] Registering agent at slave(198)@172.17.0.2:58922 (b909d5e22907) with id c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.315827 32558 registrar.cpp:495] Applied 1 operations in 55999ns; attempting to update the registry
I0912 05:40:15.316412 32558 registrar.cpp:552] Successfully updated the registry in 528896ns
I0912 05:40:15.316654 32557 master.cpp:5914] Admitted agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.317286 32554 slave.cpp:4970] Received ping from slave-observer(191)@172.17.0.2:58922
I0912 05:40:15.317461 32554 slave.cpp:1174] Registered with master master@172.17.0.2:58922; given agent ID c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.317587 32553 status_update_manager.cpp:184] Resuming sending status updates
I0912 05:40:15.317279 32557 master.cpp:5945] Registered agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907) with [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
I0912 05:40:15.317819 32562 hierarchical.cpp:593] Added agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 (b909d5e22907) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})
I0912 05:40:15.317857 32554 slave.cpp:1194] Checkpointing SlaveInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/slave.info'
I0912 05:40:15.318280 32554 slave.cpp:1243] Forwarding total oversubscribed resources {}
I0912 05:40:15.318450 32554 master.cpp:6683] Received update of agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907) with total oversubscribed resources {}
I0912 05:40:15.319030 32562 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:15.319090 32562 hierarchical.cpp:1468] Performed allocation for 1 agents in 1.101144ms
I0912 05:40:15.319267 32562 hierarchical.cpp:660] Agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 (b909d5e22907) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
I0912 05:40:15.319643 32555 master.cpp:7662] Sending 1 offers to framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:15.320127 32561 sched.cpp:933] Scheduler::resourceOffers took 109341ns
I0912 05:40:15.322115 32550 master.cpp:9159] Removing offer c23ff8cf-cb2f-40d0-8f18-871a41f128cf-O0
I0912 05:40:15.322265 32550 master.cpp:4153] Processing ACCEPT call for offers: [ c23ff8cf-cb2f-40d0-8f18-871a41f128cf-O0 ] on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907) for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:15.322368 32550 master.cpp:3530] Authorizing framework principal 'test-principal' to launch task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec
I0912 05:40:15.324560 32550 master.cpp:9719] Adding task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}] on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.325297 32550 master.cpp:4816] Launching task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}] on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.327203 32560 slave.cpp:1736] Got assigned task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.327380 32560 slave.cpp:7175] Checkpointing FrameworkInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.info'
I0912 05:40:15.327888 32560 slave.cpp:7186] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.pid'
I0912 05:40:15.327944 32550 hierarchical.cpp:887] Updated allocation of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 from cpus(allocated: *):2; mem(allocated: *):1024; disk(allocated: *):1024; ports(allocated: *):[31000-32000] to cpus(allocated: *):2; mem(allocated: *):1024; disk(allocated: *):1024; ports(allocated: *):[31000-32000]
I0912 05:40:15.328968 32560 slave.cpp:2003] Authorizing task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.329071 32560 slave.cpp:6794] Authorizing framework principal 'test-principal' to launch task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec
I0912 05:40:15.330121 32553 slave.cpp:2171] Launching task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.330823 32553 paths.cpp:578] Trying to chown '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5' to user 'mesos'
I0912 05:40:15.331084 32553 slave.cpp:7757] Checkpointing ExecutorInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/executor.info'
I0912 05:40:15.331904 32553 slave.cpp:7256] Launching executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":32.0},""type"":""SCALAR""}] in work directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'
I0912 05:40:15.332718 32553 slave.cpp:2858] Launching container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 for executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.333190 32554 containerizer.cpp:1083] Starting container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5
I0912 05:40:15.333230 32553 slave.cpp:7800] Checkpointing TaskInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5/tasks/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/task.info'
I0912 05:40:15.333696 32554 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from PROVISIONING to PREPARING
I0912 05:40:15.333937 32553 slave.cpp:2400] Queued task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.334064 32553 slave.cpp:924] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'
I0912 05:40:15.334168 32553 slave.cpp:924] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'
I0912 05:40:15.338408 32556 containerizer.cpp:1681] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/build\/src""],""shell"":false,""value"":""\/mesos\/build\/src\/mesos-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.17.0.2:58922""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""1""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""\/tmp\/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V\/slaves\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0\/frameworks\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000\/executors\/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec\/runs\/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""8fc99bc8-a2b6-498b-8bb2-af5d92e78cec""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_RECOVERY_TIMEOUT"",""type"":""VALUE"",""value"":""15mins""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(198)@172.17.0.2:58922""},{""name"":""MESOS_SUBSCRIPTION_BACKOFF_MAX"",""type"":""VALUE"",""value"":""2secs""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/tmp\/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V\/slaves\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0\/frameworks\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000\/executors\/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec\/runs\/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5""}]},""task_environment"":{},""user"":""mesos"",""working_directory"":""\/tmp\/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V\/slaves\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0\/frameworks\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000\/executors\/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec\/runs\/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5""}"" --pipe_read=""6"" --pipe_write=""7"" --runtime_directory=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/containers/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5"" --unshare_namespace_mnt=""false""'
I0912 05:40:15.340767 32556 launcher.cpp:140] Forked child with pid '1772' for container '69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'
I0912 05:40:15.340893 32556 containerizer.cpp:1773] Checkpointing container's forked pid 1772 to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5/pids/forked.pid'
I0912 05:40:15.341821 32556 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from PREPARING to ISOLATING
I0912 05:40:15.343189 32558 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from ISOLATING to FETCHING
I0912 05:40:15.343369 32560 fetcher.cpp:377] Starting to fetch URIs for container: 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5, directory: /tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5
I0912 05:40:15.344462 32549 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from FETCHING to RUNNING
I0912 05:40:15.504098  1787 exec.cpp:162] Version: 1.4.0
I0912 05:40:15.510535 32550 slave.cpp:3935] Got registration for executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from executor(1)@172.17.0.2:33722
I0912 05:40:15.511157 32550 slave.cpp:4021] Checkpointing executor pid 'executor(1)@172.17.0.2:33722' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5/pids/libprocess.pid'
I0912 05:40:15.513628 32552 slave.cpp:2605] Sending queued task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' to executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 at executor(1)@172.17.0.2:33722
I0912 05:40:15.517511  1780 exec.cpp:237] Executor registered on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.521653  1774 executor.cpp:171] Received SUBSCRIBED event
I0912 05:40:15.522095  1774 executor.cpp:175] Subscribed executor on b909d5e22907
I0912 05:40:15.522334  1774 executor.cpp:171] Received LAUNCH event
I0912 05:40:15.522544  1774 executor.cpp:633] Starting task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec
I0912 05:40:15.528475  1774 executor.cpp:477] Running '/mesos/build/src/mesos-containerizer launch <POSSIBLY-SENSITIVE-DATA>'
I0912 05:40:15.531814  1774 executor.cpp:646] Forked command at 1791
I0912 05:40:15.538535 32556 slave.cpp:4399] Handling status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from executor(1)@172.17.0.2:33722
I0912 05:40:15.540377 32548 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.540426 32548 status_update_manager.cpp:500] Creating StatusUpdate stream for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.541287 32548 status_update_manager.cpp:834] Checkpointing UPDATE for status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.541561 32548 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to the agent
I0912 05:40:15.541859 32559 slave.cpp:4880] Forwarding the update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to master@172.17.0.2:58922
I0912 05:40:15.542114 32559 slave.cpp:4774] Status update manager successfully handled status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.542174 32559 slave.cpp:4790] Sending acknowledgement for status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to executor(1)@172.17.0.2:33722
I0912 05:40:15.542295 32552 master.cpp:6841] Status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.542371 32552 master.cpp:6903] Forwarding status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.542628 32552 master.cpp:8928] Updating the state of task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (latest st:
ate: TASK_RUNNING, status update state: TASK_RUNNING)
I0912 05:40:15.542891 32563 sched.cpp:1041] Scheduler::statusUpdate took 114540ns
I0912 05:40:15.543287 32550 master.cpp:5479] Processing ACKNOWLEDGE call b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.543305 32547 slave.cpp:843] Agent terminating
I0912 05:40:15.543632 32550 master.cpp:1318] Agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907) disconnected
I0912 05:40:15.543651 32550 master.cpp:3301] Disconnecting agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.543767 32547 containerizer.cpp:246] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni,environment_secret
I0912 05:40:15.543817 32550 master.cpp:3320] Deactivating agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.543967 32560 hierarchical.cpp:690] Agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 deactivated
W0912 05:40:15.544199 32547 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges
W0912 05:40:15.544307 32547 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I0912 05:40:15.544339 32547 provisioner.cpp:255] Using default backend 'copy'
W0912 05:40:15.551013 32547 process.cpp:3196] Attempted to spawn already running process files@172.17.0.2:58922
I0912 05:40:15.551386 32547 cluster.cpp:448] Creating default 'local' authorizer
I0912 05:40:15.554386 32555 slave.cpp:250] Mesos agent started on (199)@172.17.0.2:58922
I0912 05:40:15.554404 32555 slave.cpp:251] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V""
I0912 05:40:15.554872 32555 credentials.hpp:86] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/credential'
I0912 05:40:15.555035 32555 slave.cpp:283] Agent using credential for: test-principal
I0912 05:40:15.555052 32555 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/http_credentials'
I0912 05:40:15.555235 32555 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0912 05:40:15.555388 32555 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0912 05:40:15.556735 32555 slave.cpp:565] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
I0912 05:40:15.556988 32555 slave.cpp:573] Agent attributes: [  ]
I0912 05:40:15.557003 32555 slave.cpp:582] Agent hostname: b909d5e22907
I0912 05:40:15.557221 32560 status_update_manager.cpp:177] Pausing sending status updates
I0912 05:40:15.558465 32558 state.cpp:64] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta'
I0912 05:40:15.558528 32558 state.cpp:722] No committed checkpointed resources found at '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/resources/resources.info'
I0912 05:40:15.561717 32551 slave.cpp:6386] Recovering framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.561810 32551 slave.cpp:7335] Recovering executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.562430 32552 status_update_manager.cpp:203] Recovering status update manager
I0912 05:40:15.562449 32552 status_update_manager.cpp:211] Recovering executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.562503 32552 status_update_manager.cpp:500] Creating StatusUpdate stream for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.562918 32552 status_update_manager.cpp:810] Replaying status update stream for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec
I0912 05:40:15.563284 32556 containerizer.cpp:609] Recovering containerizer
I0912 05:40:15.563344 32556 containerizer.cpp:665] Recovering container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 for executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.565598 32551 provisioner.cpp:416] Provisioner recovery complete
I0912 05:40:15.566550 32553 slave.cpp:6179] Sending reconnect request to executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 at executor(1)@172.17.0.2:33722
I0912 05:40:15.567891  1775 exec.cpp:283] Received reconnect request from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.568752 32558 slave.cpp:4327] Cleaning up un-reregistered executors
I0912 05:40:15.568778 32558 slave.cpp:4345] Killing un-reregistered executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 at executor(1)@172.17.0.2:33722
I0912 05:40:15.568904 32551 containerizer.cpp:2166] Destroying container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 in RUNNING state
I0912 05:40:15.568922 32559 hierarchical.cpp:1925] No allocations performed
I0912 05:40:15.569078 32559 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:15.568987 32551 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from RUNNING to DESTROYING
I0912 05:40:15.569145 32559 hierarchical.cpp:1468] Performed allocation for 1 agents in 332649ns
I0912 05:40:15.569416 32551 launcher.cpp:156] Asked to destroy container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5
I0912 05:40:15.568934 32558 slave.cpp:6295] Finished recovery
I0912 05:40:15.572386 32558 slave.cpp:6477] Querying resource estimator for oversubscribable resources
I0912 05:40:15.572738 32558 slave.cpp:4109] Received re-registration message from executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
W0912 05:40:15.572798 32558 slave.cpp:4161] Shutting down executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 at executor(1)@172.17.0.2:33722 because it is in unexpected state TERMINATING
I0912 05:40:15.573163 32558 slave.cpp:971] New master detected at master@172.17.0.2:58922
I0912 05:40:15.573194 32562 status_update_manager.cpp:177] Pausing sending status updates
I0912 05:40:15.573314 32558 slave.cpp:1006] Detecting new master
I0912 05:40:15.573434 32558 slave.cpp:6491] Received oversubscribable resources {} from the resource estimator
I0912 05:40:15.573761  1789 exec.cpp:435] Executor asked to shutdown
I0912 05:40:15.574031  1782 executor.cpp:171] Received SHUTDOWN event
I0912 05:40:15.574048  1782 executor.cpp:743] Shutting down
I0912 05:40:15.574089  1782 executor.cpp:850] Sending SIGTERM to process tree at pid 1791
I0912 05:40:15.580627 32553 slave.cpp:1033] Authenticating with master master@172.17.0.2:58922
I0912 05:40:15.580713 32553 slave.cpp:1044] Using default CRAM-MD5 authenticatee
I0912 05:40:15.581008 32556 authenticatee.cpp:121] Creating new client SASL connection
I0912 05:40:15.581377 32555 master.cpp:7832] Authenticating slave(199)@172.17.0.2:58922
I0912 05:40:15.581524 32561 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(408)@172.17.0.2:58922
I0912 05:40:15.581822 32563 authenticator.cpp:98] Creating new server SASL connection
I0912 05:40:15.582089 32554 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0912 05:40:15.582123 32554 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0912 05:40:15.582270 32549 authenticator.cpp:204] Received SASL authentication start
I0912 05:40:15.582330 32549 authenticator.cpp:326] Authentication requires more steps
I0912 05:40:15.582463 32549 authenticatee.cpp:259] Received SASL authentication step
I0912 05:40:15.582597 32560 authenticator.cpp:232] Received SASL authentication step
I0912 05:40:15.582625 32560 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0912 05:40:15.582641 32560 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0912 05:40:15.582676 32560 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0912 05:40:15.582695 32560 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0912 05:40:15.582702 32560 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0912 05:40:15.582707 32560 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0912 05:40:15.582720 32560 authenticator.cpp:318] Authentication success
I0912 05:40:15.582815 32562 authenticatee.cpp:299] Authentication success
I0912 05:40:15.582855 32552 master.cpp:7862] Successfully authenticated principal 'test-principal' at slave(199)@172.17.0.2:58922
I0912 05:40:15.582882 32558 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(408)@172.17.0.2:58922
I0912 05:40:15.583106 32562 slave.cpp:1128] Successfully authenticated with master master@172.17.0.2:58922
I0912 05:40:15.583451 32562 slave.cpp:1607] Will retry registration in 949976ns if necessary
I0912 05:40:15.583799 32561 master.cpp:6014] Received re-register agent message from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.584031 32561 master.cpp:3803] Authorizing agent with principal 'test-principal'
I0912 05:40:15.584475 32554 master.cpp:6083] Authorized re-registration of agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.584573 32554 master.cpp:6148] Re-registering agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.584985 32557 hierarchical.cpp:678] Agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 reactivated
I0912 05:40:15.584985 32554 master.cpp:6581] Sending updated checkpointed resources {} to agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.585088 32559 slave.cpp:1607] Will retry registration in 30.237322ms if necessary
I0912 05:40:15.585434 32559 slave.cpp:1286] Re-registered with master master@172.17.0.2:58922
I0912 05:40:15.585535 32559 slave.cpp:1323] Forwarding total oversubscribed resources {}
I0912 05:40:15.585543 32556 status_update_manager.cpp:184] Resuming sending status updates
I0912 05:40:15.585543 32554 master.cpp:6014] Received re-register agent message from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
W0912 05:40:15.585662 32556 status_update_manager.cpp:191] Resending status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.585732 32556 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to the agent
I0912 05:40:15.585903 32554 master.cpp:3803] Authorizing agent with principal 'test-principal'
I0912 05:40:15.585963 32559 slave.cpp:3430] Ignoring new checkpointed resources identical to the current version: {}
I0912 05:40:15.586246 32554 master.cpp:6683] Received update of agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907) with total oversubscribed resources {}
I0912 05:40:15.586230 32559 slave.cpp:4880] Forwarding the update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to master@172.17.0.2:58922
I0912 05:40:15.586472 32554 master.cpp:6083] Authorized re-registration of agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.586551 32556 hierarchical.cpp:660] Agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 (b909d5e22907) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
I0912 05:40:15.586566 32554 master.cpp:6148] Re-registering agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.586849 32554 master.cpp:6581] Sending updated checkpointed resources {} to agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5:
e22907)
W0912 05:40:15.586864 32563 slave.cpp:1304] Already re-registered with master master@172.17.0.2:58922
I0912 05:40:15.586884 32563 slave.cpp:1323] Forwarding total oversubscribed resources {}
I0912 05:40:15.587103 32563 slave.cpp:3366] Updating info for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.587175 32563 slave.cpp:7175] Checkpointing FrameworkInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.info'
I0912 05:40:15.587147 32554 master.cpp:6841] Status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.587221 32554 master.cpp:6903] Forwarding status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.587436 32554 master.cpp:8928] Updating the state of task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0912 05:40:15.587570 32554 master.cpp:6683] Received update of agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907) with total oversubscribed resources {}
I0912 05:40:15.587620 32557 sched.cpp:1041] Scheduler::statusUpdate took 30617ns
I0912 05:40:15.587770 32563 slave.cpp:7186] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.pid'
I0912 05:40:15.587935 32554 master.cpp:5479] Processing ACKNOWLEDGE call b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.587941 32560 hierarchical.cpp:660] Agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 (b909d5e22907) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
I0912 05:40:15.588253 32553 status_update_manager.cpp:184] Resuming sending status updates
W0912 05:40:15.588287 32553 status_update_manager.cpp:191] Resending status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.588327 32563 slave.cpp:3366] Updating info for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 with pid updated to scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:15.588326 32553 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to the agent
I0912 05:40:15.588404 32563 slave.cpp:7175] Checkpointing FrameworkInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.info'
I0912 05:40:15.588801 32563 slave.cpp:7186] Checkpointing framework pid 'scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.pid'
I0912 05:40:15.589190 32563 slave.cpp:3430] Ignoring new checkpointed resources identical to the current version: {}
I0912 05:40:15.589197 32552 status_update_manager.cpp:184] Resuming sending status updates
W0912 05:40:15.589220 32552 status_update_manager.cpp:191] Resending status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.589243 32552 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to the agent
I0912 05:40:15.589283 32563 slave.cpp:4948] Sending message for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:15.589493 32548 sched.cpp:1177] Scheduler::frameworkMessage took 29470ns
I0912 05:40:15.589572 32551 status_update_manager.cpp:395] Received status update acknowledgement (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.589622 32563 slave.cpp:4880] Forwarding the update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to master@172.17.0.2:58922
I0912 05:40:15.589694 32551 status_update_manager.cpp:834] Checkpointing ACK for status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.589879 32563 slave.cpp:4880] Forwarding the update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to master@172.17.0.2:58922
I0912 05:40:15.589992 32555 master.cpp:6841] Status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.590051 32563 slave.cpp:3663] Status update manager successfully handled status update acknowledgement (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.590047 32555 master.cpp:6903] Forwarding status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.590234 32555 master.cpp:8928] Updating the state of task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0912 05:40:15.590378 32559 sched.cpp:1041] Scheduler::statusUpdate took 14489ns
I0912 05:40:15.590432 32555 master.cpp:6841] Status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.590487 32555 master.cpp:6903] Forwarding status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.590644 32555 master.cpp:8928] Updating the state of task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0912 05:40:15.590761 32556 sched.cpp:1041] Scheduler::statusUpdate took 13603ns
I0912 05:40:15.590807 32555 master.cpp:5479] Processing ACKNOWLEDGE call b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.591056 32555 master.cpp:5479] Processing ACKNOWLEDGE call b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.591071 32562 status_update_manager.cpp:395] Received status update acknowledgement (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.591341 32550 status_update_manager.cpp:395] Received status update acknowledgement (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
E0912 05:40:15.591368 32561 slave.cpp:3656] Failed to handle status update acknowledgement (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000: Unexpected status update acknowledgment (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
E0912 05:40:15.591498 32549 slave.cpp:3656] Failed to handle status update acknowledgement (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000: Unexpected status update acknowledgment (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.669543 32554 containerizer.cpp:2612] Container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 has exited
I0912 05:40:15.671640 32560 provisioner.cpp:490] Ignoring destroy request for unknown container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5
I0912 05:40:15.672369 32563 slave.cpp:5412] Executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 terminated with signal Killed
I0912 05:40:15.672497 32563 slave.cpp:4399] Handling status update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from @0.0.0.0:0
W0912 05:40:15.673275 32550 containerizer.cpp:1976] Ignoring update for unknown container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5
I0912 05:40:15.673615 32557 status_update_manager.cpp:323] Received status update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.673672 32557 status_update_manager.cpp:834] Checkpointing UPDATE for status update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.673820 32557 status_update_manager.cpp:377] Forwarding update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to the agent
I0912 05:40:15.674010 32552 slave.cpp:4880] Forwarding the update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to master@172.17.0.2:58922
I0912 05:40:15.674181 32552 slave.cpp:4774] Status update manager successfully handled status update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.674340 32549 master.cpp:6841] Status update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.674394 32549 master.cpp:6903] Forwarding status update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.674536 32549 master.cpp:8928] Updating the state of task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (latest state: TASK_LOST, status update state: TASK_LOST)
I0912 05:40:15.674707 32553 sched.cpp:1041] Scheduler::statusUpdate took 23659ns
I0912 05:40:15.675499 32549 master.cpp:5479] Processing ACKNOWLEDGE call 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9 for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
I0912 05:40:15.675812 32548 hierarchical.cpp:1152] Recovered cpus(allocated: *):2; mem(allocated: *):1024; disk(allocated: *):1024; ports(allocated: *):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: {}) on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 from framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.675567 32549 master.cpp:9022] Removing task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}] of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(199)@172.17.0.2:58922 (b909d5e22907)
I0912 05:40:15.676257 32560 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.676344 32560 status_update_manager.cpp:834] Checkpointing ACK for status update TASK_LOST (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.676432 32560 status_update_manager.cpp:531] Cleaning up status update stream for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.676857 32558 slave.cpp:3663] Status update manager successfully handled status update acknowledgement (UUID: 1bc4622e-9dd3-49ef-9bfd-a875fb36bcd9) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.676908 32558 slave.cpp:7738] Completing task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec
I0912 05:40:15.676949 32558 slave.cpp:5516] Cleaning up executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 at executor(1)@172.17.0.2:33722
I0912 05:40:15.677388 32555 gc.cpp:59] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5' for gc 6.99999216112296days in the future
I0912 05:40:15.677546 32555 gc.cpp:59] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for gc 6.99999215893333days in the future
I0912 05:40:15.677711 32555 gc.cpp:59] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5' for gc 6.99999215732444days in the future
I0912 05:40:15.677775 32558 slave.cpp:5612] Cleaning up framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.677812 32555 gc.cpp:59] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for gc 6.99999215597333days in the future
I0912 05:40:15.677863 32559 status_update_manager.cpp:285] Closing status update streams for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:15.677960 32555 gc.cpp:59] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000' for gc 6.99999215403852days in the future
I0912 05:40:15.678086 32555 gc.cpp:59] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2:
f-40d0-8f18-871a41f128cf-0000' for gc 6.99999215262519days in the future
I0912 05:40:16.571491 32549 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:16.571547 32549 hierarchical.cpp:1468] Performed allocation for 1 agents in 1.096296ms
I0912 05:40:16.572053 32554 master.cpp:7662] Sending 1 offers to framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:16.572510 32560 sched.cpp:933] Scheduler::resourceOffers took 24477ns
I0912 05:40:17.573107 32562 hierarchical.cpp:1925] No allocations performed
I0912 05:40:17.573153 32562 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:17.573195 32562 hierarchical.cpp:1468] Performed allocation for 1 agents in 211498ns
I0912 05:40:18.574553 32548 hierarchical.cpp:1925] No allocations performed
I0912 05:40:18.574599 32548 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:18.574641 32548 hierarchical.cpp:1468] Performed allocation for 1 agents in 180072ns
I0912 05:40:19.576134 32562 hierarchical.cpp:1925] No allocations performed
I0912 05:40:19.576189 32562 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:19.576248 32562 hierarchical.cpp:1468] Performed allocation for 1 agents in 203826ns
I0912 05:40:20.576812 32555 hierarchical.cpp:1925] No allocations performed
I0912 05:40:20.576858 32555 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:20.576900 32555 hierarchical.cpp:1468] Performed allocation for 1 agents in 180929ns
I0912 05:40:21.577955 32560 hierarchical.cpp:1925] No allocations performed
I0912 05:40:21.578001 32560 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:21.578043 32560 hierarchical.cpp:1468] Performed allocation for 1 agents in 181224ns
I0912 05:40:22.579715 32553 hierarchical.cpp:1925] No allocations performed
I0912 05:40:22.579761 32553 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:22.579803 32553 hierarchical.cpp:1468] Performed allocation for 1 agents in 186117ns
I0912 05:40:23.581313 32561 hierarchical.cpp:1925] No allocations performed
I0912 05:40:23.581360 32561 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:23.581403 32561 hierarchical.cpp:1468] Performed allocation for 1 agents in 191563ns
I0912 05:40:24.582953 32559 hierarchical.cpp:1925] No allocations performed
I0912 05:40:24.583000 32559 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:24.583042 32559 hierarchical.cpp:1468] Performed allocation for 1 agents in 180449ns
I0912 05:40:25.584481 32551 hierarchical.cpp:1925] No allocations performed
I0912 05:40:25.584528 32551 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:25.584570 32551 hierarchical.cpp:1468] Performed allocation for 1 agents in 186677ns
I0912 05:40:26.585813 32552 hierarchical.cpp:1925] No allocations performed
I0912 05:40:26.585860 32552 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:26.585903 32552 hierarchical.cpp:1468] Performed allocation for 1 agents in 204077ns
I0912 05:40:27.586802 32556 hierarchical.cpp:1925] No allocations performed
I0912 05:40:27.586848 32556 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:27.586890 32556 hierarchical.cpp:1468] Performed allocation for 1 agents in 189229ns
I0912 05:40:28.588395 32559 hierarchical.cpp:1925] No allocations performed
I0912 05:40:28.588441 32559 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:28.588484 32559 hierarchical.cpp:1468] Performed allocation for 1 agents in 178527ns
I0912 05:40:29.590046 32551 hierarchical.cpp:1925] No allocations performed
I0912 05:40:29.590092 32551 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:29.590134 32551 hierarchical.cpp:1468] Performed allocation for 1 agents in 179207ns
I0912 05:40:30.574189 32550 slave.cpp:6477] Querying resource estimator for oversubscribable resources
I0912 05:40:30.574484 32557 slave.cpp:6491] Received oversubscribable resources {} from the resource estimator
/mesos/src/tests/slave_tests.cpp:5501: Failure
Failed to wait 15secs for executorToFrameworkMessage1
I0912 05:40:30.590958 32549 master.cpp:1432] Framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 disconnected
I0912 05:40:30.591058 32549 master.cpp:3264] Deactivating framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:30.591497 32555 hierarchical.cpp:412] Deactivated framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:30.591819 32555 hierarchical.cpp:1925] No allocations performed
I0912 05:40:30.591877 32555 hierarchical.cpp:2015] No inverse offers to send out!
I0912 05:40:30.592000 32555 hierarchical.cpp:1468] Performed allocation for 1 agents in 359046ns
I0912 05:40:30.592222 32549 master.cpp:9159] Removing offer c23ff8cf-cb2f-40d0-8f18-871a41f128cf-O1
I0912 05:40:30.592332 32549 master.cpp:3241] Disconnecting framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:30.592635 32549 master.cpp:1447] Giving framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 0ns to failover
I0912 05:40:30.593114 32562 slave.cpp:843] Agent terminating
I0912 05:40:30.593736 32555 hierarchical.cpp:1152] Recovered cpus(allocated: *):2; mem(allocated: *):1024; disk(allocated: *):1024; ports(allocated: *):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: {}) on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 from framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:30.594408 32560 master.cpp:7494] Framework failover timeout, removing framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:30.594449 32560 master.cpp:8355] Removing framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922
I0912 05:40:30.595404 32548 hierarchical.cpp:355] Removed framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000
I0912 05:40:30.600559 32547 master.cpp:1160] Master terminating
I0912 05:40:30.601553 32559 hierarchical.cpp:626] Removed agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0
/mesos/3rdparty/libprocess/include/process/gmock.hpp:467: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <48-04 06-D4 37-2B 00-00>, 1, 1)
         Expected: to be called once
           Actual: never called - unsatisfied and active
/mesos/3rdparty/libprocess/include/process/gmock.hpp:467: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <48-04 06-D4 37-2B 00-00>, 1, 1-byte object <28>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] SlaveTest.HTTPSchedulerSlaveRestart (15325 ms)
{code}"	MESOS	Resolved	3	1	2732	flaky-test, mesosphere
12833150	Slave should expose metrics about oversubscribed resources	metrics/snapshot should expose metrics on oversubscribed resources (allocated and available). 	MESOS	Resolved	3	3	2732	twitter
13017879	MasterMaintenanceTest.InverseOffersFilters is flaky.	"This test can crash when launching two executors concurrently because the test containerizer is not thread-safe! (see MESOS-6545).

{noformat}
[...truncated 78174 lines...]
I1103 01:40:55.530350 29098 slave.cpp:974] Authenticating with master master@172.17.0.2:58302
I1103 01:40:55.530432 29098 slave.cpp:985] Using default CRAM-MD5 authenticatee
I1103 01:40:55.530627 29098 slave.cpp:947] Detecting new master
I1103 01:40:55.530675 29108 authenticatee.cpp:121] Creating new client SASL connection
I1103 01:40:55.530743 29098 slave.cpp:5587] Received oversubscribable resources {} from the resource estimator
I1103 01:40:55.530961 29099 master.cpp:6742] Authenticating slave(150)@172.17.0.2:58302
I1103 01:40:55.531070 29112 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(357)@172.17.0.2:58302
I1103 01:40:55.531328 29106 authenticator.cpp:98] Creating new server SASL connection
I1103 01:40:55.531561 29108 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1103 01:40:55.531604 29108 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1103 01:40:55.531713 29101 authenticator.cpp:204] Received SASL authentication start
I1103 01:40:55.531805 29101 authenticator.cpp:326] Authentication requires more steps
I1103 01:40:55.531921 29108 authenticatee.cpp:259] Received SASL authentication step
I1103 01:40:55.532120 29101 authenticator.cpp:232] Received SASL authentication step
I1103 01:40:55.532155 29101 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a1c598ce334' server FQDN: '3a1c598ce334' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I1103 01:40:55.532179 29101 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1103 01:40:55.532233 29101 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1103 01:40:55.532266 29101 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a1c598ce334' server FQDN: '3a1c598ce334' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I1103 01:40:55.532289 29101 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1103 01:40:55.532305 29101 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1103 01:40:55.532335 29101 authenticator.cpp:318] Authentication success
I1103 01:40:55.532413 29110 authenticatee.cpp:299] Authentication success
I1103 01:40:55.532467 29108 master.cpp:6772] Successfully authenticated principal 'test-principal' at slave(150)@172.17.0.2:58302
I1103 01:40:55.532536 29111 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(357)@172.17.0.2:58302
I1103 01:40:55.532755 29098 slave.cpp:1069] Successfully authenticated with master master@172.17.0.2:58302
I1103 01:40:55.532997 29098 slave.cpp:1483] Will retry registration in 12.590371ms if necessary
I1103 01:40:55.533179 29108 master.cpp:5151] Registering agent at slave(150)@172.17.0.2:58302 (maintenance-host-2) with id 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1
I1103 01:40:55.533572 29112 registrar.cpp:461] Applied 1 operations in 94467ns; attempting to update the registry
I1103 01:40:55.546341 29107 slave.cpp:1483] Will retry registration in 36.501523ms if necessary
I1103 01:40:55.546461 29099 master.cpp:5139] Ignoring register agent message from slave(150)@172.17.0.2:58302 (maintenance-host-2) as admission is already in progress
I1103 01:40:55.565403 29097 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 48.099208ms
I1103 01:40:55.565495 29097 replica.cpp:708] Persisted action TRUNCATE at position 4
I1103 01:40:55.566788 29097 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1103 01:40:55.583937 29101 slave.cpp:1483] Will retry registration in 26.127711ms if necessary
I1103 01:40:55.584123 29112 master.cpp:5139] Ignoring register agent message from slave(150)@172.17.0.2:58302 (maintenance-host-2) as admission is already in progress
I1103 01:40:55.609695 29097 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 42.905697ms
I1103 01:40:55.609860 29097 leveldb.cpp:399] Deleting ~2 keys from leveldb took 96623ns
I1103 01:40:55.609899 29097 replica.cpp:708] Persisted action TRUNCATE at position 4
I1103 01:40:55.611063 29106 log.cpp:577] Attempting to append 513 bytes to the log
I1103 01:40:55.611229 29097 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I1103 01:40:55.611498 29100 slave.cpp:1483] Will retry registration in 85.55417ms if necessary
I1103 01:40:55.612069 29105 master.cpp:5139] Ignoring register agent message from slave(150)@172.17.0.2:58302 (maintenance-host-2) as admission is already in progress
I1103 01:40:55.612313 29100 replica.cpp:537] Replica received write request for position 5 from __req_res__(2518)@172.17.0.2:58302
I1103 01:40:55.657845 29100 leveldb.cpp:341] Persisting action (532 bytes) to leveldb took 45.517822ms
I1103 01:40:55.657938 29100 replica.cpp:708] Persisted action APPEND at position 5
I1103 01:40:55.658681 29103 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I1103 01:40:55.698050 29112 slave.cpp:1483] Will retry registration in 100.32384ms if necessary
I1103 01:40:55.698319 29101 master.cpp:5139] Ignoring register agent message from slave(150)@172.17.0.2:58302 (maintenance-host-2) as admission is already in progress
I1103 01:40:55.705086 29103 leveldb.cpp:341] Persisting action (534 bytes) to leveldb took 46.384557ms
I1103 01:40:55.705157 29103 replica.cpp:708] Persisted action APPEND at position 5
I1103 01:40:55.707480 29098 registrar.cpp:506] Successfully updated the registry in 173824us
I1103 01:40:55.707741 29100 log.cpp:596] Attempting to truncate the log to 5
I1103 01:40:55.708029 29110 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I1103 01:40:55.708501 29097 slave.cpp:4251] Received ping from slave-observer(151)@172.17.0.2:58302
I1103 01:40:55.708528 29112 master.cpp:5222] Registered agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 at slave(150)@172.17.0.2:58302 (maintenance-host-2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1103 01:40:55.708796 29097 slave.cpp:1115] Registered with master master@172.17.0.2:58302; given agent ID 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1
I1103 01:40:55.708822 29097 fetcher.cpp:86] Clearing fetcher cache
I1103 01:40:55.708889 29103 replica.cpp:537] Replica received write request for position 6 from __req_res__(2519)@172.17.0.2:58302
I1103 01:40:55.708935 29100 hierarchical.cpp:485] Added agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 (maintenance-host-2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I1103 01:40:55.709065 29105 status_update_manager.cpp:184] Resuming sending status updates
I1103 01:40:55.709139 29100 hierarchical.cpp:1694] No allocations performed
I1103 01:40:55.709204 29100 hierarchical.cpp:1309] Performed allocation for agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 in 229932ns
I1103 01:40:55.709316 29097 slave.cpp:1138] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffersFilters_xHOK8S/meta/slaves/3167a687-904b-4b57-bc0f-91b67dc7e41d-S1/slave.info'
I1103 01:40:55.709650 29097 slave.cpp:1175] Forwarding total oversubscribed resources {}
I1103 01:40:55.709787 29097 master.cpp:5621] Received update of agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 at slave(150)@172.17.0.2:58302 (maintenance-host-2) with total oversubscribed resources {}
I1103 01:40:55.710000 29108 hierarchical.cpp:555] Agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 (maintenance-host-2) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {})
I1103 01:40:55.710144 29108 hierarchical.cpp:1694] No allocations performed
I1103 01:40:55.710199 29108 hierarchical.cpp:1309] Performed allocation for agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 in 157491ns
I1103 01:40:55.711632 29105 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'
I1103 01:40:55.712441 29111 http.cpp:391] HTTP POST for /master/maintenance/schedule from 172.17.0.2:45755
I1103 01:40:55.713325 29110 registrar.cpp:461] Applied 1 operations in 165622ns; attempting to update the registry
I1103 01:40:55.756808 29103 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 47.899715ms
I1103 01:40:55.756888 29103 replica.cpp:708] Persisted action TRUNCATE at position 6
I1103 01:40:55.758083 29103 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I1103 01:40:55.807322 29103 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 49.23844ms
I1103 01:40:55.807481 29103 leveldb.cpp:399] Deleting ~2 keys from leveldb took 90240ns
I1103 01:40:55.807517 29103 replica.cpp:708] Persisted action TRUNCATE at position 6
I1103 01:40:55.808619 29101 log.cpp:577] Attempting to append 732 bytes to the log
I1103 01:40:55.808737 29097 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I1103 01:40:55.809571 29109 replica.cpp:537] Replica received write request for position 7 from __req_res__(2520)@172.17.0.2:58302
I1103 01:40:55.810784 29098 hierarchical.cpp:1694] No allocations performed
I1103 01:40:55.810860 29098 hierarchical.cpp:1286] Performed allocation for 2 agents in 282405ns
I1103 01:40:55.870082 29109 leveldb.cpp:341] Persisting action (751 bytes) to leveldb took 60.489282ms
I1103 01:40:55.870177 29109 replica.cpp:708] Persisted action APPEND at position 7
I1103 01:40:55.871421 29109 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I1103 01:40:55.976619 29109 leveldb.cpp:341] Persisting action (753 bytes) to leveldb took 105.201942ms
I1103 01:40:55.976718 29109 replica.cpp:708] Persisted action APPEND at position 7
I1103 01:40:55.979708 29109 registrar.cpp:506] Successfully updated the registry in 266.301952ms
I1103 01:40:55.979878 29103 log.cpp:596] Attempting to truncate the log to 7
I1103 01:40:55.979991 29106 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I1103 01:40:55.980255 29112 master.cpp:5683] Updating unavailability of agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S0 at slave(149)@172.17.0.2:58302 (maintenance-host), starting at 2449.0107815972weeks
I1103 01:40:55.980481 29112 master.cpp:5683] Updating unavailability of agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 at slave(150)@172.17.0.2:58302 (maintenance-host-2), starting at 2449.0107815972weeks
I1103 01:40:55.980818 29097 replica.cpp:537] Replica received write request for position 8 from __req_res__(2521)@172.17.0.2:58302
I1103 01:40:55.981092 29103 hierarchical.cpp:1694] No allocations performed
I1103 01:40:55.981150 29103 hierarchical.cpp:1309] Performed allocation for agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S0 in 199516ns
I1103 01:40:55.981317 29103 hierarchical.cpp:1694] No allocations performed
I1103 01:40:55.981367 29103 hierarchical.cpp:1309] Performed allocation for agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 in 144365ns
I1103 01:40:55.982864 29079 scheduler.cpp:176] Version: 1.2.0
I1103 01:40:55.983412 29112 scheduler.cpp:469] New master detected at master@172.17.0.2:58302
I1103 01:40:55.983438 29112 scheduler.cpp:478] Waiting for 0ns before initiating a re-(connection) attempt with the master
I1103 01:40:55.984688 29098 scheduler.cpp:353] Connected with the master at http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:55.985872 29109 scheduler.cpp:235] Sending SUBSCRIBE call to http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:55.986587 29099 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1103 01:40:55.987659 29108 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.2:45771
I1103 01:40:55.988019 29108 master.cpp:2329] Received subscription request for HTTP framework 'default'
I1103 01:40:55.988088 29108 master.cpp:2069] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1103 01:40:55.988632 29099 master.cpp:2427] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1103 01:40:55.989305 29105 hierarchical.cpp:275] Added framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:55.989459 29099 master.hpp:2161] Sending heartbeat to 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:55.990151 29108 scheduler.cpp:675] Enqueuing event SUBSCRIBED received from http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:55.990648 29108 scheduler.cpp:675] Enqueuing event HEARTBEAT received from http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:55.991364 29105 hierarchical.cpp:1286] Performed allocation for 2 agents in 2.044414ms
I1103 01:40:55.992657 29100 master.cpp:6571] Sending 2 offers to framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (default)
I1103 01:40:55.994067 29100 master.cpp:6661] Sending 2 inverse offers to framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (default)
I1103 01:40:55.995620 29101 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:55.996536 29101 scheduler.cpp:675] Enqueuing event INVERSE_OFFERS received from http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:56.060276 29097 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 79.452553ms
I1103 01:40:56.060681 29097 replica.cpp:708] Persisted action TRUNCATE at position 8
I1103 01:40:56.061957 29102 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I1103 01:40:56.118927 29102 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 56.925572ms
I1103 01:40:56.119022 29102 leveldb.cpp:399] Deleting ~2 keys from leveldb took 51598ns
I1103 01:40:56.119046 29102 replica.cpp:708] Persisted action TRUNCATE at position 8
I1103 01:40:56.120704 29099 hierarchical.cpp:1694] No allocations performed
I1103 01:40:56.121099 29099 hierarchical.cpp:1286] Performed allocation for 2 agents in 610047ns
I1103 01:40:56.122525 29097 scheduler.cpp:235] Sending ACCEPT call to http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:56.123138 29097 scheduler.cpp:235] Sending ACCEPT call to http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:56.124460 29098 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1103 01:40:56.124822 29098 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1103 01:40:56.126590 29098 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.2:45772
I1103 01:40:56.127682 29098 master.cpp:3581] Processing ACCEPT call for offers: [ 3167a687-904b-4b57-bc0f-91b67dc7e41d-O0 ] on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S0 at slave(149)@172.17.0.2:58302 (maintenance-host) for framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (default)
I1103 01:40:56.127856 29098 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task 19b41cf9-ba0f-472e-8cae-5ea70574b1b4
I1103 01:40:56.128916 29098 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.2:45772
I1103 01:40:56.129622 29098 master.cpp:3581] Processing ACCEPT call for offers: [ 3167a687-904b-4b57-bc0f-91b67dc7e41d-O1 ] on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 at slave(150)@172.17.0.2:58302 (maintenance-host-2) for framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (default)
I1103 01:40:56.129730 29098 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task 846a812b-b562-4782-b0fd-9e760a52306e
W1103 01:40:56.131443 29098 validation.cpp:920] Executor 'executor-1' for task '19b41cf9-ba0f-472e-8cae-5ea70574b1b4' uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1103 01:40:56.131489 29098 validation.cpp:932] Executor 'executor-1' for task '19b41cf9-ba0f-472e-8cae-5ea70574b1b4' uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1103 01:40:56.132071 29098 master.cpp:8334] Adding task 19b41cf9-ba0f-472e-8cae-5ea70574b1b4 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S0 (maintenance-host)
I1103 01:40:56.132442 29098 master.cpp:4230] Launching task 19b41cf9-ba0f-472e-8cae-5ea70574b1b4 of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S0 at slave(149)@172.17.0.2:58302 (maintenance-host)
W1103 01:40:56.133824 29098 validation.cpp:920] Executor 'executor-2' for task '846a812b-b562-4782-b0fd-9e760a52306e' uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1103 01:40:56.133858 29098 validation.cpp:932] Executor 'executor-2' for task '846a812b-b562-4782-b0fd-9e760a52306e' uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1103 01:40:56.134299 29097 slave.cpp:1547] Got assigned task '19b41cf9-ba0f-472e-8cae-5ea70574b1b4' for framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.134332 29098 master.cpp:8334] Adding task 846a812b-b562-4782-b0fd-9e760a52306e with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 (maintenance-host-2)
I1103 01:40:56.134572 29098 master.cpp:4230] Launching task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 at slave(150)@172.17.0.2:58302 (maintenance-host-2)
I1103 01:40:56.135084 29097 slave.cpp:1709] Launching task '19b41cf9-ba0f-472e-8cae-5ea70574b1b4' for framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.135145 29098 slave.cpp:1547] Got assigned task '846a812b-b562-4782-b0fd-9e760a52306e' for framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.135660 29098 slave.cpp:1709] Launching task '846a812b-b562-4782-b0fd-9e760a52306e' for framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.135871 29097 paths.cpp:536] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffersFilters_B4KX1H/slaves/3167a687-904b-4b57-bc0f-91b67dc7e41d-S0/frameworks/3167a687-904b-4b57-bc0f-91b67dc7e41d-0000/executors/executor-1/runs/fbb81e53-b134-4196-9385-791612794545' to user 'mesos'
I1103 01:40:56.138550 29098 paths.cpp:536] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffersFilters_xHOK8S/slaves/3167a687-904b-4b57-bc0f-91b67dc7e41d-S1/frameworks/3167a687-904b-4b57-bc0f-91b67dc7e41d-0000/executors/executor-2/runs/c7d8cc2e-b173-4dbe-b5ae-e45402c1cdf4' to user 'mesos'
I1103 01:40:56.144282 29098 slave.cpp:6307] Launching executor 'executor-2' of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 with resources {} in work directory '/tmp/MasterMaintenanceTest_InverseOffersFilters_xHOK8S/slaves/3167a687-904b-4b57-bc0f-91b67dc7e41d-S1/frameworks/3167a687-904b-4b57-bc0f-91b67dc7e41d-0000/executors/executor-2/runs/c7d8cc2e-b173-4dbe-b5ae-e45402c1cdf4'
I1103 01:40:56.144282 29097 slave.cpp:6307] Launching executor 'executor-1' of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 with resources {} in work directory '/tmp/MasterMaintenanceTest_InverseOffersFilters_B4KX1H/slaves/3167a687-904b-4b57-bc0f-91b67dc7e41d-S0/frameworks/3167a687-904b-4b57-bc0f-91b67dc7e41d-0000/executors/executor-1/runs/fbb81e53-b134-4196-9385-791612794545'
F1103 01:40:56.144891 29097 owned.hpp:110] Check failed: 'get()' Must be non NULL
*** Check failure stack trace: ***
I1103 01:40:56.147233 29098 exec.cpp:162] Version: 1.2.0
I1103 01:40:56.147598 29112 exec.cpp:212] Executor started at: executor(60)@172.17.0.2:58302 with pid 29079
I1103 01:40:56.147956 29098 slave.cpp:2031] Queued task '846a812b-b562-4782-b0fd-9e760a52306e' for executor 'executor-2' of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.148043 29098 slave.cpp:868] Successfully attached file '/tmp/MasterMaintenanceTest_InverseOffersFilters_xHOK8S/slaves/3167a687-904b-4b57-bc0f-91b67dc7e41d-S1/frameworks/3167a687-904b-4b57-bc0f-91b67dc7e41d-0000/executors/executor-2/runs/c7d8cc2e-b173-4dbe-b5ae-e45402c1cdf4'
I1103 01:40:56.148277 29098 slave.cpp:3305] Got registration for executor 'executor-2' of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 from executor(60)@172.17.0.2:58302
I1103 01:40:56.148833 29105 exec.cpp:237] Executor registered on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1
I1103 01:40:56.148895 29105 exec.cpp:249] Executor::registered took 29894ns
I1103 01:40:56.149114 29098 slave.cpp:2247] Sending queued task '846a812b-b562-4782-b0fd-9e760a52306e' to executor 'executor-2' of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 at executor(60)@172.17.0.2:58302
I1103 01:40:56.149495 29106 exec.cpp:324] Executor asked to run task '846a812b-b562-4782-b0fd-9e760a52306e'
I1103 01:40:56.149608 29106 exec.cpp:333] Executor::launchTask took 98162ns
I1103 01:40:56.149705 29106 exec.cpp:550] Executor sending status update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.149987 29106 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 from executor(60)@172.17.0.2:58302
I1103 01:40:56.150545 29098 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.150606 29098 status_update_manager.cpp:500] Creating StatusUpdate stream for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.151116 29098 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 to the agent
    @     0x2b56852902fd  google::LogMessage::Fail()
I1103 01:40:56.151470 29101 slave.cpp:4169] Forwarding the update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 to master@172.17.0.2:58302
I1103 01:40:56.151757 29101 slave.cpp:4063] Status update manager successfully handled status update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.151860 29101 slave.cpp:4079] Sending acknowledgement for status update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 to executor(60)@172.17.0.2:58302
I1103 01:40:56.151954 29111 master.cpp:5757] Status update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 from agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1 at slave(150)@172.17.0.2:58302 (maintenance-host-2)
I1103 01:40:56.152041 29111 master.cpp:5819] Forwarding status update TASK_RUNNING (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.152123 29100 exec.cpp:373] Executor received status update acknowledgement 1fe05bba-4be1-4c4e-a847-e4e3289b2d26 for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.152546 29111 master.cpp:7712] Updating the state of task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1103 01:40:56.153616 29103 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.2:58302/master/api/v1/scheduler
    @     0x2b568528f6de  google::LogMessage::SendToLog()
I1103 01:40:56.164335 29109 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.2:58302/master/api/v1/scheduler
I1103 01:40:56.165396 29109 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1103 01:40:56.167019 29109 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.2:45772
I1103 01:40:56.167212 29109 master.cpp:4867] Processing ACKNOWLEDGE call 1fe05bba-4be1-4c4e-a847-e4e3289b2d26 for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000 (default) on agent 3167a687-904b-4b57-bc0f-91b67dc7e41d-S1
I1103 01:40:56.168145 29110 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
I1103 01:40:56.168666 29110 slave.cpp:3022] Status update manager successfully handled status update acknowledgement (UUID: 1fe05bba-4be1-4c4e-a847-e4e3289b2d26) for task 846a812b-b562-4782-b0fd-9e760a52306e of framework 3167a687-904b-4b57-bc0f-91b67dc7e41d-0000
    @     0x2b568528ffbd  google::LogMessage::Flush()
    @     0x2b5685293438  google::LogMessageFatal::~LogMessageFatal()
    @           0xcb1fc4  google::CheckNotNull<>()
    @           0xc93ba6  process::Owned<>::operator->()
    @           0xc9100c  mesos::internal::tests::TestContainerizer::_launch()
    @           0xc98182  _ZN7testing8internal12InvokeHelperIN7process6FutureIbEESt5tupleIJRKN5mesos11ContainerIDERK6OptionINS6_8TaskInfoEERKNS6_12ExecutorInfoERKSsRKSA_ISsERKNS6_7SlaveIDERKSt3mapISsSsSt4lessISsESaISt4pairISI_SsEEEbEEE12InvokeMethodINS6_8internal5tests17TestContainerizerEMS14_FS4_S9_SE_SH_SJ_SM_SP_SY_bEEES4_PT_T0_RKSZ_
    @           0xc98014  _ZNK7testing8internal18InvokeMethodActionIN5mesos8internal5tests17TestContainerizerEMS5_FN7process6FutureIbEERKNS2_11ContainerIDERK6OptionINS2_8TaskInfoEERKNS2_12ExecutorInfoERKSsRKSC_ISsERKNS2_7SlaveIDERKSt3mapISsSsSt4lessISsESaISt4pairISK_SsEEEbEE7PerformIS8_St5tupleIJSB_SG_SJ_SL_SO_SR_S10_bEEEET_RKT0_
    @           0xc97fb6  _ZN7testing17PolymorphicActionINS_8internal18InvokeMethodActionIN5mesos8internal5tests17TestContainerizerEMS6_FN7process6FutureIbEERKNS3_11ContainerIDERK6OptionINS3_8TaskInfoEERKNS3_12ExecutorInfoERKSsRKSD_ISsERKNS3_7SlaveIDERKSt3mapISsSsSt4lessISsESaISt4pairISL_SsEEEbEEEE15MonomorphicImplIFS9_SC_SH_SK_SM_SP_SS_S11_bEE7PerformERKSt5tupleIJSC_SH_SK_SM_SP_SS_S11_bEE
    @           0xb51d58  _ZNK7testing6ActionIFN7process6FutureIbEERKN5mesos11ContainerIDERK6OptionINS4_8TaskInfoEERKNS4_12ExecutorInfoERKSsRKS8_ISsERKNS4_7SlaveIDERKSt3mapISsSsSt4lessISsESaISt4pairISG_SsEEEbEE7PerformERKSt5tupleIJS7_SC_SF_SH_SK_SN_SW_bEE
    @           0xb51b6a  testing::internal::ActionResultHolder<>::PerformAction<>()
    @           0xb4ca1c  testing::internal::FunctionMockerBase<>::UntypedPerformAction()
    @          0x1daa619  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0xb4995b  _ZN7testing8internal18FunctionMockerBaseIFN7process6FutureIbEERKN5mesos11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKS9_ISsERKNS5_7SlaveIDERKSt3mapISsSsSt4lessISsESaISt4pairISH_SsEEEbEE10InvokeWithERKSt5tupleIJS8_SD_SG_SI_SL_SO_SX_bEE
    @           0xb49916  testing::internal::FunctionMocker<>::Invoke()
    @           0xc9712f  mesos::internal::tests::TestContainerizer::launch()
    @     0x2b5684298b9b  mesos::internal::slave::Framework::launchExecutor()
    @     0x2b568429607e  mesos::internal::slave::Slave::_run()
    @     0x2b56843265a0  _ZZN7process8dispatchIN5mesos8internal5slave5SlaveERKNS_6FutureIbEERKNS1_13FrameworkInfoERKNS1_12ExecutorInfoERK6OptionINS1_8TaskInfoEERKSF_INS1_13TaskGroupInfoEES6_S9_SC_SH_SL_EEvRKNS_3PIDIT_EEMSP_FvT0_T1_T2_T3_T4_ET5_T6_T7_T8_T9_ENKUlPNS_11ProcessBaseEE_clES16_
    @     0x2b5684326062  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveERKNS0_6FutureIbEERKNS5_13FrameworkInfoERKNS5_12ExecutorInfoERK6OptionINS5_8TaskInfoEERKSJ_INS5_13TaskGroupInfoEESA_SD_SG_SL_SP_EEvRKNS0_3PIDIT_EEMST_FvT0_T1_T2_T3_T4_ET5_T6_T7_T8_T9_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2b56851b2dd8  std::function<>::operator()()
    @     0x2b5685199f74  process::ProcessBase::visit()
    @     0x2b56851f84ee  process::DispatchEvent::visit()
    @           0x8dabe1  process::ProcessBase::serve()
    @     0x2b5685197c64  process::ProcessManager::resume()
    @     0x2b56851a28cc  process::ProcessManager::init_threads()::$_0::operator()()
    @     0x2b56851a27d5  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvE3$_0vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x2b56851a27a5  std::_Bind_simple<>::operator()()
    @     0x2b56851a277c  std::thread::_Impl<>::_M_run()
    @     0x2b5686d10a60  (unknown)
    @     0x2b5687487184  start_thread
{noformat}

I suspect the following line is being performed concurrently to trip the crash in CI:

https://github.com/apache/mesos/blob/44242c058158727ce013bd51764368f5e120ee75/src/tests/containerizer.cpp#L131"	MESOS	Resolved	3	1	2732	tech-debt
13118644	Strip (Offer|Resource).allocation_info for non-MULTI_ROLE schedulers.	"In support of MULTI_ROLE capable frameworks, a Resource.allocation_info field was added and the Resource math of the Mesos library was updated to check for matching allocation_info when checking for (in)equality, addability, subtractability, containment, etc. To compensate for these changes, the demo frameworks of Mesos were updated to set the allocation_info for Resource objects during the ""matching phase"" in which offers' resources are evaluated in order for the framework to launch tasks. The Mesos demo frameworks NEEDED to be updated because the Resource algebra within Mesos now depended on matching allocation_info fields of Resource objects when executing algebraic operations. See https://github.com/apache/mesos/commit/c20744a9976b5e83698e9c6062218abb4d2e6b25#diff-298cc6a77862b7ff3422cd06c215ef28R91 .

This poses a unique problem for **external** libraries that both aim to support various frameworks, some that DO and some that DO NOT opt-in to the MULTI_ROLE capability; specifically those external libraries that implement Resource algebra that's consistent with what Mesos implements internally. One such example of a library is mesos-go, though there are undoubtedly others. The problem can be explained via this scenario: 
{quote}
Flo's mesos-go framework is running well, it doesn't opt-in to MULTI_ROLE because it doesn't need multiple roles. His framework runs on a version of Mesos that existed prior to integration of MULTI_ROLE support. His DC operator upgrades the mesos cluster to the latest version. Flo rebuilds his framework on the latest version of mesos-go and re-launches it on the cluster. He observes that his framework receives offers, but rejects ALL of them. Digging into the code he realizes that Mesos is injecting allocation_info into Resource objects being offered to his framework, and mesos-go considers allocation_info when comparing Resource objects (because it's MULTI_ROLE compatible now), but his framework doesn't take this into consideration when preparing its own Resource objects prior to the ""resource matching phase"". The consequence is that Flo's framework is trying to match against Resources that will never align because his framework isn't setting an allocation_info that might possibly match the allocation_info that Mesos is always injecting - regardless of the MULTI_ROLE capability (or lack thereof in this case) of his framework.
{quote}

If Mesos were to strip the allocation_info from Resource objects, prior to offering them to non-multi-role frameworks, then the problem illustrated above would go away.
"	MESOS	Resolved	3	1	2732	mesosphere
12843918	0.22.x scheduler driver drops 0.23.x reconciliation status updates due to missing StatusUpdate.uuid.	"In the process of fixing MESOS-2940, we accidentally introduced a non-backwards compatible change:

--> StatusUpdate.uuid was required in 0.22.x and was always set.
--> StatusUpdate.uuid is optional in 0.23.x and the master is not setting it for master-generated updates.

In 0.22.x, the scheduler driver ignores the 'uuid' for master/driver generated updates already. I'd suggest the following fix:

# In 0.23.x, rather than not setting StatusUpdate.uuid, set it to an empty string.
# In 0.23.x, ensure the scheduler driver also ignores empty StatusUpdate.uuids.
# In 0.24.x, stop setting StatusUpdate.uuid."	MESOS	Resolved	1	1	2732	twitter
13010144	ContentType/AgentAPITest.NestedContainerLaunch/1 is flaky	"{{ContentType/AgentAPITest.NestedContainerLaunch/1}} is flaky, saw this fail in ASF CI (https://builds.apache.org/job/mesos-reviewbot/15545/)

{code}
../../src/tests/api_tests.cpp:3552: Failure
(wait).failure(): Unexpected response status 404 Not Found
{code}"	MESOS	Resolved	3	1	2732	flaky-test
12667627	"Master should properly consolidate ""slaves"" and ""deactivated"" maps"	"Currently, the master keeps track of active slaves with ""slaves"" map and deactivated slaves with ""deactivated"" map. While the former is indexed on SlaveID the latter is index on pid. This could lead to inconsistencies regarding the state of the slaves.

We have seen this in production at Twitter. 

Slave was given id 201308072143-2082809866-5050-35234-5186 at 16:35:59. After ~22 minutes master removed the slave, presumably because of network partition. The slave received shutdown and restarted at 17:08:01. It then registered with the master at 17:08:31 and got a new id 201308072143-2082809866-5050-35234-5193. But then it was immediately considered ""disconnected"" (not sure why) by the master and removed. When the slave came back up it got yet another pid 201308072143-2082809866-5050-35234-5194.

The surprising bit is that at 17:08:32 it got another re-register message (probably backed up somewhere in the network?) from the same slave with the old pid 201308072143-2082809866-5050-35234-5186. Since this id doesn't exist in the master's slaves map, master thought it was a new slave and added it. When the slave got the ack for this re-registration message it committed suicide (as expected) because the id it received was un-expected. Now the master removed the slave with id 201308072143-2082809866-5050-35234-5186 from its slaves map based on the pid. Note that was completely arbitrary, because the master could just as well have removed the slave id 201308072143-2082809866-5050-35234-5194 from its map. This is because the master just loops through all entries in ""slaves"" and picks the first one that matches the pid.

At this point the slave's pid was added to ""deactivated"" but there exists a slave (201308072143-2082809866-5050-35234-5194) in the slaves map with the same pid!
When it eventually received a status update from the slave, the master crashed (as expected) because the message was from a slave whose pid is in ""deactivated"" but present in ""slaves""."	MESOS	Resolved	3	1	2732	twitter
13171249	Improve performance of json parsing by avoiding conversion cost.	"Stout's JSON parsing function parses into picojson::value followed by converting to JSON::Value. This carries a significant conversion cost (almost doubling the parse time).

We can leverage picojson's parsing context to avoid this cost."	MESOS	Resolved	3	4	2732	performance
12779984	Typo in Committer's Guide (article 5, line 2)	"http://mesos.apache.org/documentation/latest/committers-guide/

""..explicit in the commit message and (b) include the link to the review and  use 72 character.."" 
 ""  "" should be changed to "" (c) ""
 "	MESOS	Resolved	5	20	2732	documentation, newbie
12834896	Log framework capabilities in the master.	"Now that {{Capabilities}} has been added to FrameworkInfo, we should log these in the master when a framework (re-)registers (i.e. which capabilities are enabled and disabled). This would make debugging easier for framework developers.

Ideally, folding in the old {{checkpoint}} capability and logging that as well. In the past, the fact that {{checkpoint}} defaults to false has tripped up a lot of developers."	MESOS	Resolved	4	4	2732	twitter
12779208	Improve support for streaming HTTP Responses in libprocess.	"Currently libprocess' HTTP::Response supports a PIPE construct for doing streaming responses:

{code}
struct Response
{
  ...

  // Either provide a ""body"", an absolute ""path"" to a file, or a
  // ""pipe"" for streaming a response. Distinguish between the cases
  // using 'type' below.
  //
  // BODY: Uses 'body' as the body of the response. These may be
  // encoded using gzip for efficiency, if 'Content-Encoding' is not
  // already specified.
  //
  // PATH: Attempts to perform a 'sendfile' operation on the file
  // found at 'path'.
  //
  // PIPE: Splices data from 'pipe' using 'Transfer-Encoding=chunked'.
  // Note that the read end of the pipe will be closed by libprocess
  // either after the write end has been closed or if the socket the
  // data is being spliced to has been closed (i.e., nobody is
  // listening any longer). This can cause writes to the pipe to
  // generate a SIGPIPE (which will terminate your program unless you
  // explicitly ignore them or handle them).
  //
  // In all cases (BODY, PATH, PIPE), you are expected to properly
  // specify the 'Content-Type' header, but the 'Content-Length' and
  // or 'Transfer-Encoding' headers will be filled in for you.
  enum {
    NONE,
    BODY,
    PATH,
    PIPE
  } type;

  ...
};
{code}

This interface is too low level and difficult to program against:

* Connection closure is signaled with SIGPIPE, which is difficult for callers to deal with (must suppress SIGPIPE locally or globally in order to get EPIPE instead).
* Pipes are generally for inter-process communication, and the pipe has finite size. With a blocking pipe the caller must deal with blocking when the pipe's buffer limit is exceeded. With a non-blocking pipe, the caller must deal with retrying the write.

We'll want to consider a few use cases:
# Sending an HTTP::Response with streaming data.
# Making a request with http::get and http::post in which the data is returned in a streaming manner.
# Making a request in which the request content is streaming.

This ticket will focus on 1 as it is required for the HTTP API."	MESOS	Resolved	3	4	2732	twitter
12718042	Remove 'offer_id' field from LaunchTasksMessage.	"The scheduler driver has been using the new 'offer_ids' field since 0.18.0 so we should follow up and remove the deprecated 'offer_id' field. This will simplify the master logic and the driver logic.

[1] https://github.com/apache/mesos/blob/0.18.0/src/sched/sched.cpp#L887"	MESOS	Resolved	3	3	2732	tech-debt
12965871	libevent builds may prevent new connections	"When using an SSL-enabled build of Mesos in combination with SSL-downgrading support, any connection that does not actually transmit data will hang the runnable (e.g. master).

For reproducing the issue (on any platform)...

Spin up a master with enabled SSL-downgrading:
{noformat}
$ export SSL_ENABLED=true
$ export SSL_SUPPORT_DOWNGRADE=true
$ export SSL_KEY_FILE=/path/to/your/foo.key
$ export SSL_CERT_FILE=/path/to/your/foo.crt
$ export SSL_CA_FILE=/path/to/your/ca.crt
$ ./bin/mesos-master.sh --work_dir=/tmp/foo
{noformat}

Create some artificial HTTP request load for quickly spotting the problem in both, the master logs as well as the output of CURL itself:
{noformat}
$ while true; do sleep 0.1; echo $( date +"">%H:%M:%S.%3N""; curl -s -k -A ""SSL Debug"" http://localhost:5050/master/slaves; echo ;date +""<%H:%M:%S.%3N""; echo); done
{noformat}

Now create a connection to the master that does not transmit any data:
{noformat}
$ telnet localhost 5050
{noformat}

You should now see the CURL requests hanging, the master stops responding to new connections. This will persist until either some data is transmitted via the above telnet connection or it is closed.

This problem has initially been observed when running Mesos on an AWS cluster with enabled load-balancer (which uses an idle, persistent connection) for the master node. Such connection does naturally not transmit any data as long as there are no external requests routed via the load-balancer. AWS allows setting up a timeout for those connections and in our test environment, this duration was set to 60 seconds and hence we were seeing our master getting repetitively unresponsive for 60 seconds, then getting ""unstuck"" for a brief period until it got stuck again.
"	MESOS	Resolved	1	1	2732	mesosphere, security, ssl
12781595	Add ability to distinguish slave removals metrics by reason.	"Currently we only expose a single removal metric ({{""master/slave_removals""}}) which makes it difficult to distinguish between removal reasons in the alerting.

Currently, a slave can be removed for the following reasons:

# Health checks failed.
# Slave unregistered.
# Slave was replaced by a new slave (on the same endpoint).

In the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures."	MESOS	Resolved	3	4	2732	twitter
12720503	Add operational documentation for running HA masters.	"Now that the master has replicated state on the disk, we should add documentation that guides operators for common maintenance work:

* Swapping a master in the ensemble.
* Growing the master ensemble.
* Shrinking the master ensemble.

This would help craft similar documentation for users of the replicated log."	MESOS	Resolved	3	20	2732	twitter
13300036	OpenSSLSocketImpl on Windows with 'support_downgrade' is incorrectly polling for read readiness.	"OpenSSLSocket is currently using the zero byte read trick on Windows to poll for read readiness when peaking at the data to determine whether the incoming connection is performing an SSL handshake. However, io::read is designed to provide consistent semantics for a zero byte read across posix and windows, which is to return immediately.

To fix this, we can either:

(1) Have different semantics for zero byte io::read on posix / windows, where we just let it fall through to the system calls. This might be confusing for users, but it's unlikely that a caller would perform a zero byte read in typical code so the confusion is probably avoided.

(2) Implement io::poll for reads on windows. This would make the caller code consistent and is probably less confusing to users."	MESOS	Resolved	3	1	2732	windows
13242238	Expose quota consumption in /roles endpoint.	As part of exposing quota consumption to users and displaying quota consumption in the ui, we will need to add it to the /roles endpoint (which is currently what the ui uses for the roles table).	MESOS	Resolved	3	3	2732	resource-management
12782431	Performance issue in the master when a large number of slaves are registering.	"For large clusters, when a lot of slaves are registering, the master gets backlogged processing registration requests. {{perf}} revealed the following:

{code}
Events: 14K cycles
 25.44%  libmesos-0.22.0-x.so  [.] mesos::internal::master::Master::registerSlave(process::UPID const&, mesos::SlaveInfo const&, std::vector<mesos::Resource, std::allocator<mesos::Resource> > cons
 11.18%  libmesos-0.22.0-x.so  [.] pipecb
  5.88%  libc-2.5.so             [.] malloc_consolidate
  5.33%  libc-2.5.so             [.] _int_free
  5.25%  libc-2.5.so             [.] malloc
  5.23%  libc-2.5.so             [.] _int_malloc
  4.11%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)
  3.22%  libmesos-0.22.0-x.so  [.] mesos::Resource::SharedDtor()
  3.10%  [kernel]                [k] _raw_spin_lock
  1.97%  libmesos-0.22.0-x.so  [.] mesos::Attribute::SharedDtor()
  1.28%  libc-2.5.so             [.] memcmp
  1.08%  libc-2.5.so             [.] free
{code}

This is likely because we loop over all the slaves for each registration:

{code}
void Master::registerSlave(
    const UPID& from,
    const SlaveInfo& slaveInfo,
    const vector<Resource>& checkpointedResources,
    const string& version)
{
  // ...

  // Check if this slave is already registered (because it retries).
  foreachvalue (Slave* slave, slaves.registered) {
    if (slave->pid == from) {
      // ...
    }
  }
  // ...
}
{code}"	MESOS	Resolved	3	4	2732	scalability, twitter
13185403	Improve sorting performance in the DRF sorter.	"The sorting performance of the DRF sorter is negatively affected by the use of hashmaps introduced originally in MESOS-4964.

Storing a cache-friendlier data structure and avoiding hashing overhead would improve the sort performance."	MESOS	Resolved	2	4	2732	performance
12787115	Document tips, best practices, guidelines for doing code reviews.	"We currently have a [""Committers Guide""|https://github.com/apache/mesos/blob/0.22.0/docs/committers-guide.md], however most of this information is relevant to all contributors looking to be participating in the code review process.

I'm proposing we extract much of this information into a more general ""Code Reviewing"" document, and include additional tips, best practices, lessons learned from members of the community.

This would be a great pre-requisite for on-boarding more committers and adding [MAINTAINERS|http://mail-archives.apache.org/mod_mbox/mesos-dev/201502.mbox/%3CCA+8RcoReugMVqoOpsnB8WGYBELa5fHwPA=J=YHJE22iwZvsbeQ@mail.gmail.com%3E].

The committers guide can be more specific to our expectations of committers, so we may want to make this into a ""committership"" document to help set expectations for contributors looking to become committers."	MESOS	Resolved	3	4	2732	tech-debt
12675143	Capture memory usage statistics before OOM	Since 0.14.2-rc1 we collect memory usage statistics after we get OOM notifications. There is concern that these stats might be stale. If thats the case it would be nice to get a snapshot of the stats based on memory threshold notifications.	MESOS	Resolved	3	4	2732	twitter
12777027	Rate limit slaves removals during master recovery.	"Much like we rate limit slave removals in the common path (MESOS-1148), we need to rate limit slave removals that occur during master recovery. When a master recovers and is using a strict registry, slaves that do not re-register within a timeout will be removed.

Currently there is a safeguard in place to abort when too many slaves have not re-registered. However, in the case of a transient partition, we don't want to remove large sections of slaves without rate limiting."	MESOS	Resolved	3	4	2732	twitter
12527244	Executor resource monitoring and local reporting of usage stats	"Implement reporting of resource usage on executors and log them to a local log file (for now). The eventual usage of this will be to report these statistics to the Mesos master in order to build either or both a timeline for the webui and/or a top-like command-line interface. This improvement ticket is just for the local monitoring and log file reporting. A reporting system (to the master node) will be a later improvement ticket.

With the current version of Mesos, it is not possible to monitor individual tasks. Therefore the best this sort of system can do is monitor the usage of an individual executor and aggregate the resource usage of over the executor's tasks and resource allocations. If frameworks have a 1-to-1 relationship of a job to an executor, then the aggregate statistics will be more meaningful.

Reporting will be available for both lxc isolation and process-based isolation. For lxc isolation the task is easier because of the isolation facilities of lxc. Process-based isolation is more difficult as processes can become re-parented from the process tree of the executor (e.g. double fork). The session ID and the process group ID will likely still be the same as that of the executor except for the uncommon case of the process resetting both of those.

When usage statistics are eventually reported to the Mesos master, it may be possible to use them to oversubscribe slave nodes."	MESOS	Resolved	3	16	2732	monitoring
12849499	Provide a means to check http connection equality for streaming connections.	"If one uses an http::Pipe::Writer to stream a response, one cannot compare the writer with another to see if the connection has changed.

This is useful for example, in the master's http api when there is asynchronous disconnection logic. When we handle the disconnection, it's possible for the scheduler to have re-subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action."	MESOS	Resolved	3	3	2732	twitter
13224708	Quota is not enforced properly when subroles have reservations.	"Note: the discussion here concerns quota enforcement for top-level role, setting quota on sublevel role is not supported.

If a subrole directly makes a reservation, the accounting of `roleConsumedQuota` will be off:

https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/hierarchical.cpp#L1703-L1705

Specifically, in this formula:
`Consumed Quota = reservations + allocation - allocated reservations`

The `reservations` part does not account subrole's reservation to its ancestors. If a reservation is made directly for role ""a/b"", its reservation is accounted only for ""a/b"" but not for ""a"". Similarly, if a top role ( ""a"") reservation is refined to a subrole (""a/b""), the current code first subtracts the reservation from ""a"" and then track that under ""a/b"".

We should make it hierarchical-aware.

The ""allocation"" and ""allocated reservations"" are both tracked in the sorter where the hierarchical relationship is considered -- allocations are added hierarchically."	MESOS	Resolved	2	1	2732	resource-management
13244666	Master CPU high due to unexpected foreachkey behaviour in Master::__reregisterSlave.	"At https://github.com/apache/mesos/blob/9932550e9632e7fbb9a45b217793c7f508f57001/src/master/master.cpp#L7707-L7708

{code}
void Master::__reregisterSlave(
...
    foreachkey (FrameworkID frameworkId,
               slaves.unreachableTasks.at(slaveInfo.id())) {
        ...
        foreach (TaskID taskId,
                 slaves.unreachableTasks.at(slaveInfo.id()).get(frameworkId)) {
{code}

Our case is when network flapping, 3~4 agents reregister, then master would CPU full and could not process any requests during that period.

After change 
{code}
-    foreachkey (FrameworkID frameworkId,
-               slaves.unreachableTasks.at(slaveInfo.id())) {
+    foreach (FrameworkID frameworkId,
+               slaves.unreachableTasks.at(slaveInfo.id()).keys()) {
{code}

The problem gone."	MESOS	Resolved	2	1	2732	foundations
12920638	HTTPConnectionTest.ClosingResponse is flaky	"Output of the test:
{code}
[ RUN      ] HTTPConnectionTest.ClosingResponse
I1210 01:20:27.048532 26671 process.cpp:3077] Handling HTTP event for process '(22)' with path: '/(22)/get'
../../../3rdparty/libprocess/src/tests/http_tests.cpp:919: Failure
Actual function call count doesn't match EXPECT_CALL(*http.process, get(_))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
[  FAILED  ] HTTPConnectionTest.ClosingResponse (43 ms)
{code}"	MESOS	Resolved	4	1	2732	flaky, flaky-test, mesosphere, newbie, test
12661970	AllocatorTest/0.RoleTest is flaky.	"Looks like this test if flaky on Jenkins:

https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-In-Src-Set-JAVA_HOME/1309/consoleFull

[ RUN      ] AllocatorTest/0.RoleTest
I0805 20:31:40.486978 22444 master.cpp:230] Master started on 67.195.138.8:58067
I0805 20:31:40.487063 22443 sched.cpp:178] New master at master@67.195.138.8:58067
I0805 20:31:40.487066 22444 master.cpp:245] Master ID: 201308052031-143311683-58067-22417
I0805 20:31:40.487548 22444 master.cpp:593] Elected as master!
W0805 20:31:40.487632 22441 master.cpp:83] No whitelist given. Advertising offers for all slaves
I0805 20:31:40.487825 22441 sched.cpp:482] Got error 'Role 'role1' is not valid.'
I0805 20:31:40.487865 22441 sched.cpp:493] Scheduler::error took 8.598us
I0805 20:31:40.487948 22441 sched.cpp:522] Aborting framework ''
I0805 20:31:40.488014 22441 sched.cpp:527] Not sending a deactivate message as master is disconnected
I0805 20:31:40.488276 22439 sched.cpp:178] New master at master@67.195.138.8:58067
I0805 20:31:40.488396 22439 master.cpp:643] Registering framework 201308052031-143311683-58067-22417-0000 at scheduler(100)@67.195.138.8:58067
I0805 20:31:40.488472 22439 sched.cpp:237] Framework registered with 201308052031-143311683-58067-22417-0000
I0805 20:31:40.488509 22439 sched.cpp:251] Scheduler::registered took 9.903us
I0805 20:31:40.488657 22417 master.cpp:446] Master terminating
I0805 20:31:40.488690 22417 master.cpp:209] Shutting down master
I0805 20:31:40.488709 22439 sched.cpp:498] Stopping framework '201308052031-143311683-58067-22417-0000'
I0805 20:31:40.488744 22442 sched.cpp:498] Stopping framework ''
I0805 20:31:40.489085 22444 hierarchical_allocator_process.hpp:287] Initializing hierarchical allocator process with master : master@67.195.138.8:58067
tests/allocator_tests.cpp:1640: Failure
Actual function call count doesn't match EXPECT_CALL(this->allocator, frameworkAdded(_, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] AllocatorTest/0.RoleTest, where TypeParam = mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> (3 ms)
[----------] 10 tests from AllocatorTest/0 (473 ms total)"	MESOS	Resolved	3	1	2732	twitter
13265924	Improve v1 operator API read performance.	"Currently, the v1 operator API has poor performance relative to the v0 json API. The following initial numbers were provided by [~Will Mahler]from our state serving benchmark:


|OPTIMIZED - Master (baseline)|||||
|Test setup|1000 agents with a total of 10000 running tasks and 10000 completed tasks|10000 agents with a total of 100000 running tasks and 100000 completed tasks|20000 agents with a total of 200000 running tasks and 200000 completed tasks|40000 agents with a total of 400000 running tasks and 400000 completed tasks|
|v0 'state' response|0.17|1.66|8.96|12.42|
|v1 x-protobuf|0.35|3.21|9.47|19.09|
|v1 json|0.45|4.72|10.81|31.43|


There is quite a lot of variance, but v1 protobuf consistently slower than v0 (sometimes significantly so) and v1 json is consistently slower than v1 protobuf (sometimes significantly so).

The reason that the v1 operator API is slower is that it does the following:

(1) Construct temporary unversioned state response object by copying in-memory un-versioned state into overall response object. (expensive!)
(2) Evolve it to v1: serialize, de-serialize into v1 overall state object. (expensive!)
(3) Serialize the overall v1 state object to protobuf or json.
(4) Destruct the temporaries (expensive! but is done after response starts serving)

On the other hand, the v0 jsonify approach does the following:

(1) Serialize the in-memory unversioned state into json, by traversing state and accumulating the overall serialized json.

This means that v1 has substantial overhead vs v0, and we need to remove it to bring v1 on-par or better than v0. v1 should serialize directly to json (straightforward with jsonify) or protobuf (this can be done via a io::CodedOutputStream)."	MESOS	Resolved	3	4	2732	foundations
13191561	FsTest.Used is flaky	"The stout test \{{FsTest.Used}} is flaky,
{code}
[ RUN ] FsTest.Used
../3rdparty/stout/tests/os/filesystem_tests.cpp:817: Failure
 Expected: used.get() / b.f_frsize
 Which is: 1246447B
To be equal to: b.f_blocks - b.f_bfree
 Which is: 1246384
[ FAILED ] FsTest.Used (0 ms)
{code}

Looking at the test implementation this appears to be due to the test acquiring the two compared values non-atomically,
{code}
TEST_F(FsTest, Used)
{
 Try<Bytes> used = fs::used(""."");
 ASSERT_SOME(used);

struct statvfs b;
 ASSERT_EQ(0, ::statvfs(""."", &b));

// Check that the block counts match.
 EXPECT_EQ(used.get() / b.f_frsize, b.f_blocks - b.f_bfree);
}
{code}"	MESOS	Resolved	3	1	2732	flaky, flaky-test
12774399	Add ability for schedulers to explicitly acknowledge status updates on the driver.	"In order for schedulers to be able to handle status updates in a scalable manner, they need the ability to send acknowledgements through the driver. This enables optimizations in schedulers (e.g. process status updates asynchronously w/o backing up the driver, processing/acking updates in batch).

Without this, an implicit reconciliation can overload a scheduler (hence the motivation for MESOS-2308)."	MESOS	Resolved	3	4	2732	twitter
12709130	Make check failure on OSX - IO error: Too many open files	"Make check runs into an abort:

{noformat}
$ make check
[...]
[       OK ] CoordinatorTest.LearnedOnOneReplica_NotLearnedOnAnother_AnotherFailsAndRecovers (0 ms)
[----------] 21 tests from CoordinatorTest (816 ms total)

[----------] 2 tests from RecoverTest
[ RUN      ] RecoverTest.RacingCatchup
F0417 21:45:21.254204 1980908304 replica.cpp:709] CHECK_SOME(state): IO error: /private/tmp/RecoverTest_RacingCatchup_if5Cz6/.log4/LOCK: Too many open filesFailed to recover the log
*** Check failure stack trace: ***
    @        0x10a2f9434  google::LogMessage::SendToLog()
    @        0x10a2f9963  google::LogMessage::Flush()
    @        0x10a2fcaff  google::LogMessageFatal::~LogMessageFatal()
    @        0x10a2fa059  google::LogMessageFatal::~LogMessageFatal()
    @        0x109dd8479  _CheckFatal::~_CheckFatal()
    @        0x109dd8349  _CheckFatal::~_CheckFatal()
    @        0x10a1b379a  mesos::internal::log::ReplicaProcess::restore()
    @        0x10a1b3241  mesos::internal::log::ReplicaProcess::ReplicaProcess()
    @        0x10a1b696b  mesos::internal::log::Replica::Replica()
    @        0x1091ebd9a  RecoverTest_RacingCatchup_Test::TestBody()
    @        0x10945234c  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x1094431ea  testing::Test::Run()
    @        0x109443e72  testing::TestInfo::Run()
    @        0x1094444b0  testing::TestCase::Run()
    @        0x109449d05  testing::internal::UnitTestImpl::RunAllTests()
    @        0x109452b14  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x109449a39  testing::UnitTest::Run()
    @        0x10922a270  main
    @     0x7fff8a98d5fd  start
    @                0x1  (unknown)
make[3]: *** [check-local] Abort trap: 6
{noformat}

That test does not fail when being run individually, hinting that we got some general file-handle leakage problem.

The exact test that throws the abort bomb is machine and dependent. Tried it on two MBP's and one fails a few tests earlier than the other."	MESOS	Resolved	3	1	2732	build, build-failure, mesos, unit-test
12767847	SlaveTest.MesosExecutorGracefulShutdown is flaky	"Observed this on internal CI

{noformat}
[ RUN      ] SlaveTest.MesosExecutorGracefulShutdown
Using temporary directory '/tmp/SlaveTest_MesosExecutorGracefulShutdown_AWdtVJ'
I0124 08:14:04.399211  7926 leveldb.cpp:176] Opened db in 27.364056ms
I0124 08:14:04.402632  7926 leveldb.cpp:183] Compacted db in 3.357646ms
I0124 08:14:04.402691  7926 leveldb.cpp:198] Created db iterator in 23822ns
I0124 08:14:04.402708  7926 leveldb.cpp:204] Seeked to beginning of db in 1913ns
I0124 08:14:04.402716  7926 leveldb.cpp:273] Iterated through 0 keys in the db in 458ns
I0124 08:14:04.402767  7926 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0124 08:14:04.403728  7951 recover.cpp:449] Starting replica recovery
I0124 08:14:04.404011  7951 recover.cpp:475] Replica is in EMPTY status
I0124 08:14:04.407765  7950 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0124 08:14:04.408710  7951 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0124 08:14:04.419666  7951 recover.cpp:566] Updating replica status to STARTING
I0124 08:14:04.429719  7953 master.cpp:262] Master 20150124-081404-16842879-47787-7926 (utopic) started on 127.0.1.1:47787
I0124 08:14:04.429790  7953 master.cpp:308] Master only allowing authenticated frameworks to register
I0124 08:14:04.429802  7953 master.cpp:313] Master only allowing authenticated slaves to register
I0124 08:14:04.429826  7953 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_AWdtVJ/credentials'
I0124 08:14:04.430277  7953 master.cpp:357] Authorization enabled
I0124 08:14:04.432682  7953 master.cpp:1219] The newly elected leader is master@127.0.1.1:47787 with id 20150124-081404-16842879-47787-7926
I0124 08:14:04.432816  7953 master.cpp:1232] Elected as the leading master!
I0124 08:14:04.432894  7953 master.cpp:1050] Recovering from registrar
I0124 08:14:04.433212  7950 registrar.cpp:313] Recovering registrar
I0124 08:14:04.434226  7951 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.323302ms
I0124 08:14:04.434270  7951 replica.cpp:323] Persisted replica status to STARTING
I0124 08:14:04.434489  7951 recover.cpp:475] Replica is in STARTING status
I0124 08:14:04.436164  7951 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0124 08:14:04.439368  7947 recover.cpp:195] Received a recover response from a replica in STARTING status
I0124 08:14:04.440626  7947 recover.cpp:566] Updating replica status to VOTING
I0124 08:14:04.443667  7947 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 2.698664ms
I0124 08:14:04.443759  7947 replica.cpp:323] Persisted replica status to VOTING
I0124 08:14:04.443925  7947 recover.cpp:580] Successfully joined the Paxos group
I0124 08:14:04.444160  7947 recover.cpp:464] Recover process terminated
I0124 08:14:04.444543  7949 log.cpp:660] Attempting to start the writer
I0124 08:14:04.446331  7949 replica.cpp:477] Replica received implicit promise request with proposal 1
I0124 08:14:04.449329  7949 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 2.690453ms
I0124 08:14:04.449388  7949 replica.cpp:345] Persisted promised to 1
I0124 08:14:04.450637  7947 coordinator.cpp:230] Coordinator attemping to fill missing position
I0124 08:14:04.452271  7949 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0124 08:14:04.455124  7949 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 2.593522ms
I0124 08:14:04.455157  7949 replica.cpp:679] Persisted action at 0
I0124 08:14:04.456594  7951 replica.cpp:511] Replica received write request for position 0
I0124 08:14:04.456657  7951 leveldb.cpp:438] Reading position from leveldb took 30358ns
I0124 08:14:04.464860  7951 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 8.164646ms
I0124 08:14:04.464903  7951 replica.cpp:679] Persisted action at 0
I0124 08:14:04.465947  7949 replica.cpp:658] Replica received learned notice for position 0
I0124 08:14:04.471567  7949 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.587838ms
I0124 08:14:04.471601  7949 replica.cpp:679] Persisted action at 0
I0124 08:14:04.471622  7949 replica.cpp:664] Replica learned NOP action at position 0
I0124 08:14:04.472682  7951 log.cpp:676] Writer started with ending position 0
I0124 08:14:04.473919  7951 leveldb.cpp:438] Reading position from leveldb took 28676ns
I0124 08:14:04.491591  7951 registrar.cpp:346] Successfully fetched the registry (0B) in 58.337024ms
I0124 08:14:04.491704  7951 registrar.cpp:445] Applied 1 operations in 28163ns; attempting to update the 'registry'
I0124 08:14:04.493938  7953 log.cpp:684] Attempting to append 118 bytes to the log
I0124 08:14:04.494122  7953 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0124 08:14:04.495069  7953 replica.cpp:511] Replica received write request for position 1
I0124 08:14:04.500089  7953 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 4.989356ms
I0124 08:14:04.500123  7953 replica.cpp:679] Persisted action at 1
I0124 08:14:04.501271  7950 replica.cpp:658] Replica received learned notice for position 1
I0124 08:14:04.505698  7950 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 4.396221ms
I0124 08:14:04.505734  7950 replica.cpp:679] Persisted action at 1
I0124 08:14:04.505755  7950 replica.cpp:664] Replica learned APPEND action at position 1
I0124 08:14:04.507313  7950 registrar.cpp:490] Successfully updated the 'registry' in 15.52896ms
I0124 08:14:04.507478  7953 log.cpp:703] Attempting to truncate the log to 1
I0124 08:14:04.507848  7953 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0124 08:14:04.508743  7953 replica.cpp:511] Replica received write request for position 2
I0124 08:14:04.509214  7950 registrar.cpp:376] Successfully recovered registrar
I0124 08:14:04.509682  7946 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0124 08:14:04.514654  7953 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.880031ms
I0124 08:14:04.514689  7953 replica.cpp:679] Persisted action at 2
I0124 08:14:04.515736  7953 replica.cpp:658] Replica received learned notice for position 2
I0124 08:14:04.522014  7953 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 6.245138ms
I0124 08:14:04.522086  7953 leveldb.cpp:401] Deleting ~1 keys from leveldb took 37803ns
I0124 08:14:04.522107  7953 replica.cpp:679] Persisted action at 2
I0124 08:14:04.522128  7953 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0124 08:14:04.531460  7926 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0124 08:14:04.547194  7951 slave.cpp:173] Slave started on 208)@127.0.1.1:47787
I0124 08:14:04.555682  7951 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/credential'
I0124 08:14:04.556622  7951 slave.cpp:282] Slave using credential for: test-principal
I0124 08:14:04.557052  7951 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0124 08:14:04.557842  7951 slave.cpp:329] Slave hostname: utopic
I0124 08:14:04.558091  7951 slave.cpp:330] Slave checkpoint: false
W0124 08:14:04.558352  7951 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0124 08:14:04.566864  7948 state.cpp:33] Recovering state from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/meta'
I0124 08:14:04.575711  7951 status_update_manager.cpp:197] Recovering status update manager
I0124 08:14:04.575904  7951 containerizer.cpp:300] Recovering containerizer
I0124 08:14:04.577112  7951 slave.cpp:3519] Finished recovery
I0124 08:14:04.577374  7926 sched.cpp:151] Version: 0.22.0
I0124 08:14:04.578663  7950 sched.cpp:248] New master detected at master@127.0.1.1:47787
I0124 08:14:04.578759  7950 sched.cpp:304] Authenticating with master master@127.0.1.1:47787
I0124 08:14:04.578781  7950 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0124 08:14:04.579071  7950 authenticatee.hpp:138] Creating new client SASL connection
I0124 08:14:04.579550  7947 master.cpp:4129] Authenticating scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.579582  7947 master.cpp:4140] Using default CRAM-MD5 authenticator
I0124 08:14:04.580031  7947 authenticator.hpp:170] Creating new server SASL connection
I0124 08:14:04.580402  7947 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0124 08:14:04.580430  7947 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0124 08:14:04.580538  7947 authenticator.hpp:276] Received SASL authentication start
I0124 08:14:04.580581  7947 authenticator.hpp:398] Authentication requires more steps
I0124 08:14:04.580651  7947 authenticatee.hpp:275] Received SASL authentication step
I0124 08:14:04.580746  7947 authenticator.hpp:304] Received SASL authentication step
I0124 08:14:04.580837  7947 authenticator.hpp:390] Authentication success
I0124 08:14:04.580940  7947 authenticatee.hpp:315] Authentication success
I0124 08:14:04.581009  7947 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.581328  7947 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:47787
I0124 08:14:04.581509  7947 master.cpp:1420] Received registration request for framework 'default' at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.581585  7947 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0124 08:14:04.582033  7947 master.cpp:1484] Registering framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.582595  7947 hierarchical_allocator_process.hpp:319] Added framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.583051  7947 sched.cpp:442] Framework registered with 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.584087  7951 slave.cpp:613] New master detected at master@127.0.1.1:47787
I0124 08:14:04.584388  7951 slave.cpp:676] Authenticating with master master@127.0.1.1:47787
I0124 08:14:04.584564  7951 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0124 08:14:04.584951  7951 slave.cpp:649] Detecting new master
I0124 08:14:04.585219  7951 status_update_manager.cpp:171] Pausing sending status updates
I0124 08:14:04.585604  7951 authenticatee.hpp:138] Creating new client SASL connection
I0124 08:14:04.587666  7953 master.cpp:4129] Authenticating slave(208)@127.0.1.1:47787
I0124 08:14:04.587702  7953 master.cpp:4140] Using default CRAM-MD5 authenticator
I0124 08:14:04.588434  7953 authenticator.hpp:170] Creating new server SASL connection
I0124 08:14:04.588764  7953 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0124 08:14:04.588790  7953 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0124 08:14:04.588896  7953 authenticator.hpp:276] Received SASL authentication start
I0124 08:14:04.588935  7953 authenticator.hpp:398] Authentication requires more steps
I0124 08:14:04.589005  7953 authenticatee.hpp:275] Received SASL authentication step
I0124 08:14:04.589082  7953 authenticator.hpp:304] Received SASL authentication step
I0124 08:14:04.589140  7953 authenticator.hpp:390] Authentication success
I0124 08:14:04.589232  7953 authenticatee.hpp:315] Authentication success
I0124 08:14:04.589300  7953 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(208)@127.0.1.1:47787
I0124 08:14:04.589587  7953 slave.cpp:747] Successfully authenticated with master master@127.0.1.1:47787
I0124 08:14:04.589913  7953 master.cpp:3275] Registering slave at slave(208)@127.0.1.1:47787 (utopic) with id 20150124-081404-16842879-47787-7926-S0
I0124 08:14:04.590322  7953 registrar.cpp:445] Applied 1 operations in 60404ns; attempting to update the 'registry'
I0124 08:14:04.595336  7948 log.cpp:684] Attempting to append 283 bytes to the log
I0124 08:14:04.595552  7948 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0124 08:14:04.596535  7948 replica.cpp:511] Replica received write request for position 3
I0124 08:14:04.597846  7951 master.cpp:3263] Ignoring register slave message from slave(208)@127.0.1.1:47787 (utopic) as admission is already in progress
I0124 08:14:04.602326  7948 leveldb.cpp:343] Persisting action (302 bytes) to leveldb took 5.758211ms
I0124 08:14:04.602363  7948 replica.cpp:679] Persisted action at 3
I0124 08:14:04.603492  7951 replica.cpp:658] Replica received learned notice for position 3
I0124 08:14:04.608952  7951 leveldb.cpp:343] Persisting action (304 bytes) to leveldb took 5.427195ms
I0124 08:14:04.608985  7951 replica.cpp:679] Persisted action at 3
I0124 08:14:04.609007  7951 replica.cpp:664] Replica learned APPEND action at position 3
I0124 08:14:04.610643  7951 registrar.cpp:490] Successfully updated the 'registry' in 20.258048ms
I0124 08:14:04.610800  7948 log.cpp:703] Attempting to truncate the log to 3
I0124 08:14:04.611184  7948 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0124 08:14:04.612076  7948 replica.cpp:511] Replica received write request for position 4
I0124 08:14:04.613061  7946 master.cpp:3329] Registered slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0124 08:14:04.613299  7946 hierarchical_allocator_process.hpp:453] Added slave 20150124-081404-16842879-47787-7926-S0 (utopic) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0124 08:14:04.613688  7946 slave.cpp:781] Registered with master master@127.0.1.1:47787; given slave ID 20150124-081404-16842879-47787-7926-S0
I0124 08:14:04.614112  7946 master.cpp:4071] Sending 1 offers to framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.614228  7946 status_update_manager.cpp:178] Resuming sending status updates
I0124 08:14:04.617481  7947 master.cpp:2677] Processing ACCEPT call for offers: [ 20150124-081404-16842879-47787-7926-O0 ] on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) for framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.617535  7947 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 7c16772d-4aed-4719-81c4-658a2cc22543 as user 'jenkins'
I0124 08:14:04.618736  7947 master.hpp:782] Adding task 7c16772d-4aed-4719-81c4-658a2cc22543 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150124-081404-16842879-47787-7926-S0 (utopic)
I0124 08:14:04.618854  7947 master.cpp:2885] Launching task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:04.619209  7947 slave.cpp:1130] Got assigned task 7c16772d-4aed-4719-81c4-658a2cc22543 for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.619472  7948 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.364828ms
I0124 08:14:04.619941  7948 replica.cpp:679] Persisted action at 4
I0124 08:14:04.624851  7953 replica.cpp:658] Replica received learned notice for position 4
I0124 08:14:04.625757  7947 slave.cpp:1245] Launching task 7c16772d-4aed-4719-81c4-658a2cc22543 for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.630590  7953 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.705336ms
I0124 08:14:04.630805  7953 leveldb.cpp:401] Deleting ~2 keys from leveldb took 51263ns
I0124 08:14:04.630828  7953 replica.cpp:679] Persisted action at 4
I0124 08:14:04.630851  7953 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0124 08:14:04.633968  7947 slave.cpp:3921] Launching executor 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 in work directory '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543/runs/53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.634963  7951 containerizer.cpp:445] Starting container '53887a08-f11d-4a2f-a659-a715d9fcf3d2' for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework '20150124-081404-16842879-47787-7926-0000'
W0124 08:14:04.636931  7951 containerizer.cpp:296] CommandInfo.grace_period flag is not set, using default value: 3secs
I0124 08:14:04.655591  7947 slave.cpp:1368] Queuing task '7c16772d-4aed-4719-81c4-658a2cc22543' for executor 7c16772d-4aed-4719-81c4-658a2cc22543 of framework '20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.656992  7951 launcher.cpp:137] Forked child with pid '11030' for container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.673646  7951 slave.cpp:2890] Monitoring executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework '20150124-081404-16842879-47787-7926-0000' in container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.964946 11044 exec.cpp:147] Version: 0.22.0
I0124 08:14:05.113059  7948 slave.cpp:1912] Got registration for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:05.121086  7948 slave.cpp:2031] Flushing queued task 7c16772d-4aed-4719-81c4-658a2cc22543 for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.266849 11062 exec.cpp:221] Executor registered on slave 20150124-081404-16842879-47787-7926-S0
Shutdown timeout is set to 3secsRegistered executor on utopic
Starting task 7c16772d-4aed-4719-81c4-658a2cc22543
Forked command at 11067
sh -c 'sleep 1000'
I0124 08:14:05.492084  7953 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:05.492805  7953 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.493762  7953 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to master@127.0.1.1:47787
I0124 08:14:05.493948  7953 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to executor(1)@127.0.1.1:49174
I0124 08:14:05.495378  7949 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.495584  7949 master.cpp:3624] Status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:05.495678  7949 master.cpp:4934] Updating the latest state of task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to TASK_RUNNING
I0124 08:14:05.496422  7949 master.cpp:3125] Forwarding status update acknowledgement 54742a87-ef02-4e72-a19b-83b0eeb62568 for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 to slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:05.497735  7946 master.cpp:2961] Asked to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.497859  7946 master.cpp:3021] Telling slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:05.498589  7947 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.499006  7953 slave.cpp:1424] Asked to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
Shutting down
Sending SIGTERM to process tree at pid 11067
Killing the following process trees:
[ 
-+- 11067 sh -c sleep 1000 
 \--- 11068 sleep 1000 
]
2015-01-24 08:14:07,295:7926(0x7f30b1b34700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:57753] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
Process 11067 did not terminate after 3secs, sending SIGKILL to process tree at 11067
Killed the following process trees:
[ 
-+- 11067 sh -c sleep 1000 
 \--- 11068 sleep 1000 
]
Command terminated with signal Killed (pid: 11067)
I0124 08:14:09.063453  7953 slave.cpp:2265] Handling status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:09.069545  7953 status_update_manager.cpp:317] Received status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.070265  7953 slave.cpp:2508] Forwarding the update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to master@127.0.1.1:47787
I0124 08:14:09.070996  7947 master.cpp:3652] Forwarding status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.071182  7947 master.cpp:3624] Status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:09.071260  7947 master.cpp:4934] Updating the latest state of task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to TASK_KILLED
I0124 08:14:09.072052  7947 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150124-081404-16842879-47787-7926-S0 from framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.072449  7947 master.cpp:4993] Removing task 7c16772d-4aed-4719-81c4-658a2cc22543 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150124-081404-16842879-47787-7926-0000 on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:09.072700  7947 master.cpp:3125] Forwarding status update acknowledgement 4bd05372-2705-46e5-8182-5cb6907fbab3 for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 to slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
../../src/tests/slave_tests.cpp:1736: Failure
Expected: (std::string::npos) != (statusKilled.get().message().find(""Terminated"")), actual: 18446744073709551615 vs 18446744073709551615
I0124 08:14:09.073422  7926 sched.cpp:1471] Asked to stop the driver
I0124 08:14:09.073629  7926 master.cpp:654] Master terminating
I0124 08:14:09.075768  7950 sched.cpp:808] Stopping framework '20150124-081404-16842879-47787-7926-0000'
I0124 08:14:09.079352  7953 slave.cpp:2441] Sending acknowledgement for status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to executor(1)@127.0.1.1:49174
I0124 08:14:09.085199  7953 slave.cpp:2673] master@127.0.1.1:47787 exited
W0124 08:14:09.085232  7953 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0124 08:14:09.085263  7953 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.120879  7946 containerizer.cpp:879] Destroying container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:09.216553  7952 containerizer.cpp:1084] Executor for container '53887a08-f11d-4a2f-a659-a715d9fcf3d2' has exited
I0124 08:14:09.218641  7952 slave.cpp:2948] Executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000 terminated with signal Killed
I0124 08:14:09.218855  7952 slave.cpp:3057] Cleaning up executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.223268  7947 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543/runs/53887a08-f11d-4a2f-a659-a715d9fcf3d2' for gc 6.99999746482667days in the future
I0124 08:14:09.224205  7947 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543' for gc 6.99999746293926days in the future
I0124 08:14:09.227552  7952 slave.cpp:3136] Cleaning up framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.229786  7949 status_update_manager.cpp:279] Closing status update streams for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.230849  7952 slave.cpp:495] Slave terminating
I0124 08:14:09.230989  7952 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000' for gc 6.99999732935407days in the future
[  FAILED  ] SlaveTest.MesosExecutorGracefulShutdown (4881 ms)
{noformat}"	MESOS	Resolved	3	1	2732	twitter
12848417	Allow slave to forward messages through the master for HTTP schedulers.	"The master currently has no install handler for {{ExecutorToFramework}} messages and the slave directly sends these messages to the scheduler driver, bypassing the master entirely.

We need to preserve this behavior for the driver, but HTTP schedulers will not have a libprocess 'pid'. We'll have to ensure that the {{RunTaskMessage}} and {{UpdateFrameworkMessage}} have an optional pid. For now the master will continue to set the pid, but 0.24.0 slaves will know to send messages through the master when the 'pid' is not available."	MESOS	Resolved	3	3	2732	twitter
13134910	Provide a task scheduler to simplify the tests.	"Currently, there are a lot of tests that just want to launch a task in order to test some behavior of the system. These tests have to create their own v0 or v1 scheduler and invoke the necessary calls on it and expect the necessary calls / messages back. This is rather verbose.

It would be helpful to have some better abstractions here, like a TestScheduler that can launch tasks and exposes the status updates for them, along with other interesting information. E.g.

{code}
class TestScheduler
{
  // Add the task to the queue of tasks that need to be launched.
  // Returns the stream of status updates for this task.
  Queue<StatusUpdate> addTask(const TaskInfo& t);

  etc
}
{code}

Probably this could be implemented against both v0 and v1, if we want to parameterize the tests."	MESOS	Reviewable	3	4	2732	foundations, tech-debt
13218696	Display quota consumption in the webui.	"Currently, the Roles table in the webui displays allocation and quota guarantees / limits. However, quota ""consumption"" is different from allocation, in that reserved resources are always considered consumed against the quota.

This discrepancy has led to confusion from users. One exampled occurred when an agent was added with a large reservation exceeding the memory quota guarantee. The user sees memory chopping in offers, and since the scheduler didn't want to use the reservation, it can't launch its tasks.

If consumption is shown in the UI, we should include a tool tip that indicates how consumed is calculated so that users know how to interpret it."	MESOS	Resolved	3	4	2732	resource-management
13131615	ExecutorAuthorizationTest.RunTaskGroup segfaults.	"{noformat}
14:32:50 *** Aborted at 1516199570 (unix time) try ""date -d @1516199570"" if you are using GNU date ***
14:32:50 PC: @     0x7f36ef13f8b0 std::_Hashtable<>::count()
14:32:50 *** SIGSEGV (@0x107c7f88978) received by PID 19547 (TID 0x7f36e2722700) from PID 18446744072769538424; stack trace: ***
14:32:50     @     0x7f36dcc763fd (unknown)
14:32:50     @     0x7f36dcc7b419 (unknown)
14:32:50     @     0x7f36dcc6f918 (unknown)
14:32:50     @     0x7f36eb99e330 (unknown)
14:32:50     @     0x7f36ef13f8b0 std::_Hashtable<>::count()
14:32:50     @     0x7f36ef12bd22 _ZZN7process11ProcessBase8_consumeERKNS0_12HttpEndpointERKSsRKNS_5OwnedINS_4http7RequestEEEENKUlRK6OptionINS7_14authentication20AuthenticationResultEEE0_clESH_
14:32:50     @     0x7f36ef12c834 _ZNO6lambda12CallableOnceIFN7process6FutureINS1_4http8ResponseEEEvEE10CallableFnINS_8internal7PartialIZNS1_11ProcessBase8_consumeERKNSB_12HttpEndpointERKSsRKNS1_5OwnedINS3_7RequestEEEEUlRK6OptionINS3_14authentication20AuthenticationResultEEE0_JSP_EEEEclEv
14:32:50     @     0x7f36ee1c1e8a _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureINS1_4http8ResponseEEEEclINS0_IFSE_vEEEEESE_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISD_EESt14default_deleteISQ_EEOSI_S3_E_JST_SI_St12_PlaceholderILi1EEEEEEclEOS3_
14:32:50     @     0x7f36ef118711 process::ProcessBase::consume()
14:32:50     @     0x7f36ef1309a2 process::ProcessManager::resume()
14:32:50     @     0x7f36ef134216 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
14:32:50     @     0x7f36ec15a5b0 (unknown)
14:32:50     @     0x7f36eb996184 start_thread
14:32:50     @     0x7f36eb6c2ffd (unknown)
{noformat}
Full log attached."	MESOS	Resolved	3	1	2732	flaky-test
12852842	some variables in version.hpp use `Type &var` instead of `Type& var`	"Some variables in 
3rdparty/libprocess/3rdparty/stout/include/stout/version.hpp violate Mesos code style of biding '&' and '*' to the type name  (as opposed to binding to the variable name)."	MESOS	Resolved	4	1	2732	mesosphere, newbie
12860408	Support HTTP Pipelining in libprocess (http::post)	"Currently , {{http::post}} in libprocess, does not support HTTP pipelining. Each call as of know sends in the {{Connection: close}} header, thereby, signaling to the server to close the TCP socket after the response.

We either need to create a new interface for supporting HTTP pipelining , or modify the existing {{http::post}} to do so.

This is needed for the Scheduler/Executor library implementations to make sure ""Calls"" are sent in order to the master. Currently, in order to do so, we send in the next request only after we have received a response for an earlier call that results in degraded performance.

"	MESOS	Resolved	3	3	2732	twitter
12837533	Slave should send oversubscribed resource information after master failover.	After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.	MESOS	Resolved	2	1	2732	twitter
13171260	Add a metrics benchmark in libprocess.	Libprocess metrics scalability is being worked on and currently a mesos specific benchmark is being used. It would be nice to have a benchmark in libprocess to iterate on the library improvements in a more minimal benchmark.	MESOS	Resolved	3	3	2732	performance
12823202	Implement a stand alone test framework that uses revocable cpu resources	"Ideally this would be an example framework (or stand alone binary like load generator framework) that helps us evaluate oversubscription in a real cluster.

We need to come up with metrics that need to be exposed by this framework for evaluation (e.g., how many revocable offers, rescinds, preemptions etc)."	MESOS	Resolved	3	3	2732	twitter
13074010	Add executor reconnection retry logic to the agent	Currently, the agent sends a single {{ReconnectExecutorMessage}} to PID-based executors during recovery. It would be more robust to have the agent retry these messages until {{executor_reregister_timeout}} has elapsed.	MESOS	Resolved	3	4	2732	mesosphere
13173068	Adopt rapidjson for improved json serialization performance.	"The master state query benchmark shows a roughly 50% reduction in time to serve v0 /state when jsonify uses rapidjson under the covers. We should consider adopting rapidjson in order to get this performance benefit to users:

https://github.com/Tencent/rapidjson

License wise, we just have to exclude a third party bundled binary, so we'll need to document how to strip the releases prior to upgrading for posterity."	MESOS	Resolved	3	4	2732	performance
13223224	Deprecate v0 quota calls.	Once we introduce the new quota APIs in MESOS-8068, we should deprecate the `/quota` endpoint. We should mark this as deprecated and hide it in our documentation.	MESOS	Resolved	3	4	2732	mesosphere, resource-management
12667919	SlaveRecoveryTest/0.RecoveryTimeout Java SIGSEGV	Assigning to Vinod to suppress or remove this test in 0.14.0. If suppressed, we should include a note about the relevant TODO in libprocess.	MESOS	Accepted	3	1	2732	disabled-test, foundations, mesosphere
13261128	Very large quota values can crash master.	"We are observing the following crash on the 1.9.1 master:

{code}
I1008 10:12:15.148486  4687 http.cpp:1115] HTTP POST for /master/api/v1?_ts=1570529541073&UPDATE_QUOTA from 10.0.7.253:35410 with User-Agent='Mozilla/5.0 (Windows NT 6.1; Win64; x64) Ap>
I1008 10:12:15.148665  4687 http.cpp:263] Processing call UPDATE_QUOTA
I1008 10:12:15.148756  4687 quota_handler.cpp:1136] Authorizing principal 'bootstrapuser' to update quota config for role 's1'
I1008 10:12:15.149169  4685 registrar.cpp:487] Applied 1 operations in 56277ns; attempting to update the registry
I1008 10:12:15.149338  4681 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 13
I1008 10:12:15.149467  4689 replica.cpp:541] Replica received write request for position 13 from __req_res__(29)@10.0.7.253:5050
I1008 10:12:15.151820  4683 replica.cpp:695] Replica received learned notice for position 13 from log-network(2)@10.0.7.253:5050
I1008 10:12:15.153559  4679 registrar.cpp:544] Successfully updated the registry in 4.348928ms
I1008 10:12:15.153592  4678 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 14
I1008 10:12:15.153715  4679 hierarchical.cpp:1619] Updated quota for role 's1',  guarantees: {} limits: cpus:2; disk:-9.22337203685478e+15; gpus:3; mem:1000000000000
I1008 10:12:15.153796  4677 replica.cpp:541] Replica received write request for position 14 from __req_res__(30)@10.0.7.253:5050
I1008 10:12:15.155380  4691 replica.cpp:695] Replica received learned notice for position 14 from log-network(2)@10.0.7.253:5050
I1008 10:12:15.249722  4677 authenticator.cpp:324] dstip=10.0.7.253 type=audit timestamp=2019-10-08 10:12:15.249673984+00:00 reason=""Valid authentication token"" uid=""bootstrapuser"" obje>
I1008 10:12:15.249956  4682 http.cpp:1115] HTTP GET for /master/state-summary?_ts=1570529541169 from 10.0.7.253:35414 with User-Agent='Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebK>
I1008 10:12:15.250633  4691 http.cpp:1132] HTTP GET for /master/state-summary?_ts=1570529541169 from 10.0.7.253:35414: '200 OK' after 1.72621ms
I1008 10:12:15.570379  4689 hierarchical.cpp:1908] Before allocation, required quota headroom is {} and available quota headroom is cpus:0.9; disk:75853; mem:5507
F1008 10:12:15.570580  4689 resource_quantities.cpp:330] Check failed: scalar >= Value::Scalar() (-9.22337203685478e+15 vs. 0)
*** Check failure stack trace: ***
    @     0x7fc786f0148d  google::LogMessage::Fail()
    @     0x7fc786f036e8  google::LogMessage::SendToLog()
    @     0x7fc786f01023  google::LogMessage::Flush()
    @     0x7fc786f04029  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fc785954dfa  mesos::ResourceQuantities::add()
    @     0x7fc785954fb6  mesos::ResourceQuantities::fromScalarResource()
    @     0x7fc78595e135  mesos::shrinkResources()
    @     0x7fc785a874a9  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::__allocate()
    @     0x7fc785a88089  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::_allocate()
    @     0x7fc785a93882  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchI7NothingN5mesos8internal6master9allocator8internal28Hier>
    @     0x7fc786e49e21  process::ProcessBase::consume()
    @     0x7fc786e6141b  process::ProcessManager::resume()
    @     0x7fc786e670b6  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
    @     0x7fc782a28b22  (unknown)
    @     0x7fc7821be94a  (unknown)
    @     0x7fc781eef07f  clone
{code}

Note that the value of disk quota limit is *logged* as ""negative"".

Update: we figured out that in reality the quota limit on that master has been set to an insanely large value.

The situation is exacerbated by the fact that the crash is not guaranteed to occur immediately, i.e. these values might become persisted in the registry."	MESOS	Resolved	1	1	2732	resource-management
13240459	Slow memory growth in master due to deferred deletion of offer filters and timers.	"The allocator does not keep a handle to the offer filter timer, which means it cannot remove the timer overhead (in this case memory) when removing the offer filter earlier (e.g. due to revive):

https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1338-L1352

In addition, the offer filter is allocated on the heap but not deleted until the timer fires (which might take forever!):

https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1321
https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1408-L1413
https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L2249

We'll need to try to backport this to all active release branches."	MESOS	Resolved	2	1	2732	resource-management
12987596	ExamplesTest.DynamicReservationFramework is flaky	"Showed up on ASF CI:
https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2466/changes

{code}
[ RUN      ] ExamplesTest.DynamicReservationFramework
Using temporary directory '/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9'
/mesos/mesos-1.0.0/src/tests/dynamic_reservation_framework_test.sh: line 19: /mesos/mesos-1.0.0/_build/src/colors.sh: No such file or directory
/mesos/mesos-1.0.0/src/tests/dynamic_reservation_framework_test.sh: line 20: /mesos/mesos-1.0.0/_build/src/atexit.sh: No such file or directory
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0707 19:30:31.102650 29946 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I0707 19:30:31.125845 29946 process.cpp:1066] libprocess is initialized on 172.17.0.7:37568 with 16 worker threads
I0707 19:30:31.125954 29946 logging.cpp:199] Logging to STDERR
I0707 19:30:31.237936 29946 leveldb.cpp:174] Opened db in 101.67046ms
I0707 19:30:31.272083 29946 leveldb.cpp:181] Compacted db in 34.088797ms
I0707 19:30:31.272655 29946 leveldb.cpp:196] Created db iterator in 104307ns
I0707 19:30:31.272855 29946 leveldb.cpp:202] Seeked to beginning of db in 20581ns
I0707 19:30:31.273027 29946 leveldb.cpp:271] Iterated through 0 keys in the db in 13839ns
I0707 19:30:31.273460 29946 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0707 19:30:31.277535 29979 recover.cpp:451] Starting replica recovery
I0707 19:30:31.279044 29979 recover.cpp:477] Replica is in EMPTY status
I0707 19:30:31.285576 29984 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (3)@172.17.0.7:37568
I0707 19:30:31.290812 29983 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0707 19:30:31.300268 29972 recover.cpp:568] Updating replica status to STARTING
I0707 19:30:31.307143 29946 local.cpp:255] Creating default 'local' authorizer
I0707 19:30:31.324632 29972 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.394808ms
I0707 19:30:31.325036 29972 replica.cpp:320] Persisted replica status to STARTING
I0707 19:30:31.325812 29972 recover.cpp:477] Replica is in STARTING status
I0707 19:30:31.328284 29972 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.7:37568
I0707 19:30:31.328945 29972 recover.cpp:197] Received a recover response from a replica in STARTING status
I0707 19:30:31.329859 29972 recover.cpp:568] Updating replica status to VOTING
I0707 19:30:31.335539 29974 master.cpp:382] Master 443ee691-d272-454c-90fe-959c95948252 (89b080073abb) started on 172.17.0.7:37568
I0707 19:30:31.335839 29974 master.cpp:384] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""false"" --authenticate_frameworks=""false"" --authenticate_http=""false"" --authenticate_http_frameworks=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""20secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.0.0/src/webui"" --work_dir=""/tmp/mesos-zPIQS8"" --zk_session_timeout=""10secs""
I0707 19:30:31.337158 29974 master.cpp:436] Master allowing unauthenticated frameworks to register
I0707 19:30:31.337323 29974 master.cpp:450] Master allowing unauthenticated agents to register
I0707 19:30:31.337527 29974 master.cpp:464] Master allowing HTTP frameworks to register without authentication
I0707 19:30:31.337689 29974 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9/credentials'
W0707 19:30:31.337962 29974 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0707 19:30:31.338336 29974 master.cpp:506] Using default 'crammd5' authenticator
I0707 19:30:31.338723 29974 authenticator.cpp:519] Initializing server SASL
I0707 19:30:31.340744 29974 auxprop.cpp:73] Initialized in-memory auxiliary property plugin
I0707 19:30:31.341084 29974 master.cpp:705] Authorization enabled
I0707 19:30:31.342696 29971 hierarchical.cpp:151] Initialized hierarchical allocator process
I0707 19:30:31.342895 29977 whitelist_watcher.cpp:77] No whitelist given
I0707 19:30:31.358129 29972 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 27.780299ms
I0707 19:30:31.358496 29972 replica.cpp:320] Persisted replica status to VOTING
I0707 19:30:31.358949 29972 recover.cpp:582] Successfully joined the Paxos group
I0707 19:30:31.359601 29972 recover.cpp:466] Recover process terminated
I0707 19:30:31.365345 29946 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni
W0707 19:30:31.368975 29946 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W0707 19:30:31.369699 29946 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges
I0707 19:30:31.393633 29977 slave.cpp:205] Agent started on 1)@172.17.0.7:37568
I0707 19:30:31.394129 29977 slave.cpp:206] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-zPIQS8/0""
I0707 19:30:31.395762 29977 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 19:30:31.396198 29977 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 19:30:31.397099 29977 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:31.397364 29977 slave.cpp:602] Agent attributes: [  ]
I0707 19:30:31.397557 29977 slave.cpp:607] Agent hostname: 89b080073abb
I0707 19:30:31.403342 29981 state.cpp:57] Recovering state from '/tmp/mesos-zPIQS8/0/meta'
I0707 19:30:31.411643 29973 status_update_manager.cpp:200] Recovering status update manager
I0707 19:30:31.412467 29983 containerizer.cpp:522] Recovering containerizer
I0707 19:30:31.417868 29975 provisioner.cpp:253] Provisioner recovery complete
I0707 19:30:31.419260 29977 slave.cpp:4856] Finished recovery
I0707 19:30:31.420929 29977 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0707 19:30:31.422238 29970 status_update_manager.cpp:174] Pausing sending status updates
I0707 19:30:31.422533 29977 slave.cpp:969] New master detected at master@172.17.0.7:37568
I0707 19:30:31.422721 29977 slave.cpp:990] No credentials provided. Attempting to register without authentication
I0707 19:30:31.422902 29977 slave.cpp:1001] Detecting new master
I0707 19:30:31.423362 29977 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0707 19:30:31.429898 29974 master.cpp:1973] The newly elected leader is master@172.17.0.7:37568 with id 443ee691-d272-454c-90fe-959c95948252
I0707 19:30:31.429949 29974 master.cpp:1986] Elected as the leading master!
I0707 19:30:31.429968 29974 master.cpp:1673] Recovering from registrar
I0707 19:30:31.431020 29976 registrar.cpp:332] Recovering registrar
I0707 19:30:31.433168 29971 log.cpp:553] Attempting to start the writer
I0707 19:30:31.439359 29982 replica.cpp:493] Replica received implicit promise request from (21)@172.17.0.7:37568 with proposal 1
I0707 19:30:31.441862 29946 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni
W0707 19:30:31.443104 29946 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W0707 19:30:31.443366 29946 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges
I0707 19:30:31.457201 29975 slave.cpp:205] Agent started on 2)@172.17.0.7:37568
I0707 19:30:31.457254 29975 slave.cpp:206] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-zPIQS8/1""
I0707 19:30:31.458678 29982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 19.283309ms
I0707 19:30:31.458717 29982 replica.cpp:342] Persisted promised to 1
I0707 19:30:31.461284 29969 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0707 19:30:31.461690 29975 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 19:30:31.461866 29975 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 19:30:31.462319 29975 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:31.462396 29975 slave.cpp:602] Agent attributes: [  ]
I0707 19:30:31.464599 29975 slave.cpp:607] Agent hostname: 89b080073abb
I0707 19:30:31.466464 29978 replica.cpp:388] Replica received explicit promise request from (33)@172.17.0.7:37568 for position 0 with proposal 2
I0707 19:30:31.468361 29975 state.cpp:57] Recovering state from '/tmp/mesos-zPIQS8/1/meta'
I0707 19:30:31.468951 29975 status_update_manager.cpp:200] Recovering status update manager
I0707 19:30:31.469187 29975 containerizer.cpp:522] Recovering containerizer
I0707 19:30:31.472386 29969 provisioner.cpp:253] Provisioner recovery complete
I0707 19:30:31.473125 29969 slave.cpp:4856] Finished recovery
I0707 19:30:31.473996 29969 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0707 19:30:31.474643 29982 slave.cpp:969] New master detected at master@172.17.0.7:37568
I0707 19:30:31.474673 29982 slave.cpp:990] No credentials provided. Attempting to register without authentication
I0707 19:30:31.474726 29982 slave.cpp:1001] Detecting new master
I0707 19:30:31.474833 29982 status_update_manager.cpp:174] Pausing sending status updates
I0707 19:30:31.475157 29969 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0707 19:30:31.479303 29946 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni
W0707 19:30:31.484933 29946 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W0707 19:30:31.485230 29946 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges
I0707 19:30:31.492482 29978 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 25.968225ms
I0707 19:30:31.492543 29978 replica.cpp:712] Persisted action at 0
I0707 19:30:31.495333 29972 replica.cpp:537] Replica received write request for position 0 from (46)@172.17.0.7:37568
I0707 19:30:31.495918 29972 leveldb.cpp:436] Reading position from leveldb took 553942ns
I0707 19:30:31.505445 29973 slave.cpp:205] Agent started on 3)@172.17.0.7:37568
I0707 19:30:31.505492 29973 slave.cpp:206] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-zPIQS8/2""
I0707 19:30:31.506813 29973 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 19:30:31.506990 29973 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 19:30:31.507602 29973 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:31.507680 29973 slave.cpp:602] Agent attributes: [  ]
I0707 19:30:31.507695 29973 slave.cpp:607] Agent hostname: 89b080073abb
I0707 19:30:31.510499 29973 state.cpp:57] Recovering state from '/tmp/mesos-zPIQS8/2/meta'
I0707 19:30:31.511034 29973 status_update_manager.cpp:200] Recovering status update manager
I0707 19:30:31.511270 29973 containerizer.cpp:522] Recovering containerizer
I0707 19:30:31.514657 29984 provisioner.cpp:253] Provisioner recovery complete
I0707 19:30:31.515745 29970 slave.cpp:4856] Finished recovery
I0707 19:30:31.516332 29970 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0707 19:30:31.517103 29970 slave.cpp:969] New master detected at master@172.17.0.7:37568
I0707 19:30:31.517134 29970 slave.cpp:990] No credentials provided. Attempting to register without authentication
I0707 19:30:31.517190 29970 slave.cpp:1001] Detecting new master
I0707 19:30:31.517294 29970 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0707 19:30:31.517375 29970 status_update_manager.cpp:174] Pausing sending status updates
I0707 19:30:31.519979 29946 sched.cpp:226] Version: 1.0.0
I0707 19:30:31.521474 29980 sched.cpp:330] New master detected at master@172.17.0.7:37568
I0707 19:30:31.521586 29980 sched.cpp:341] No credentials provided. Attempting to register without authentication
I0707 19:30:31.521613 29980 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.7:37568
I0707 19:30:31.521769 29980 sched.cpp:853] Will retry registration in 898.210224ms if necessary
I0707 19:30:31.521977 29980 master.cpp:1500] Dropping 'mesos.scheduler.Call' message since not recovered yet
I0707 19:30:31.522469 29972 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 26.469135ms
I0707 19:30:31.522514 29972 replica.cpp:712] Persisted action at 0
I0707 19:30:31.523948 29980 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0707 19:30:31.538797 29972 slave.cpp:1529] Will retry registration in 1.972934225secs if necessary
I0707 19:30:31.538925 29972 master.cpp:1500] Dropping 'mesos.internal.RegisterSlaveMessage' message since not recovered yet
I0707 19:30:31.555934 29980 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 31.978704ms
I0707 19:30:31.556016 29980 replica.cpp:712] Persisted action at 0
I0707 19:30:31.556066 29980 replica.cpp:697] Replica learned NOP action at position 0
I0707 19:30:31.557960 29980 log.cpp:569] Writer started with ending position 0
I0707 19:30:31.561957 29976 leveldb.cpp:436] Reading position from leveldb took 90775ns
I0707 19:30:31.566825 29979 slave.cpp:1529] Will retry registration in 382.223275ms if necessary
I0707 19:30:31.566967 29979 master.cpp:1500] Dropping 'mesos.internal.RegisterSlaveMessage' message since not recovered yet
I0707 19:30:31.582073 29981 registrar.cpp:365] Successfully fetched the registry (0B) in 150.98496ms
I0707 19:30:31.582437 29981 registrar.cpp:464] Applied 1 operations in 94170ns; attempting to update the 'registry'
I0707 19:30:31.587924 29975 log.cpp:577] Attempting to append 168 bytes to the log
I0707 19:30:31.588234 29975 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0707 19:30:31.589561 29978 replica.cpp:537] Replica received write request for position 1 from (51)@172.17.0.7:37568
I0707 19:30:31.621119 29978 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 31.540172ms
I0707 19:30:31.621209 29978 replica.cpp:712] Persisted action at 1
I0707 19:30:31.623564 29978 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0707 19:30:31.656234 29978 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 32.657222ms
I0707 19:30:31.656424 29978 replica.cpp:712] Persisted action at 1
I0707 19:30:31.656786 29978 replica.cpp:697] Replica learned APPEND action at position 1
I0707 19:30:31.660815 29978 registrar.cpp:509] Successfully updated the 'registry' in 78.219008ms
I0707 19:30:31.661057 29978 registrar.cpp:395] Successfully recovered registrar
I0707 19:30:31.661593 29978 log.cpp:596] Attempting to truncate the log to 1
I0707 19:30:31.662271 29978 master.cpp:1781] Recovered 0 agents from the Registry (129B) ; allowing 10mins for agents to re-register
I0707 19:30:31.662566 29978 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0707 19:30:31.663004 29978 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
I0707 19:30:31.664005 29975 replica.cpp:537] Replica received write request for position 2 from (52)@172.17.0.7:37568
I0707 19:30:31.696493 29975 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 32.24974ms
I0707 19:30:31.696583 29975 replica.cpp:712] Persisted action at 2
I0707 19:30:31.698271 29984 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0707 19:30:31.731513 29984 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 32.894448ms
I0707 19:30:31.731775 29984 leveldb.cpp:399] Deleting ~1 keys from leveldb took 95908ns
I0707 19:30:31.732022 29984 replica.cpp:712] Persisted action at 2
I0707 19:30:31.732120 29984 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0707 19:30:31.950920 29984 slave.cpp:1529] Will retry registration in 3.638047644secs if necessary
I0707 19:30:31.951601 29983 master.cpp:4676] Registering agent at slave(3)@172.17.0.7:37568 (89b080073abb) with id 443ee691-d272-454c-90fe-959c95948252-S0
I0707 19:30:31.953089 29974 registrar.cpp:464] Applied 1 operations in 182983ns; attempting to update the 'registry'
I0707 19:30:31.957223 29983 log.cpp:577] Attempting to append 337 bytes to the log
I0707 19:30:31.957545 29983 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0707 19:30:31.958920 29983 replica.cpp:537] Replica received write request for position 3 from (53)@172.17.0.7:37568
I0707 19:30:31.989977 29983 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 30.902846ms
I0707 19:30:31.990154 29983 replica.cpp:712] Persisted action at 3
I0707 19:30:31.991781 29974 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0707 19:30:32.024132 29974 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 32.308737ms
I0707 19:30:32.024305 29974 replica.cpp:712] Persisted action at 3
I0707 19:30:32.024449 29974 replica.cpp:697] Replica learned APPEND action at position 3
I0707 19:30:32.027683 29975 registrar.cpp:509] Successfully updated the 'registry' in 74.444032ms
I0707 19:30:32.029734 29974 log.cpp:596] Attempting to truncate the log to 3
I0707 19:30:32.030093 29974 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0707 19:30:32.030804 29974 slave.cpp:3760] Received ping from slave-observer(1)@172.17.0.7:37568
I0707 19:30:32.031373 29974 slave.cpp:1169] Registered with master master@172.17.0.7:37568; given agent ID 443ee691-d272-454c-90fe-959c95948252-S0
I0707 19:30:32.031460 29974 fetcher.cpp:86] Clearing fetcher cache
I0707 19:30:32.032008 29974 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/mesos-zPIQS8/2/meta/slaves/443ee691-d272-454c-90fe-959c95948252-S0/slave.info'
I0707 19:30:32.031088 29975 master.cpp:4745] Registered agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:32.033082 29975 hierarchical.cpp:478] Added agent 443ee691-d272-454c-90fe-959c95948252-S0 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0707 19:30:32.033608 29975 hierarchical.cpp:1537] No allocations performed
I0707 19:30:32.033747 29975 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S0 in 584676ns
I0707 19:30:32.034116 29975 status_update_manager.cpp:181] Resuming sending status updates
I0707 19:30:32.034010 29974 slave.cpp:1229] Forwarding total oversubscribed resources 
I0707 19:30:32.034950 29974 master.cpp:5128] Received update of agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb) with total oversubscribed resources 
I0707 19:30:32.035320 29975 replica.cpp:537] Replica received write request for position 4 from (54)@172.17.0.7:37568
I0707 19:30:32.036041 29971 hierarchical.cpp:542] Agent 443ee691-d272-454c-90fe-959c95948252-S0 (89b080073abb) updated with oversubscribed resources  (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0707 19:30:32.036212 29971 hierarchical.cpp:1537] No allocations performed
I0707 19:30:32.036327 29971 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S0 in 212809ns
I0707 19:30:32.196679 29976 master.cpp:4676] Registering agent at slave(2)@172.17.0.7:37568 (89b080073abb) with id 443ee691-d272-454c-90fe-959c95948252-S1
I0707 19:30:32.196384 29979 slave.cpp:1529] Will retry registration in 1.893622708secs if necessary
I0707 19:30:32.197633 29976 registrar.cpp:464] Applied 1 operations in 273890ns; attempting to update the 'registry'
I0707 19:30:32.343791 29979 hierarchical.cpp:1537] No allocations performed
I0707 19:30:32.344105 29979 hierarchical.cpp:1172] Performed allocation for 1 agents in 555357ns
I0707 19:30:32.373800 29975 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 338.056804ms
I0707 19:30:32.373987 29975 replica.cpp:712] Persisted action at 4
I0707 19:30:32.387712 29973 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0707 19:30:32.420934 29981 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.7:37568
I0707 19:30:32.421331 29981 sched.cpp:853] Will retry registration in 2.058099434secs if necessary
I0707 19:30:32.421792 29981 master.cpp:2550] Received SUBSCRIBE call for framework 'Dynamic Reservation Framework (C++)' at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:32.421934 29981 master.cpp:2012] Authorizing framework principal 'test' to receive offers for role 'test'
I0707 19:30:32.423535 29981 master.cpp:2626] Subscribing framework Dynamic Reservation Framework (C++) with checkpointing disabled and capabilities [  ]
I0707 19:30:32.425323 29976 hierarchical.cpp:271] Added framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:32.426686 29973 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 38.63187ms
I0707 19:30:32.426865 29973 leveldb.cpp:399] Deleting ~2 keys from leveldb took 95262ns
I0707 19:30:32.426981 29973 replica.cpp:712] Persisted action at 4
I0707 19:30:32.427096 29973 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0707 19:30:32.428614 29973 log.cpp:577] Attempting to append 503 bytes to the log
I0707 19:30:32.426307 29981 sched.cpp:743] Framework registered with 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:32.428905 29981 dynamic_reservation_framework.cpp:73] Registered!
I0707 19:30:32.429059 29981 sched.cpp:757] Scheduler::registered took 167468ns
I0707 19:30:32.429239 29981 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0707 19:30:32.431745 29976 hierarchical.cpp:1632] No inverse offers to send out!
I0707 19:30:32.432610 29984 master.cpp:5835] Sending 1 offers to framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:32.433627 29984 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O0 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:32.434248 29984 sched.cpp:917] Scheduler::resourceOffers took 642030ns
I0707 19:30:32.436048 29984 master.cpp:3468] Processing ACCEPT call for offers: [ 443ee691-d272-454c-90fe-959c95948252-O0 ] on agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb) for framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:32.436368 29984 master.cpp:3144] Authorizing principal 'test' to reserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 19:30:32.438547 29976 hierarchical.cpp:1172] Performed allocation for 1 agents in 12.203221ms
I0707 19:30:32.432860 29981 replica.cpp:537] Replica received write request for position 5 from (55)@172.17.0.7:37568
I0707 19:30:32.439970 29984 master.cpp:3695] Applying RESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 to agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:32.440765 29984 master.cpp:7098] Sending checkpointed resources cpus(test, test):1; mem(test, test):128 to agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:32.444211 29976 hierarchical.cpp:683] Updated allocation of framework 443ee691-d272-454c-90fe-959c95948252-0000 on agent 443ee691-d272-454c-90fe-959c95948252-S0 from cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] to cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128
I0707 19:30:32.444527 29984 slave.cpp:2600] Updated checkpointed resources from  to cpus(test, test):1; mem(test, test):128
I0707 19:30:32.445664 29976 hierarchical.cpp:924] Recovered cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 443ee691-d272-454c-90fe-959c95948252-S0 from framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:32.467499 29981 leveldb.cpp:341] Persisting action (522 bytes) to leveldb took 28.613107ms
I0707 19:30:32.467705 29981 replica.cpp:712] Persisted action at 5
I0707 19:30:32.483840 29971 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0707 19:30:32.511849 29971 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 27.859875ms
I0707 19:30:32.512235 29971 replica.cpp:712] Persisted action at 5
I0707 19:30:32.512511 29971 replica.cpp:697] Replica learned APPEND action at position 5
I0707 19:30:32.516393 29971 registrar.cpp:509] Successfully updated the 'registry' in 318.636032ms
I0707 19:30:32.517113 29971 log.cpp:596] Attempting to truncate the log to 5
I0707 19:30:32.518293 29971 master.cpp:4745] Registered agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:32.518659 29971 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0707 19:30:32.519564 29971 hierarchical.cpp:478] Added agent 443ee691-d272-454c-90fe-959c95948252-S1 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0707 19:30:32.520804 29971 hierarchical.cpp:1632] No inverse offers to send out!
I0707 19:30:32.521106 29971 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S1 in 1.298948ms
I0707 19:30:32.521436 29971 slave.cpp:1169] Registered with master master@172.17.0.7:37568; given agent ID 443ee691-d272-454c-90fe-959c95948252-S1
I0707 19:30:32.521669 29971 fetcher.cpp:86] Clearing fetcher cache
I0707 19:30:32.522266 29971 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/mesos-zPIQS8/1/meta/slaves/443ee691-d272-454c-90fe-959c95948252-S1/slave.info'
I0707 19:30:32.523712 29971 slave.cpp:1229] Forwarding total oversubscribed resources 
I0707 19:30:32.523080 29981 master.cpp:5835] Sending 1 offers to framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:32.524303 29979 status_update_manager.cpp:181] Resuming sending status updates
I0707 19:30:32.524688 29981 master.cpp:5128] Received update of agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb) with total oversubscribed resources 
I0707 19:30:32.525228 29970 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O1 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:32.525902 29970 sched.cpp:917] Scheduler::resourceOffers took 691140ns
I0707 19:30:32.525388 29971 slave.cpp:3760] Received ping from slave-observer(2)@172.17.0.7:37568
I0707 19:30:32.527039 29982 replica.cpp:537] Replica received write request for position 6 from (58)@172.17.0.7:37568
I0707 19:30:32.528058 29981 master.cpp:3468] Processing ACCEPT call for offers: [ 443ee691-d272-454c-90fe-959c95948252-O1 ] on agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb) for framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:32.528295 29979 hierarchical.cpp:542] Agent 443ee691-d272-454c-90fe-959c95948252-S1 (89b080073abb) updated with oversubscribed resources  (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000])
I0707 19:30:32.529708 29979 hierarchical.cpp:1537] No allocations performed
I0707 19:30:32.529754 29979 hierarchical.cpp:1632] No inverse offers to send out!
I0707 19:30:32.529820 29979 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S1 in 1.473485ms
I0707 19:30:32.529899 29981 master.cpp:3144] Authorizing principal 'test' to reserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 19:30:32.531919 29984 master.cpp:3695] Applying RESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 to agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:32.532374 29984 master.cpp:7098] Sending checkpointed resources cpus(test, test):1; mem(test, test):128 to agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:32.534451 29977 hierarchical.cpp:683] Updated allocation of framework 443ee691-d272-454c-90fe-959c95948252-0000 on agent 443ee691-d272-454c-90fe-959c95948252-S1 from cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] to cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128
I0707 19:30:32.537169 29980 hierarchical.cpp:924] Recovered cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 443ee691-d272-454c-90fe-959c95948252-S1 from framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:32.535399 29984 slave.cpp:2600] Updated checkpointed resources from  to cpus(test, test):1; mem(test, test):128
I0707 19:30:32.554222 29982 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 27.170492ms
I0707 19:30:32.554395 29982 replica.cpp:712] Persisted action at 6
I0707 19:30:32.556767 29970 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0707 19:30:32.579500 29970 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 22.578289ms
I0707 19:30:32.579659 29970 leveldb.cpp:399] Deleting ~2 keys from leveldb took 86499ns
I0707 19:30:32.579692 29970 replica.cpp:712] Persisted action at 6
I0707 19:30:32.579746 29970 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0707 19:30:33.347929 29970 hierarchical.cpp:1632] No inverse offers to send out!
I0707 19:30:33.349206 29970 hierarchical.cpp:1172] Performed allocation for 2 agents in 3.521151ms
I0707 19:30:33.349076 29977 master.cpp:5835] Sending 2 offers to framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:33.350098 29977 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O2 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:33.350462 29977 dynamic_reservation_framework.cpp:150] Launching task 0 using offer 443ee691-d272-454c-90fe-959c95948252-O2
I0707 19:30:33.350879 29977 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O3 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:33.351199 29977 dynamic_reservation_framework.cpp:150] Launching task 1 using offer 443ee691-d272-454c-90fe-959c95948252-O3
I0707 19:30:33.351398 29977 sched.cpp:917] Scheduler::resourceOffers took 1.321372ms
I0707 19:30:33.352967 29977 master.cpp:3468] Processing ACCEPT call for offers: [ 443ee691-d272-454c-90fe-959c95948252-O2 ] on agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb) for framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:33.353111 29977 master.cpp:3106] Authorizing framework principal 'test' to launch task 0
I0707 19:30:33.355710 29977 master.cpp:3468] Processing ACCEPT call for offers: [ 443ee691-d272-454c-90fe-959c95948252-O3 ] on agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb) for framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:33.355829 29977 master.cpp:3106] Authorizing framework principal 'test' to launch task 1
I0707 19:30:33.359690 29977 master.cpp:7565] Adding task 0 with resources cpus(test, test):1; mem(test, test):128 on agent 443ee691-d272-454c-90fe-959c95948252-S0 (89b080073abb)
I0707 19:30:33.359900 29977 master.cpp:3957] Launching task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 with resources cpus(test, test):1; mem(test, test):128 on agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:33.360642 29970 slave.cpp:1569] Got assigned task 0 for framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.361142 29970 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 19:30:33.362133 29983 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: cpus(test, test):1; mem(test, test):128) on agent 443ee691-d272-454c-90fe-959c95948252-S0 from framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.362866 29970 slave.cpp:1688] Launching task 0 for framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.362995 29970 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 19:30:33.367036 29977 master.cpp:7565] Adding task 1 with resources cpus(test, test):1; mem(test, test):128 on agent 443ee691-d272-454c-90fe-959c95948252-S1 (89b080073abb)
I0707 19:30:33.367182 29977 master.cpp:3957] Launching task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 with resources cpus(test, test):1; mem(test, test):128 on agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:33.367564 29978 slave.cpp:1569] Got assigned task 1 for framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.367811 29978 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 19:30:33.368335 29978 slave.cpp:1688] Launching task 1 for framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.368511 29978 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 19:30:33.373251 29970 paths.cpp:528] Trying to chown '/tmp/mesos-zPIQS8/2/slaves/443ee691-d272-454c-90fe-959c95948252-S0/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/0/runs/5f6efb5e-5357-4514-964a-5af3d0ec33f1' to user 'mesos'
I0707 19:30:33.376608 29977 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: cpus(test, test):1; mem(test, test):128) on agent 443ee691-d272-454c-90fe-959c95948252-S1 from framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.376976 29978 paths.cpp:528] Trying to chown '/tmp/mesos-zPIQS8/1/slaves/443ee691-d272-454c-90fe-959c95948252-S1/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/1/runs/34287f06-c6d4-4ab6-b706-5abf0e314655' to user 'mesos'
I0707 19:30:33.378888 29970 slave.cpp:5748] Launching executor 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-zPIQS8/2/slaves/443ee691-d272-454c-90fe-959c95948252-S0/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/0/runs/5f6efb5e-5357-4514-964a-5af3d0ec33f1'
I0707 19:30:33.380379 29980 containerizer.cpp:781] Starting container '5f6efb5e-5357-4514-964a-5af3d0ec33f1' for executor '0' of framework '443ee691-d272-454c-90fe-959c95948252-0000'
I0707 19:30:33.383648 29978 slave.cpp:5748] Launching executor 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-zPIQS8/1/slaves/443ee691-d272-454c-90fe-959c95948252-S1/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/1/runs/34287f06-c6d4-4ab6-b706-5abf0e314655'
I0707 19:30:33.384750 29971 containerizer.cpp:781] Starting container '34287f06-c6d4-4ab6-b706-5abf0e314655' for executor '1' of framework '443ee691-d272-454c-90fe-959c95948252-0000'
I0707 19:30:33.384395 29978 slave.cpp:1914] Queuing task '1' for executor '1' of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.385623 29978 slave.cpp:922] Successfully attached file '/tmp/mesos-zPIQS8/1/slaves/443ee691-d272-454c-90fe-959c95948252-S1/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/1/runs/34287f06-c6d4-4ab6-b706-5abf0e314655'
I0707 19:30:33.399920 29979 containerizer.cpp:1284] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/mesos-1.0.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.0.0\/_build\/src\/mesos-executor""}"" --help=""false"" --pipe_read=""9"" --pipe_write=""12"" --pre_exec_commands=""[]"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/mesos-zPIQS8/2/slaves/443ee691-d272-454c-90fe-959c95948252-S0/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/0/runs/5f6efb5e-5357-4514-964a-5af3d0ec33f1""'
I0707 19:30:33.403643 29971 containerizer.cpp:1284] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/mesos-1.0.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.0.0\/_build\/src\/mesos-executor""}"" --help=""false"" --pipe_read=""13"" --pipe_write=""14"" --pre_exec_commands=""[]"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/mesos-zPIQS8/1/slaves/443ee691-d272-454c-90fe-959c95948252-S1/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/1/runs/34287f06-c6d4-4ab6-b706-5abf0e314655""'
I0707 19:30:33.406067 29979 launcher.cpp:126] Forked child with pid '29991' for container '5f6efb5e-5357-4514-964a-5af3d0ec33f1'
I0707 19:30:33.407141 29971 launcher.cpp:126] Forked child with pid '29992' for container '34287f06-c6d4-4ab6-b706-5abf0e314655'
I0707 19:30:33.405388 29970 slave.cpp:1914] Queuing task '0' for executor '0' of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.408761 29970 slave.cpp:922] Successfully attached file '/tmp/mesos-zPIQS8/2/slaves/443ee691-d272-454c-90fe-959c95948252-S0/frameworks/443ee691-d272-454c-90fe-959c95948252-0000/executors/0/runs/5f6efb5e-5357-4514-964a-5af3d0ec33f1'
I0707 19:30:33.512244 29979 slave.cpp:1529] Will retry registration in 109.135061ms if necessary
I0707 19:30:33.512642 29979 master.cpp:4676] Registering agent at slave(1)@172.17.0.7:37568 (89b080073abb) with id 443ee691-d272-454c-90fe-959c95948252-S2
I0707 19:30:33.513365 29979 registrar.cpp:464] Applied 1 operations in 161276ns; attempting to update the 'registry'
I0707 19:30:33.522032 29979 log.cpp:577] Attempting to append 669 bytes to the log
I0707 19:30:33.522296 29979 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0707 19:30:33.525583 29975 replica.cpp:537] Replica received write request for position 7 from (67)@172.17.0.7:37568
I0707 19:30:33.556705 29975 leveldb.cpp:341] Persisting action (688 bytes) to leveldb took 31.103148ms
I0707 19:30:33.556799 29975 replica.cpp:712] Persisted action at 7
I0707 19:30:33.558219 29982 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I0707 19:30:33.590251 29982 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 32.026154ms
I0707 19:30:33.590347 29982 replica.cpp:712] Persisted action at 7
I0707 19:30:33.590395 29982 replica.cpp:697] Replica learned APPEND action at position 7
I0707 19:30:33.595444 29975 log.cpp:596] Attempting to truncate the log to 7
I0707 19:30:33.595649 29975 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0707 19:30:33.596961 29975 replica.cpp:537] Replica received write request for position 8 from (68)@172.17.0.7:37568
I0707 19:30:33.595394 29982 registrar.cpp:509] Successfully updated the 'registry' in 81.936128ms
I0707 19:30:33.598295 29969 master.cpp:4745] Registered agent 443ee691-d272-454c-90fe-959c95948252-S2 at slave(1)@172.17.0.7:37568 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:33.599622 29984 hierarchical.cpp:478] Added agent 443ee691-d272-454c-90fe-959c95948252-S2 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0707 19:30:33.600498 29978 slave.cpp:1169] Registered with master master@172.17.0.7:37568; given agent ID 443ee691-d272-454c-90fe-959c95948252-S2
I0707 19:30:33.600530 29978 fetcher.cpp:86] Clearing fetcher cache
I0707 19:30:33.601135 29978 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/mesos-zPIQS8/0/meta/slaves/443ee691-d272-454c-90fe-959c95948252-S2/slave.info'
I0707 19:30:33.601351 29981 status_update_manager.cpp:181] Resuming sending status updates
I0707 19:30:33.601753 29978 slave.cpp:1229] Forwarding total oversubscribed resources 
I0707 19:30:33.601856 29978 slave.cpp:3760] Received ping from slave-observer(3)@172.17.0.7:37568
I0707 19:30:33.602028 29978 master.cpp:5128] Received update of agent 443ee691-d272-454c-90fe-959c95948252-S2 at slave(1)@172.17.0.7:37568 (89b080073abb) with total oversubscribed resources 
I0707 19:30:33.602550 29984 hierarchical.cpp:1632] No inverse offers to send out!
I0707 19:30:33.602638 29984 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S2 in 2.976114ms
I0707 19:30:33.602772 29984 hierarchical.cpp:542] Agent 443ee691-d272-454c-90fe-959c95948252-S2 (89b080073abb) updated with oversubscribed resources  (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000])
I0707 19:30:33.603118 29984 hierarchical.cpp:1537] No allocations performed
I0707 19:30:33.603154 29984 hierarchical.cpp:1632] No inverse offers to send out!
I0707 19:30:33.603205 29984 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S2 in 384101ns
I0707 19:30:33.604754 29984 master.cpp:5835] Sending 1 offers to framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:33.605154 29984 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O4 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:33.605554 29984 sched.cpp:917] Scheduler::resourceOffers took 415259ns
I0707 19:30:33.606514 29984 master.cpp:3468] Processing ACCEPT call for offers: [ 443ee691-d272-454c-90fe-959c95948252-O4 ] on agent 443ee691-d272-454c-90fe-959c95948252-S2 at slave(1)@172.17.0.7:37568 (89b080073abb) for framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:33.606920 29984 master.cpp:3144] Authorizing principal 'test' to reserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 19:30:33.616320 29979 master.cpp:3695] Applying RESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 to agent 443ee691-d272-454c-90fe-959c95948252-S2 at slave(1)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:33.616832 29979 master.cpp:7098] Sending checkpointed resources cpus(test, test):1; mem(test, test):128 to agent 443ee691-d272-454c-90fe-959c95948252-S2 at slave(1)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:33.620553 29979 hierarchical.cpp:683] Updated allocation of framework 443ee691-d272-454c-90fe-959c95948252-0000 on agent 443ee691-d272-454c-90fe-959c95948252-S2 from cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] to cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128
I0707 19:30:33.621477 29979 hierarchical.cpp:924] Recovered cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 443ee691-d272-454c-90fe-959c95948252-S2 from framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.626524 29969 slave.cpp:2600] Updated checkpointed resources from  to cpus(test, test):1; mem(test, test):128
I0707 19:30:33.632431 29975 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 35.442219ms
I0707 19:30:33.632493 29975 replica.cpp:712] Persisted action at 8
I0707 19:30:33.634073 29975 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I0707 19:30:33.662283 29975 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 28.207233ms
I0707 19:30:33.662469 29975 leveldb.cpp:399] Deleting ~2 keys from leveldb took 111311ns
I0707 19:30:33.662504 29975 replica.cpp:712] Persisted action at 8
I0707 19:30:33.662557 29975 replica.cpp:697] Replica learned TRUNCATE action at position 8
I0707 19:30:33.750787 29992 exec.cpp:161] Version: 1.0.0
I0707 19:30:33.758561 29975 slave.cpp:2902] Got registration for executor '1' of framework 443ee691-d272-454c-90fe-959c95948252-0000 from executor(1)@172.17.0.7:38689
I0707 19:30:33.766023 29975 slave.cpp:2079] Sending queued task '1' to executor '1' of framework 443ee691-d272-454c-90fe-959c95948252-0000 at executor(1)@172.17.0.7:38689
I0707 19:30:33.774989 30060 exec.cpp:236] Executor registered on agent 443ee691-d272-454c-90fe-959c95948252-S1
Received SUBSCRIBED event
Subscribed executor on 89b080073abb
Received LAUNCH event
Starting task 1
/mesos/mesos-1.0.0/_build/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""echo hello""}"" --help=""false"" --unshare_namespace_mnt=""false""
Forked command at 30062
I0707 19:30:33.842072 29981 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from executor(1)@172.17.0.7:38689
I0707 19:30:33.847599 29974 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.847682 29974 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.849678 29974 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to the agent
I0707 19:30:33.850505 29973 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to master@172.17.0.7:37568
I0707 19:30:33.850747 29973 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.850805 29973 slave.cpp:3588] Sending acknowledgement for status update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to executor(1)@172.17.0.7:38689
I0707 19:30:33.851368 29973 master.cpp:5273] Status update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:33.852810 29973 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.853047 29973 master.cpp:6959] Updating the state of task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0707 19:30:33.853245 29973 dynamic_reservation_framework.cpp:211] Task 1 is in state TASK_RUNNING
I0707 19:30:33.853276 29973 sched.cpp:1025] Scheduler::statusUpdate took 41044ns
I0707 19:30:33.854579 29970 master.cpp:4388] Processing ACKNOWLEDGE call 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14 for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 on agent 443ee691-d272-454c-90fe-959c95948252-S1
I0707 19:30:33.855051 29970 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:33.855461 29972 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 1a550a5a-a5c4-4d37-9018-f6a41ca4eb14) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.054215 30083 exec.cpp:161] Version: 1.0.0
I0707 19:30:34.069156 29972 slave.cpp:2902] Got registration for executor '0' of framework 443ee691-d272-454c-90fe-959c95948252-0000 from executor(1)@172.17.0.7:37892
I0707 19:30:34.077040 29972 slave.cpp:2079] Sending queued task '0' to executor '0' of framework 443ee691-d272-454c-90fe-959c95948252-0000 at executor(1)@172.17.0.7:37892
I0707 19:30:34.082531 30081 exec.cpp:236] Executor registered on agent 443ee691-d272-454c-90fe-959c95948252-S0
Received SUBSCRIBED event
Subscribed executor on 89b080073abb
Received LAUNCH event
Starting task 0
/mesos/mesos-1.0.0/_build/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""echo hello""}"" --help=""false"" --unshare_namespace_mnt=""false""
Forked command at 30093
I0707 19:30:34.121834 29971 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from executor(1)@172.17.0.7:37892
I0707 19:30:34.125562 29982 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.125622 29982 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.126216 29982 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to the agent
I0707 19:30:34.126847 29971 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to master@172.17.0.7:37568
I0707 19:30:34.127034 29971 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.127084 29971 slave.cpp:3588] Sending acknowledgement for status update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to executor(1)@172.17.0.7:37892
I0707 19:30:34.128938 29971 master.cpp:5273] Status update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:34.128993 29971 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.129176 29971 master.cpp:6959] Updating the state of task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0707 19:30:34.129348 29971 dynamic_reservation_framework.cpp:211] Task 0 is in state TASK_RUNNING
I0707 19:30:34.129369 29971 sched.cpp:1025] Scheduler::statusUpdate took 31771ns
I0707 19:30:34.130749 29971 master.cpp:4388] Processing ACKNOWLEDGE call 4b52fae2-8c9c-4dd5-8459-729d84b86a2e for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 on agent 443ee691-d272-454c-90fe-959c95948252-S0
I0707 19:30:34.131034 29971 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.131378 29971 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 4b52fae2-8c9c-4dd5-8459-729d84b86a2e) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
hello
Command exited with status 0 (pid: 30062)
I0707 19:30:34.343664 29977 slave.cpp:3285] Handling status update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from executor(1)@172.17.0.7:38689
I0707 19:30:34.346058 29984 slave.cpp:6088] Terminating task 1
I0707 19:30:34.350986 29984 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.351263 29984 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to the agent
I0707 19:30:34.353025 29984 slave.cpp:3678] Forwarding the update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to master@172.17.0.7:37568
I0707 19:30:34.353231 29984 slave.cpp:3572] Status update manager successfully handled status update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.353282 29984 slave.cpp:3588] Sending acknowledgement for status update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to executor(1)@172.17.0.7:38689
I0707 19:30:34.353657 29969 master.cpp:5273] Status update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:34.353708 29969 master.cpp:5321] Forwarding status update TASK_FINISHED (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.353883 29969 master.cpp:6959] Updating the state of task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0707 19:30:34.354302 29969 dynamic_reservation_framework.cpp:208] Task 1 is finished at agent 443ee691-d272-454c-90fe-959c95948252-S1
I0707 19:30:34.354327 29969 sched.cpp:1025] Scheduler::statusUpdate took 42112ns
I0707 19:30:34.354652 29969 master.cpp:4388] Processing ACKNOWLEDGE call 08fecbb0-8539-4123-916a-37cda28ec934 for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 on agent 443ee691-d272-454c-90fe-959c95948252-S1
I0707 19:30:34.354730 29969 master.cpp:7025] Removing task 1 with resources cpus(test, test):1; mem(test, test):128 of framework 443ee691-d272-454c-90fe-959c95948252-0000 on agent 443ee691-d272-454c-90fe-959c95948252-S1 at slave(2)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:34.357210 29977 hierarchical.cpp:1632] No inverse offers to send out!
I0707 19:30:34.357326 29977 hierarchical.cpp:1172] Performed allocation for 3 agents in 7.731795ms
I0707 19:30:34.358654 29969 master.cpp:5835] Sending 3 offers to framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568
I0707 19:30:34.359386 29969 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O5 with cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:34.359557 29969 dynamic_reservation_framework.cpp:164] The task on 443ee691-d272-454c-90fe-959c95948252-S0 is running, waiting for task done
I0707 19:30:34.359571 29969 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O6 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 19:30:34.359845 29969 dynamic_reservation_framework.cpp:150] Launching task 2 using offer 443ee691-d272-454c-90fe-959c95948252-O6
I0707 19:30:34.360051 29969 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O7 with cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
F0707 19:30:34.360167 29969 dynamic_reservation_framework.cpp:135] Check failed: reserved.contains(taskResources) 
*** Check failure stack trace: ***
I0707 19:30:34.361974 29975 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.362541 29975 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.362946 29977 hierarchical.cpp:924] Recovered cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142) on agent 443ee691-d272-454c-90fe-959c95948252-S1 from framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.363346 29975 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 08fecbb0-8539-4123-916a-37cda28ec934) for task 1 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.363533 29975 slave.cpp:6129] Completing task 1
hello
Command exited with status 0 (pid: 30093)
I0707 19:30:34.421588 29981 slave.cpp:3285] Handling status update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from executor(1)@172.17.0.7:37892
I0707 19:30:34.424176 29981 slave.cpp:6088] Terminating task 0
I0707 19:30:34.427489 29970 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.427922 29970 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to the agent
I0707 19:30:34.428794 29970 slave.cpp:3678] Forwarding the update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to master@172.17.0.7:37568
I0707 19:30:34.429209 29970 slave.cpp:3572] Status update manager successfully handled status update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.429819 29970 slave.cpp:3588] Sending acknowledgement for status update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 to executor(1)@172.17.0.7:37892
I0707 19:30:34.429739 29971 master.cpp:5273] Status update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 from agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb)
I0707 19:30:34.430163 29971 master.cpp:5321] Forwarding status update TASK_FINISHED (UUID: 14598eb7-e5a3-4aec-9b92-abe6c26957c9) for task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000
I0707 19:30:34.430559 29971 master.cpp:6959] Updating the state of task 0 of framework 443ee691-d272-454c-90fe-959c95948252-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0707 19:30:34.432127 29971 hierarchical.cpp:924] Recovered cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142) on agent 443ee691-d272-454c-90fe-959c95948252-S0 from framework 443ee691-d272-454c-90fe-959c95948252-0000
    @     0x2b569c2093ed  google::LogMessage::Fail()
    @     0x2b569c2087ce  google::LogMessage::SendToLog()
    @     0x2b569c2090ad  google::LogMessage::Flush()
    @     0x2b569c20c528  google::LogMessageFatal::~LogMessageFatal()
    @           0x45dba0  DynamicReservationScheduler::resourceOffers()
    @     0x2b569b0a75d8  mesos::internal::SchedulerProcess::resourceOffers()
    @     0x2b569b0bb3cf  ProtobufProcess<>::handler2<>()
    @     0x2b569b0bcdcd  _ZNSt5_BindIFPFvPN5mesos8internal16SchedulerProcessEMS2_FvRKN7process4UPIDERKSt6vectorINS0_5OfferESaIS9_EERKS8_ISsSaISsEEEMNS1_21ResourceOffersMessageEKFRKN6google8protobuf16RepeatedPtrFieldIS9_EEvEMSK_KFRKNSN_ISsEEvES7_RKSsES3_SJ_SS_SX_St12_PlaceholderILi1EES12_ILi2EEEE6__callIvJS7_SZ_EJLm0ELm1ELm2ELm3ELm4ELm5EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @     0x2b569b0bcc16  _ZNSt5_BindIFPFvPN5mesos8internal16SchedulerProcessEMS2_FvRKN7process4UPIDERKSt6vectorINS0_5OfferESaIS9_EERKS8_ISsSaISsEEEMNS1_21ResourceOffersMessageEKFRKN6google8protobuf16RepeatedPtrFieldIS9_EEvEMSK_KFRKNSN_ISsEEvES7_RKSsES3_SJ_SS_SX_St12_PlaceholderILi1EES12_ILi2EEEEclIJS7_SZ_EvEET0_DpOT_
    @     0x2b569b0bc9b7  std::_Function_handler<>::_M_invoke()
    @     0x2b569a964bb0  std::function<>::operator()()
    @     0x2b569b0a2acb  ProtobufProcess<>::visit()
    @     0x2b569b0a2be7  ProtobufProcess<>::visit()
    @     0x2b569af25d8e  process::MessageEvent::visit()
    @     0x2b569a95aad1  process::ProcessBase::serve()
    @     0x2b569c120984  process::ProcessManager::resume()
    @     0x2b569c12b6fc  process::ProcessManager::init_threads()::$_0::operator()()
    @     0x2b569c12b605  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvE3$_0vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x2b569c12b5d5  std::_Bind_simple<>::operator()()
    @     0x2b569c12b5ac  std::thread::_Impl<>::_M_run()
    @     0x2b569d80aa60  (unknown)
    @     0x2b569df81184  start_thread
    @     0x2b569e29137d  (unknown)
../../src/tests/script.cpp:80: Failure
Failed
dynamic_reservation_framework_test.sh terminated with signal Aborted
[  FAILED  ] ExamplesTest.DynamicReservationFramework (7146 ms)
{code}

Logs from a previous good run:
{code}
[ RUN      ] ExamplesTest.DynamicReservationFramework
Using temporary directory '/tmp/ExamplesTest_DynamicReservationFramework_mXcx0v'
/mesos/mesos-1.0.0/src/tests/dynamic_reservation_framework_test.sh: line 19: /mesos/mesos-1.0.0/_build/src/colors.sh: No such file or directory
/mesos/mesos-1.0.0/src/tests/dynamic_reservation_framework_test.sh: line 20: /mesos/mesos-1.0.0/_build/src/atexit.sh: No such file or directory
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0707 18:06:58.103094 31500 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I0707 18:06:58.116397 31500 process.cpp:1066] libprocess is initialized on 172.17.0.7:39581 with 16 worker threads
I0707 18:06:58.116585 31500 logging.cpp:199] Logging to STDERR
I0707 18:06:58.206984 31500 leveldb.cpp:174] Opened db in 84.981237ms
I0707 18:06:58.240731 31500 leveldb.cpp:181] Compacted db in 33.702091ms
I0707 18:06:58.240833 31500 leveldb.cpp:196] Created db iterator in 66372ns
I0707 18:06:58.240985 31500 leveldb.cpp:202] Seeked to beginning of db in 4465ns
I0707 18:06:58.241019 31500 leveldb.cpp:271] Iterated through 0 keys in the db in 450ns
I0707 18:06:58.241217 31500 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0707 18:06:58.244328 31500 local.cpp:255] Creating default 'local' authorizer
I0707 18:06:58.244483 31529 recover.cpp:451] Starting replica recovery
I0707 18:06:58.245290 31529 recover.cpp:477] Replica is in EMPTY status
I0707 18:06:58.248134 31529 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.7:39581
I0707 18:06:58.249553 31534 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0707 18:06:58.250586 31525 recover.cpp:568] Updating replica status to STARTING
I0707 18:06:58.252516 31533 master.cpp:382] Master 7892fbb2-1ac1-450f-8576-10c1df35f765 (753c2ae3a486) started on 172.17.0.7:39581
I0707 18:06:58.252545 31533 master.cpp:384] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""false"" --authenticate_frameworks=""false"" --authenticate_http=""false"" --authenticate_http_frameworks=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_DynamicReservationFramework_mXcx0v/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""20secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.0.0/src/webui"" --work_dir=""/tmp/mesos-IFR4rG"" --zk_session_timeout=""10secs""
I0707 18:06:58.253370 31533 master.cpp:436] Master allowing unauthenticated frameworks to register
I0707 18:06:58.253386 31533 master.cpp:450] Master allowing unauthenticated agents to register
I0707 18:06:58.253397 31533 master.cpp:464] Master allowing HTTP frameworks to register without authentication
I0707 18:06:58.253464 31533 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_DynamicReservationFramework_mXcx0v/credentials'
W0707 18:06:58.253582 31533 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_DynamicReservationFramework_mXcx0v/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0707 18:06:58.253777 31533 master.cpp:506] Using default 'crammd5' authenticator
I0707 18:06:58.253957 31533 authenticator.cpp:519] Initializing server SASL
I0707 18:06:58.259035 31533 auxprop.cpp:73] Initialized in-memory auxiliary property plugin
I0707 18:06:58.259171 31533 master.cpp:705] Authorization enabled
I0707 18:06:58.260638 31538 hierarchical.cpp:151] Initialized hierarchical allocator process
I0707 18:06:58.260769 31538 whitelist_watcher.cpp:77] No whitelist given
I0707 18:06:58.261309 31500 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni
W0707 18:06:58.266885 31500 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W0707 18:06:58.267235 31500 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges
I0707 18:06:58.274190 31525 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.019016ms
I0707 18:06:58.274230 31525 replica.cpp:320] Persisted replica status to STARTING
I0707 18:06:58.274695 31527 recover.cpp:477] Replica is in STARTING status
I0707 18:06:58.276667 31531 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (15)@172.17.0.7:39581
I0707 18:06:58.277261 31538 recover.cpp:197] Received a recover response from a replica in STARTING status
I0707 18:06:58.277840 31536 recover.cpp:568] Updating replica status to VOTING
I0707 18:06:58.279667 31532 slave.cpp:205] Agent started on 1)@172.17.0.7:39581
I0707 18:06:58.279690 31532 slave.cpp:206] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-IFR4rG/0""
I0707 18:06:58.280802 31532 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 18:06:58.280975 31532 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 18:06:58.281785 31532 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:06:58.281886 31532 slave.cpp:602] Agent attributes: [  ]
I0707 18:06:58.281918 31532 slave.cpp:607] Agent hostname: 753c2ae3a486
I0707 18:06:58.285878 31500 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni
I0707 18:06:58.286325 31535 state.cpp:57] Recovering state from '/tmp/mesos-IFR4rG/0/meta'
I0707 18:06:58.289796 31525 master.cpp:1973] The newly elected leader is master@172.17.0.7:39581 with id 7892fbb2-1ac1-450f-8576-10c1df35f765
I0707 18:06:58.289837 31525 master.cpp:1986] Elected as the leading master!
I0707 18:06:58.289881 31525 master.cpp:1673] Recovering from registrar
I0707 18:06:58.290081 31528 registrar.cpp:332] Recovering registrar
I0707 18:06:58.298210 31532 status_update_manager.cpp:200] Recovering status update manager
I0707 18:06:58.298463 31532 containerizer.cpp:522] Recovering containerizer
I0707 18:06:58.310628 31530 provisioner.cpp:253] Provisioner recovery complete
I0707 18:06:58.311688 31531 slave.cpp:4856] Finished recovery
I0707 18:06:58.314482 31538 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 36.508519ms
I0707 18:06:58.314538 31538 replica.cpp:320] Persisted replica status to VOTING
I0707 18:06:58.314743 31538 recover.cpp:582] Successfully joined the Paxos group
I0707 18:06:58.315083 31538 recover.cpp:466] Recover process terminated
I0707 18:06:58.316979 31538 log.cpp:553] Attempting to start the writer
W0707 18:06:58.319859 31500 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W0707 18:06:58.320173 31500 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges
I0707 18:06:58.325480 31531 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0707 18:06:58.326918 31525 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0707 18:06:58.332331 31531 status_update_manager.cpp:174] Pausing sending status updates
I0707 18:06:58.332393 31534 slave.cpp:969] New master detected at master@172.17.0.7:39581
I0707 18:06:58.332491 31534 slave.cpp:990] No credentials provided. Attempting to register without authentication
I0707 18:06:58.332593 31534 slave.cpp:1001] Detecting new master
I0707 18:06:58.333231 31539 replica.cpp:493] Replica received implicit promise request from (30)@172.17.0.7:39581 with proposal 1
I0707 18:06:58.340519 31527 slave.cpp:205] Agent started on 2)@172.17.0.7:39581
I0707 18:06:58.340543 31527 slave.cpp:206] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-IFR4rG/1""
I0707 18:06:58.341367 31527 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 18:06:58.343765 31527 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 18:06:58.582685 31527 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:06:58.582808 31527 slave.cpp:602] Agent attributes: [  ]
I0707 18:06:58.582825 31527 slave.cpp:607] Agent hostname: 753c2ae3a486
I0707 18:06:58.583901 31536 slave.cpp:1529] Will retry registration in 1.442011578secs if necessary
I0707 18:06:58.584017 31536 master.cpp:1500] Dropping 'mesos.internal.RegisterSlaveMessage' message since not recovered yet
I0707 18:06:58.586772 31527 state.cpp:57] Recovering state from '/tmp/mesos-IFR4rG/1/meta'
I0707 18:06:58.587353 31527 status_update_manager.cpp:200] Recovering status update manager
I0707 18:06:58.587736 31525 containerizer.cpp:522] Recovering containerizer
I0707 18:06:58.590937 31500 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni
I0707 18:06:58.592527 31529 provisioner.cpp:253] Provisioner recovery complete
I0707 18:06:58.594597 31527 slave.cpp:4856] Finished recovery
I0707 18:06:58.595170 31527 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0707 18:06:58.596602 31529 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
W0707 18:06:58.597026 31500 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
I0707 18:06:58.597048 31527 slave.cpp:969] New master detected at master@172.17.0.7:39581
I0707 18:06:58.597074 31527 slave.cpp:990] No credentials provided. Attempting to register without authentication
I0707 18:06:58.597183 31529 status_update_manager.cpp:174] Pausing sending status updates
W0707 18:06:58.597195 31500 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges
I0707 18:06:58.597316 31527 slave.cpp:1001] Detecting new master
I0707 18:06:58.622475 31528 slave.cpp:205] Agent started on 3)@172.17.0.7:39581
I0707 18:06:58.622517 31528 slave.cpp:206] Flags at startup: --acls=""permissive: true
register_frameworks {
  principals {
    type: ANY
  }
  roles {
    type: SOME
    values: ""test""
  }
}
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-IFR4rG/2""
I0707 18:06:58.623533 31528 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 18:06:58.623703 31528 resources.cpp:572] Parsing resources as JSON failed: 
Trying semicolon-delimited string format instead
I0707 18:06:58.624135 31528 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:06:58.624209 31528 slave.cpp:602] Agent attributes: [  ]
I0707 18:06:58.624224 31528 slave.cpp:607] Agent hostname: 753c2ae3a486
I0707 18:06:58.628917 31533 state.cpp:57] Recovering state from '/tmp/mesos-IFR4rG/2/meta'
I0707 18:06:58.629308 31533 status_update_manager.cpp:200] Recovering status update manager
I0707 18:06:58.629530 31533 containerizer.cpp:522] Recovering containerizer
I0707 18:06:58.631003 31500 sched.cpp:226] Version: 1.0.0
I0707 18:06:58.632382 31536 provisioner.cpp:253] Provisioner recovery complete
I0707 18:06:58.632655 31530 sched.cpp:330] New master detected at master@172.17.0.7:39581
I0707 18:06:58.632758 31530 sched.cpp:341] No credentials provided. Attempting to register without authentication
I0707 18:06:58.632786 31530 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.7:39581
I0707 18:06:58.632921 31530 sched.cpp:853] Will retry registration in 1.972934225secs if necessary
I0707 18:06:58.633108 31535 master.cpp:1500] Dropping 'mesos.scheduler.Call' message since not recovered yet
I0707 18:06:58.633489 31530 slave.cpp:4856] Finished recovery
I0707 18:06:58.633956 31530 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0707 18:06:58.634172 31530 slave.cpp:969] New master detected at master@172.17.0.7:39581
I0707 18:06:58.634199 31530 slave.cpp:990] No credentials provided. Attempting to register without authentication
I0707 18:06:58.634241 31530 slave.cpp:1001] Detecting new master
I0707 18:06:58.634254 31538 status_update_manager.cpp:174] Pausing sending status updates
I0707 18:06:58.634354 31530 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0707 18:06:58.646832 31538 slave.cpp:1529] Will retry registration in 421.765838ms if necessary
I0707 18:06:58.647001 31538 master.cpp:1500] Dropping 'mesos.internal.RegisterSlaveMessage' message since not recovered yet
I0707 18:06:58.814708 31539 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 481.415169ms
I0707 18:06:58.814787 31539 replica.cpp:342] Persisted promised to 1
I0707 18:06:58.816249 31539 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0707 18:06:58.820055 31525 replica.cpp:388] Replica received explicit promise request from (49)@172.17.0.7:39581 for position 0 with proposal 2
I0707 18:06:58.865041 31525 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 44.725594ms
I0707 18:06:58.865135 31525 replica.cpp:712] Persisted action at 0
I0707 18:06:58.868293 31531 replica.cpp:537] Replica received write request for position 0 from (50)@172.17.0.7:39581
I0707 18:06:58.868521 31531 leveldb.cpp:436] Reading position from leveldb took 71896ns
I0707 18:06:58.901504 31531 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 32.785731ms
I0707 18:06:58.901585 31531 replica.cpp:712] Persisted action at 0
I0707 18:06:58.903040 31534 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0707 18:06:58.934504 31534 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 31.403773ms
I0707 18:06:58.934590 31534 replica.cpp:712] Persisted action at 0
I0707 18:06:58.934640 31534 replica.cpp:697] Replica learned NOP action at position 0
I0707 18:06:58.936309 31534 log.cpp:569] Writer started with ending position 0
I0707 18:06:58.941576 31534 leveldb.cpp:436] Reading position from leveldb took 74550ns
I0707 18:06:58.950364 31534 registrar.cpp:365] Successfully fetched the registry (0B) in 660.20608ms
I0707 18:06:58.952555 31534 registrar.cpp:464] Applied 1 operations in 61140ns; attempting to update the 'registry'
I0707 18:06:58.955271 31535 log.cpp:577] Attempting to append 168 bytes to the log
I0707 18:06:58.955555 31534 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0707 18:06:58.958256 31534 replica.cpp:537] Replica received write request for position 1 from (51)@172.17.0.7:39581
I0707 18:06:59.000967 31534 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 42.408267ms
I0707 18:06:59.001049 31534 replica.cpp:712] Persisted action at 1
I0707 18:06:59.004665 31535 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0707 18:06:59.051337 31535 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 46.611618ms
I0707 18:06:59.051431 31535 replica.cpp:712] Persisted action at 1
I0707 18:06:59.051483 31535 replica.cpp:697] Replica learned APPEND action at position 1
I0707 18:06:59.054003 31537 registrar.cpp:509] Successfully updated the 'registry' in 101.27104ms
I0707 18:06:59.054268 31537 registrar.cpp:395] Successfully recovered registrar
I0707 18:06:59.054762 31525 log.cpp:596] Attempting to truncate the log to 1
I0707 18:06:59.055158 31537 master.cpp:1781] Recovered 0 agents from the Registry (129B) ; allowing 10mins for agents to re-register
I0707 18:06:59.055218 31539 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0707 18:06:59.055346 31525 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
I0707 18:06:59.056404 31539 replica.cpp:537] Replica received write request for position 2 from (52)@172.17.0.7:39581
I0707 18:06:59.069908 31537 slave.cpp:1529] Will retry registration in 2.057528722secs if necessary
I0707 18:06:59.070539 31530 master.cpp:4676] Registering agent at slave(2)@172.17.0.7:39581 (753c2ae3a486) with id 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:06:59.071838 31530 registrar.cpp:464] Applied 1 operations in 139897ns; attempting to update the 'registry'
I0707 18:06:59.101510 31539 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 44.978496ms
I0707 18:06:59.101680 31539 replica.cpp:712] Persisted action at 2
I0707 18:06:59.102838 31527 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0707 18:06:59.165279 31527 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 62.339577ms
I0707 18:06:59.165451 31527 leveldb.cpp:399] Deleting ~1 keys from leveldb took 89301ns
I0707 18:06:59.165479 31527 replica.cpp:712] Persisted action at 2
I0707 18:06:59.165531 31527 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0707 18:06:59.167273 31533 log.cpp:577] Attempting to append 337 bytes to the log
I0707 18:06:59.167647 31526 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0707 18:06:59.169075 31526 replica.cpp:537] Replica received write request for position 3 from (53)@172.17.0.7:39581
I0707 18:06:59.224467 31526 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 55.235676ms
I0707 18:06:59.224548 31526 replica.cpp:712] Persisted action at 3
I0707 18:06:59.226044 31526 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0707 18:06:59.262912 31525 hierarchical.cpp:1537] No allocations performed
I0707 18:06:59.263056 31525 hierarchical.cpp:1172] Performed allocation for 0 agents in 345421ns
I0707 18:06:59.283505 31526 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 57.362529ms
I0707 18:06:59.283589 31526 replica.cpp:712] Persisted action at 3
I0707 18:06:59.283638 31526 replica.cpp:697] Replica learned APPEND action at position 3
I0707 18:06:59.287037 31538 registrar.cpp:509] Successfully updated the 'registry' in 215.051008ms
I0707 18:06:59.287451 31533 log.cpp:596] Attempting to truncate the log to 3
I0707 18:06:59.288493 31539 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0707 18:06:59.289965 31532 replica.cpp:537] Replica received write request for position 4 from (54)@172.17.0.7:39581
I0707 18:06:59.291105 31533 master.cpp:4745] Registered agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:06:59.291481 31533 slave.cpp:1169] Registered with master master@172.17.0.7:39581; given agent ID 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:06:59.291512 31533 fetcher.cpp:86] Clearing fetcher cache
I0707 18:06:59.291749 31539 hierarchical.cpp:478] Added agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 (753c2ae3a486) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0707 18:06:59.292026 31533 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/mesos-IFR4rG/1/meta/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/slave.info'
I0707 18:06:59.292233 31525 status_update_manager.cpp:181] Resuming sending status updates
I0707 18:06:59.292330 31539 hierarchical.cpp:1537] No allocations performed
I0707 18:06:59.292726 31533 slave.cpp:1229] Forwarding total oversubscribed resources 
I0707 18:06:59.292819 31539 hierarchical.cpp:1195] Performed allocation for agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 in 968472ns
I0707 18:06:59.292860 31533 slave.cpp:3760] Received ping from slave-observer(1)@172.17.0.7:39581
I0707 18:06:59.293042 31525 master.cpp:5128] Received update of agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486) with total oversubscribed resources 
I0707 18:06:59.293349 31525 hierarchical.cpp:542] Agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 (753c2ae3a486) updated with oversubscribed resources  (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0707 18:06:59.294507 31525 hierarchical.cpp:1537] No allocations performed
I0707 18:06:59.294569 31525 hierarchical.cpp:1195] Performed allocation for agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 in 1.17629ms
I0707 18:06:59.343272 31539 slave.cpp:1529] Will retry registration in 1.912221755secs if necessary
I0707 18:06:59.343785 31528 master.cpp:4676] Registering agent at slave(3)@172.17.0.7:39581 (753c2ae3a486) with id 7892fbb2-1ac1-450f-8576-10c1df35f765-S1
I0707 18:06:59.344733 31528 registrar.cpp:464] Applied 1 operations in 125766ns; attempting to update the 'registry'
I0707 18:06:59.362001 31532 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 71.968888ms
I0707 18:06:59.362097 31532 replica.cpp:712] Persisted action at 4
I0707 18:06:59.363359 31527 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0707 18:06:59.412232 31527 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 48.705675ms
I0707 18:06:59.412684 31527 leveldb.cpp:399] Deleting ~2 keys from leveldb took 95246ns
I0707 18:06:59.412853 31527 replica.cpp:712] Persisted action at 4
I0707 18:06:59.413059 31527 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0707 18:06:59.414814 31536 log.cpp:577] Attempting to append 503 bytes to the log
I0707 18:06:59.414932 31527 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0707 18:06:59.415906 31536 replica.cpp:537] Replica received write request for position 5 from (55)@172.17.0.7:39581
I0707 18:06:59.462564 31536 leveldb.cpp:341] Persisting action (522 bytes) to leveldb took 46.531781ms
I0707 18:06:59.462729 31536 replica.cpp:712] Persisted action at 5
I0707 18:06:59.463939 31536 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0707 18:06:59.534988 31536 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 70.884462ms
I0707 18:06:59.535076 31536 replica.cpp:712] Persisted action at 5
I0707 18:06:59.535125 31536 replica.cpp:697] Replica learned APPEND action at position 5
I0707 18:06:59.540696 31533 registrar.cpp:509] Successfully updated the 'registry' in 195.819008ms
I0707 18:06:59.541802 31533 log.cpp:596] Attempting to truncate the log to 5
I0707 18:06:59.544545 31533 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0707 18:06:59.545786 31536 master.cpp:4745] Registered agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:06:59.546710 31536 hierarchical.cpp:478] Added agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 (753c2ae3a486) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0707 18:06:59.547562 31536 hierarchical.cpp:1537] No allocations performed
I0707 18:06:59.547972 31536 hierarchical.cpp:1195] Performed allocation for agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 in 540048ns
I0707 18:06:59.547883 31533 slave.cpp:1169] Registered with master master@172.17.0.7:39581; given agent ID 7892fbb2-1ac1-450f-8576-10c1df35f765-S1
I0707 18:06:59.549479 31533 fetcher.cpp:86] Clearing fetcher cache
I0707 18:06:59.549988 31533 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/mesos-IFR4rG/2/meta/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/slave.info'
I0707 18:06:59.550889 31533 slave.cpp:1229] Forwarding total oversubscribed resources 
I0707 18:06:59.551502 31533 slave.cpp:3760] Received ping from slave-observer(2)@172.17.0.7:39581
I0707 18:06:59.550431 31536 replica.cpp:537] Replica received write request for position 6 from (56)@172.17.0.7:39581
I0707 18:06:59.551846 31533 status_update_manager.cpp:181] Resuming sending status updates
I0707 18:06:59.552496 31533 master.cpp:5128] Received update of agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486) with total oversubscribed resources 
I0707 18:06:59.552784 31533 hierarchical.cpp:542] Agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 (753c2ae3a486) updated with oversubscribed resources  (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0707 18:06:59.553241 31533 hierarchical.cpp:1537] No allocations performed
I0707 18:06:59.553311 31533 hierarchical.cpp:1195] Performed allocation for agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 in 481606ns
I0707 18:06:59.586164 31536 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 34.414546ms
I0707 18:06:59.586247 31536 replica.cpp:712] Persisted action at 6
I0707 18:06:59.587699 31536 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0707 18:06:59.619674 31536 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 31.87307ms
I0707 18:06:59.619837 31536 leveldb.cpp:399] Deleting ~2 keys from leveldb took 82424ns
I0707 18:06:59.619864 31536 replica.cpp:712] Persisted action at 6
I0707 18:06:59.619913 31536 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0707 18:07:00.026949 31524 slave.cpp:1529] Will retry registration in 2.762006963secs if necessary
I0707 18:07:00.027307 31524 master.cpp:4676] Registering agent at slave(1)@172.17.0.7:39581 (753c2ae3a486) with id 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:00.028275 31525 registrar.cpp:464] Applied 1 operations in 158666ns; attempting to update the 'registry'
I0707 18:07:00.032213 31524 log.cpp:577] Attempting to append 669 bytes to the log
I0707 18:07:00.032482 31525 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0707 18:07:00.034119 31525 replica.cpp:537] Replica received write request for position 7 from (57)@172.17.0.7:39581
I0707 18:07:00.048461 31525 leveldb.cpp:341] Persisting action (688 bytes) to leveldb took 14.18893ms
I0707 18:07:00.048629 31525 replica.cpp:712] Persisted action at 7
I0707 18:07:00.050050 31530 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I0707 18:07:00.113382 31530 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 63.230657ms
I0707 18:07:00.113576 31530 replica.cpp:712] Persisted action at 7
I0707 18:07:00.113853 31530 replica.cpp:697] Replica learned APPEND action at position 7
I0707 18:07:00.117030 31526 registrar.cpp:509] Successfully updated the 'registry' in 88.66816ms
I0707 18:07:00.117674 31530 log.cpp:596] Attempting to truncate the log to 7
I0707 18:07:00.117910 31528 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0707 18:07:00.118896 31526 master.cpp:4745] Registered agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:00.119174 31526 replica.cpp:537] Replica received write request for position 8 from (58)@172.17.0.7:39581
I0707 18:07:00.119213 31531 slave.cpp:1169] Registered with master master@172.17.0.7:39581; given agent ID 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:00.119254 31531 fetcher.cpp:86] Clearing fetcher cache
I0707 18:07:00.119482 31527 status_update_manager.cpp:181] Resuming sending status updates
I0707 18:07:00.119736 31531 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/mesos-IFR4rG/0/meta/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/slave.info'
I0707 18:07:00.120182 31531 slave.cpp:1229] Forwarding total oversubscribed resources 
I0707 18:07:00.120273 31531 slave.cpp:3760] Received ping from slave-observer(3)@172.17.0.7:39581
I0707 18:07:00.120461 31531 master.cpp:5128] Received update of agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486) with total oversubscribed resources 
I0707 18:07:00.120740 31539 hierarchical.cpp:478] Added agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 (753c2ae3a486) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0707 18:07:00.120904 31539 hierarchical.cpp:1537] No allocations performed
I0707 18:07:00.121006 31539 hierarchical.cpp:1195] Performed allocation for agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 in 222368ns
I0707 18:07:00.121196 31539 hierarchical.cpp:542] Agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 (753c2ae3a486) updated with oversubscribed resources  (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0707 18:07:00.121511 31539 hierarchical.cpp:1537] No allocations performed
I0707 18:07:00.121609 31539 hierarchical.cpp:1195] Performed allocation for agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 in 178299ns
I0707 18:07:00.168759 31526 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 49.379822ms
I0707 18:07:00.168941 31526 replica.cpp:712] Persisted action at 8
I0707 18:07:00.173840 31535 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I0707 18:07:00.227267 31535 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 53.368171ms
I0707 18:07:00.227453 31535 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98129ns
I0707 18:07:00.227483 31535 replica.cpp:712] Persisted action at 8
I0707 18:07:00.227536 31535 replica.cpp:697] Replica learned TRUNCATE action at position 8
I0707 18:07:00.264490 31537 hierarchical.cpp:1537] No allocations performed
I0707 18:07:00.264606 31537 hierarchical.cpp:1172] Performed allocation for 3 agents in 410062ns
I0707 18:07:00.606685 31537 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.7:39581
I0707 18:07:00.606863 31537 sched.cpp:853] Will retry registration in 3.19389747secs if necessary
I0707 18:07:00.607161 31537 master.cpp:2550] Received SUBSCRIBE call for framework 'Dynamic Reservation Framework (C++)' at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:00.607316 31537 master.cpp:2012] Authorizing framework principal 'test' to receive offers for role 'test'
I0707 18:07:00.608633 31537 master.cpp:2626] Subscribing framework Dynamic Reservation Framework (C++) with checkpointing disabled and capabilities [  ]
I0707 18:07:00.610199 31535 hierarchical.cpp:271] Added framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:00.611109 31537 sched.cpp:743] Framework registered with 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:00.611136 31537 dynamic_reservation_framework.cpp:73] Registered!
I0707 18:07:00.611152 31537 sched.cpp:757] Scheduler::registered took 18259ns
I0707 18:07:00.613695 31535 hierarchical.cpp:1632] No inverse offers to send out!
I0707 18:07:00.613819 31535 hierarchical.cpp:1172] Performed allocation for 3 agents in 3.590355ms
I0707 18:07:00.615160 31535 master.cpp:5835] Sending 3 offers to framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:00.616071 31535 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O0 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:00.616686 31535 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O1 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:00.617288 31535 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O2 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:00.617588 31535 sched.cpp:917] Scheduler::resourceOffers took 1.516257ms
I0707 18:07:00.619355 31531 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O0 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:00.619647 31531 master.cpp:3144] Authorizing principal 'test' to reserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 18:07:00.621302 31531 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O1 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:00.621472 31531 master.cpp:3144] Authorizing principal 'test' to reserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 18:07:00.622601 31531 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O2 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:00.622714 31531 master.cpp:3144] Authorizing principal 'test' to reserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 18:07:00.623579 31531 master.cpp:3695] Applying RESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:00.624171 31531 master.cpp:7098] Sending checkpointed resources cpus(test, test):1; mem(test, test):128 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:00.624826 31531 master.cpp:3695] Applying RESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:00.625102 31524 slave.cpp:2600] Updated checkpointed resources from  to cpus(test, test):1; mem(test, test):128
I0707 18:07:00.625347 31531 master.cpp:7098] Sending checkpointed resources cpus(test, test):1; mem(test, test):128 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:00.625932 31531 master.cpp:3695] Applying RESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:00.626421 31531 master.cpp:7098] Sending checkpointed resources cpus(test, test):1; mem(test, test):128 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:00.626845 31537 slave.cpp:2600] Updated checkpointed resources from  to cpus(test, test):1; mem(test, test):128
I0707 18:07:00.627255 31538 hierarchical.cpp:683] Updated allocation of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] to cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128
I0707 18:07:00.627341 31531 slave.cpp:2600] Updated checkpointed resources from  to cpus(test, test):1; mem(test, test):128
I0707 18:07:00.628175 31538 hierarchical.cpp:924] Recovered cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:00.630285 31538 hierarchical.cpp:683] Updated allocation of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 from cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] to cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128
I0707 18:07:00.631186 31538 hierarchical.cpp:924] Recovered cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:00.633533 31538 hierarchical.cpp:683] Updated allocation of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] to cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128
I0707 18:07:00.634449 31538 hierarchical.cpp:924] Recovered cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.268144 31537 hierarchical.cpp:1632] No inverse offers to send out!
I0707 18:07:01.268594 31537 hierarchical.cpp:1172] Performed allocation for 3 agents in 3.471848ms
I0707 18:07:01.269795 31532 master.cpp:5835] Sending 3 offers to framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:01.272722 31532 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O3 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:01.273185 31532 dynamic_reservation_framework.cpp:150] Launching task 0 using offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O3
I0707 18:07:01.274802 31532 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O4 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:01.275229 31532 dynamic_reservation_framework.cpp:150] Launching task 1 using offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O4
I0707 18:07:01.275583 31532 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O5 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:01.276630 31532 dynamic_reservation_framework.cpp:150] Launching task 2 using offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O5
I0707 18:07:01.277556 31532 sched.cpp:917] Scheduler::resourceOffers took 4.841343ms
I0707 18:07:01.279577 31529 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O3 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:01.279718 31529 master.cpp:3106] Authorizing framework principal 'test' to launch task 0
I0707 18:07:01.282876 31529 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O4 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:01.282953 31529 master.cpp:3106] Authorizing framework principal 'test' to launch task 1
I0707 18:07:01.287205 31529 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O5 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:01.287295 31529 master.cpp:3106] Authorizing framework principal 'test' to launch task 2
I0707 18:07:01.292567 31529 master.cpp:7565] Adding task 0 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 (753c2ae3a486)
I0707 18:07:01.292743 31529 master.cpp:3957] Launching task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.294683 31539 slave.cpp:1569] Got assigned task 0 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.295222 31539 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:01.297250 31532 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: cpus(test, test):1; mem(test, test):128) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.298192 31539 slave.cpp:1688] Launching task 0 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.298318 31539 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:01.302230 31529 master.cpp:7565] Adding task 1 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 (753c2ae3a486)
I0707 18:07:01.303160 31539 paths.cpp:528] Trying to chown '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/0/runs/bdca1a15-bdb1-45bb-b19e-df07fb74e5db' to user 'mesos'
I0707 18:07:01.302377 31529 master.cpp:3957] Launching task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.304913 31527 slave.cpp:1569] Got assigned task 1 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.305199 31527 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:01.307775 31539 slave.cpp:5748] Launching executor 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/0/runs/bdca1a15-bdb1-45bb-b19e-df07fb74e5db'
I0707 18:07:01.309219 31527 slave.cpp:1688] Launching task 1 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.309334 31527 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:01.309659 31536 containerizer.cpp:781] Starting container 'bdca1a15-bdb1-45bb-b19e-df07fb74e5db' for executor '0' of framework '7892fbb2-1ac1-450f-8576-10c1df35f765-0000'
I0707 18:07:01.312726 31539 slave.cpp:1914] Queuing task '0' for executor '0' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.312937 31539 slave.cpp:922] Successfully attached file '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/0/runs/bdca1a15-bdb1-45bb-b19e-df07fb74e5db'
I0707 18:07:01.317831 31528 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: cpus(test, test):1; mem(test, test):128) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.319255 31529 master.cpp:7565] Adding task 2 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 (753c2ae3a486)
I0707 18:07:01.319463 31529 master.cpp:3957] Launching task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.319934 31526 slave.cpp:1569] Got assigned task 2 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.320222 31526 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:01.320818 31526 slave.cpp:1688] Launching task 2 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.320926 31526 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:01.322808 31524 containerizer.cpp:1284] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/mesos-1.0.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.0.0\/_build\/src\/mesos-executor""}"" --help=""false"" --pipe_read=""12"" --pipe_write=""13"" --pre_exec_commands=""[]"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/0/runs/bdca1a15-bdb1-45bb-b19e-df07fb74e5db""'
I0707 18:07:01.325748 31524 launcher.cpp:126] Forked child with pid '31544' for container 'bdca1a15-bdb1-45bb-b19e-df07fb74e5db'
I0707 18:07:01.328867 31533 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: cpus(test, test):1; mem(test, test):128) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.351835 31526 paths.cpp:528] Trying to chown '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/2/runs/cffaea8f-effc-4388-902d-1ae39d1e5bfb' to user 'mesos'
I0707 18:07:01.353647 31527 paths.cpp:528] Trying to chown '/tmp/mesos-IFR4rG/2/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/1/runs/cde8072a-fc80-426d-833f-57e3cf50f368' to user 'mesos'
I0707 18:07:01.360844 31526 slave.cpp:5748] Launching executor 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/2/runs/cffaea8f-effc-4388-902d-1ae39d1e5bfb'
I0707 18:07:01.362640 31525 containerizer.cpp:781] Starting container 'cffaea8f-effc-4388-902d-1ae39d1e5bfb' for executor '2' of framework '7892fbb2-1ac1-450f-8576-10c1df35f765-0000'
I0707 18:07:01.370136 31525 containerizer.cpp:1284] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/mesos-1.0.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.0.0\/_build\/src\/mesos-executor""}"" --help=""false"" --pipe_read=""9"" --pipe_write=""12"" --pre_exec_commands=""[]"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/2/runs/cffaea8f-effc-4388-902d-1ae39d1e5bfb""'
I0707 18:07:01.370545 31526 slave.cpp:1914] Queuing task '2' for executor '2' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.370671 31526 slave.cpp:922] Successfully attached file '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/2/runs/cffaea8f-effc-4388-902d-1ae39d1e5bfb'
I0707 18:07:01.372215 31525 launcher.cpp:126] Forked child with pid '31562' for container 'cffaea8f-effc-4388-902d-1ae39d1e5bfb'
I0707 18:07:01.385299 31527 slave.cpp:5748] Launching executor 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-IFR4rG/2/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/1/runs/cde8072a-fc80-426d-833f-57e3cf50f368'
I0707 18:07:01.386274 31527 slave.cpp:1914] Queuing task '1' for executor '1' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.387212 31527 slave.cpp:922] Successfully attached file '/tmp/mesos-IFR4rG/2/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/1/runs/cde8072a-fc80-426d-833f-57e3cf50f368'
I0707 18:07:01.386729 31536 containerizer.cpp:781] Starting container 'cde8072a-fc80-426d-833f-57e3cf50f368' for executor '1' of framework '7892fbb2-1ac1-450f-8576-10c1df35f765-0000'
I0707 18:07:01.393424 31530 containerizer.cpp:1284] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/mesos-1.0.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.0.0\/_build\/src\/mesos-executor""}"" --help=""false"" --pipe_read=""9"" --pipe_write=""12"" --pre_exec_commands=""[]"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/mesos-IFR4rG/2/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/1/runs/cde8072a-fc80-426d-833f-57e3cf50f368""'
I0707 18:07:01.395277 31530 launcher.cpp:126] Forked child with pid '31567' for container 'cde8072a-fc80-426d-833f-57e3cf50f368'
I0707 18:07:01.597575 31544 exec.cpp:161] Version: 1.0.0
I0707 18:07:01.603597 31530 slave.cpp:2902] Got registration for executor '0' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:59475
I0707 18:07:01.610890 31530 slave.cpp:2079] Sending queued task '0' to executor '0' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:59475
I0707 18:07:01.610896 31623 exec.cpp:236] Executor registered on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
Received SUBSCRIBED event
Subscribed executor on 753c2ae3a486
Received LAUNCH event
Starting task 0
/mesos/mesos-1.0.0/_build/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""echo hello""}"" --help=""false"" --unshare_namespace_mnt=""false""
Forked command at 31646
I0707 18:07:01.638350 31527 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:59475
I0707 18:07:01.643460 31532 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.643530 31532 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.644738 31532 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:01.646585 31527 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:01.646842 31527 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.646900 31527 slave.cpp:3588] Sending acknowledgement for status update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:59475
I0707 18:07:01.647038 31538 master.cpp:5273] Status update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.647094 31538 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.647313 31538 master.cpp:6959] Updating the state of task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0707 18:07:01.647377 31527 dynamic_reservation_framework.cpp:211] Task 0 is in state TASK_RUNNING
I0707 18:07:01.647423 31527 sched.cpp:1025] Scheduler::statusUpdate took 33812ns
I0707 18:07:01.647711 31527 master.cpp:4388] Processing ACKNOWLEDGE call 24b1f98c-f325-4ecc-b839-07580dcccd52 for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:07:01.648241 31529 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.648927 31529 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 24b1f98c-f325-4ecc-b839-07580dcccd52) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.688079 31670 exec.cpp:161] Version: 1.0.0
I0707 18:07:01.701555 31529 slave.cpp:2902] Got registration for executor '2' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:51409
I0707 18:07:01.716229 31668 exec.cpp:236] Executor registered on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:01.719995 31529 slave.cpp:2079] Sending queued task '2' to executor '2' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:51409
I0707 18:07:01.740218 31567 exec.cpp:161] Version: 1.0.0
I0707 18:07:01.745529 31534 slave.cpp:2902] Got registration for executor '1' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:34513
hello
I0707 18:07:01.747939 31539 slave.cpp:2079] Sending queued task '1' to executor '1' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:34513
I0707 18:07:01.749034 31677 exec.cpp:236] Executor registered on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1
Received SUBSCRIBED event
Subscribed executor on 753c2ae3a486
Received LAUNCH event
Starting task 2
/mesos/mesos-1.0.0/_build/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""echo hello""}"" --help=""false"" --unshare_namespace_mnt=""false""
Forked command at 31694
I0707 18:07:01.763070 31533 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:51409
I0707 18:07:01.765763 31531 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.765916 31531 status_update_manager.cpp:497] Creating StatusUpdate stream for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.766629 31531 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:01.767174 31531 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:01.767484 31531 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.767638 31531 slave.cpp:3588] Sending acknowledgement for status update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:51409
I0707 18:07:01.767709 31529 master.cpp:5273] Status update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.768045 31529 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.768604 31535 dynamic_reservation_framework.cpp:211] Task 2 is in state TASK_RUNNING
I0707 18:07:01.768832 31535 sched.cpp:1025] Scheduler::statusUpdate took 272020ns
I0707 18:07:01.769253 31529 master.cpp:6959] Updating the state of task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0707 18:07:01.769649 31529 master.cpp:4388] Processing ACKNOWLEDGE call c30ef1c4-d5f2-4556-875b-df44c8e586d5 for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:01.770125 31529 status_update_manager.cpp:392] Received status update acknowledgement (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.770845 31524 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: c30ef1c4-d5f2-4556-875b-df44c8e586d5) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
Received SUBSCRIBED event
Subscribed executor on 753c2ae3a486
Received LAUNCH event
Starting task 1
/mesos/mesos-1.0.0/_build/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""echo hello""}"" --help=""false"" --unshare_namespace_mnt=""false""
Forked command at 31702
I0707 18:07:01.786492 31528 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:34513
I0707 18:07:01.789589 31535 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.789639 31535 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.790278 31535 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:01.790938 31539 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:01.791113 31539 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.791162 31539 slave.cpp:3588] Sending acknowledgement for status update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:34513
I0707 18:07:01.792708 31539 master.cpp:5273] Status update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.792759 31539 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.792935 31539 master.cpp:6959] Updating the state of task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0707 18:07:01.793094 31539 dynamic_reservation_framework.cpp:211] Task 1 is in state TASK_RUNNING
I0707 18:07:01.793119 31539 sched.cpp:1025] Scheduler::statusUpdate took 26767ns
I0707 18:07:01.793341 31539 master.cpp:4388] Processing ACKNOWLEDGE call 41438479-6ebc-417c-a357-9446db661f27 for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1
I0707 18:07:01.793712 31539 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.794035 31539 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 41438479-6ebc-417c-a357-9446db661f27) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
Command exited with status 0 (pid: 31646)
I0707 18:07:01.849810 31535 slave.cpp:3285] Handling status update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:59475
I0707 18:07:01.854346 31535 slave.cpp:6088] Terminating task 0
I0707 18:07:01.860373 31530 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.861690 31530 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:01.862217 31530 slave.cpp:3678] Forwarding the update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:01.863664 31533 master.cpp:5273] Status update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.863718 31533 master.cpp:5321] Forwarding status update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.863888 31533 master.cpp:6959] Updating the state of task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0707 18:07:01.864351 31533 dynamic_reservation_framework.cpp:208] Task 0 is finished at agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:07:01.864377 31533 sched.cpp:1025] Scheduler::statusUpdate took 42319ns
I0707 18:07:01.865952 31533 hierarchical.cpp:924] Recovered cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.866158 31533 master.cpp:4388] Processing ACKNOWLEDGE call 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:07:01.866231 31533 master.cpp:7025] Removing task 0 with resources cpus(test, test):1; mem(test, test):128 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.866461 31530 slave.cpp:3572] Status update manager successfully handled status update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.866519 31530 slave.cpp:3588] Sending acknowledgement for status update TASK_FINISHED (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:59475
I0707 18:07:01.869339 31533 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.869580 31533 status_update_manager.cpp:528] Cleaning up status update stream for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.870098 31533 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 2c0ce9f5-7167-4fb9-8636-ae0e465d08fc) for task 0 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.870162 31533 slave.cpp:6129] Completing task 0
hello
Command exited with status 0 (pid: 31694)
hello
I0707 18:07:01.965605 31533 slave.cpp:3285] Handling status update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:51409
I0707 18:07:01.968528 31535 slave.cpp:6088] Terminating task 2
I0707 18:07:01.970181 31539 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.970440 31539 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:01.970832 31533 slave.cpp:3678] Forwarding the update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:01.971072 31533 slave.cpp:3572] Status update manager successfully handled status update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.971158 31533 slave.cpp:3588] Sending acknowledgement for status update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:51409
I0707 18:07:01.971346 31535 master.cpp:5273] Status update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.971515 31535 master.cpp:5321] Forwarding status update TASK_FINISHED (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.971796 31535 master.cpp:6959] Updating the state of task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0707 18:07:01.971863 31539 dynamic_reservation_framework.cpp:208] Task 2 is finished at agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:01.971887 31539 sched.cpp:1025] Scheduler::statusUpdate took 36513ns
I0707 18:07:01.972437 31535 master.cpp:4388] Processing ACKNOWLEDGE call 60c0ce74-18df-458f-bef4-1f82fb03412e for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:01.972524 31528 hierarchical.cpp:924] Recovered cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.972553 31535 master.cpp:7025] Removing task 2 with resources cpus(test, test):1; mem(test, test):128 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.973103 31535 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.973393 31535 status_update_manager.cpp:528] Cleaning up status update stream for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.973930 31535 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 60c0ce74-18df-458f-bef4-1f82fb03412e) for task 2 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.974081 31535 slave.cpp:6129] Completing task 2
Command exited with status 0 (pid: 31702)
I0707 18:07:01.988955 31530 slave.cpp:3285] Handling status update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:34513
I0707 18:07:01.990218 31534 slave.cpp:6088] Terminating task 1
I0707 18:07:01.991665 31534 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.991874 31534 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:01.992138 31531 slave.cpp:3678] Forwarding the update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:01.992455 31531 slave.cpp:3572] Status update manager successfully handled status update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.992539 31531 slave.cpp:3588] Sending acknowledgement for status update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:34513
I0707 18:07:01.992573 31535 master.cpp:5273] Status update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.992619 31535 master.cpp:5321] Forwarding status update TASK_FINISHED (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.992776 31535 master.cpp:6959] Updating the state of task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0707 18:07:01.993170 31535 dynamic_reservation_framework.cpp:208] Task 1 is finished at agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1
I0707 18:07:01.993191 31535 sched.cpp:1025] Scheduler::statusUpdate took 32857ns
I0707 18:07:01.993369 31535 master.cpp:4388] Processing ACKNOWLEDGE call 9e47c164-0f30-4b73-aab4-249a77bdc04d for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1
I0707 18:07:01.993633 31531 hierarchical.cpp:924] Recovered cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.994060 31535 master.cpp:7025] Removing task 1 with resources cpus(test, test):1; mem(test, test):128 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:01.994530 31535 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.994729 31535 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.995079 31537 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 9e47c164-0f30-4b73-aab4-249a77bdc04d) for task 1 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:01.995172 31537 slave.cpp:6129] Completing task 1
I0707 18:07:02.274876 31537 hierarchical.cpp:1632] No inverse offers to send out!
I0707 18:07:02.275038 31537 hierarchical.cpp:1172] Performed allocation for 3 agents in 4.318297ms
I0707 18:07:02.278640 31537 master.cpp:5835] Sending 3 offers to framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:02.280704 31524 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O6 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:02.280966 31524 dynamic_reservation_framework.cpp:150] Launching task 3 using offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O6
I0707 18:07:02.281510 31524 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O7 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:02.281695 31524 dynamic_reservation_framework.cpp:150] Launching task 4 using offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O7
I0707 18:07:02.282440 31524 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O8 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:02.282804 31524 sched.cpp:917] Scheduler::resourceOffers took 2.103878ms
I0707 18:07:02.285826 31537 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O6 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:02.285923 31537 master.cpp:3106] Authorizing framework principal 'test' to launch task 3
I0707 18:07:02.291869 31537 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O7 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:02.291971 31537 master.cpp:3106] Authorizing framework principal 'test' to launch task 4
I0707 18:07:02.297622 31537 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O8 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:02.299481 31537 master.cpp:3201] Authorizing principal 'test' to unreserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 18:07:02.304149 31537 master.cpp:7565] Adding task 3 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 (753c2ae3a486)
I0707 18:07:02.304298 31537 master.cpp:3957] Launching task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.305838 31532 slave.cpp:1569] Got assigned task 3 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.306040 31532 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:02.307746 31532 slave.cpp:1688] Launching task 3 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.307853 31532 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:02.312942 31532 paths.cpp:528] Trying to chown '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/3/runs/e31dfccd-5f2a-40e1-95a0-b5b253fac912' to user 'mesos'
I0707 18:07:02.317132 31531 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: cpus(test, test):1; mem(test, test):128) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.318964 31532 slave.cpp:5748] Launching executor 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/3/runs/e31dfccd-5f2a-40e1-95a0-b5b253fac912'
I0707 18:07:02.319262 31537 master.cpp:7565] Adding task 4 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 (753c2ae3a486)
I0707 18:07:02.319537 31537 master.cpp:3957] Launching task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 with resources cpus(test, test):1; mem(test, test):128 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.319990 31527 slave.cpp:1569] Got assigned task 4 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.320173 31537 master.cpp:3747] Applying UNRESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.321169 31539 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):15; mem(*):47142 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: cpus(test, test):1; mem(test, test):128) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.321388 31527 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:02.322088 31537 master.cpp:7098] Sending checkpointed resources  to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 at slave(3)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.322193 31527 slave.cpp:1688] Launching task 4 for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.322312 31527 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0707 18:07:02.322660 31537 slave.cpp:2600] Updated checkpointed resources from cpus(test, test):1; mem(test, test):128 to 
I0707 18:07:02.323094 31527 paths.cpp:528] Trying to chown '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/4/runs/f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84' to user 'mesos'
I0707 18:07:02.324539 31532 slave.cpp:1914] Queuing task '3' for executor '3' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.324769 31528 containerizer.cpp:781] Starting container 'e31dfccd-5f2a-40e1-95a0-b5b253fac912' for executor '3' of framework '7892fbb2-1ac1-450f-8576-10c1df35f765-0000'
I0707 18:07:02.324848 31532 slave.cpp:922] Successfully attached file '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/3/runs/e31dfccd-5f2a-40e1-95a0-b5b253fac912'
I0707 18:07:02.326263 31539 hierarchical.cpp:683] Updated allocation of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 from cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000] to ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):16; mem(*):47270
I0707 18:07:02.327944 31539 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):16; mem(*):47270 (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.330935 31527 slave.cpp:5748] Launching executor 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/4/runs/f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84'
I0707 18:07:02.331660 31527 slave.cpp:1914] Queuing task '4' for executor '4' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.331791 31527 slave.cpp:922] Successfully attached file '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/4/runs/f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84'
I0707 18:07:02.331665 31536 containerizer.cpp:781] Starting container 'f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84' for executor '4' of framework '7892fbb2-1ac1-450f-8576-10c1df35f765-0000'
I0707 18:07:02.334722 31530 containerizer.cpp:1284] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/mesos-1.0.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.0.0\/_build\/src\/mesos-executor""}"" --help=""false"" --pipe_read=""17"" --pipe_write=""18"" --pre_exec_commands=""[]"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/3/runs/e31dfccd-5f2a-40e1-95a0-b5b253fac912""'
I0707 18:07:02.335965 31530 launcher.cpp:126] Forked child with pid '31726' for container 'e31dfccd-5f2a-40e1-95a0-b5b253fac912'
I0707 18:07:02.340095 31536 containerizer.cpp:1284] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/mesos-1.0.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.0.0\/_build\/src\/mesos-executor""}"" --help=""false"" --pipe_read=""19"" --pipe_write=""20"" --pre_exec_commands=""[]"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/4/runs/f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84""'
I0707 18:07:02.341588 31536 launcher.cpp:126] Forked child with pid '31727' for container 'f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84'
I0707 18:07:02.587986 31726 exec.cpp:161] Version: 1.0.0
I0707 18:07:02.592061 31727 exec.cpp:161] Version: 1.0.0
I0707 18:07:02.592990 31535 slave.cpp:2902] Got registration for executor '3' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:55420
I0707 18:07:02.595854 31533 slave.cpp:2902] Got registration for executor '4' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:56283
I0707 18:07:02.599692 31535 slave.cpp:2079] Sending queued task '3' to executor '3' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:55420
I0707 18:07:02.600880 31811 exec.cpp:236] Executor registered on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:02.601768 31533 slave.cpp:2079] Sending queued task '4' to executor '4' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:56283
I0707 18:07:02.603651 31792 exec.cpp:236] Executor registered on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
Received SUBSCRIBED event
Subscribed executor on 753c2ae3a486
Received LAUNCH event
Starting task 3
Received SUBSCRIBED event
Subscribed executor on 753c2ae3a486
Received LAUNCH event
Starting task 4
/mesos/mesos-1.0.0/_build/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""echo hello""}"" --help=""false"" --unshare_namespace_mnt=""false""
Forked command at 31815
/mesos/mesos-1.0.0/_build/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""echo hello""}"" --help=""false"" --unshare_namespace_mnt=""false""
Forked command at 31814
I0707 18:07:02.632139 31530 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:55420
I0707 18:07:02.634009 31537 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:56283
I0707 18:07:02.636554 31527 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.636744 31527 status_update_manager.cpp:497] Creating StatusUpdate stream for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.637475 31527 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:02.640130 31527 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:02.641506 31527 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.643518 31527 slave.cpp:3588] Sending acknowledgement for status update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:56283
I0707 18:07:02.642694 31529 master.cpp:5273] Status update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.644038 31529 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.644603 31524 dynamic_reservation_framework.cpp:211] Task 4 is in state TASK_RUNNING
I0707 18:07:02.644876 31524 sched.cpp:1025] Scheduler::statusUpdate took 310563ns
I0707 18:07:02.645233 31529 master.cpp:6959] Updating the state of task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0707 18:07:02.645633 31529 master.cpp:4388] Processing ACKNOWLEDGE call f7349218-9f36-4867-8b7e-b980f525c673 for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:02.648191 31530 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.648598 31530 status_update_manager.cpp:497] Creating StatusUpdate stream for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.648950 31527 status_update_manager.cpp:392] Received status update acknowledgement (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.649775 31527 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: f7349218-9f36-4867-8b7e-b980f525c673) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.650616 31530 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:02.651362 31530 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:02.651851 31530 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.652447 31530 slave.cpp:3588] Sending acknowledgement for status update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:55420
I0707 18:07:02.652318 31538 master.cpp:5273] Status update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.652909 31538 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.653316 31538 master.cpp:6959] Updating the state of task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0707 18:07:02.653714 31526 dynamic_reservation_framework.cpp:211] Task 3 is in state TASK_RUNNING
I0707 18:07:02.653985 31526 sched.cpp:1025] Scheduler::statusUpdate took 274166ns
I0707 18:07:02.654438 31526 master.cpp:4388] Processing ACKNOWLEDGE call 58bae39d-f2fa-4160-84d2-ddf3d1c728bf for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:07:02.656904 31525 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.657546 31530 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 58bae39d-f2fa-4160-84d2-ddf3d1c728bf) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
hello
hello
Command exited with status 0 (pid: 31814)
Command exited with status 0 (pid: 31815)
I0707 18:07:02.834297 31526 slave.cpp:3285] Handling status update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:55420
I0707 18:07:02.836560 31529 slave.cpp:6088] Terminating task 3
I0707 18:07:02.837365 31526 slave.cpp:3285] Handling status update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from executor(1)@172.17.0.7:56283
I0707 18:07:02.838392 31534 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.838690 31534 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:02.839052 31537 slave.cpp:3678] Forwarding the update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:02.839562 31537 slave.cpp:3572] Status update manager successfully handled status update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.839733 31526 master.cpp:5273] Status update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.839855 31526 master.cpp:5321] Forwarding status update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.839866 31537 slave.cpp:3588] Sending acknowledgement for status update TASK_FINISHED (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:55420
I0707 18:07:02.840096 31526 master.cpp:6959] Updating the state of task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0707 18:07:02.840512 31535 dynamic_reservation_framework.cpp:208] Task 3 is finished at agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:07:02.840540 31535 sched.cpp:1025] Scheduler::statusUpdate took 46734ns
I0707 18:07:02.840790 31535 master.cpp:4388] Processing ACKNOWLEDGE call 5c12302d-6330-4df9-bf8f-d3443e6bccc6 for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:07:02.840873 31535 master.cpp:7025] Removing task 3 with resources cpus(test, test):1; mem(test, test):128 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.841006 31526 hierarchical.cpp:924] Recovered cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.841898 31532 slave.cpp:6088] Terminating task 4
I0707 18:07:02.843639 31535 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.843842 31535 status_update_manager.cpp:528] Cleaning up status update stream for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.844398 31535 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 5c12302d-6330-4df9-bf8f-d3443e6bccc6) for task 3 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.844481 31535 slave.cpp:6129] Completing task 3
I0707 18:07:02.845114 31532 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.845590 31532 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to the agent
I0707 18:07:02.846128 31532 slave.cpp:3678] Forwarding the update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to master@172.17.0.7:39581
I0707 18:07:02.846406 31532 slave.cpp:3572] Status update manager successfully handled status update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.846519 31532 slave.cpp:3588] Sending acknowledgement for status update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 to executor(1)@172.17.0.7:56283
I0707 18:07:02.846554 31527 master.cpp:5273] Status update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 from agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.846730 31527 master.cpp:5321] Forwarding status update TASK_FINISHED (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.846974 31527 master.cpp:6959] Updating the state of task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0707 18:07:02.847877 31524 hierarchical.cpp:924] Recovered cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.848093 31532 dynamic_reservation_framework.cpp:208] Task 4 is finished at agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:02.848188 31532 dynamic_reservation_framework.cpp:226] All tasks done, waiting for unreserving resources
I0707 18:07:02.848259 31532 sched.cpp:1025] Scheduler::statusUpdate took 173818ns
I0707 18:07:02.848578 31532 master.cpp:4388] Processing ACKNOWLEDGE call 7e8d7fb0-613e-4a73-b07f-d3eee49496d5 for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:02.848701 31532 master.cpp:7025] Removing task 4 with resources cpus(test, test):1; mem(test, test):128 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:02.849139 31532 slave.cpp:3806] executor(1)@172.17.0.7:59475 exited
I0707 18:07:02.849176 31532 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.849354 31532 status_update_manager.cpp:528] Cleaning up status update stream for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.850083 31532 slave.cpp:2671] Status update manager successfully handled status update acknowledgement (UUID: 7e8d7fb0-613e-4a73-b07f-d3eee49496d5) for task 4 of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:02.850138 31532 slave.cpp:6129] Completing task 4
I0707 18:07:02.947499 31529 containerizer.cpp:1863] Executor for container 'bdca1a15-bdb1-45bb-b19e-df07fb74e5db' has exited
I0707 18:07:02.947567 31529 containerizer.cpp:1622] Destroying container 'bdca1a15-bdb1-45bb-b19e-df07fb74e5db'
I0707 18:07:02.960427 31539 provisioner.cpp:411] Ignoring destroy request for unknown container bdca1a15-bdb1-45bb-b19e-df07fb74e5db
I0707 18:07:02.961381 31526 slave.cpp:4163] Executor '0' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 exited with status 0
I0707 18:07:02.961753 31526 slave.cpp:4267] Cleaning up executor '0' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:59475
I0707 18:07:02.962579 31535 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/0/runs/bdca1a15-bdb1-45bb-b19e-df07fb74e5db' for gc 6.99998886310222days in the future
I0707 18:07:02.963564 31535 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/0' for gc 6.99998885004741days in the future
I0707 18:07:02.973430 31538 slave.cpp:3806] executor(1)@172.17.0.7:51409 exited
I0707 18:07:02.995627 31536 slave.cpp:3806] executor(1)@172.17.0.7:34513 exited
I0707 18:07:03.050102 31538 containerizer.cpp:1863] Executor for container 'cffaea8f-effc-4388-902d-1ae39d1e5bfb' has exited
I0707 18:07:03.050295 31538 containerizer.cpp:1622] Destroying container 'cffaea8f-effc-4388-902d-1ae39d1e5bfb'
I0707 18:07:03.052471 31531 containerizer.cpp:1863] Executor for container 'cde8072a-fc80-426d-833f-57e3cf50f368' has exited
I0707 18:07:03.052614 31531 containerizer.cpp:1622] Destroying container 'cde8072a-fc80-426d-833f-57e3cf50f368'
I0707 18:07:03.068886 31533 provisioner.cpp:411] Ignoring destroy request for unknown container cffaea8f-effc-4388-902d-1ae39d1e5bfb
I0707 18:07:03.069725 31524 slave.cpp:4163] Executor '2' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 exited with status 0
I0707 18:07:03.069861 31524 slave.cpp:4267] Cleaning up executor '2' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:51409
I0707 18:07:03.071491 31524 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/2/runs/cffaea8f-effc-4388-902d-1ae39d1e5bfb' for gc 6.9999991881837days in the future
I0707 18:07:03.071869 31524 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/2' for gc 6.99999918568296days in the future
I0707 18:07:03.084206 31528 provisioner.cpp:411] Ignoring destroy request for unknown container cde8072a-fc80-426d-833f-57e3cf50f368
I0707 18:07:03.084844 31530 slave.cpp:4163] Executor '1' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 exited with status 0
I0707 18:07:03.085223 31530 slave.cpp:4267] Cleaning up executor '1' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:34513
I0707 18:07:03.085597 31526 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/2/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/1/runs/cde8072a-fc80-426d-833f-57e3cf50f368' for gc 6.99999901009185days in the future
I0707 18:07:03.085814 31530 slave.cpp:4355] Cleaning up framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.086110 31530 status_update_manager.cpp:282] Closing status update streams for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.086287 31526 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/2/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/1' for gc 6.99999900754667days in the future
I0707 18:07:03.086448 31526 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/2/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S1/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000' for gc 6.99999900508148days in the future
I0707 18:07:03.282027 31534 hierarchical.cpp:1632] No inverse offers to send out!
I0707 18:07:03.282196 31534 hierarchical.cpp:1172] Performed allocation for 3 agents in 6.269452ms
I0707 18:07:03.285627 31534 master.cpp:5835] Sending 3 offers to framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:03.288548 31534 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O9 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:03.288714 31534 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O10 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:03.289059 31534 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O11 with cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:03.289353 31534 sched.cpp:917] Scheduler::resourceOffers took 808575ns
I0707 18:07:03.295822 31534 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O10 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:03.296022 31534 master.cpp:3201] Authorizing principal 'test' to unreserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 18:07:03.299430 31534 master.cpp:3468] Processing ACCEPT call for offers: [ 7892fbb2-1ac1-450f-8576-10c1df35f765-O11 ] on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486) for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:03.299563 31534 master.cpp:3201] Authorizing principal 'test' to unreserve resources 'cpus(test, test):1; mem(test, test):128'
I0707 18:07:03.300375 31534 master.cpp:3747] Applying UNRESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:03.302352 31534 master.cpp:7098] Sending checkpointed resources  to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 at slave(2)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:03.303211 31534 master.cpp:3747] Applying UNRESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581 to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:03.303390 31527 slave.cpp:2600] Updated checkpointed resources from cpus(test, test):1; mem(test, test):128 to 
I0707 18:07:03.304319 31528 hierarchical.cpp:683] Updated allocation of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000] to ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):16; mem(*):47270
I0707 18:07:03.305121 31534 master.cpp:7098] Sending checkpointed resources  to agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 at slave(1)@172.17.0.7:39581 (753c2ae3a486)
I0707 18:07:03.305532 31531 slave.cpp:2600] Updated checkpointed resources from cpus(test, test):1; mem(test, test):128 to 
I0707 18:07:03.306602 31528 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):16; mem(*):47270 (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.311444 31528 hierarchical.cpp:683] Updated allocation of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from cpus(test, test):1; mem(test, test):128; cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000] to ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):16; mem(*):47270
I0707 18:07:03.312047 31528 hierarchical.cpp:924] Recovered ports(*):[31000-32000]; disk(*):3.70122e+06; cpus(*):16; mem(*):47270 (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.841215 31530 slave.cpp:3806] executor(1)@172.17.0.7:55420 exited
I0707 18:07:03.841861 31533 slave.cpp:3806] executor(1)@172.17.0.7:56283 exited
I0707 18:07:03.857960 31525 containerizer.cpp:1863] Executor for container 'e31dfccd-5f2a-40e1-95a0-b5b253fac912' has exited
I0707 18:07:03.858224 31525 containerizer.cpp:1622] Destroying container 'e31dfccd-5f2a-40e1-95a0-b5b253fac912'
I0707 18:07:03.858165 31537 containerizer.cpp:1863] Executor for container 'f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84' has exited
I0707 18:07:03.858378 31537 containerizer.cpp:1622] Destroying container 'f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84'
I0707 18:07:03.866737 31532 provisioner.cpp:411] Ignoring destroy request for unknown container f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84
I0707 18:07:03.867043 31527 slave.cpp:4163] Executor '4' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 exited with status 0
I0707 18:07:03.867420 31527 slave.cpp:4267] Cleaning up executor '4' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:56283
I0707 18:07:03.867700 31536 provisioner.cpp:411] Ignoring destroy request for unknown container e31dfccd-5f2a-40e1-95a0-b5b253fac912
I0707 18:07:03.868448 31538 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/4/runs/f4bdc7d6-e7b1-4bd7-84e3-a7353f8b3e84' for gc 6.99998995501037days in the future
I0707 18:07:03.868978 31531 slave.cpp:4163] Executor '3' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 exited with status 0
I0707 18:07:03.869102 31531 slave.cpp:4267] Cleaning up executor '3' of framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 at executor(1)@172.17.0.7:55420
I0707 18:07:03.869125 31533 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/4' for gc 6.99998994908444days in the future
I0707 18:07:03.869405 31533 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/3/runs/e31dfccd-5f2a-40e1-95a0-b5b253fac912' for gc 6.99998993851852days in the future
I0707 18:07:03.869010 31527 slave.cpp:4355] Cleaning up framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.869788 31526 status_update_manager.cpp:282] Closing status update streams for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.869946 31531 slave.cpp:4355] Cleaning up framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.870136 31539 status_update_manager.cpp:282] Closing status update streams for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:03.870043 31538 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000/executors/3' for gc 6.99998993607704days in the future
I0707 18:07:03.870403 31538 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/1/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S0/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000' for gc 6.99998992845926days in the future
I0707 18:07:03.870625 31533 gc.cpp:55] Scheduling '/tmp/mesos-IFR4rG/0/slaves/7892fbb2-1ac1-450f-8576-10c1df35f765-S2/frameworks/7892fbb2-1ac1-450f-8576-10c1df35f765-0000' for gc 6.99998993375111days in the future
I0707 18:07:04.284621 31537 hierarchical.cpp:1632] No inverse offers to send out!
I0707 18:07:04.284754 31537 hierarchical.cpp:1172] Performed allocation for 3 agents in 1.977235ms
I0707 18:07:04.285658 31534 master.cpp:5835] Sending 2 offers to framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:04.286209 31534 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O12 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:04.286334 31534 dynamic_reservation_framework.cpp:84] Received offer 7892fbb2-1ac1-450f-8576-10c1df35f765-O13 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]
I0707 18:07:04.286553 31534 sched.cpp:1987] Asked to stop the driver
I0707 18:07:04.286667 31534 sched.cpp:917] Scheduler::resourceOffers took 458638ns
I0707 18:07:04.286797 31534 sched.cpp:1187] Stopping framework '7892fbb2-1ac1-450f-8576-10c1df35f765-0000'
I0707 18:07:04.287029 31539 master.cpp:6410] Processing TEARDOWN call for framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:04.287063 31539 master.cpp:6422] Removing framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 (Dynamic Reservation Framework (C++)) at scheduler-c51aa6e6-f5b6-4bfc-982c-9a71ea56a862@172.17.0.7:39581
I0707 18:07:04.288715 31539 hierarchical.cpp:382] Deactivated framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.289376 31539 hierarchical.cpp:924] Recovered cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.290086 31539 hierarchical.cpp:924] Recovered cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.290809 31539 hierarchical.cpp:924] Recovered cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1 from framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.291211 31539 hierarchical.cpp:333] Removed framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.291306 31539 slave.cpp:2292] Asked to shut down framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 by master@172.17.0.7:39581
W0707 18:07:04.291337 31539 slave.cpp:2307] Cannot shut down unknown framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.291376 31539 slave.cpp:2292] Asked to shut down framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 by master@172.17.0.7:39581
W0707 18:07:04.291400 31539 slave.cpp:2307] Cannot shut down unknown framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.291467 31539 slave.cpp:2292] Asked to shut down framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000 by master@172.17.0.7:39581
W0707 18:07:04.291494 31539 slave.cpp:2307] Cannot shut down unknown framework 7892fbb2-1ac1-450f-8576-10c1df35f765-0000
I0707 18:07:04.292186 31500 sched.cpp:1987] Asked to stop the driver
I0707 18:07:04.292228 31500 sched.cpp:1990] Ignoring stop because the status of the driver is DRIVER_STOPPED
I0707 18:07:04.292887 31537 master.cpp:1218] Master terminating
I0707 18:07:04.293656 31525 hierarchical.cpp:510] Removed agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S2
I0707 18:07:04.293948 31525 hierarchical.cpp:510] Removed agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S1
I0707 18:07:04.294634 31526 hierarchical.cpp:510] Removed agent 7892fbb2-1ac1-450f-8576-10c1df35f765-S0
I0707 18:07:04.295132 31535 slave.cpp:3806] master@172.17.0.7:39581 exited
I0707 18:07:04.295198 31534 slave.cpp:3806] master@172.17.0.7:39581 exited
I0707 18:07:04.295245 31526 slave.cpp:3806] master@172.17.0.7:39581 exited
W0707 18:07:04.300513 31535 slave.cpp:3811] Master disconnected! Waiting for a new master to be elected
W0707 18:07:04.300745 31534 slave.cpp:3811] Master disconnected! Waiting for a new master to be elected
W0707 18:07:04.300839 31526 slave.cpp:3811] Master disconnected! Waiting for a new master to be elected
I0707 18:07:04.300978 31534 slave.cpp:841] Agent terminating
I0707 18:07:04.305454 31538 slave.cpp:841] Agent terminating
I0707 18:07:04.308804 31500 slave.cpp:841] Agent terminating
[       OK ] ExamplesTest.DynamicReservationFramework (10418 ms)
{code}"	MESOS	Resolved	3	1	2732	mesosphere, resource-management
13155542	Introduce a push-based gauge.	"Currently, we only have pull-based gauges which have significant performance downsides.

A push-based gauge differs from a pull-based gauge in that the client is responsible for pushing the latest value into the gauge whenever it changes. This can be challenging in some cases as it requires the client to have a good handle on when the gauge value changes (rather than just computing the current value when asked).

It is highly recommended to use push-based gauges if possible as they provide significant performance benefits over pull-based gauges. Pull-based gauge suffer from delays getting processed on the event queue of a Process, as well as incur computation cost on the Process each time the metrics are collected. Push-based gauges, on the other hand, incur no cost to the owning Process when metrics are collected, and instead incur a trivial cost when the Process pushes new values in."	MESOS	Resolved	3	4	2732	mesosphere, metrics
13118660	Add recommended accept/decline/suppress/revive behavior to scheduler docs	We should update the scheduler documentation to provide recommendations to framework authors regarding how to use the ACCEPT / DECLINE / SUPPRESS / REVIVE calls, in order to maximize resource availability.	MESOS	Reviewable	3	3	2732	mesosphere
12665374	Type resolution issue on 32 bit systems	"Cast call to min for 32 bit systems.

The call to min(ssize_t, long int) won't compile on 32 bit systems because ssize_t resolves to int on those systems. h/t to woggle on irc for helping me find this.
"	MESOS	Resolved	4	1	2732	32bit, build-failure
12843366	Support existing message passing optimization with Event/Call.	"See the thread here:
http://markmail.org/thread/wvapc7vkbv7z6gbx

The scheduler driver currently sends framework messages directly to the slave, when possible:

{noformat}
                  (through master)
    Scheduler  > Master  >  Slave >  Executor
     Driver    >                Driver
                   (skip master)
{noformat}

The slave always sends messages directly to the scheduler driver:
{noformat}
    Scheduler         Master          Slave <  Executor
     Driver    <                Driver
                   (skip master)
{noformat}

In order for the scheduler driver to receive Events from the master, it needs enough information to continue directly sending messages to slaves. This was previously accomplished by sending the slave's pid inside the [offer message|https://github.com/apache/mesos/blob/0.23.0-rc1/src/messages/messages.proto#L168]:

{code}
message ResourceOffersMessage {
  repeated Offer offers = 1;
  repeated string pids = 2;
}
{code}

We could add an 'Address' to the Offer protobuf to provide the scheduler driver with the same information:

{code}
message Address {
  required string ip;
  required string hostname;
  required uint32_t port;

  // All HTTP requests to this address must begin with this prefix.
  required string path_prefix;
}

message Offer {
  required OfferID id = 1;
  required FrameworkID framework_id = 2;
  required SlaveID slave_id = 3;
  required string hostname = 4;   // Deprecated in favor of 'address'.
  optional Address address = 8;  // Obviates 'hostname'.
  ...
}
{code}

The path prefix is required for testing purposes, where we can have multiple slaves within a process (e.g. {{localhost:5051/slave(1)/state.json}} vs. {{localhost:5051/slave(2)/state.json}}).

This provides enough information to allow the scheduler driver to continue to directly send messages to the slaves, which unblocks MESOS-2910."	MESOS	Resolved	3	3	2732	twitter
12962399	Need to add REMOVE semantics to the copy backend	"Some Dockerfiles run the `rm` command to remove files from the base image using the ""RUN"" directive in the Dockerfile. An example can be found here:
https://github.com/ngineered/nginx-php-fpm.git

In the final rootfs the removed files should not be present. Presence of these files in the final image can make the container misbehave. For example, the nginx-php-fpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different HTML root. If the default nginx config is still present after the building the image, nginx will start pointing to a different HTML root than the one set in the Dockerfile.


Currently the copy backend cannot handle removal of files from intermediate layers. This can cause issues with docker images built using a Dockerfile similar to the one listed here. Hence, we need to add REMOVE semantics to the copy backend.  "	MESOS	Resolved	3	1	4582	mesosphere
13179583	Unkillable pod container stuck in ISOLATING	"We have a simple test that launches a pod with two containers (one writes in a file and the other reads it). This test is flaky because the container sometimes fails to start.
Marathon app definition:

{code:java}
{
  ""id"": ""/simple-pod"",
  ""scaling"": {
    ""kind"": ""fixed"",
    ""instances"": 1
  },
  ""environment"": {
    ""PING"": ""PONG""
  },
  ""containers"": [
    {
      ""name"": ""ct1"",
      ""resources"": {
        ""cpus"": 0.1,
        ""mem"": 32
      },
      ""image"": {
        ""kind"": ""DOCKER"",
        ""id"": ""busybox""
      },
      ""exec"": {
        ""command"": {
          ""shell"": ""while true; do echo the current time is $(date) > ./test-v1/clock; sleep 1; done""
        }
      },
      ""volumeMounts"": [
        {
          ""name"": ""v1"",
          ""mountPath"": ""test-v1""
        }
      ]
    },
    {
      ""name"": ""ct2"",
      ""resources"": {
        ""cpus"": 0.1,
        ""mem"": 32
      },
      ""exec"": {
        ""command"": {
          ""shell"": ""while true; do echo -n $PING ' '; cat ./etc/clock; sleep 1; done""
        }
      },
      ""volumeMounts"": [
        {
          ""name"": ""v1"",
          ""mountPath"": ""etc""
        },
        {
          ""name"": ""v2"",
          ""mountPath"": ""docker""
        }
      ]
    }
  ],
  ""networks"": [
    {
      ""mode"": ""host""
    }
  ],
  ""volumes"": [
    {
      ""name"": ""v1""
    },
    {
      ""name"": ""v2"",
      ""host"": ""/var/lib/docker""
    }
  ]
}
{code}

During the test, Marathon tries to launch the pod but doesn't receive a {{TASK_RUNNING}} for the first container and so after 2min decides to kill the pod which also fails. 

Agent sandbox (attached to this ticket minus docker layers, since they're too big to attach) shows that one of the containers wasn't started properly - the last line in the agent log says:
{code}
Transitioning the state of container ff4f4fdc-9327-42fb-be40-29e919e15aee.e9b05652-e779-46f8-9b76-b2e1ce7e5940 from PREPARING to ISOLATING
{code}
Until then the log looks pretty unspektakular. 

Afterwards, Marathon tries to kill the container repeatedly, but doesn't succeed - the executor receives the reuests but doesn't send anything back:
{code}
I0816 22:52:53.111995     4 default_executor.cpp:204] Received SUBSCRIBED event
I0816 22:52:53.112520     4 default_executor.cpp:208] Subscribed executor on 10.10.0.222
I0816 22:52:53.112783     4 default_executor.cpp:204] Received LAUNCH_GROUP event
I0816 22:52:53.116516    11 default_executor.cpp:428] Setting 'MESOS_CONTAINER_IP' to: 10.10.0.222
I0816 22:52:53.169596     4 default_executor.cpp:204] Received ACKNOWLEDGED event
I0816 22:52:53.194416    10 default_executor.cpp:204] Received ACKNOWLEDGED event
I0816 22:54:50.559470     8 default_executor.cpp:204] Received KILL event
I0816 22:54:50.559496     8 default_executor.cpp:1251] Received kill for task 'simple-pod-bcc8f180b611494aa972520b8b650ca9.instance-1ad9ecbb-a1a7-11e8-b35a-6e17842c13e2.ct1'
I0816 22:54:50.559737     4 default_executor.cpp:204] Received KILL event
I0816 22:54:50.559751     4 default_executor.cpp:1251] Received kill for task 'simple-pod-bcc8f180b611494aa972520b8b650ca9.instance-1ad9ecbb-a1a7-11e8-b35a-6e17842c13e2.ct2'
...
{code}

Relevant Ids for grepping the logs:
{code}
Marathon app id: /simple-pod-bcc8f180b611494aa972520b8b650ca9
Mesos tasks id: simple-pod-bcc8f180b611494aa972520b8b650ca9.instance-1ad9ecbb-a1a7-11e8-b35a-6e17842c13e2.ct1
Mesos container id: e9b05652-e779-46f8-9b76-b2e1ce7e5940
{code}"	MESOS	Resolved	3	1	4582	container-stuck
13057788	Unified containerizer does not support docker registry version < 2.3.	"in file `src/uri/fetchers/docker.cpp`

```
    Option<string> contentType = response.headers.get(""Content-Type"");  
        if (contentType.isSome() &&  
            !strings::startsWith(  
                contentType.get(),  
                ""application/vnd.docker.distribution.manifest.v1"")) {  
          return Failure(  
              ""Unsupported manifest MIME type: "" + contentType.get());  
        }  
```

Docker fetcher check the contentType strictly, while docker registry with version < 2.3 returns manifests with contentType `application/json`, that leading failure like `E0321 13:27:27.572402 40370 slave.cpp:4650] Container 'xxx' for executor 'xxx' of framework xxx failed to start: Unsupported manifest MIME type: application/json; charset=utf-8`."	MESOS	Resolved	1	1	4582	easyfix
13048405	Persistent volume ownership is set to root when task is running with non-root user	"Im running docker container in universal containerizer, mesos 1.1.0. switch_user=true, isolator=filesystem/linux,docker/runtime.  Container is launched with marathon, user:someappuser. Id want to use persistent volume, but its exposed to container with root user permissions even if root folder is created with someppuser ownership (looks like mesos do chown to this folder). 

here logs for my container:
{code}
I0305 22:51:36.414655 10175 slave.cpp:1701] Launching task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' for framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.415118 10175 paths.cpp:536] Trying to chown '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a' to user 'root'
I0305 22:51:36.422992 10175 slave.cpp:6179] Launching executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a'
I0305 22:51:36.424278 10175 slave.cpp:1987] Queued task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' for executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.424347 10158 docker.cpp:1000] Skipping non-docker container
I0305 22:51:36.425639 10142 containerizer.cpp:938] Starting container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a for executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.428725 10166 provisioner.cpp:294] Provisioning image rootfs '/export/intssd/mesos-slave/workdir/provisioner/containers/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/backends/copy/rootfses/0e2181e9-1bf2-42d4-8cb0-ee70e466c3ae' for container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a
I0305 22:51:42.981240 10149 linux.cpp:695] Changing the ownership of the persistent volume at '/export/intssd/mesos-slave/data/volumes/roles/general_marathon_service_role/md_hdfs_journal#data#23f813aa-01dd-11e7-a012-0242ce94d92a' with uid 0 and gid 0
I0305 22:51:42.986593 10136 linux_launcher.cpp:421] Launching container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a and cloning with namespaces CLONE_NEWNS
{code}

{code}
ls -la /export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/
drwxr-xr-x 3 someappuser someappgroup   4096 22:51 .
drwxr-xr-x 3 root     root            4096 22:51 ..
drwxr-xr-x 2 root     root            4096 22:51 data
-rw-r--r-- 1 root     root             169 22:51 stderr
-rw-r--r-- 1 root     root          183012 23:00 stdout
{code}"	MESOS	Resolved	2	1	4582	user
13070132	Provisioner recover should not always assume 'rootfses' dir exists.	"The mesos agent would restart due to many reasons (e.g., disk full). Always assume the provisioner 'rootfses' dir exists would block the agent to recover.

{noformat}
Failed to perform recovery: Collect failed: Unable to list rootfses belonged to container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847: Unable to list the backend directory: Failed to opendir '/var/lib/mesos/slave/provisioner/containers/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/backends/overlay/rootfses': No such file or directory
{noformat}

This issue may occur due to the race between removing the provisioner container dir and the agent restarts:
{noformat}
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.058349 11441 linux_launcher.cpp:429] Launching container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.072191 11441 systemd.cpp:96] Assigned child process '11577' to 'mesos_executors.slice'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.075932 11439 containerizer.cpp:1592] Checkpointing container's forked pid 11577 to '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008/executors/node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05/runs/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/pids/forked.pid'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.081516 11438 linux_launcher.cpp:429] Launching container 03a57a37-eede-46ec-8420-dda3cc54e2e0 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.083516 11438 systemd.cpp:96] Assigned child process '11579' to 'mesos_executors.slice'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.087345 11444 containerizer.cpp:1592] Checkpointing container's forked pid 11579 to '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/36a25adb-4ea2-49d3-a195-448cff1dc146-0002/executors/66897/runs/03a57a37-eede-46ec-8420-dda3cc54e2e0/pids/forked.pid'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: W0505 02:14:32.213049 11440 fetcher.cpp:896] Begin fetcher log (stderr in sandbox) for container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac from running command: /opt/mesosphere/packages/mesos--aaedd03eee0d57f5c0d49c74ff1e5721862cad98/libexec/mesos/mesos-fetcher
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.006201 11561 fetcher.cpp:531] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/36a25adb-4ea2-49d3-a195-448cff1dc146-S34\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""https:\/\/downloads.mesosphere.com\/libmesos-bundle\/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz""}},{""action"":""BYPASS_CACHE"",
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009678 11561 fetcher.cpp:442] Fetching URI 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009693 11561 fetcher.cpp:283] Fetching directly into the sandbox directory
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009711 11561 fetcher.cpp:220] Fetching URI 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009723 11561 fetcher.cpp:163] Downloading resource from 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz' to '/var/lib/mesos/slave/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011/executors/hello__91922a16-889e-4e94-9dab-9f6754f091de/
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: Failed to fetch 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz': Error downloading resource: Failed writing received data to disk/application
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: End fetcher log for container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.213114 11440 fetcher.cpp:558] Failed to run mesos-fetcher: Failed to fetch all URIs for container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' with exit status: 256
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.213351 11444 slave.cpp:4642] Container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' for executor 'hello__91922a16-889e-4e94-9dab-9f6754f091de' of framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011 failed to start: Failed to fetch all URIs for container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' with exit status: 256
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.213614 11443 containerizer.cpp:2071] Destroying container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac in FETCHING state
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.213977 11443 linux_launcher.cpp:505] Asked to destroy container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.214757 11443 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.216047 11444 cgroups.cpp:2692] Freezing cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.218407 11443 cgroups.cpp:1405] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac after 2.326016ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.220391 11445 cgroups.cpp:2710] Thawing cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.222124 11445 cgroups.cpp:1434] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac after 1.693952ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.239018 11441 fetcher.cpp:558] Failed to run mesos-fetcher: Failed to create 'stdout' file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.239162 11442 slave.cpp:4642] Container 'a30b74d5-53ac-4fbf-b8f3-5cfba58ea847' for executor 'node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05' of framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008 failed to start: Failed to create 'stdout' file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.239284 11445 containerizer.cpp:2071] Destroying container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 in FETCHING state
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.239390 11444 linux_launcher.cpp:505] Asked to destroy container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.240103 11444 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.241353 11440 cgroups.cpp:2692] Freezing cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.243120 11444 cgroups.cpp:1405] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 after 1.726976ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.245045 11440 cgroups.cpp:2710] Thawing cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.246800 11440 cgroups.cpp:1434] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 after 1.715968ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.285477 11438 slave.cpp:1625] Got assigned task 'dse-1-agent__720d6f09-9d60-4667-b224-abcd495e0e58' for framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0009
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: F0505 02:14:32.296481 11438 slave.cpp:6381] CHECK_SOME(state::checkpoint(path, info)): Failed to create temporary file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: *** Check failure stack trace: ***
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856be857d  google::LogMessage::Fail()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856bea3ad  google::LogMessage::SendToLog()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856be816c  google::LogMessage::Flush()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856beaca9  google::LogMessageFatal::~LogMessageFatal()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5855e4b5e9  _CheckFatal::~_CheckFatal()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.314082 11445 containerizer.cpp:2434] Container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac has exited
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.314826 11440 containerizer.cpp:2434] Container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 has exited
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316660 11439 container_assigner.cpp:101] Unregistering container_id[value: ""6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac""].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316761 11474 container_assigner_strategy.cpp:202] Closing ephemeral-port reader for container[value: ""6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac""] at endpoint[198.51.100.1:34273].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316804 11474 container_reader_impl.cpp:38] Triggering ContainerReader shutdown
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316833 11474 sync_util.hpp:39] Dispatching and waiting <=5s for ticket 7: ~ContainerReaderImpl:shutdown
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316769 11439 container_assigner.cpp:101] Unregistering container_id[value: ""a30b74d5-53ac-4fbf-b8f3-5cfba58ea847""].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316864 11474 container_reader
{noformat}

In provisioner recover, when listing the container rootfses, it is possible that the 'rootfses' dir does not exist. Because a possible race between the provisioner destroy and the agent restart. For instance, while the provisioner is destroying the container dir the agent restarts. Due to os::rmdir() is recursive by traversing the FTS tree, it is possible that 'rootfses' dir is removed but the others (e.g., scratch dir) are not.

Currently, we are returning an error if the 'rootfses' dir does not exist, which blocks the agent from recovery. We should skip it if 'rootfses' does not exist."	MESOS	Resolved	2	1	4582	provisioner
13005237	Authentication in v2 protobuf should not be `required`.	"I was testing the mesos GPU support.  However, I have notice the issue between different docker repository.  The public docker hub works fine, but the private docker repository by JFrog doesn't work as expected.

I have setup the environment according to this document.
https://mesosphere.github.io/marathon/docs/native-docker.html

Tested with mesos-execute command:
billz2:/etc/mesos-slave$ sudo mesos-execute       --master=bz01.apple.com:5050       --name=gpu-test       --docker_image=docker.apple.com/nvidia/cuda       --command=""nvidia-smi""       --framework_capabilities=""GPU_RESOURCES""       --resources=""gpus:2""
I0914 18:32:51.571482 26084 scheduler.cpp:172] Version: 1.0.1
I0914 18:32:51.579815 26087 scheduler.cpp:461] New master detected at master@17.x.x.x:5050
Subscribed with ID 'c0968c96-cc66-4990-9c49-d5ef26d07a07-0015'
Submitted task 'gpu-test' to agent 'c0968c96-cc66-4990-9c49-d5ef26d07a07-S17370'
Received status update TASK_FAILED for task 'gpu-test'
  message: 'Failed to launch container: Failed to parse the image manifest: Protobuf parse failed: Missing required fields: signatures[0].header.jwk.kid; Container destroyed while provisioning images'
  source: SOURCE_AGENT
  reason: REASON_CONTAINER_LAUNCH_FAILED

The authentication in v2 protobuf should
not be `required`."	MESOS	Resolved	3	1	4582	containerizer, docker, mesosphere
12993470	"Unable to run ""scratch"" Dockerfiles with Unified Containerizer."	"It is not possible to run Docker containers that are based upon the ""scratch"" container.

Setup: Mesos 1.0.0 with the following Mesos settings:

{code:none}
echo 'docker' | sudo tee /etc/mesos-slave/image_providers
echo 'filesystem/linux,docker/runtime' | sudo tee /etc/mesos-slave/isolation
{code}

Recreate: From a Master or Slave, run:

{code:none}
mesos-execute --command='echo ok' --docker_image=hello-seattle --master=localhost:5050 --name=test
{code}

Effect: The container will crash with messages from Mesos reporting it can't mount folder x/y/z. E.g. can't mount /tmp. This means you can't run any container that is not a ""fat"" container (i.e. one with a full OS). E.g. error: 
bq. Failed to enter chroot '/var/lib/mesos/provisioner/containers/fed6add8-0126-40e6-ae81-5859a0c1a2d4/backends/copy/rootfses/4feefc8b-fd5a-4835-95db-165e675f11cd': /tmp in chroot does not existI0729 07:49:56.753474  4362 exec.cpp:413] Executor asked to shutdown

Expected: Run without issues.

Use case: We use scratch based containers with static binaries to keep the image size down. This is a common practice."	MESOS	Resolved	3	1	4582	containerizer, filesystem, mesosphere
12915505	Refactor registry client/puller to avoid JSON and struct.	We should get rid of all JSON and struct for message passing as function returned type. By using the methods provided by spec.hpp to refactor all unnecessary JSON message and struct in registry client and registry puller. Also, remove all redundant check in registry client that are already checked by spec validation. 	MESOS	Resolved	3	4	4582	mesosphere
12905599	MESOS_NATIVE_JAVA_LIBRARY not set on MesosContainerize tasks with --executor_environmnent_variables	"When using --executor_environment_variables, and having MESOS_NATIVE_JAVA_LIBRARY in the environment of mesos-slave, the mesos containerizer does not set MESOS_NATIVE_JAVA_LIBRARY itself.

Relevant code: https://github.com/apache/mesos/blob/14f7967ef307f3d98e3a4b93d92d6b3a56399b20/src/slave/containerizer/containerizer.cpp#L281

It sees that the variable is in the mesos-slave's environment (os::getenv), rather than checking if it is set in the environment variable set."	MESOS	Resolved	3	1	4582	mesosphere, newbie
13152892	Support docker image tarball hdfs based fetching.	Support docker image tarball hdfs based fetching.	MESOS	Resolved	1	3	4582	containerizer, fetcher, hdfs, mesosphere
12934242	Introduce docker runtime isolator.	Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.	MESOS	Resolved	3	1	4582	mesosphere
13036377	Support linux filesystem type detection.	We should support detecting a linux filesystem type (e.g., xfs, extfs) and its filesystem id mapping.	MESOS	Resolved	2	3	4582	filesystem, linux
13008779	Support nested containers for logger in Mesos Containerizer.	"Currently, there are two issues in mesos containerizer using logger for nested contaienrs:

1. An empty executorinfo is passed to logger when launching a nested container, it would potentially break some logger modules if any module tries to access the required proto field (e.g., executorId).

2. The logger does not reocver the nested containers yet in MesosContainerizer::recover."	MESOS	Resolved	1	1	4582	containerizer, logger, mesosphere
13044988	The agent may be flapping after the machine reboots due to provisioner recover.	"After the agent machine reboots, if the agent work dir survives (e.g., /var/lib/mesos) and the container runtime directory is gone (an empty SlaveState as well), the provisioner recover() would get into segfault because that case break the semantic that a child container should always be cleaned up before it parent container.

This is a particular case which only happens if the machine reboots and the provisioner directory survives.

{noformat}
F0217 01:10:18.423238 30099 provisioner.cpp:504] Check failed: entry.parent() != containerId Failed to destroy container 1 since its nested container 1.2 has not been destroyed yet
*** Check failure stack trace: ***
    @     0x7fceb444121d  google::LogMessage::Fail()
    @     0x7fceb44405ee  google::LogMessage::SendToLog()
    @     0x7fceb4440eed  google::LogMessage::Flush()
    @     0x7fceb4444368  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fceb36137f9  mesos::internal::slave::ProvisionerProcess::destroy()
    @     0x7fceb36126f0  mesos::internal::slave::ProvisionerProcess::recover()
    @     0x7fceb3637fc6  _ZZN7process8dispatchI7NothingN5mesos8internal5slave18ProvisionerProcessERK7hashsetINS2_11ContainerIDESt4hashIS7_ESt8equal_toIS7_EESC_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSJ_FSH_T1_ET2_ENKUlPNS_11ProcessBaseEE_clESS_
    @     0x7fceb3637bc2  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingN5mesos8internal5slave18ProvisionerProcessERK7hashsetINS6_11ContainerIDESt4hashISB_ESt8equal_toISB_EESG_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSN_FSL_T1_ET2_EUlS2_E_E9_M_invokeERKSt9_Any_dataOS2_
    @     0x7fceb43848e4  std::function<>::operator()()
    @     0x7fceb436baf4  process::ProcessBase::visit()
    @     0x7fceb43e5fde  process::DispatchEvent::visit()
    @           0x9e4101  process::ProcessBase::serve()
    @     0x7fceb4369007  process::ProcessManager::resume()
    @     0x7fceb4377a8c  process::ProcessManager::init_threads()::$_2::operator()()
    @     0x7fceb4377995  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvE3$_2vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7fceb4377965  std::_Bind_simple<>::operator()()
    @     0x7fceb437793c  std::thread::_Impl<>::_M_run()
    @     0x7fceadefa030  (unknown)
    @     0x7fcead70b6aa  start_thread
    @     0x7fcead440e9d  (unknown)
{noformat}

The provisioner directory is supposed to be under the container runtime directory. However, this is not backward compatible. We can only change it after a deprecation cycle.

For now, we have to three options:
1. make provisioner::destroy() recursive.
2. sort the container during recovery to guarantee `child before parent` semantic.
3. remove the check-failure since the while provisioner dir will be removed eventually at the end (not recommended).

Recommend (1)."	MESOS	Resolved	1	1	4582	nested, provisioner
13085606	Persistent volume might not be mounted if there is a sandbox volume whose source is the same as the target of the persistent volume.	"This issue is only for Mesos Containerizer.

If the source of a sandbox volume is a relative path, we'll create the directory in the sandbox in Isolator::prepare method:
https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L480-L485

And then, we'll try to mount persistent volumes. However, because of this TODO in the code:
https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L726-L739

We'll skip mounting the persistent volume. That will cause a silent failure.

This is important because the workaround we suggest folks to solve MESOS-4016 is to use an additional sandbox volume."	MESOS	Resolved	2	1	4582	mesosphere, persistent-volumes
12981167	MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky.	"{noformat}
[03:36:29] :	 [Step 10/10] [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.461802  2797 cluster.cpp:155] Creating default 'local' authorizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.469468  2797 leveldb.cpp:174] Opened db in 7.527163ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470188  2797 leveldb.cpp:181] Compacted db in 699544ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470206  2797 leveldb.cpp:196] Created db iterator in 4293ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470211  2797 leveldb.cpp:202] Seeked to beginning of db in 535ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470216  2797 leveldb.cpp:271] Iterated through 0 keys in the db in 321ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470230  2797 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470510  2815 recover.cpp:451] Starting replica recovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470592  2817 recover.cpp:477] Replica is in EMPTY status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471029  2813 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (19800)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471139  2816 recover.cpp:197] Received a recover response from a replica in EMPTY status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471271  2818 recover.cpp:568] Updating replica status to STARTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471606  2811 master.cpp:382] Master 6d44b7c1-ac0b-4409-97df-a53fa2e39d09 (ip-172-30-2-29.mesosphere.io) started on 172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471619  2811 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/baXWq5/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/baXWq5/master"" --zk_session_timeout=""10secs""
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471745  2811 master.cpp:434] Master only allowing authenticated frameworks to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471753  2811 master.cpp:448] Master only allowing authenticated agents to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471757  2811 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471761  2811 credentials.hpp:37] Loading credentials for authentication from '/tmp/baXWq5/credentials'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471829  2811 master.cpp:506] Using default 'crammd5' authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471868  2811 master.cpp:578] Using default 'basic' HTTP authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471941  2811 master.cpp:658] Using default 'basic' HTTP framework authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471977  2811 master.cpp:705] Authorization enabled
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472034  2817 hierarchical.cpp:142] Initialized hierarchical allocator process
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472038  2814 whitelist_watcher.cpp:77] No whitelist given
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472506  2811 master.cpp:1969] The newly elected leader is master@172.30.2.29:37328 with id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472522  2811 master.cpp:1982] Elected as the leading master!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472527  2811 master.cpp:1669] Recovering from registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472573  2812 registrar.cpp:332] Recovering registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473511  2816 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.195002ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473527  2816 replica.cpp:320] Persisted replica status to STARTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473578  2816 recover.cpp:477] Replica is in STARTING status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473877  2815 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (19803)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473989  2814 recover.cpp:197] Received a recover response from a replica in STARTING status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474126  2817 recover.cpp:568] Updating replica status to VOTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474735  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547332ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474748  2811 replica.cpp:320] Persisted replica status to VOTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474783  2811 recover.cpp:582] Successfully joined the Paxos group
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474829  2811 recover.cpp:466] Recover process terminated
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474969  2818 log.cpp:553] Attempting to start the writer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475361  2811 replica.cpp:493] Replica received implicit promise request from (19804)@172.30.2.29:37328 with proposal 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475944  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 559444ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475956  2811 replica.cpp:342] Persisted promised to 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.476215  2815 coordinator.cpp:238] Coordinator attempting to fill missing positions
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.476660  2816 replica.cpp:388] Replica received explicit promise request from (19805)@172.30.2.29:37328 for position 0 with proposal 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477262  2816 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 584333ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477273  2816 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477699  2815 replica.cpp:537] Replica received write request for position 0 from (19806)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477726  2815 leveldb.cpp:436] Reading position from leveldb took 8842ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478277  2815 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 537361ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478291  2815 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478569  2811 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479132  2811 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 545208ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479146  2811 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479152  2811 replica.cpp:697] Replica learned NOP action at position 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479317  2814 log.cpp:569] Writer started with ending position 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479568  2811 leveldb.cpp:436] Reading position from leveldb took 8325ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479786  2814 registrar.cpp:365] Successfully fetched the registry (0B) in 7.192064ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479822  2814 registrar.cpp:464] Applied 1 operations in 3018ns; attempting to update the 'registry'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479995  2818 log.cpp:577] Attempting to append 205 bytes to the log
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480044  2818 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480309  2811 replica.cpp:537] Replica received write request for position 1 from (19807)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480928  2811 leveldb.cpp:341] Persisting action (224 bytes) to leveldb took 596433ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480942  2811 replica.cpp:712] Persisted action at 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481148  2815 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481710  2815 leveldb.cpp:341] Persisting action (226 bytes) to leveldb took 545656ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481722  2815 replica.cpp:712] Persisted action at 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481727  2815 replica.cpp:697] Replica learned APPEND action at position 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481958  2816 registrar.cpp:509] Successfully updated the 'registry' in 2.119168ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482014  2816 registrar.cpp:395] Successfully recovered registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482045  2817 log.cpp:596] Attempting to truncate the log to 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482117  2817 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482166  2816 master.cpp:1777] Recovered 0 agents from the Registry (166B) ; allowing 10mins for agents to re-register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482177  2817 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482404  2817 replica.cpp:537] Replica received write request for position 2 from (19808)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482975  2817 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 552763ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482986  2817 replica.cpp:712] Persisted action at 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483301  2813 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483870  2813 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 547529ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483896  2813 leveldb.cpp:399] Deleting ~1 keys from leveldb took 12161ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483904  2813 replica.cpp:712] Persisted action at 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483911  2813 replica.cpp:697] Replica learned TRUNCATE action at position 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.492995  2797 containerizer.cpp:201] Using isolation: cgroups/mem,filesystem/posix,network/cni
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.496548  2797 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503572  2797 cluster.cpp:432] Creating default 'local' authorizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503936  2817 slave.cpp:203] Agent started on 488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503952  2817 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL""
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504148  2817 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504189  2817 slave.cpp:341] Agent using credential for: test-principal
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504199  2817 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504245  2817 slave.cpp:393] Using default 'basic' HTTP authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504410  2797 sched.cpp:224] Version: 1.0.0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504416  2817 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504580  2818 sched.cpp:328] New master detected at master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504613  2818 sched.cpp:394] Authenticating with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504622  2818 sched.cpp:401] Using default CRAM-MD5 authenticatee
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504649  2817 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504673  2817 slave.cpp:600] Agent attributes: [  ]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504678  2817 slave.cpp:605] Agent hostname: ip-172-30-2-29.mesosphere.io
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504703  2816 authenticatee.cpp:121] Creating new client SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504830  2818 master.cpp:5943] Authenticating scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504887  2816 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(991)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504982  2811 authenticator.cpp:98] Creating new server SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505004  2816 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505105  2813 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505131  2813 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505138  2818 status_update_manager.cpp:200] Recovering status update manager
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505167  2813 authenticator.cpp:204] Received SASL authentication start
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2813 authenticator.cpp:326] Authentication requires more steps
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2814 containerizer.cpp:514] Recovering containerizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505241  2813 authenticatee.cpp:259] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505300  2812 authenticator.cpp:232] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505317  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505323  2812 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505331  2812 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505337  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505342  2812 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505347  2812 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505355  2812 authenticator.cpp:318] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505399  2813 authenticatee.cpp:299] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505421  2811 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(991)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505436  2812 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505534  2816 sched.cpp:484] Successfully authenticated with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505553  2816 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505591  2816 sched.cpp:833] Will retry registration in 11.319315ms if necessary
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505672  2815 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505702  2815 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505854  2818 master.cpp:2615] Subscribing framework default with checkpointing enabled and capabilities [  ]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506031  2818 sched.cpp:723] Framework registered with 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506050  2816 hierarchical.cpp:264] Added framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506072  2816 hierarchical.cpp:1488] No allocations performed
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506073  2818 sched.cpp:737] Scheduler::registered took 28711ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506093  2816 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506126  2816 hierarchical.cpp:1139] Performed allocation for 0 agents in 59667ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506428  2818 provisioner.cpp:253] Provisioner recovery complete
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506570  2815 slave.cpp:4845] Finished recovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506747  2815 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506878  2813 slave.cpp:967] New master detected at master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506886  2814 status_update_manager.cpp:174] Pausing sending status updates
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506903  2813 slave.cpp:1029] Authenticating with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506924  2813 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506976  2813 slave.cpp:1002] Detecting new master
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506989  2816 authenticatee.cpp:121] Creating new client SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507069  2813 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507145  2815 master.cpp:5943] Authenticating slave(488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507202  2811 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(992)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507264  2817 authenticator.cpp:98] Creating new server SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507374  2817 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507387  2817 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507433  2813 authenticator.cpp:204] Received SASL authentication start
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507467  2813 authenticator.cpp:326] Authentication requires more steps
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507511  2813 authenticatee.cpp:259] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507578  2811 authenticator.cpp:232] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507597  2811 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507606  2811 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507617  2811 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507629  2811 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507640  2811 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507648  2811 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507686  2811 authenticator.cpp:318] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507750  2817 authenticatee.cpp:299] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507766  2811 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(992)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507786  2813 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507863  2817 slave.cpp:1108] Successfully authenticated with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507910  2817 slave.cpp:1511] Will retry registration in 10.588836ms if necessary
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507966  2812 master.cpp:4653] Registering agent at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508059  2817 registrar.cpp:464] Applied 1 operations in 13429ns; attempting to update the 'registry'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508244  2812 log.cpp:577] Attempting to append 390 bytes to the log
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508296  2817 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508546  2815 replica.cpp:537] Replica received write request for position 3 from (19831)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509158  2815 leveldb.cpp:341] Persisting action (409 bytes) to leveldb took 589901ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509171  2815 replica.cpp:712] Persisted action at 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509403  2815 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509980  2815 leveldb.cpp:341] Persisting action (411 bytes) to leveldb took 558737ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509992  2815 replica.cpp:712] Persisted action at 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509999  2815 replica.cpp:697] Replica learned APPEND action at position 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510262  2818 registrar.cpp:509] Successfully updated the 'registry' in 2.178048ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510313  2811 log.cpp:596] Attempting to truncate the log to 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510375  2817 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510486  2818 slave.cpp:3747] Received ping from slave-observer(447)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510519  2816 master.cpp:4721] Registered agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510540  2818 slave.cpp:1152] Registered with master master@172.30.2.29:37328; given agent ID 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510577  2818 fetcher.cpp:86] Clearing fetcher cache
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510577  2815 hierarchical.cpp:473] Added agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510639  2811 replica.cpp:537] Replica received write request for position 4 from (19832)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510658  2816 status_update_manager.cpp:181] Resuming sending status updates
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510730  2815 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510747  2815 hierarchical.cpp:1162] Performed allocation for agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 in 127305ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510766  2818 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/slave.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510848  2816 master.cpp:5772] Sending 1 offers to framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510892  2818 slave.cpp:1212] Forwarding total oversubscribed resources 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510956  2818 master.cpp:5066] Received update of agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with total oversubscribed resources 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510987  2817 sched.cpp:897] Scheduler::resourceOffers took 30391ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511080  2816 hierarchical.cpp:531] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511124  2816 hierarchical.cpp:1488] No allocations performed
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511132  2797 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:256;disk:1024
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511133  2816 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511167  2816 hierarchical.cpp:1162] Performed allocation for agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 in 57933ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511201  2811 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 542938ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511214  2811 replica.cpp:712] Persisted action at 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511431  2818 master.cpp:3457] Processing ACCEPT call for offers: [ 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-O0 ] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511461  2818 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task e9fcbad2-73bf-409e-9f71-023b826b5286
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511560  2816 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511827  2811 master.hpp:177] Adding task e9fcbad2-73bf-409e-9f71-023b826b5286 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511859  2811 master.cpp:3946] Launching task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511968  2814 slave.cpp:1551] Got assigned task e9fcbad2-73bf-409e-9f71-023b826b5286 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511984  2815 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512009  2815 hierarchical.cpp:928] Framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 filtered agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for 5secs
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512022  2814 slave.cpp:5654] Checkpointing FrameworkInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512127  2816 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 544409ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512138  2814 slave.cpp:5665] Checkpointing framework pid 'scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328' to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.pid'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512153  2816 leveldb.cpp:399] Deleting ~2 keys from leveldb took 13134ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512162  2816 replica.cpp:712] Persisted action at 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512167  2816 replica.cpp:697] Replica learned TRUNCATE action at position 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512245  2814 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512377  2814 slave.cpp:1670] Launching task e9fcbad2-73bf-409e-9f71-023b826b5286 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512408  2814 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512596  2814 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' to user 'root'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517411  2814 slave.cpp:6136] Checkpointing ExecutorInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/executor.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517659  2814 slave.cpp:5734] Launching executor e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517853  2814 slave.cpp:6159] Checkpointing TaskInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/tasks/e9fcbad2-73bf-409e-9f71-023b826b5286/task.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517861  2818 containerizer.cpp:773] Starting container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework '6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.518013  2814 slave.cpp:1896] Queuing task 'e9fcbad2-73bf-409e-9f71-023b826b5286' for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.518056  2814 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.519455  2817 mem.cpp:602] Started listening for OOM events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.519815  2817 mem.cpp:722] Started listening on low memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520133  2817 mem.cpp:722] Started listening on medium memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520447  2817 mem.cpp:722] Started listening on critical memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520769  2817 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521339  2817 mem.cpp:388] Updated 'memory.limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521926  2816 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""119"" --pipe_write=""120"" --sandbox=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82"" --user=""root""'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521984  2816 linux_launcher.cpp:281] Cloning child process with flags = 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.544052  2816 containerizer.cpp:1302] Checkpointing executor's forked pid 20673 to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/forked.pid'
[03:36:29]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.603862 20687 process.cpp:1060] libprocess is initialized on 172.30.2.29:44617 with 8 worker threads
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.605692 20687 logging.cpp:199] Logging to STDERR
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606240 20687 exec.cpp:161] Version: 1.0.0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606302 20704 exec.cpp:211] Executor started at: executor(1)@172.30.2.29:44617 with pid 20687
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606724  2814 slave.cpp:2884] Got registration for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606885  2814 slave.cpp:2970] Checkpointing executor pid 'executor(1)@172.30.2.29:44617' to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/libprocess.pid'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.607306 20703 exec.cpp:236] Executor registered on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.607925  2815 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608141 20703 exec.cpp:248] Executor::registered took 89576ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608538  2816 slave.cpp:2061] Sending queued task 'e9fcbad2-73bf-409e-9f71-023b826b5286' to executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608767 20705 exec.cpp:323] Executor asked to run task 'e9fcbad2-73bf-409e-9f71-023b826b5286'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608811 20705 exec.cpp:332] Executor::launchTask took 26475ns
[03:36:29] :	 [Step 10/10] Received SUBSCRIBED event
[03:36:29] :	 [Step 10/10] Subscribed executor on ip-172-30-2-29.mesosphere.io
[03:36:29] :	 [Step 10/10] Received LAUNCH event
[03:36:29] :	 [Step 10/10] Starting task e9fcbad2-73bf-409e-9f71-023b826b5286
[03:36:29] :	 [Step 10/10] Forked command at 20710
[03:36:29] :	 [Step 10/10] sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.611716 20705 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.611974  2815 slave.cpp:3267] Handling status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612499  2818 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612527  2818 status_update_manager.cpp:497] Creating StatusUpdate stream for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612751  2818 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725725  2818 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to the agent
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725908  2817 slave.cpp:3665] Forwarding the update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725999  2817 slave.cpp:3559] Status update manager successfully handled status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726016  2817 slave.cpp:3575] Sending acknowledgement for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726124  2813 master.cpp:5211] Status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726157  2813 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726238  2813 master.cpp:6871] Updating the state of task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726300 20701 exec.cpp:369] Executor received status update acknowledgement bea75e2e-9827-4410-9864-288f29c0a618 for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726363  2818 sched.cpp:1005] Scheduler::statusUpdate took 77055ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726517  2814 master.cpp:4365] Processing ACKNOWLEDGE call bea75e2e-9827-4410-9864-288f29c0a618 for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726757  2816 status_update_manager.cpp:392] Received status update acknowledgement (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726812  2816 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472790  2817 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472841  2817 hierarchical.cpp:1488] No allocations performed
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472847  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472864  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 181038ns
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474026  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474076  2814 hierarchical.cpp:1488] No allocations performed
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474083  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474097  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 180187ns
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475332  2817 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475383  2817 hierarchical.cpp:1488] No allocations performed
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475389  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475402  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 176560ns
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476011  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476059  2814 hierarchical.cpp:1488] No allocations performed
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476066  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476080  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 194002ns
[03:36:33]W:	 [Step 10/10] 512+0 records in
[03:36:33]W:	 [Step 10/10] 512+0 records out
[03:36:33]W:	 [Step 10/10] 536870912 bytes (537 MB, 512 MiB) copied, 4.23412 s, 127 MB/s
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477355  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477406  2814 hierarchical.cpp:1488] No allocations performed
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477413  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477427  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 184403ns
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477726  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477774  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 202326ns
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477824  2818 master.cpp:5772] Sending 1 offers to framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477948  2818 sched.cpp:897] Scheduler::resourceOffers took 9712ns
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478219  2814 hierarchical.cpp:1488] No allocations performed
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478235  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478245  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 47187ns
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478663  2811 hierarchical.cpp:1488] No allocations performed
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478678  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478693  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 45629ns
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479481  2817 hierarchical.cpp:1488] No allocations performed
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479516  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479532  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 98966ns
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480494  2813 hierarchical.cpp:1488] No allocations performed
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480526  2813 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480543  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 87017ns
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481472  2812 hierarchical.cpp:1488] No allocations performed
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481504  2812 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481519  2812 hierarchical.cpp:1139] Performed allocation for 1 agents in 122806ns
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482342  2813 hierarchical.cpp:1488] No allocations performed
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482378  2813 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482393  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 98739ns
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483055  2817 hierarchical.cpp:1488] No allocations performed
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483083  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483095  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 73620ns
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483800  2811 hierarchical.cpp:1488] No allocations performed
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483837  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483853  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 103486ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484480  2818 hierarchical.cpp:1488] No allocations performed
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484508  2818 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484522  2818 hierarchical.cpp:1139] Performed allocation for 1 agents in 76447ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.507843  2815 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.507937  2815 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.511128  2812 slave.cpp:3747] Received ping from slave-observer(447)@172.30.2.29:37328
[03:36:44] :	 [Step 10/10] ../../src/tests/containerizer/memory_pressure_tests.cpp:263: Failure
[03:36:44] :	 [Step 10/10] Failed to wait 15secs for _statusUpdateAcknowledgement
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727337  2815 master.cpp:1406] Framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 disconnected
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727363  2815 master.cpp:2840] Disconnecting framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727396  2815 master.cpp:2864] Deactivating framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727478  2814 hierarchical.cpp:375] Deactivated framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] W0618 03:36:44.727489  2815 master.hpp:1967] Master attempted to send message to disconnected framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727519  2815 master.cpp:1419] Giving framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 0ns to failover
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727556  2814 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727741  2814 containerizer.cpp:1576] Destroying container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728740  2813 master.cpp:5624] Framework failover timeout, removing framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728765  2813 master.cpp:6354] Removing framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728817  2813 master.cpp:6871] Updating the state of task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728827  2817 slave.cpp:2274] Asked to shut down framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 by master@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728853  2817 slave.cpp:2299] Shutting down framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728869  2817 slave.cpp:4470] Shutting down executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728896  2811 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728937  2815 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):256; disk(*):1024 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44] :	 [Step 10/10] Received SHUTDOWN event
[03:36:44] :	 [Step 10/10] Shutting down
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728950  2813 master.cpp:6937] Removing task e9fcbad2-73bf-409e-9f71-023b826b5286 with resources cpus(*):1; mem(*):256; disk(*):1024 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:44] :	 [Step 10/10] Sending SIGTERM to process tree at pid 20710
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729131 20707 exec.cpp:410] Executor asked to shutdown
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729141  2815 hierarchical.cpp:326] Removed framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729179 20707 exec.cpp:425] Executor::shutdown took 6153ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729199 20707 exec.cpp:92] Scheduling shutdown of the executor in 5secs
[03:36:45]W:	 [Step 10/10] I0618 03:36:45.485015  2818 hierarchical.cpp:1488] No allocations performed
[03:36:45]W:	 [Step 10/10] I0618 03:36:45.485038  2818 hierarchical.cpp:1139] Performed allocation for 1 agents in 47043ns
[03:36:46]W:	 [Step 10/10] I0618 03:36:46.485332  2811 hierarchical.cpp:1488] No allocations performed
[03:36:46]W:	 [Step 10/10] I0618 03:36:46.485350  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 33542ns
[03:36:47]W:	 [Step 10/10] I0618 03:36:47.486548  2817 hierarchical.cpp:1488] No allocations performed
[03:36:47]W:	 [Step 10/10] I0618 03:36:47.486588  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 84621ns
[03:36:48]W:	 [Step 10/10] I0618 03:36:48.487707  2813 hierarchical.cpp:1488] No allocations performed
[03:36:48]W:	 [Step 10/10] I0618 03:36:48.487751  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 83039ns
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.488706  2812 hierarchical.cpp:1488] No allocations performed
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.488745  2812 hierarchical.cpp:1139] Performed allocation for 1 agents in 78192ns
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.729018  2811 slave.cpp:4543] Killing executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:50]W:	 [Step 10/10] I0618 03:36:50.489168  2817 hierarchical.cpp:1488] No allocations performed
[03:36:50]W:	 [Step 10/10] I0618 03:36:50.489207  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 87236ns
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.369570  2818 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.430644  2813 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 after 6.70171904secs
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.431812  2818 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.432981  2817 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 after 1.140992ms
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.433709  2816 slave.cpp:3793] executor(1)@172.30.2.29:44617 exited
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.443989  2813 containerizer.cpp:1812] Executor for container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' has exited
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446597  2818 provisioner.cpp:411] Ignoring destroy request for unknown container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446734  2813 slave.cpp:4152] Executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 terminated with signal Killed
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446758  2813 slave.cpp:4256] Cleaning up executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446943  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for gc 6.99999482767407days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447018  2813 slave.cpp:4344] Cleaning up framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447038  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286' for gc 6.9999948270963days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447082  2816 status_update_manager.cpp:282] Closing status update streams for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447098  2816 status_update_manager.cpp:528] Cleaning up status update stream for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447100  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for gc 6.99999482669037days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447103  2813 slave.cpp:839] Agent terminating
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447149  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286' for gc 6.99999482630815days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447190  2816 master.cpp:1367] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) disconnected
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447209  2816 master.cpp:2899] Disconnecting agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447211  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000' for gc 6.99999482555556days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447237  2816 master.cpp:2918] Deactivating agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447254  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000' for gc 6.99999482534815days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447300  2816 hierarchical.cpp:560] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 deactivated
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.448766  2797 master.cpp:1214] Master terminating
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.448875  2814 hierarchical.cpp:505] Removed agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.460062  2813 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.562192  2816 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 after 102.104064ms
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.563100  2816 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.564021  2815 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 after 901888ns
[03:36:51] :	 [Step 10/10] [  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (22119 ms)
{noformat}"	MESOS	Resolved	3	1	4582	cgroups, flaky-test, mesosphere
12846958	Move all MesosContainerizer related files under src/slave/containerizer/mesos	"Currently, some MesosContainerizer specific files are not in the correct location. For example:
{noformat} 
src/slave/containerizer/isolators/*
src/slave/containerizer/provisioner.hpp|cpp
{noformat}

They should be put under src/slave/containerizer/mesos/"	MESOS	Resolved	3	3	4582	mesosphere
12923531	Exposed docker/appc image manifest to mesos containerizer.	Collect docker image manifest from disk(which contains all runtime configurations), and pass it back to provisioner, so that mesos containerizer can grab all necessary info from provisioner.	MESOS	Resolved	3	4	4582	mesosphere, unified-containerizer-mvp
13138092	Container stuck in PULLING when Docker daemon hangs	"When the {{force}} argument is not set to {{true}}, {{Docker::pull}} will always perform a {{docker inspect}} call before it does a {{docker pull}}. If either of these two Docker CLI calls hangs indefinitely, the Docker container will be stuck in the PULLING state. This means that we make no further progress in the {{launch()}} call path, so the executor binary is never executed, the {{Future}} associated with the {{launch()}} call is never failed or satisfied, and {{wait()}} is never called on the container. The agent chains the executor cleanup onto that {{wait()}} call which is never made. So, when the executor registration timeout elapses, {{containerizer->destroy()}} is called on the executor container, but the rest of the executor cleanup is never performed, and no terminal task status update is sent.

This leaves the task destined for that Docker executor stuck in TASK_STAGING from the framework's perspective, and attempts to kill the task will fail."	MESOS	Resolved	3	4	4582	mesosphere
13178394	Container stuck at ISOLATING due to FD leak	"When containers are launching on a single agent at scale, one container stuck at ISOLATING could occasionally happen. And this container becomes un-destroyable due to containerizer destroy always wait for isolate() finish to continue.

We add more logging to debug this issue:
{noformat}
Aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:28.050068  2995 collect.hpp:271] $$$$: AwaitProcess waited invoked for ProcessBase ID: __await__(26651); futures size: 3; future: Ready; future index: 2; ready count: 1
Aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:28.414436  2998 collect.hpp:271] $$$$: AwaitProcess waited invoked for ProcessBase ID: __await__(26651); futures size: 3; future: Ready; future index: 0; ready count: 2
{noformat} 
which shows that the await() in CNI::attach() stuck at the second future (io::read() for stdout).

By looking at the df of this stdout:
{noformat}
Aug 10 17:23:27 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:27.657501  2995 cni.cpp:1287] !!!!: Start to await for plugin '/opt/mesosphere/active/mesos/libexec/mesos/mesos-cni-port-mapper' to finish for container 1c8abf4c-f71a-4704-9a73-1ab0dd709c62 with pid '16644'; stdout fd: 1781; stderr fd: 1800
{noformat}

We found
{noformat}
core@ip-10-0-1-129 ~ $ ps aux | grep mesos-agent
core      1674  0.0  0.0   6704   864 pts/0    S+   20:00   0:00 grep --colour=auto mesos-agent
root      2974 16.4  2.5 1211096 414048 ?      Ssl  17:02  29:11 /opt/mesosphere/packages/mesos--61265af3be37861f26b657c1f9800293b86a0374/bin/mesos-agent
core@ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep 1781
l-wx------. 1 root root 64 Aug 10 19:38 1781 -> /var/lib/mesos/slave/meta/slaves/d3089315-8e34-40b4-b1f7-0ac6a624d7db-S0/frameworks/d3089315-8e34-40b4-b1f7-0ac6a624d7db-0000/executors/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/runs/38e9270d-ebda-4758-ad96-40c5b84bffdc/tasks/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/task.updates
{noformat}

{noformat}
core@ip-10-0-1-129 ~ $ ps aux | grep 27981
core      2201  0.0  0.0   6704   884 pts/0    S+   20:06   0:00 grep --colour=auto 27981
root     27981  0.0  0.0   1516     4 ?        Ss   17:25   0:00 sleep 10000
core@ip-10-0-1-129 ~ $ cat /proc/s^C       
core@ip-10-0-1-129 ~ $ sudo -s
ip-10-0-1-129 core # ls -al /proc/27981/fd | grep 275230
lr-x------. 1 root root 64 Aug 10 20:05 1781 -> pipe:[275230]
l-wx------. 1 root root 64 Aug 10 20:05 1787 -> pipe:[275230]
{noformat}

{noformat}
core@ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep pipe
lr-x------. 1 root root 64 Aug 10 17:02 11 -> pipe:[49380]
l-wx------. 1 root root 64 Aug 10 17:02 14 -> pipe:[49380]
lr-x------. 1 root root 64 Aug 10 17:02 17 -> pipe:[48909]
lr-x------. 1 root root 64 Aug 10 19:38 1708 -> pipe:[275089]
l-wx------. 1 root root 64 Aug 10 19:38 1755 -> pipe:[275089]
lr-x------. 1 root root 64 Aug 10 19:38 1787 -> pipe:[275230]
l-wx------. 1 root root 64 Aug 10 17:02 19 -> pipe:[48909]
{noformat}

pipe 275230 is held by the agent process and the sleep process at the same time!

The reason why the leak is possible is because we don't use `pipe2` to create a pipe with `O_CLOEXEC` in subprocess:
https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess_posix.cpp#L61

Although we do set cloexec on those fds later:
https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess.cpp#L366-L373

There is a race where a fork happens after `pipe()` call, but before cloexec is called later. This is more likely on a busy system (this explains why it's not hard to repo the issue when launching a lot of containers on a single box)."	MESOS	Resolved	2	1	4582	container-stuck, containerizer, mesosphere
13032328	"Introduce ""HOST VOLUME"" as a volume type in mesos protobufs"	"The `MesosContainerizer` supports ""host volumes"". However, there is no specific volume type defined for ""host volumes"" (https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto#L1921). Instead, the `MesosContainerizer` relies on inferring the mount paths from the ""host_path"" to decipher if the user wants a ""host volume"". 

It would be better to be more explicit and have a volume source type of ""HOST_VOLUME"" in the volume types.
"	MESOS	Open	3	4	4582	mesosphere, storage
13135296	`prepareMounts` in Mesos containerizer is flaky.	"The [{{prepareMount()}}|https://github.com/apache/mesos/blob/1.5.x/src/slave/containerizer/mesos/launch.cpp#L244] function in {{src/slave/containerizer/mesos/launch.cpp}} sometimes fails with the following error:
{noformat}
Failed to prepare mounts: Failed to mark '/home/docker/containers/af78db6ebc1aff572e576b773d1378121a66bb755ed63b3278e759907e5fe7b6/shm' as slave: Invalid argument
{noformat}
The error message comes from https://github.com/apache/mesos/blob/1.5.x/src/slave/containerizer/mesos/launch.cpp#L#L326.

Although it does not happen frequently, it can be reproduced by running tests that need to clone mount namespaces in repetition. For example, I just reproduced the bug with the following command after 17 minutes:
{noformat}
sudo bin/mesos-tests.sh --gtest_filter='*ROOT_PublishResourcesRecovery' --gtest_break_on_failure --gtest_repeat=-1 --verbose
{noformat}

No that in this example, the test itself does not involve any docker image or docker containerizer."	MESOS	Open	3	1	4582	containerization, mesosphere
12957846	LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint is flaky	"Observed on the internal Mesosphere CI:
{code}
[07:12:07] :	 [Step 11/11] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint
[07:12:08]W:	 [Step 11/11] I0410 07:12:08.906998 32129 linux.cpp:81] Making '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH' a shared mount
[07:12:08]W:	 [Step 11/11] I0410 07:12:08.923028 32129 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[07:12:08]W:	 [Step 11/11] I0410 07:12:08.923751 32144 containerizer.cpp:682] Starting container '86d04a91-e7b0-4b8f-9706-b9969796b5d1' for executor 'test_executor' of framework ''
[07:12:08]W:	 [Step 11/11] I0410 07:12:08.924296 32148 provisioner.cpp:285] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/provisioner/containers/86d04a91-e7b0-4b8f-9706-b9969796b5d1/backends/copy/rootfses/104f1991-f54a-4dd0-ab92-48ff2d3bebab' for container 86d04a91-e7b0-4b8f-9706-b9969796b5d1
[07:12:08]W:	 [Step 11/11] I0410 07:12:08.924885 32145 copy.cpp:127] Copying layer path '/tmp/WwQa3Q/test_image' to rootfs '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/provisioner/containers/86d04a91-e7b0-4b8f-9706-b9969796b5d1/backends/copy/rootfses/104f1991-f54a-4dd0-ab92-48ff2d3bebab'
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.627612 32145 linux.cpp:355] Bind mounting work directory from '/tmp/WwQa3Q/sandbox' to '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/provisioner/containers/86d04a91-e7b0-4b8f-9706-b9969796b5d1/backends/copy/rootfses/104f1991-f54a-4dd0-ab92-48ff2d3bebab/mnt/mesos/sandbox' for container 86d04a91-e7b0-4b8f-9706-b9969796b5d1
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.648669 32147 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWNS
[07:12:13]W:	 [Step 11/11] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
[07:12:13]W:	 [Step 11/11] + grep -E /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/.+ /proc/self/mountinfo
[07:12:13]W:	 [Step 11/11] + grep -v 86d04a91-e7b0-4b8f-9706-b9969796b5d1
[07:12:13]W:	 [Step 11/11] + cut '-d ' -f5
[07:12:13]W:	 [Step 11/11] + xargs --no-run-if-empty umount -l
[07:12:13]W:	 [Step 11/11] + mount -n --rbind /tmp/WwQa3Q /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/provisioner/containers/86d04a91-e7b0-4b8f-9706-b9969796b5d1/backends/copy/rootfses/104f1991-f54a-4dd0-ab92-48ff2d3bebab/mnt/mesos/sandbox/mountpoint
[07:12:13] :	 [Step 11/11] Changing root to /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/provisioner/containers/86d04a91-e7b0-4b8f-9706-b9969796b5d1/backends/copy/rootfses/104f1991-f54a-4dd0-ab92-48ff2d3bebab
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.827551 32145 containerizer.cpp:1674] Executor for container '86d04a91-e7b0-4b8f-9706-b9969796b5d1' has exited
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.827607 32145 containerizer.cpp:1439] Destroying container '86d04a91-e7b0-4b8f-9706-b9969796b5d1'
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.830469 32145 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos/86d04a91-e7b0-4b8f-9706-b9969796b5d1
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.832928 32143 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/86d04a91-e7b0-4b8f-9706-b9969796b5d1 after 2.412032ms
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.835292 32150 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos/86d04a91-e7b0-4b8f-9706-b9969796b5d1
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.837411 32150 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/86d04a91-e7b0-4b8f-9706-b9969796b5d1 after 2.07616ms
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.840045 32148 linux.cpp:817] Unmounting sandbox/work directory '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/provisioner/containers/86d04a91-e7b0-4b8f-9706-b9969796b5d1/backends/copy/rootfses/104f1991-f54a-4dd0-ab92-48ff2d3bebab/mnt/mesos/sandbox' for container 86d04a91-e7b0-4b8f-9706-b9969796b5d1
[07:12:13]W:	 [Step 11/11] I0410 07:12:13.840504 32150 provisioner.cpp:330] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_VolumeFromHostSandboxMountPoint_aSovaH/provisioner/containers/86d04a91-e7b0-4b8f-9706-b9969796b5d1/backends/copy/rootfses/104f1991-f54a-4dd0-ab92-48ff2d3bebab' for container 86d04a91-e7b0-4b8f-9706-b9969796b5d1
[07:12:28] :	 [Step 11/11] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:961: Failure
[07:12:28] :	 [Step 11/11] Failed to wait 15secs for wait
[07:12:30] :	 [Step 11/11] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint (23703 ms)
{code}"	MESOS	Resolved	3	1	4582	filesystem, isolation, mesosphere
12947798	Implement runtime isolator tests.	There different cases in docker runtime isolator. Some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct.	MESOS	Resolved	3	3	4582	containerizer, mesosphere
13008101	Mesos containerizer should figure out the correct sandbox directory for nested launch.	Currently the mesos containerizer take the sandbox directory from the agent. Ideally, a nested sandbox dir can be figured out by the containerizer. And there is no need to pass it from the agent. We should remove the `directory` parameter in nested launch interface.	MESOS	Resolved	3	1	4582	containerizer, mesosphere
12901114	add test cases for sha256/sha512 digest verifier	add test cases for sha256/sha512 digest verifier, to read from a file path and verify with corresponding string digest.	MESOS	Accepted	3	4	4582	mesosphere
12996356	Consolidate two `Containerizer::launch` methods into one.	"Looks like keeping both of them is not necessary.
{code}
  // Launch a containerized executor. Returns true if launching this
  // ExecutorInfo is supported and it has been launched, otherwise
  // false or a failure is something went wrong.
  virtual process::Future<bool> launch(
      const ContainerID& containerId,
      const ExecutorInfo& executorInfo,
      const std::string& directory,
      const Option<std::string>& user,
      const SlaveID& slaveId,
      const process::PID<Slave>& slavePid,
      bool checkpoint) = 0;

  // Launch a containerized task. Returns true if launching this
  // TaskInfo/ExecutorInfo is supported and it has been launched,
  // otherwise false or a failure is something went wrong.
  // TODO(nnielsen): Obsolete the executorInfo argument when the slave
  // doesn't require executors to run standalone tasks.
  virtual process::Future<bool> launch(
      const ContainerID& containerId,
      const TaskInfo& taskInfo,
      const ExecutorInfo& executorInfo,
      const std::string& directory,
      const Option<std::string>& user,
      const SlaveID& slaveId,
      const process::PID<Slave>& slavePid,
      bool checkpoint) = 0;
{code}

We can just make `taskInfo` optional."	MESOS	Resolved	3	3	4582	mesosphere
12960548	CHECK failure in AppcProvisionerIntegrationTest.ROOT_SimpleLinuxImageTest	"Observed on the Mesosphere internal CI:

{noformat}
[22:56:28]W:     [Step 10/10] F0420 22:56:28.056788   629 containerizer.cpp:1634] Check failed: containers_.contains(containerId)
{noformat}

Complete test log will be attached as a file."	MESOS	Resolved	3	1	4582	containerizer, flaky, mesosphere
12841854	Support container image caching 	"Each image provisioner need to implement its own storing and fetching images, and in some level need to implement caching and concurrent downloads of the same layer/image. 
We already have fetcher cache, and we should consider if we can reuse this. And if not we still should have some primitives that all the provisioners can reuse around caching.
"	MESOS	Resolved	3	4	4582	mesosphere
13191770	UCR container launch stuck at PROVISIONING during image fetching.	"We observed mesos containerizer stuck at PROVISIONING when launching a mesos container using docker image: `kvish/jenkins-dev:595c74f713f609fd1d3b05a40d35113fc03227c9`:

The image pulling never finishes. Insufficient image contents are still in image store staging directory /var/lib/mesos/slave/store/docker/staging/egLYqO, forever.
{noformat}
OK-22:50:06-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/staging/egLYqO # ls -alh
total 1.1G
drwx------. 2 root root 4.0K Oct 15 13:02 .
drwxr-xr-x. 3 root root   20 Oct 15 22:40 ..
-rw-r--r--. 1 root root  59K Oct 15 13:02 manifest
-rw-r--r--. 1 root root 2.6K Oct 15 13:02 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63
-rw-r--r--. 1 root root  440 Oct 15 13:02 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66
-rw-r--r--. 1 root root  248 Oct 15 13:02 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a
-rw-r--r--. 1 root root  240 Oct 15 13:02 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb
-rw-r--r--. 1 root root  562 Oct 15 13:02 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1
-rw-r--r--. 1 root root  11M Oct 15 13:02 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d
-rw-r--r--. 1 root root  130 Oct 15 13:02 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50
-rw-r--r--. 1 root root  176 Oct 15 13:02 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312
-rw-r--r--. 1 root root  380 Oct 15 13:02 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a
-rw-r--r--. 1 root root  71M Oct 15 13:02 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604
-rw-r--r--. 1 root root 1.4K Oct 15 13:02 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1
-rw-r--r--. 1 root root 653K Oct 15 13:02 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01
-rw-r--r--. 1 root root  184 Oct 15 13:02 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90
-rw-r--r--. 1 root root 366K Oct 15 13:02 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94
-rw-r--r--. 1 root root  23K Oct 15 13:02 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2
-rw-r--r--. 1 root root 384M Oct 15 13:02 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0
-rw-r--r--. 1 root root 1.5K Oct 15 13:02 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc
-rw-r--r--. 1 root root  48M Oct 15 13:02 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c
-rw-r--r--. 1 root root  30M Oct 15 13:02 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf
-rw-r--r--. 1 root root 306M Oct 15 13:02 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f
-rw-r--r--. 1 root root  435 Oct 15 13:02 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d
-rw-r--r--. 1 root root 5.5K Oct 15 13:02 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5
-rw-r--r--. 1 root root  39M Oct 15 13:02 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10
-rw-r--r--. 1 root root  615 Oct 15 13:02 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b
-rw-r--r--. 1 root root  712 Oct 15 13:02 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2
-rw-r--r--. 1 root root  12K Oct 15 13:02 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728
-rw-r--r--. 1 root root  861 Oct 15 13:02 sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352
-rw-r--r--. 1 root root   32 Oct 15 13:02 sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
-rw-r--r--. 1 root root 266K Oct 15 13:02 sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276
-rw-r--r--. 1 root root 1.6K Oct 15 13:02 sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74
-rw-r--r--. 1 root root 4.2M Oct 15 13:02 sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c
-rw-r--r--. 1 root root 1.1K Oct 15 13:02 sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58
-rw-r--r--. 1 root root 2.8K Oct 15 13:02 sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747
-rw-r--r--. 1 root root 6.3M Oct 15 13:02 sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5
-rw-r--r--. 1 root root 1.8K Oct 15 13:02 sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215
-rw-r--r--. 1 root root 4.1K Oct 15 13:02 sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac
-rw-r--r--. 1 root root  355 Oct 15 13:02 sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87
-rw-r--r--. 1 root root 165M Oct 15 13:02 sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3
-rw-r--r--. 1 root root 872K Oct 15 13:02 sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac
-rw-r--r--. 1 root root  431 Oct 15 13:02 sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a
-rw-r--r--. 1 root root  19M Oct 15 13:02 sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6
-rw-r--r--. 1 root root  198 Oct 15 13:02 sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec
-rw-r--r--. 1 root root 550K Oct 15 13:02 sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320
-rw-r--r--. 1 root root  676 Oct 15 13:02 sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa
{noformat}

It is not clear yet why the SHA pulling does not finish, so we use the same image on another empty machine with UCR. The other machine has the container RUNNING correctly, and has the following staging directory before moving to the layers dir:
{noformat}
-rw-r--r--. 1 root root 2.6K Oct 15 18:03 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63
-rw-r--r--. 1 root root  440 Oct 15 18:03 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66
-rw-r--r--. 1 root root  248 Oct 15 18:03 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a
-rw-r--r--. 1 root root  240 Oct 15 18:03 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb
-rw-r--r--. 1 root root  562 Oct 15 18:03 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1
-rw-r--r--. 1 root root  11M Oct 15 18:03 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d
-rw-r--r--. 1 root root  130 Oct 15 18:03 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50
-rw-r--r--. 1 root root  176 Oct 15 18:03 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312
-rw-r--r--. 1 root root  380 Oct 15 18:03 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a
-rw-r--r--. 1 root root  71M Oct 15 18:03 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604
-rw-r--r--. 1 root root 1.4K Oct 15 18:03 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1
-rw-r--r--. 1 root root 653K Oct 15 18:03 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01
-rw-r--r--. 1 root root  184 Oct 15 18:03 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90
-rw-r--r--. 1 root root 366K Oct 15 18:03 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94
-rw-r--r--. 1 root root  23K Oct 15 18:03 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2
-rw-r--r--. 1 root root 122M Oct 15 18:03 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0
-rw-r--r--. 1 root root 1.5K Oct 15 18:03 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc
-rw-r--r--. 1 root root  48M Oct 15 18:03 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c
-rw-r--r--. 1 root root  30M Oct 15 18:03 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf
-rw-r--r--. 1 root root  92M Oct 15 18:03 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f
-rw-r--r--. 1 root root  435 Oct 15 18:03 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d
-rw-r--r--. 1 root root 5.5K Oct 15 18:03 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5
-rw-r--r--. 1 root root  39M Oct 15 18:03 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10
-rw-r--r--. 1 root root  615 Oct 15 18:03 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b
-rw-r--r--. 1 root root  712 Oct 15 18:03 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2
-rw-r--r--. 1 root root  12K Oct 15 18:03 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728
-rw-r--r--. 1 root root  861 Oct 15 18:03 sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352
-rw-r--r--. 1 root root   32 Oct 15 18:03 sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
-rw-r--r--. 1 root root 266K Oct 15 18:03 sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276
-rw-r--r--. 1 root root 1.6K Oct 15 18:03 sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74
-rw-r--r--. 1 root root 4.2M Oct 15 18:03 sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c
-rw-r--r--. 1 root root 1.1K Oct 15 18:03 sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58
-rw-r--r--. 1 root root 2.8K Oct 15 18:03 sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747
-rw-r--r--. 1 root root 6.3M Oct 15 18:03 sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5
-rw-r--r--. 1 root root 1.8K Oct 15 18:03 sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215
-rw-r--r--. 1 root root  44M Oct 15 18:03 sha256:c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b
-rw-r--r--. 1 root root 4.1K Oct 15 18:03 sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac
-rw-r--r--. 1 root root  355 Oct 15 18:03 sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87
-rw-r--r--. 1 root root  82M Oct 15 18:03 sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3
-rw-r--r--. 1 root root 872K Oct 15 18:03 sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac
-rw-r--r--. 1 root root  431 Oct 15 18:03 sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a
-rw-r--r--. 1 root root  19M Oct 15 18:03 sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6
-rw-r--r--. 1 root root  198 Oct 15 18:03 sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec
-rw-r--r--. 1 root root 550K Oct 15 18:03 sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320
-rw-r--r--. 1 root root  676 Oct 15 18:03 sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa
{noformat}

By comparing two cases, we can see one layer `8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324` is missing on the problematic agent node, and it is the last layer to fetch.

Here is the manifest as a reference:
{noformat}
OK-17:42:20-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/staging/egLYqO # cat manifest 
{
   ""schemaVersion"": 1,
   ""name"": ""kvish/jenkins-dev"",
   ""tag"": ""595c74f713f609fd1d3b05a40d35113fc03227c9"",
   ""architecture"": ""amd64"",
   ""fsLayers"": [
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0""
      },
      {
         ""blobSum"": ""sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215""
      },
      {
         ""blobSum"": ""sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2""
      },
      {
         ""blobSum"": ""sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320""
      },
      {
         ""blobSum"": ""sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5""
      },
      {
         ""blobSum"": ""sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f""
      },
      {
         ""blobSum"": ""sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58""
      },
      {
         ""blobSum"": ""sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d""
      },
      {
         ""blobSum"": ""sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87""
      },
      {
         ""blobSum"": ""sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1""
      },
      {
         ""blobSum"": ""sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa""
      },
      {
         ""blobSum"": ""sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312""
      },
      {
         ""blobSum"": ""sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2""
      },
      {
         ""blobSum"": ""sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1""
      },
      {
         ""blobSum"": ""sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b""
      },
      {
         ""blobSum"": ""sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63""
      },
      {
         ""blobSum"": ""sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728""
      },
      {
         ""blobSum"": ""sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec""
      },
      {
         ""blobSum"": ""sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf""
      },
      {
         ""blobSum"": ""sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10""
      },
      {
         ""blobSum"": ""sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747""
      },
      {
         ""blobSum"": ""sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a""
      },
      {
         ""blobSum"": ""sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352""
      },
      {
         ""blobSum"": ""sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a""
      },
      {
         ""blobSum"": ""sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94""
      },
      {
         ""blobSum"": ""sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01""
      },
      {
         ""blobSum"": ""sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276""
      },
      {
         ""blobSum"": ""sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50""
      },
      {
         ""blobSum"": ""sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac""
      },
      {
         ""blobSum"": ""sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c""
      },
      {
         ""blobSum"": ""sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c""
      },
      {
         ""blobSum"": ""sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b""
      }
   ],
   ""history"": [
      {
         ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""nobody\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""ExposedPorts\"":{\""50000/tcp\"":{},\""8080/tcp\"":{}},\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""LANG=C.UTF-8\"",\""JAVA_HOME=/docker-java-home\"",\""JAVA_VERSION=8u162\"",\""JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\"",\""CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\"",\""JENKINS_HOME=/var/jenkinsdcos_home\"",\""JENKINS_SLAVE_AGENT_PORT=50000\"",\""JENKINS_VERSION=2.107.2\"",\""JENKINS_UC=https://updates.jenkins.io\"",\""JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\"",\""COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\"",\""JENKINS_FOLDER=/usr/share/jenkins\"",\""JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""/usr/local/jenkins/bin/run.sh\""],\""ArgsEscaped\"":true,\""Image\"":\""sha256:c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\"",\""Volumes\"":{\""/var/jenkins_home\"":{}},\""WorkingDir\"":\""/tmp\"",\""Entrypoint\"":[\""/sbin/tini\"",\""--\"",\""/usr/local/bin/jenkins.sh\""],\""OnBuild\"":[],\""Labels\"":null},\""container\"":\""e4111508e68c304ec5b36009773b41384b96fd887b61177cd42935b9757567fd\"",\""container_config\"":{\""Hostname\"":\""e4111508e68c\"",\""Domainname\"":\""\"",\""User\"":\""nobody\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""ExposedPorts\"":{\""50000/tcp\"":{},\""8080/tcp\"":{}},\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""LANG=C.UTF-8\"",\""JAVA_HOME=/docker-java-home\"",\""JAVA_VERSION=8u162\"",\""JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\"",\""CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\"",\""JENKINS_HOME=/var/jenkinsdcos_home\"",\""JENKINS_SLAVE_AGENT_PORT=50000\"",\""JENKINS_VERSION=2.107.2\"",\""JENKINS_UC=https://updates.jenkins.io\"",\""JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\"",\""COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\"",\""JENKINS_FOLDER=/usr/share/jenkins\"",\""JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) \"",\""CMD [\\\""/bin/sh\\\"" \\\""-c\\\"" \\\""/usr/local/jenkins/bin/run.sh\\\""]\""],\""ArgsEscaped\"":true,\""Image\"":\""sha256:c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\"",\""Volumes\"":{\""/var/jenkins_home\"":{}},\""WorkingDir\"":\""/tmp\"",\""Entrypoint\"":[\""/sbin/tini\"",\""--\"",\""/usr/local/bin/jenkins.sh\""],\""OnBuild\"":[],\""Labels\"":{}},\""created\"":\""2018-09-26T17:33:57.6822239Z\"",\""docker_version\"":\""18.03.0-ce\"",\""id\"":\""fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\"",\""os\"":\""linux\"",\""parent\"":\""bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\"",\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\"",\""parent\"":\""2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\"",\""created\"":\""2018-09-26T17:33:57.3350528Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo 2.0 \\u003e /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\"",\""parent\"":\""36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\"",\""created\"":\""2018-09-26T17:33:56.0461597Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER nobody\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\"",\""parent\"":\""ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\"",\""created\"":\""2018-09-26T17:33:55.6692099Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c chmod -R ugo+rw \\\""$JENKINS_HOME\\\"" \\\""${JENKINS_FOLDER}\\\""     \\u0026\\u0026 chmod -R ugo+r \\\""${JENKINS_STAGING}\\\""     \\u0026\\u0026 chmod -R ugo+rx /usr/local/jenkins/bin/     \\u0026\\u0026 chmod -R ugo+rw /var/jenkins_home/     \\u0026\\u0026 chmod -R ugo+rw /var/lib/nginx/ /var/nginx/ /var/log/nginx     \\u0026\\u0026 chmod ugo+rx /usr/local/jenkins/bin/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\"",\""parent\"":\""c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\"",\""created\"":\""2018-09-26T17:33:49.7534514Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c groupadd -g ${gid} nobody     \\u0026\\u0026 usermod -u ${uid} -g ${gid} ${user}     \\u0026\\u0026 usermod -a -G users nobody     \\u0026\\u0026 echo \\\""nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\\\"" \\u003e\\u003e /etc/passwd\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\"",\""parent\"":\""2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\"",\""created\"":\""2018-09-26T17:33:48.3150654Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD c84b80e3ceaef7f211a221093369729eeb89e5cfc5f3d0a5cd4917e7b6c7027f in /usr/share/jenkins/ref//plugins/metrics-graphite.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\"",\""parent\"":\""a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\"",\""created\"":\""2018-09-26T17:33:47.8920446Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD f4d41c9bf39651b20107d62d85c101014320946e6a33763e5519ec18aee77858 in /usr/share/jenkins/ref//plugins/prometheus.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\"",\""parent\"":\""f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\"",\""created\"":\""2018-09-26T17:33:46.775839Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD 652f0ad5e9ad70b4db10957b64265f808b45c63d8ef07b107d3082450084164c in /usr/share/jenkins/ref//plugins/mesos.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\"",\""parent\"":\""0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\"",\""created\"":\""2018-09-26T17:33:45.5611867Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c /usr/local/bin/install-plugins.sh         blueocean-bitbucket-pipeline:${BLUEOCEAN_VERSION}      blueocean-commons:${BLUEOCEAN_VERSION}      blueocean-config:${BLUEOCEAN_VERSION}       blueocean-dashboard:${BLUEOCEAN_VERSION}    blueocean-events:${BLUEOCEAN_VERSION}       blueocean-git-pipeline:${BLUEOCEAN_VERSION}            blueocean-github-pipeline:${BLUEOCEAN_VERSION}         blueocean-i18n:${BLUEOCEAN_VERSION}         blueocean-jwt:${BLUEOCEAN_VERSION}          blueocean-jira:${BLUEOCEAN_VERSION}         blueocean-personalization:${BLUEOCEAN_VERSION}          blueocean-pipeline-api-impl:${BLUEOCEAN_VERSION}        blueocean-pipeline-editor:${BLUEOCEAN_VERSION}          blueocean-pipeline-scm-api:${BLUEOCEAN_VERSION}         blueocean-rest-impl:${BLUEOCEAN_VERSION}    blueocean-rest:${BLUEOCEAN_VERSION}         blueocean-web:${BLUEOCEAN_VERSION}          blueocean:${BLUEOCEAN_VERSION}              ant:1.8                          ansicolor:0.5.2                  antisamy-markup-formatter:1.5    artifactory:2.15.1               authentication-tokens:1.3        azure-credentials:1.6.0          azure-vm-agents:0.7.0            branch-api:2.0.19                build-name-setter:1.6.9          build-timeout:1.19               cloudbees-folder:6.4             conditional-buildstep:1.3.6      config-file-provider:2.18        copyartifact:1.39.1              cvs:2.14                         docker-build-publish:1.3.2       docker-workflow:1.15.1           durable-task:1.22                ec2:1.39                         embeddable-build-status:1.9      external-monitor-job:1.7         ghprb:1.40.0                     git:3.8.0                        git-client:2.7.1                 git-server:1.7                   github:1.29.0                    github-api:1.90                  github-branch-source:2.3.3       github-organization-folder:1.6   gitlab-plugin:1.5.5              gradle:1.28                      greenballs:1.15                  handlebars:1.1.1                 ivy:1.28                         jackson2-api:2.8.11.3            job-dsl:1.68                     jobConfigHistory:2.18            jquery:1.12.4-0                  ldap:1.20                        mapdb-api:1.0.9.0                marathon:1.6.0                   matrix-auth:2.2                  matrix-project:1.13              maven-plugin:3.1.2               metrics:3.1.2.11                 monitoring:1.72.0                nant:1.4.3                       node-iterator-api:1.5.0          pam-auth:1.3                     parameterized-trigger:2.35.2     pipeline-build-step:2.7          pipeline-github-lib:1.0          pipeline-input-step:2.8          pipeline-milestone-step:1.3.1    pipeline-model-api:1.2.8         pipeline-model-definition:1.2.8   pipeline-model-extensions:1.2.8   pipeline-rest-api:2.10           pipeline-stage-step:2.3          pipeline-stage-view:2.10         plain-credentials:1.4            prometheus:1.2.0                 rebuild:1.28                     role-strategy:2.7.0              run-condition:1.0                s3:0.11.0                        saferestart:0.3                  saml:1.0.5                       scm-api:2.2.6                    ssh-agent:1.15                   ssh-slaves:1.26                  subversion:2.10.5                timestamper:1.8.9                translation:1.16                 variant:1.1                      windows-slaves:1.3.1             workflow-aggregator:2.5          workflow-api:2.27                workflow-basic-steps:2.6         workflow-cps:2.48                workflow-cps-global-lib:2.9      workflow-durable-task-step:2.19   workflow-job:2.18                workflow-multibranch:2.17        workflow-scm-step:2.6            workflow-step-api:2.14           workflow-support:2.18\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\"",\""parent\"":\""1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\"",\""created\"":\""2018-09-26T17:31:24.2544617Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:59ced817d4cd74453e0658c69f937959d2b4d86cfe15d699cd1fdcf2f6867067 in /usr/share/jenkins/ref//init.groovy.d/mesos-auth.groovy \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\"",\""parent\"":\""35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\"",\""created\"":\""2018-09-26T17:31:23.9384301Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:8ca0529d27d0fa91b7848e39a5d04e55df01746ab31ca6bae1816f062667f8cc in /usr/share/jenkins/ref//nodeMonitors.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\"",\""parent\"":\""12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\"",\""created\"":\""2018-09-26T17:31:23.609004Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:beed7a659bf7217db04b70fa4220df32e07015c6f20edf4d73b5cab69354542e in /usr/share/jenkins/ref//jenkins.model.JenkinsLocationConfiguration.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\"",\""parent\"":\""87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\"",\""created\"":\""2018-09-26T17:31:23.3055734Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:46468ed2b6fa66eeea868396b18d952f8cbdd0df6529ec2a4d5782a1acc7ee7a in /usr/share/jenkins/ref//config.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\"",\""parent\"":\""a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\"",\""created\"":\""2018-09-26T17:31:23.003904Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:6b54409cf8c3ce4dae538b70b64f8755636613e71806e479c5d8f081224c63e9 in /var/nginx/nginx.conf \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\"",\""parent\"":\""39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\"",\""created\"":\""2018-09-26T17:31:22.6859214Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c mkdir -p /var/log/nginx/jenkins /var/nginx/\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\"",\""parent\"":\""0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\"",\""created\"":\""2018-09-26T17:31:21.2086534Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:a4cf73ccc8a0e4b1a7acef249766ce76b31bf76d03f97ac157d6eccfab30d4f5 in /usr/local/jenkins/bin/run.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\"",\""parent\"":\""f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\"",\""created\"":\""2018-09-26T17:31:20.9064351Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:3377f08a63084052efa9902be76b1eb669229849b476b52f448697333457e769 in /usr/local/jenkins/bin/dcos-account.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\"",\""parent\"":\""2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\"",\""created\"":\""2018-09-26T17:31:20.5594535Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:5814edade36c8c883f19e868796f1ae1d46d6990af813451101abec8196856d4 in /usr/local/jenkins/bin/export-libssl.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\"",\""parent\"":\""ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\"",\""created\"":\""2018-09-26T17:31:20.2349213Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:8206c6af7dc8888193958fd9428ba085ae19c8282c26eb05fb9f4c4f46973a4e in /usr/local/jenkins/bin/bootstrap.py \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\"",\""parent\"":\""9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\"",\""created\"":\""2018-07-09T20:54:30.984299193Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo 'networkaddress.cache.ttl=60' \\u003e\\u003e ${JAVA_HOME}/jre/lib/security/java.security\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\"",\""parent\"":\""e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\"",\""created\"":\""2018-07-09T20:54:29.524404063Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c mkdir -p \\\""${JENKINS_HOME}\\\"" \\\""${JENKINS_FOLDER}/war\\\""\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\"",\""parent\"":\""84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\"",\""created\"":\""2018-07-09T20:54:28.236876676Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo \\\""deb http://ftp.debian.org/debian testing main\\\"" \\u003e\\u003e /etc/apt/sources.list   \\u0026\\u0026 apt-get update \\u0026\\u0026 apt-get -t testing install -y git\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\"",\""parent\"":\""a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\"",\""created\"":\""2018-07-09T20:54:14.100019856Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c curl -fsSL \\\""$LIBMESOS_DOWNLOAD_URL\\\"" -o libmesos-bundle.tar.gz    \\u0026\\u0026 echo \\\""$LIBMESOS_DOWNLOAD_SHA256 libmesos-bundle.tar.gz\\\"" | sha256sum -c -   \\u0026\\u0026 tar -C / -xzf libmesos-bundle.tar.gz    \\u0026\\u0026 rm libmesos-bundle.tar.gz\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\"",\""parent\"":\""bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\"",\""created\"":\""2018-07-09T20:54:00.580952612Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y nginx python zip jq\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\"",\""parent\"":\""c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\"",\""created\"":\""2018-07-09T20:53:46.425927046Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER root\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\"",\""parent\"":\""662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\"",\""created\"":\""2018-07-09T20:53:46.096470837Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\"",\""parent\"":\""5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\"",\""created\"":\""2018-07-09T20:53:45.797188526Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\"",\""parent\"":\""25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\"",\""created\"":\""2018-07-09T20:53:45.462915577Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_HOME=/var/jenkinsdcos_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\"",\""parent\"":\""1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\"",\""created\"":\""2018-07-09T20:53:45.124088811Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG gid=99\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\"",\""parent\"":\""fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\"",\""created\"":\""2018-07-09T20:53:44.827537014Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG uid=99\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\"",\""parent\"":\""3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\"",\""created\"":\""2018-07-09T20:53:44.458211965Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG user=nobody\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\"",\""parent\"":\""ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\"",\""created\"":\""2018-07-09T20:53:44.10755361Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_DCOS_HOME=/var/jenkinsdcos_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\"",\""parent\"":\""b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\"",\""created\"":\""2018-07-09T20:53:43.757033301Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\"",\""parent\"":\""d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\"",\""created\"":\""2018-07-09T20:53:43.442946812Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\"",\""parent\"":\""fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\"",\""created\"":\""2018-07-09T20:53:43.116440726Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\"",\""parent\"":\""f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\"",\""created\"":\""2018-04-24T20:52:04.5174488Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_STAGING=/usr/share/jenkins/ref/\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\"",\""parent\"":\""9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\"",\""created\"":\""2018-04-24T20:52:04.1863586Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG BLUEOCEAN_VERSION=1.5.0\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\"",\""parent\"":\""72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\"",\""created\"":\""2018-04-24T20:52:03.8152478Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\"",\""parent\"":\""9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\"",\""created\"":\""2018-04-24T20:52:03.4353208Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\"",\""parent\"":\""764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\"",\""created\"":\""2018-04-24T20:52:03.0719423Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_FOLDER=/usr/share/jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\"",\""parent\"":\""7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\"",\""created\"":\""2018-04-24T20:52:02.73463Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) WORKDIR /tmp\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\"",\""parent\"":\""35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\"",\""created\"":\""2018-04-11T10:05:00.283278344Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:2874a36404a19c4075e62bf579a79bf730d317e628e80b03c676af4509481acc in /usr/local/bin/install-plugins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\"",\""parent\"":\""9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\"",\""created\"":\""2018-04-11T10:04:58.564052111Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:39d6085e6ad132734efabf90a5444f3bc74a21e8bf5a79f4d0176ac18bb98217 in /usr/local/bin/plugins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\"",\""parent\"":\""fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\"",\""created\"":\""2018-04-11T10:04:56.647913351Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENTRYPOINT [\\\""/sbin/tini\\\"" \\\""--\\\"" \\\""/usr/local/bin/jenkins.sh\\\""]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\"",\""parent\"":\""afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\"",\""created\"":\""2018-04-11T10:04:54.736575307Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:dc942ca949bb159f81bbc954773b3491e433d2d3e3ef90bac80ecf48a313c9c9 in /bin/tini \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\"",\""parent\"":\""64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\"",\""created\"":\""2018-04-11T10:04:51.974150657Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:1a73810a97d134925c37b2276c894e0a9c92125cdd8c750aaf8ef15c3c20aa72 in /usr/local/bin/jenkins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\"",\""parent\"":\""0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\"",\""created\"":\""2018-04-11T10:04:50.171056466Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:88dd96a27353c9d476981c3cfc6b39c95983c45083324afa7c8bddb682d91bff in /usr/local/bin/jenkins-support \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\"",\""parent\"":\""d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\"",\""created\"":\""2018-04-11T10:04:48.292041295Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\"",\""parent\"":\""c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\"",\""created\"":\""2018-04-11T10:04:46.288406797Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\"",\""parent\"":\""09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\"",\""created\"":\""2018-04-11T10:04:44.37013921Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  EXPOSE 50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\"",\""parent\"":\""3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\"",\""created\"":\""2018-04-11T10:04:42.447771731Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  EXPOSE 8080\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\"",\""parent\"":\""6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\"",\""created\"":\""2018-04-11T10:04:40.453492565Z\"",\""container_config\"":{\""Cmd\"":[\""|9 JENKINS_SHA=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67ed JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c chown -R ${user} \\\""$JENKINS_HOME\\\"" /usr/share/jenkins/ref\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\"",\""parent\"":\""c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\"",\""created\"":\""2018-04-11T10:04:37.42404848Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\"",\""parent\"":\""9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\"",\""created\"":\""2018-04-11T10:04:35.309385797Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_UC=https://updates.jenkins.io\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\"",\""parent\"":\""6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\"",\""created\"":\""2018-04-11T10:04:33.341878374Z\"",\""container_config\"":{\""Cmd\"":[\""|9 JENKINS_SHA=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67ed JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL ${JENKINS_URL} -o /usr/share/jenkins/jenkins.war   \\u0026\\u0026 echo \\\""${JENKINS_SHA}  /usr/share/jenkins/jenkins.war\\\"" | sha256sum -c -\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\"",\""parent\"":\""0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\"",\""created\"":\""2018-04-11T10:04:28.72473862Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\"",\""parent\"":\""14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\"",\""created\"":\""2018-04-11T10:04:26.621369421Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_SHA=2d71b8f87c8417f9303a73d52901a59678ee6c0eefcf7325efed6035ff39372a\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\"",\""parent\"":\""3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\"",\""created\"":\""2018-04-11T10:04:24.515479866Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_VERSION=2.107.2\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\"",\""parent\"":\""8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\"",\""created\"":\""2018-04-11T10:04:22.485876008Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_VERSION\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\"",\""parent\"":\""14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\"",\""created\"":\""2018-04-11T10:04:20.518174508Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:c84b91c835048a52bb864c1f4662607c56befe3c4b1520b0ea94633103a4554f in /usr/share/jenkins/ref/init.groovy.d/tcp-slave-agent-port.groovy \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\"",\""parent\"":\""f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\"",\""created\"":\""2018-04-11T10:04:18.593424219Z\"",\""container_config\"":{\""Cmd\"":[\""|7 TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture) -o /sbin/tini   \\u0026\\u0026 curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture).asc -o /sbin/tini.asc   \\u0026\\u0026 gpg --import /var/jenkins_home/tini_pub.gpg   \\u0026\\u0026 gpg --verify /sbin/tini.asc   \\u0026\\u0026 rm -rf /sbin/tini.asc /root/.gnupg   \\u0026\\u0026 chmod +x /sbin/tini\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\"",\""parent\"":\""8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\"",\""created\"":\""2018-04-11T10:04:13.905006564Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:653491cb486e752a4c2b4b407a46ec75646a54eabb597634b25c7c2b82a31424 in /var/jenkins_home/tini_pub.gpg \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\"",\""parent\"":\""1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\"",\""created\"":\""2018-04-11T10:04:11.747045116Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG TINI_VERSION=v0.16.1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\"",\""parent\"":\""9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\"",\""created\"":\""2018-04-11T10:04:09.646844829Z\"",\""container_config\"":{\""Cmd\"":[\""|6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c mkdir -p /usr/share/jenkins/ref/init.groovy.d\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\"",\""parent\"":\""cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\"",\""created\"":\""2018-04-11T10:04:05.986383436Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  VOLUME [/var/jenkins_home]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\"",\""parent\"":\""8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\"",\""created\"":\""2018-04-11T10:04:03.98242692Z\"",\""container_config\"":{\""Cmd\"":[\""|6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c groupadd -g ${gid} ${group}     \\u0026\\u0026 useradd -d \\\""$JENKINS_HOME\\\"" -u ${uid} -g ${gid} -m -s /bin/bash ${user}\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\"",\""parent\"":\""f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\"",\""created\"":\""2018-04-11T10:04:00.815710832Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_SLAVE_AGENT_PORT=50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\"",\""parent\"":\""5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\"",\""created\"":\""2018-04-11T10:03:58.893891854Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_HOME=/var/jenkins_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\"",\""parent\"":\""b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\"",\""created\"":\""2018-04-11T10:03:57.021756845Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG agent_port=50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\"",\""parent\"":\""d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\"",\""created\"":\""2018-04-11T10:03:55.096596096Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG http_port=8080\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\"",\""parent\"":\""35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\"",\""created\"":\""2018-04-11T10:03:53.140848234Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG gid=1000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\"",\""parent\"":\""d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\"",\""created\"":\""2018-04-11T10:03:51.085212134Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG uid=1000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\"",\""parent\"":\""edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\"",\""created\"":\""2018-04-11T10:03:49.08677048Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG group=jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\"",\""parent\"":\""27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\"",\""created\"":\""2018-04-11T10:03:47.139089021Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG user=jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\"",\""parent\"":\""558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\"",\""created\"":\""2018-04-11T10:03:45.06746326Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y git curl \\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\"",\""parent\"":\""180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\"",\""created\"":\""2018-03-19T21:23:43.026367652Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c /var/lib/dpkg/info/ca-certificates-java.postinst configure\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\"",\""parent\"":\""4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\"",\""created\"":\""2018-03-19T21:23:40.069312316Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c set -ex; \\t\\tif [ ! -d /usr/share/man/man1 ]; then \\t\\tmkdir -p /usr/share/man/man1; \\tfi; \\t\\tapt-get update; \\tapt-get install -y \\t\\topenjdk-8-jdk=\\\""$JAVA_DEBIAN_VERSION\\\"" \\t\\tca-certificates-java=\\\""$CA_CERTIFICATES_JAVA_VERSION\\\"" \\t; \\trm -rf /var/lib/apt/lists/*; \\t\\t[ \\\""$(readlink -f \\\""$JAVA_HOME\\\"")\\\"" = \\\""$(docker-java-home)\\\"" ]; \\t\\tupdate-alternatives --get-selections | awk -v home=\\\""$(readlink -f \\\""$JAVA_HOME\\\"")\\\"" 'index($3, home) == 1 { $2 = \\\""manual\\\""; print | \\\""update-alternatives --set-selections\\\"" }'; \\tupdate-alternatives --query java | grep -q 'Status: manual'\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\"",\""parent\"":\""ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\"",\""created\"":\""2018-03-19T21:22:53.380702822Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\"",\""parent\"":\""7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\"",\""created\"":\""2018-03-19T21:22:53.161529652Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\"",\""parent\"":\""7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\"",\""created\"":\""2018-03-19T21:22:52.921597489Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_VERSION=8u162\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\"",\""parent\"":\""a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\"",\""created\"":\""2018-03-14T11:09:02.54085877Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_HOME=/docker-java-home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\"",\""parent\"":\""f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\"",\""created\"":\""2018-03-14T11:09:02.292291489Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c ln -svT \\\""/usr/lib/jvm/java-8-openjdk-$(dpkg --print-architecture)\\\"" /docker-java-home\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\"",\""parent\"":\""b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\"",\""created\"":\""2018-03-14T11:09:01.580163972Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c { \\t\\techo '#!/bin/sh'; \\t\\techo 'set -e'; \\t\\techo; \\t\\techo 'dirname \\\""$(dirname \\\""$(readlink -f \\\""$(which javac || which java)\\\"")\\\"")\\\""'; \\t} \\u003e /usr/local/bin/docker-java-home \\t\\u0026\\u0026 chmod +x /usr/local/bin/docker-java-home\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\"",\""parent\"":\""800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\"",\""created\"":\""2018-03-14T11:09:00.816087216Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV LANG=C.UTF-8\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\"",\""parent\"":\""62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\"",\""created\"":\""2018-03-14T11:09:00.593223495Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tbzip2 \\t\\tunzip \\t\\txz-utils \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\"",\""parent\"":\""810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\"",\""created\"":\""2018-03-13T23:56:55.333999982Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tbzr \\t\\tgit \\t\\tmercurial \\t\\topenssh-client \\t\\tsubversion \\t\\t\\t\\tprocps \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\"",\""parent\"":\""e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\"",\""created\"":\""2018-03-13T23:56:22.934435097Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c set -ex; \\tif ! command -v gpg \\u003e /dev/null; then \\t\\tapt-get update; \\t\\tapt-get install -y --no-install-recommends \\t\\t\\tgnupg \\t\\t\\tdirmngr \\t\\t; \\t\\trm -rf /var/lib/apt/lists/*; \\tfi\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\"",\""parent\"":\""ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\"",\""created\"":\""2018-03-13T23:56:19.194216172Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tca-certificates \\t\\tcurl \\t\\twget \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\"",\""parent\"":\""8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\"",\""created\"":\""2018-03-13T22:26:49.547884802Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  CMD [\\\""bash\\\""]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\"",\""created\"":\""2018-03-13T22:26:49.153534342Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD file:b380df301ccb5ca09f0d7cd5697ed402fa55f3e9bc5df2f4d489ba31f28de58a in / \""]}}""
      }
   ],
   ""signatures"": [
      {
         ""header"": {
            ""jwk"": {
               ""crv"": ""P-256"",
               ""kid"": ""JTGT:L32L:BI2G:TG3A:RLO2:6H6K:OZXC:HFYY:SPZW:QXEZ:XNK3:2KAL"",
               ""kty"": ""EC"",
               ""x"": ""Q3Qr-lNb0qyOiyFBHzF5v4gxgVp_drIszYInemkB464"",
               ""y"": ""oBzQUsRherctDgDVxwOR0zkij_B7GAL9B20PWVtHzfs""
            },
            ""alg"": ""ES256""
         },
         ""signature"": ""X6BvXE9thNyPHIvyH_0GE1blPxznEcPbILpB5HBvI2339gSA5t4HAE7GMalgKLyThJbjrNjiq_PQqreFMBpqzA"",
         ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjU4ODg5LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTgtMTAtMTVUMTM6MDI6MjdaIn0""
      }
   ]
}
{noformat}

This should not be related: when we try to find the extracted layers on the layers dir, we could only find two:
{noformat}
ERROR(130)-22:27:48-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/layers # ls -alh | grep 'fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\|bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\|2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\|36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\|ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\|c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\|2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\|a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\|f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\|0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\|1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\|35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\|12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\|87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\|a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\|39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\|0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\|f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\|2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\|ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\|9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\|e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\|84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\|a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\|bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\|c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\|662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\|5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\|25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\|1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\|fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\|3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\|ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\|b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\|d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\|fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\|f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\|9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\|72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\|9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\|764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\|7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\|35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\|9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\|fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\|afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\|64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\|0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\|d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\|c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\|09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\|3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\|6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\|c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\|9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\|6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\|0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\|14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\|3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\|8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\|14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\|f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\|8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\|1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\|9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\|cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\|8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\|f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\|5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\|b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\|d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\|35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\|d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\|edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\|27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\|558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\|180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\|4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\|ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\|7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\|7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\|a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\|f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\|b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\|800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\|62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\|810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\|e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\|ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\|8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324'
drwxr-xr-x.   3 root root  40 Oct 15 10:23 8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324
drwxr-xr-x.   3 root root  40 Oct 15 10:23 ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182
{noformat}

These two are base layers that were downloaded earlier from other images. We still need to figure out why there is one layer fetch not finished. (no curl process and tar process running stuck at background)"	MESOS	Resolved	3	1	4582	containerizer
13010538	Nested containers getting killed before network isolation can be applied to them.	"Seeing this odd behavior in one of our clusters:
```
http.cpp:1948] Failed to launch nested container cb92634b-42b3-40f3-94f7-609f89a362bc.46d884e4-d0eb-4572-be1d-24414df7cb2e: Collect failed: Failed to seed container cb92634b-42b3-40f3-94f7-609f89a362bc.46d884e4-d0eb-4572-be1d-24414df7cb2e: Collect failed: Failed to setup hostname and network files: Failed to enter the mount namespace of pid 21591: Pid 21591 does not exist
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: I1007 02:05:55.894485 31531 containerizer.cpp:1931] Destroying container cb92634b-42b3-40f3-94f7-609f89a362bc.46d884e4-d0eb-4572-be1d-24414df7cb2e in ISOLATING state
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: I1007 02:05:55.894439 31531 containerizer.cpp:2300] Container cb92634b-42b3-40f3-94f7-609f89a362bc.46d884e4-d0eb-4572-be1d-24414df7cb2e has exited
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: I1007 02:05:55.854456 31534 systemd.cpp:96] Assigned child process '21591' to 'mesos_executors.slice'
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: W1007 02:05:55.831861 21580 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: I1007 02:05:55.831526 21580 openssl.cpp:432] Will only verify peer certificate if presented!
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: NOTE: Set LIBPROCESS_SSL_VERIFY_CERT=1 to enable peer certificate verification
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: I1007 02:05:55.831521 21580 openssl.cpp:426] Will not verify peer certificate!
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: I1007 02:05:55.831511 21580 openssl.cpp:421] CA directory path unspecified! NOTE: Set CA directory path with LIBPROCESS_SSL_CA_DIR=<dirpath>
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: W1007 02:05:55.831405 21580 openssl.cpp:399] Failed SSL connections will be downgraded to a non-SSL socket
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: W1007 02:05:55.828413 21581 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Oct 07 02:05:55 ip-10-10-0-207 mesos-agent[31520]: NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
```
The above log is ""reverse"" chronological order, so please read it bottom up.

The relevant log is:
```
http.cpp:1948] Failed to launch nested container cb92634b-42b3-40f3-94f7-609f89a362bc.46d884e4-d0eb-4572-be1d-24414df7cb2e: Collect failed: Failed to seed container cb92634b-42b3-40f3-94f7-609f89a362bc.46d884e4-d0eb-4572-be1d-24414df7cb2e: Collect failed: Failed to setup hostname and network files: Failed to enter the mount namespace of pid 21591: Pid 21591 does not exist
```
Looks like the nested container failed to launch because the `isolate` call to the `network/cni` isolator failed. Seems like when the isolator received the `isolate` call the PID for the nested container has already exited and it couldn't enter its mount namespace to setup the network files. 

The odd thing here is that the nested container would have been frozen, and hence was not running, so not sure what killed the nested container. My suspicion falls on systemd, since I also see this log message:
```
Oct 07 18:02:31 ip-10-10-0-207 mesos-agent[31520]: I1007 18:02:31.473656 31532 systemd.cpp:96] Assigned child process '1596' to 'mesos_executors.slice'
```

"	MESOS	Resolved	3	1	4582	mesosphere
12970113	docker containerizer should prefix relative volume.container_path values with the path to the sandbox	"docker containerizer currently requires absolute paths for values of volume.container_path. this is inconsistent with the mesos containerizer which requires relative container_path. it makes for a confusing API. both at the Mesos level as well as at the Marathon level.

ideally the docker containerizer would allow a framework to specify a relative path for volume.container_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it.

/cc [~jieyu]"	MESOS	Resolved	3	1	4582	docker, mesosphere, storage, volumes
12953223	MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.	"Observed on the Apache Jenkins.

{noformat}
[ RUN      ] MesosContainerizerProvisionerTest.ProvisionFailed
I0324 13:38:56.284261  2948 containerizer.cpp:666] Starting container 'test_container' for executor 'executor' of framework ''
I0324 13:38:56.285825  2939 containerizer.cpp:1421] Destroying container 'test_container'
I0324 13:38:56.285854  2939 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'test_container'
[       OK ] MesosContainerizerProvisionerTest.ProvisionFailed (7 ms)
[ RUN      ] MesosContainerizerProvisionerTest.DestroyWhileProvisioning
I0324 13:38:56.291187  2944 containerizer.cpp:666] Starting container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2' for executor 'executor' of framework ''
I0324 13:38:56.292157  2944 containerizer.cpp:1421] Destroying container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
I0324 13:38:56.292179  2944 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
F0324 13:38:56.292899  2944 containerizer.cpp:752] Check failed: containers_.contains(containerId)
*** Check failure stack trace: ***
    @     0x2ac9973d0ae4  google::LogMessage::Fail()
    @     0x2ac9973d0a30  google::LogMessage::SendToLog()
    @     0x2ac9973d0432  google::LogMessage::Flush()
    @     0x2ac9973d3346  google::LogMessageFatal::~LogMessageFatal()
    @     0x2ac996af897c  mesos::internal::slave::MesosContainerizerProcess::_launch()
    @     0x2ac996b1f18a  _ZZN7process8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS1_11ContainerIDERK6OptionINS1_8TaskInfoEERKNS1_12ExecutorInfoERKSsRKS8_ISsERKNS1_7SlaveIDERKNS_3PIDINS3_5SlaveEEEbRKS8_INS3_13ProvisionInfoEES5_SA_SD_SsSI_SL_SQ_bSU_EENS_6FutureIT_EERKNSO_IT0_EEMS10_FSZ_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_ENKUlPNS_11ProcessBaseEE_clES1P_
    @     0x2ac996b479d9  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKSC_ISsERKNS5_7SlaveIDERKNS0_3PIDINS7_5SlaveEEEbRKSC_INS7_13ProvisionInfoEES9_SE_SH_SsSM_SP_SU_bSY_EENS0_6FutureIT_EERKNSS_IT0_EEMS14_FS13_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2ac997334fef  std::function<>::operator()()
    @     0x2ac99731b1c7  process::ProcessBase::visit()
    @     0x2ac997321154  process::DispatchEvent::visit()
    @           0x9a699c  process::ProcessBase::serve()
    @     0x2ac9973173c0  process::ProcessManager::resume()
    @     0x2ac99731445a  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2ac997320916  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2ac9973208c6  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2ac997320858  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2ac9973207af  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2ac997320748  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2ac9989aea60  (unknown)
    @     0x2ac999125182  start_thread
    @     0x2ac99943547d  (unknown)
make[4]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[4]: *** [check-local] Aborted
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/mesos/mesos-0.29.0/_build'
make: *** [distcheck] Error 1
Build step 'Execute shell' marked build as failure
{noformat}"	MESOS	Resolved	2	1	4582	mesosphere
12928536	Protobuf parse should pass error messages when parsing nested JSON.	Currently when protobuf::parse handles nested JSON objects, it cannot pass any error message out. We should enable showing those error messages.	MESOS	Resolved	3	1	4582	mesosphere
12980726	Executors should not inherit environment variables from the agent.	"Currently executors are inheriting environment variables form the slave in mesos containerizer. This is problematic, because of two reasons:

1. When we use docker images (such as `mongo`) in unified containerizer, duplicated environment variables inherited from the slave lead to initialization failures, because LANG and/or LC_* environment variables are not set correctly.

2. When we are looking at the environment variables from the executor tasks, there are pages of environment variables listed, which is redundant and dangerous.

Depending on the reasons above, we propose that no longer allow executors to inherit environment variables from the slave. Instead, users should specify all environment variables they need by setting the slave flag `--executor_environment_variables` as a JSON format."	MESOS	Resolved	3	1	4582	containerizer, mesosphere
13161641	Improve the container preparing logging in IOSwitchboard and volume/secret isolator.	Improve the container preparing logging in IOSwitchboard and volume/secret isolator.	MESOS	Resolved	3	4	4582	containerizer
13122317	Add excluded image parameter to containerizer::pruneImages() interface.	Add excluded image parameter to containerizer::pruneImages() interface.	MESOS	Resolved	3	4	4582	containerizer, image-gc, mesosphere, uber
12927019	Update isolator prepare function to use ContainerLaunchInfo	"Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers. 

By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher."	MESOS	Resolved	1	1	4582	mesosphere, unified-containerizer-mvp
12842078	Allow runtime configuration to be returned from provisioner	"Image specs also includes execution configuration (e.g: Env, user, ports, etc).
We should support passing those information from the image provisioner back to the containerizer."	MESOS	Resolved	3	4	4582	mesosphere
13026737	Support 'Basic' auth docker private registry on Unified Containerizer.	Currently, the Unified Containerizer only supports the private docker registry with 'Bearer' authorization (token is needed from the auth server). We should support the 'Basic' auth registry as well.	MESOS	Resolved	1	4	4582	containerizer
12987960	Support mounting image volume in mesos containerizer.	Mesos containerizer should be able to support mounting image volume type. Specifically, both image rootfs and default manifest should be reachable inside container's mount namespace.	MESOS	Resolved	3	4	4582	containerizer, filesystem, isolator, mesosphere
13008814	Add unit tests for nested container case for filesystem/linux isolator.	Parameterize the existing tests so that all works for both top level container and nested container.	MESOS	Accepted	3	4	4582	isolator, mesosphere
13226302	Support docker manifest v2s2 config GC.	"After docker manifest v2s2 support, layer GC is still properly supported.

However, the manifest config is not garbage collected. Need to add the config dir to the checkpointed LAYERS_FILE to support config GC."	MESOS	Resolved	1	4	4582	containerization
12981118	Deprecate camel case proto field in isolator ContainerConfig.	Currently there are extra ExecutorInfo and TaskInfo in isolator ContaienrConfig, because a deprecation cycle is needed to deprecate camel cased proto field names. This JIRA is used for tracking this issue, which should address the TODO in isolator.proto.	MESOS	Resolved	3	4	4582	isolation, isolator, mesosphere
12962478	LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystemCommandExecutor is flaky.	"Observed on the internal Mesosphere CI:
{code}
[18:03:58] :	 [Step 10/10] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystemCommandExecutor
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.584962   642 cluster.cpp:149] Creating default 'local' authorizer
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.597232   642 leveldb.cpp:174] Opened db in 12.195009ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.598534   642 leveldb.cpp:181] Compacted db in 1.266907ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.598558   642 leveldb.cpp:196] Created db iterator in 5704ns
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.598565   642 leveldb.cpp:202] Seeked to beginning of db in 703ns
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.598570   642 leveldb.cpp:271] Iterated through 0 keys in the db in 272ns
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.598585   642 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.598815   663 recover.cpp:447] Starting replica recovery
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.598927   663 recover.cpp:473] Replica is in EMPTY status
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.599241   663 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (17941)@172.30.2.229:48705
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.599323   663 recover.cpp:193] Received a recover response from a replica in EMPTY status
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.599472   657 recover.cpp:564] Updating replica status to STARTING
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600092   661 master.cpp:382] Master 7e239aa6-d964-4f11-95a8-ba808ad23f4e (ip-172-30-2-229.mesosphere.io) started on 172.30.2.229:48705
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600105   661 master.cpp:384] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/XRr1Iz/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/XRr1Iz/master"" --zk_session_timeout=""10secs""
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600225   661 master.cpp:433] Master only allowing authenticated frameworks to register
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600231   661 master.cpp:439] Master only allowing authenticated agents to register
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600234   661 master.cpp:445] Master only allowing authenticated HTTP frameworks to register
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600239   661 credentials.hpp:37] Loading credentials for authentication from '/tmp/XRr1Iz/credentials'
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600371   661 master.cpp:489] Using default 'crammd5' authenticator
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600410   661 master.cpp:560] Using default 'basic' HTTP authenticator
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600461   661 master.cpp:640] Using default 'basic' HTTP framework authenticator
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600525   661 master.cpp:687] Authorization enabled
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600590   656 whitelist_watcher.cpp:77] No whitelist given
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600610   660 hierarchical.cpp:142] Initialized hierarchical allocator process
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600883   659 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.350635ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.600904   659 replica.cpp:320] Persisted replica status to STARTING
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601029   659 recover.cpp:473] Replica is in STARTING status
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601161   657 master.cpp:1932] The newly elected leader is master@172.30.2.229:48705 with id 7e239aa6-d964-4f11-95a8-ba808ad23f4e
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601176   657 master.cpp:1945] Elected as the leading master!
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601181   657 master.cpp:1632] Recovering from registrar
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601227   661 registrar.cpp:331] Recovering registrar
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601291   660 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (17944)@172.30.2.229:48705
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601460   656 recover.cpp:193] Received a recover response from a replica in STARTING status
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.601572   663 recover.cpp:564] Updating replica status to VOTING
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.602798   663 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.178644ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.602814   663 replica.cpp:320] Persisted replica status to VOTING
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.602845   663 recover.cpp:578] Successfully joined the Paxos group
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.602881   663 recover.cpp:462] Recover process terminated
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.603023   656 log.cpp:524] Attempting to start the writer
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.603399   663 replica.cpp:493] Replica received implicit promise request from (17945)@172.30.2.229:48705 with proposal 1
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.604593   663 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.178393ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.604607   663 replica.cpp:342] Persisted promised to 1
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.604833   662 coordinator.cpp:238] Coordinator attempting to fill missing positions
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.605226   660 replica.cpp:388] Replica received explicit promise request from (17946)@172.30.2.229:48705 for position 0 with proposal 2
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.606335   660 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.088746ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.606351   660 replica.cpp:712] Persisted action at 0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.606735   661 replica.cpp:537] Replica received write request for position 0 from (17947)@172.30.2.229:48705
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.606767   661 leveldb.cpp:436] Reading position from leveldb took 13169ns
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.608047   661 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.264327ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.608062   661 replica.cpp:712] Persisted action at 0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.608309   657 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.609447   657 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.122608ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.609462   657 replica.cpp:712] Persisted action at 0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.609467   657 replica.cpp:697] Replica learned NOP action at position 0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.609609   657 log.cpp:540] Writer started with ending position 0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.609887   661 leveldb.cpp:436] Reading position from leveldb took 8414ns
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.610115   661 registrar.cpp:364] Successfully fetched the registry (0B) in 8.865024ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.610144   661 registrar.cpp:463] Applied 1 operations in 4077ns; attempting to update the 'registry'
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.610333   659 log.cpp:548] Attempting to append 210 bytes to the log
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.610385   656 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.610631   663 replica.cpp:537] Replica received write request for position 1 from (17948)@172.30.2.229:48705
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.611784   663 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.132676ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.611799   663 replica.cpp:712] Persisted action at 1
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.612002   662 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613100   662 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.077829ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613114   662 replica.cpp:712] Persisted action at 1
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613121   662 replica.cpp:697] Replica learned APPEND action at position 1
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613335   659 registrar.cpp:508] Successfully updated the 'registry' in 3.173888ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613402   657 log.cpp:567] Attempting to truncate the log to 1
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613420   659 registrar.cpp:394] Successfully recovered registrar
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613545   663 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613566   656 master.cpp:1740] Recovered 0 agents from the Registry (171B) ; allowing 10mins for agents to re-register
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613679   659 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.613807   660 replica.cpp:537] Replica received write request for position 2 from (17949)@172.30.2.229:48705
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.615124   660 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.299877ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.615140   660 replica.cpp:712] Persisted action at 2
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.615324   657 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.616408   657 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.067973ms
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.616437   657 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14275ns
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.616446   657 replica.cpp:712] Persisted action at 2
[18:03:58]W:	 [Step 10/10] I0425 18:03:58.616451   657 replica.cpp:697] Replica learned TRUNCATE action at position 2
[18:03:59]W:	 [Step 10/10] I0425 18:03:59.601456   662 hierarchical.cpp:1488] No resources available to allocate!
[18:03:59]W:	 [Step 10/10] I0425 18:03:59.601508   662 hierarchical.cpp:1139] Performed allocation for 0 agents in 90519ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.326802   642 linux.cpp:81] Making '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj' a shared mount
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.339643   642 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.340358   642 cluster.cpp:398] Creating default 'local' authorizer
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.340817   658 slave.cpp:204] Agent started on 458)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.340831   658 slave.cpp:205] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj""
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341024   658 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/credential'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341190   658 slave.cpp:342] Agent using credential for: test-principal
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341203   658 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/http_credentials'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341292   658 slave.cpp:394] Using default 'basic' HTTP authenticator
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341378   658 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
[18:04:00]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341543   642 sched.cpp:224] Version: 0.29.0
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341552   658 slave.cpp:593] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341583   658 slave.cpp:601] Agent attributes: [  ]
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341591   658 slave.cpp:606] Agent hostname: ip-172-30-2-229.mesosphere.io
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341683   660 sched.cpp:328] New master detected at master@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341712   660 sched.cpp:384] Authenticating with master master@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341722   660 sched.cpp:391] Using default CRAM-MD5 authenticatee
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341886   661 authenticatee.cpp:121] Creating new client SASL connection
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.341940   658 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/meta'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342118   658 status_update_manager.cpp:200] Recovering status update manager
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342120   662 master.cpp:5803] Authenticating scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342188   661 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(932)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342308   660 containerizer.cpp:444] Recovering containerizer
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342308   657 authenticator.cpp:98] Creating new server SASL connection
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342531   657 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342551   657 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342634   659 authenticator.cpp:203] Received SASL authentication start
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342669   659 authenticator.cpp:325] Authentication requires more steps
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342701   659 authenticatee.cpp:258] Received SASL authentication step
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342789   656 authenticator.cpp:231] Received SASL authentication step
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342808   656 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-229.mesosphere.io' server FQDN: 'ip-172-30-2-229.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342814   656 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342825   656 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342833   656 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-229.mesosphere.io' server FQDN: 'ip-172-30-2-229.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342836   656 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342840   656 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342849   656 authenticator.cpp:317] Authentication success
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342896   659 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(932)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342897   662 authenticatee.cpp:298] Authentication success
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.342916   656 master.cpp:5833] Successfully authenticated principal 'test-principal' at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343030   663 sched.cpp:474] Successfully authenticated with master master@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343044   663 sched.cpp:783] Sending SUBSCRIBE call to master@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343091   663 sched.cpp:816] Will retry registration in 485.223658ms if necessary
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343134   656 master.cpp:2465] Received SUBSCRIBE call for framework 'default' at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343152   656 master.cpp:1971] Authorizing framework principal 'test-principal' to receive offers for role '*'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343235   656 master.cpp:2541] Subscribing framework default with checkpointing disabled and capabilities [  ]
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343372   657 hierarchical.cpp:264] Added framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343391   657 hierarchical.cpp:1488] No resources available to allocate!
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343400   657 hierarchical.cpp:1583] No inverse offers to send out!
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343417   657 hierarchical.cpp:1139] Performed allocation for 0 agents in 32147ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343487   656 sched.cpp:710] Framework registered with 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.343518   656 sched.cpp:724] Scheduler::registered took 20476ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344192   659 provisioner.cpp:245] Provisioner recovery complete
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344338   663 slave.cpp:4826] Finished recovery
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344614   663 slave.cpp:4998] Querying resource estimator for oversubscribable resources
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344729   659 slave.cpp:953] New master detected at master@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344738   662 status_update_manager.cpp:174] Pausing sending status updates
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344745   659 slave.cpp:1016] Authenticating with master master@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344753   659 slave.cpp:1021] Using default CRAM-MD5 authenticatee
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344786   659 slave.cpp:989] Detecting new master
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344811   656 authenticatee.cpp:121] Creating new client SASL connection
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344854   659 slave.cpp:5012] Received oversubscribable resources  from the resource estimator
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.344982   656 master.cpp:5803] Authenticating slave(458)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345024   656 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(933)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345121   660 authenticator.cpp:98] Creating new server SASL connection
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345310   660 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345327   660 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345374   660 authenticator.cpp:203] Received SASL authentication start
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345408   660 authenticator.cpp:325] Authentication requires more steps
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345450   660 authenticatee.cpp:258] Received SASL authentication step
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345540   658 authenticator.cpp:231] Received SASL authentication step
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345553   658 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-229.mesosphere.io' server FQDN: 'ip-172-30-2-229.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345558   658 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345566   658 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345571   658 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-229.mesosphere.io' server FQDN: 'ip-172-30-2-229.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345576   658 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345578   658 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345585   658 authenticator.cpp:317] Authentication success
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345640   659 master.cpp:5833] Successfully authenticated principal 'test-principal' at slave(458)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345652   657 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(933)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345666   661 authenticatee.cpp:298] Authentication success
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345746   661 slave.cpp:1086] Successfully authenticated with master master@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345789   661 slave.cpp:1482] Will retry registration in 19.872252ms if necessary
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345850   659 master.cpp:4514] Registering agent at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io) with id 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.345944   657 registrar.cpp:463] Applied 1 operations in 15436ns; attempting to update the 'registry'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.346144   659 log.cpp:548] Attempting to append 396 bytes to the log
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.346197   658 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.346508   658 replica.cpp:537] Replica received write request for position 3 from (17965)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.347267   658 leveldb.cpp:341] Persisting action (415 bytes) to leveldb took 736349ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.347286   658 replica.cpp:712] Persisted action at 3
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.347520   659 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348105   659 leveldb.cpp:341] Persisting action (417 bytes) to leveldb took 565502ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348116   659 replica.cpp:712] Persisted action at 3
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348121   659 replica.cpp:697] Replica learned APPEND action at position 3
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348390   658 registrar.cpp:508] Successfully updated the 'registry' in 2.422784ms
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348438   663 log.cpp:567] Attempting to truncate the log to 3
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348547   656 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348604   659 master.cpp:4582] Registered agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348654   656 slave.cpp:3701] Received ping from slave-observer(423)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348670   660 hierarchical.cpp:473] Added agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 (ip-172-30-2-229.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348711   656 slave.cpp:1130] Registered with master master@172.30.2.229:48705; given agent ID 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348726   656 fetcher.cpp:81] Clearing fetcher cache
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348769   659 replica.cpp:537] Replica received write request for position 4 from (17966)@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348834   660 hierarchical.cpp:1583] No inverse offers to send out!
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348858   660 hierarchical.cpp:1162] Performed allocation for agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 in 160800ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348888   660 status_update_manager.cpp:181] Resuming sending status updates
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348994   660 master.cpp:5632] Sending 1 offers to framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (default) at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.348999   656 slave.cpp:1153] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/meta/slaves/7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0/slave.info'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349146   660 sched.cpp:880] Scheduler::resourceOffers took 40293ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349184   656 slave.cpp:1190] Forwarding total oversubscribed resources 
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349258   656 master.cpp:4926] Received update of agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io) with total oversubscribed resources 
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349381   656 hierarchical.cpp:531] Agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 (ip-172-30-2-229.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349417   659 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 632584ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349428   659 replica.cpp:712] Persisted action at 4
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349427   656 hierarchical.cpp:1488] No resources available to allocate!
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349441   656 hierarchical.cpp:1583] No inverse offers to send out!
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349454   656 hierarchical.cpp:1162] Performed allocation for agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 in 44061ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349580   660 master.cpp:3412] Processing ACCEPT call for offers: [ 7e239aa6-d964-4f11-95a8-ba808ad23f4e-O0 ] on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io) for framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (default) at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349602   660 master.cpp:3015] Authorizing framework principal 'test-principal' to launch task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 as user 'root'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.349720   661 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350009   659 master.hpp:177] Adding task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 (ip-172-30-2-229.mesosphere.io)
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350044   659 master.cpp:3897] Launching task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (default) at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io)
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350181   658 slave.cpp:1522] Got assigned task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 for framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350261   658 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:04:00]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350317   661 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 575979ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350348   661 leveldb.cpp:399] Deleting ~2 keys from leveldb took 16773ns
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350361   661 replica.cpp:712] Persisted action at 4
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350369   661 replica.cpp:697] Replica learned TRUNCATE action at position 4
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350391   658 slave.cpp:1641] Launching task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 for framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350428   658 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:04:00]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.350800   658 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/slaves/7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0/frameworks/7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000/executors/52bf3e3e-f9e2-417c-a66d-24c5f8ec2898/runs/57198894-8507-4f3e-8c9b-428c1cdeb9bb' to user 'root'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.355469   658 slave.cpp:5646] Launching executor 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/slaves/7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0/frameworks/7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000/executors/52bf3e3e-f9e2-417c-a66d-24c5f8ec2898/runs/57198894-8507-4f3e-8c9b-428c1cdeb9bb'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.355728   658 slave.cpp:1867] Queuing task '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' for executor '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.355738   662 containerizer.cpp:703] Starting container '57198894-8507-4f3e-8c9b-428c1cdeb9bb' for executor '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' of framework '7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.355782   658 slave.cpp:906] Successfully attached file '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/slaves/7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0/frameworks/7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000/executors/52bf3e3e-f9e2-417c-a66d-24c5f8ec2898/runs/57198894-8507-4f3e-8c9b-428c1cdeb9bb'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.356014   661 provisioner.cpp:285] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/provisioner/containers/57198894-8507-4f3e-8c9b-428c1cdeb9bb/backends/copy/rootfses/264fec60-6476-4aab-94af-d459a15c2448' for container 57198894-8507-4f3e-8c9b-428c1cdeb9bb
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.356412   657 copy.cpp:128] Copying layer path '/tmp/XRr1Iz/test_image' to rootfs '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/provisioner/containers/57198894-8507-4f3e-8c9b-428c1cdeb9bb/backends/copy/rootfses/264fec60-6476-4aab-94af-d459a15c2448'
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.602272   656 hierarchical.cpp:1488] No resources available to allocate!
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.602299   656 hierarchical.cpp:1583] No inverse offers to send out!
[18:04:00]W:	 [Step 10/10] I0425 18:04:00.602319   656 hierarchical.cpp:1139] Performed allocation for 1 agents in 123386ns
[18:04:01]W:	 [Step 10/10] I0425 18:04:01.603039   659 hierarchical.cpp:1488] No resources available to allocate!
[18:04:01]W:	 [Step 10/10] I0425 18:04:01.603067   659 hierarchical.cpp:1583] No inverse offers to send out!
[18:04:01]W:	 [Step 10/10] I0425 18:04:01.603086   659 hierarchical.cpp:1139] Performed allocation for 1 agents in 138139ns
[18:04:02]W:	 [Step 10/10] I0425 18:04:02.603773   657 hierarchical.cpp:1488] No resources available to allocate!
[18:04:02]W:	 [Step 10/10] I0425 18:04:02.603811   657 hierarchical.cpp:1583] No inverse offers to send out!
[18:04:02]W:	 [Step 10/10] I0425 18:04:02.603828   657 hierarchical.cpp:1139] Performed allocation for 1 agents in 113145ns
[18:04:03]W:	 [Step 10/10] I0425 18:04:03.604910   660 hierarchical.cpp:1488] No resources available to allocate!
[18:04:03]W:	 [Step 10/10] I0425 18:04:03.604938   660 hierarchical.cpp:1583] No inverse offers to send out!
[18:04:03]W:	 [Step 10/10] I0425 18:04:03.604954   660 hierarchical.cpp:1139] Performed allocation for 1 agents in 126355ns
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.346846   661 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWNS
[18:04:04]W:	 [Step 10/10] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
[18:04:04]W:	 [Step 10/10] + grep -E /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/.+ /proc/self/mountinfo
[18:04:04]W:	 [Step 10/10] + grep -v 57198894-8507-4f3e-8c9b-428c1cdeb9bb
[18:04:04]W:	 [Step 10/10] + cut '-d ' -f5
[18:04:04]W:	 [Step 10/10] + xargs --no-run-if-empty umount -l
[18:04:04]W:	 [Step 10/10] + mount -n --rbind /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/provisioner/containers/57198894-8507-4f3e-8c9b-428c1cdeb9bb/backends/copy/rootfses/264fec60-6476-4aab-94af-d459a15c2448 /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/slaves/7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0/frameworks/7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000/executors/52bf3e3e-f9e2-417c-a66d-24c5f8ec2898/runs/57198894-8507-4f3e-8c9b-428c1cdeb9bb/.rootfs
[18:04:04]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.457139 15601 process.cpp:991] libprocess is initialized on 172.30.2.229:48674 with 8 worker threads
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.457484 15601 logging.cpp:195] Logging to STDERR
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.458220 15601 exec.cpp:150] Version: 0.29.0
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.459712 15648 exec.cpp:200] Executor started at: executor(1)@172.30.2.229:48674 with pid 15601
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.460300   656 slave.cpp:2862] Got registration for executor '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 from executor(1)@172.30.2.229:48674
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.460985   660 slave.cpp:2032] Sending queued task '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' to executor '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 at executor(1)@172.30.2.229:48674
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.461005 15647 exec.cpp:225] Executor registered on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.461822 15647 exec.cpp:237] Executor::registered took 101939ns
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.462000 15647 exec.cpp:312] Executor asked to run task '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898'
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.462044 15647 exec.cpp:321] Executor::launchTask took 30183ns
[18:04:04] :	 [Step 10/10] Registered executor on ip-172-30-2-229.mesosphere.io
[18:04:04] :	 [Step 10/10] Starting task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.463605 15647 exec.cpp:535] Executor sending status update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04] :	 [Step 10/10] Forked command at 15656
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.464066   656 slave.cpp:3221] Handling status update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 from executor(1)@172.30.2.229:48674
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.464565   658 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04] :	 [Step 10/10] sh -c 'test -d /mnt/mesos/sandbox'
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.464589   658 status_update_manager.cpp:497] Creating StatusUpdate stream for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.464783   658 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 to the agent
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.464893   661 slave.cpp:3619] Forwarding the update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 to master@172.30.2.229:48705
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.464998   661 slave.cpp:3513] Status update manager successfully handled status update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465014   661 slave.cpp:3529] Sending acknowledgement for status update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 to executor(1)@172.30.2.229:48674
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465016   658 master.cpp:5071] Status update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 from agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io)
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465035   658 master.cpp:5119] Forwarding status update TASK_RUNNING (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465086   658 master.cpp:6727] Updating the state of task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465193   661 sched.cpp:988] Scheduler::statusUpdate took 56656ns
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465309   657 master.cpp:4226] Processing ACKNOWLEDGE call b38791ab-d91d-4906-9a86-af476b83924e for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (default) at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705 on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465338 15652 exec.cpp:358] Executor received status update acknowledgement b38791ab-d91d-4906-9a86-af476b83924e for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465440   657 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.465507   658 slave.cpp:2631] Status update manager successfully handled status update acknowledgement (UUID: b38791ab-d91d-4906-9a86-af476b83924e) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04] :	 [Step 10/10] Command exited with status 0 (pid: 15656)
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.565306 15648 exec.cpp:535] Executor sending status update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.565690   657 slave.cpp:3221] Handling status update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 from executor(1)@172.30.2.229:48674
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566062   662 slave.cpp:5966] Terminating task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566371   662 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566422   662 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 to the agent
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566495   660 slave.cpp:3619] Forwarding the update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 to master@172.30.2.229:48705
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566575   660 slave.cpp:3513] Status update manager successfully handled status update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566589   660 slave.cpp:3529] Sending acknowledgement for status update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 to executor(1)@172.30.2.229:48674
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566658   659 master.cpp:5071] Status update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 from agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io)
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566684   659 master.cpp:5119] Forwarding status update TASK_FINISHED (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566733   659 master.cpp:6727] Updating the state of task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566829 15650 exec.cpp:358] Executor received status update acknowledgement 244979d6-35e7-4a28-875e-31c5289240d3 for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566850   661 sched.cpp:988] Scheduler::statusUpdate took 41212ns
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566905   662 hierarchical.cpp:891] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 from framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566972   661 master.cpp:4226] Processing ACKNOWLEDGE call 244979d6-35e7-4a28-875e-31c5289240d3 for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (default) at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705 on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.566990   661 master.cpp:6793] Removing task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 on agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io)
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567018   642 sched.cpp:1911] Asked to stop the driver
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567078   659 sched.cpp:1150] Stopping framework '7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000'
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567165   661 master.cpp:6200] Processing TEARDOWN call for framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (default) at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567178   661 master.cpp:6212] Removing framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 (default) at scheduler-46c11828-a1c6-4ea0-8ec2-2ffb638437ba@172.30.2.229:48705
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567196   663 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567248   663 status_update_manager.cpp:528] Cleaning up status update stream for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567271   656 slave.cpp:2252] Asked to shut down framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 by master@172.30.2.229:48705
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567288   656 slave.cpp:2277] Shutting down framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567302   656 slave.cpp:4451] Shutting down executor '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 at executor(1)@172.30.2.229:48674
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567414   656 slave.cpp:2631] Status update manager successfully handled status update acknowledgement (UUID: 244979d6-35e7-4a28-875e-31c5289240d3) for task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898 of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567435   656 slave.cpp:6007] Completing task 52bf3e3e-f9e2-417c-a66d-24c5f8ec2898
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567548   657 hierarchical.cpp:375] Deactivated framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567594 15647 exec.cpp:399] Executor asked to shutdown
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567625   657 hierarchical.cpp:326] Removed framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567633 15647 exec.cpp:414] Executor::shutdown took 4162ns
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567662 15647 exec.cpp:91] Scheduling shutdown of the executor in 5secs
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.567800   661 containerizer.cpp:1481] Destroying container '57198894-8507-4f3e-8c9b-428c1cdeb9bb'
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.569454   657 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos/57198894-8507-4f3e-8c9b-428c1cdeb9bb
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.570904   661 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/57198894-8507-4f3e-8c9b-428c1cdeb9bb after 1.430016ms
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.572533   663 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos/57198894-8507-4f3e-8c9b-428c1cdeb9bb
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.574110   657 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/57198894-8507-4f3e-8c9b-428c1cdeb9bb after 1.55776ms
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.575166   661 slave.cpp:3747] executor(1)@172.30.2.229:48674 exited
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.605558   661 hierarchical.cpp:1488] No resources available to allocate!
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.605597   661 hierarchical.cpp:1139] Performed allocation for 1 agents in 82646ns
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.646847   661 containerizer.cpp:1717] Executor for container '57198894-8507-4f3e-8c9b-428c1cdeb9bb' has exited
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.648252   663 linux.cpp:822] Ignoring unmounting sandbox/work directory for container 57198894-8507-4f3e-8c9b-428c1cdeb9bb
[18:04:04]W:	 [Step 10/10] I0425 18:04:04.648402   657 provisioner.cpp:338] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_nZJkSj/provisioner/containers/57198894-8507-4f3e-8c9b-428c1cdeb9bb/backends/copy/rootfses/264fec60-6476-4aab-94af-d459a15c2448' for container 57198894-8507-4f3e-8c9b-428c1cdeb9bb
[18:04:05]W:	 [Step 10/10] I0425 18:04:05.606057   660 hierarchical.cpp:1488] No resources available to allocate!
[18:04:05]W:	 [Step 10/10] I0425 18:04:05.606106   660 hierarchical.cpp:1139] Performed allocation for 1 agents in 125175ns
[18:04:06]W:	 [Step 10/10] I0425 18:04:06.606617   662 hierarchical.cpp:1488] No resources available to allocate!
[18:04:06]W:	 [Step 10/10] I0425 18:04:06.606653   662 hierarchical.cpp:1139] Performed allocation for 1 agents in 94758ns
[18:04:07]W:	 [Step 10/10] I0425 18:04:07.607285   658 hierarchical.cpp:1488] No resources available to allocate!
[18:04:07]W:	 [Step 10/10] I0425 18:04:07.607324   658 hierarchical.cpp:1139] Performed allocation for 1 agents in 104606ns
[18:04:08]W:	 [Step 10/10] I0425 18:04:08.607969   656 hierarchical.cpp:1488] No resources available to allocate!
[18:04:08]W:	 [Step 10/10] I0425 18:04:08.608006   656 hierarchical.cpp:1139] Performed allocation for 1 agents in 87577ns
[18:04:09]W:	 [Step 10/10] I0425 18:04:09.568349   657 slave.cpp:4524] Killing executor '52bf3e3e-f9e2-417c-a66d-24c5f8ec2898' of framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 at executor(1)@172.30.2.229:48674
[18:04:09]W:	 [Step 10/10] I0425 18:04:09.608618   661 hierarchical.cpp:1488] No resources available to allocate!
[18:04:09]W:	 [Step 10/10] I0425 18:04:09.608666   661 hierarchical.cpp:1139] Performed allocation for 1 agents in 107807ns
[18:04:10]W:	 [Step 10/10] I0425 18:04:10.609025   656 hierarchical.cpp:1488] No resources available to allocate!
[18:04:10]W:	 [Step 10/10] I0425 18:04:10.609067   656 hierarchical.cpp:1139] Performed allocation for 1 agents in 76953ns
[18:04:11]W:	 [Step 10/10] I0425 18:04:11.609622   662 hierarchical.cpp:1488] No resources available to allocate!
[18:04:11]W:	 [Step 10/10] I0425 18:04:11.609657   662 hierarchical.cpp:1139] Performed allocation for 1 agents in 85763ns
[18:04:12]W:	 [Step 10/10] I0425 18:04:12.610268   659 hierarchical.cpp:1488] No resources available to allocate!
[18:04:12]W:	 [Step 10/10] I0425 18:04:12.610303   659 hierarchical.cpp:1139] Performed allocation for 1 agents in 98315ns
[18:04:13]W:	 [Step 10/10] I0425 18:04:13.610864   663 hierarchical.cpp:1488] No resources available to allocate!
[18:04:13]W:	 [Step 10/10] I0425 18:04:13.610908   663 hierarchical.cpp:1139] Performed allocation for 1 agents in 136620ns
[18:04:14]W:	 [Step 10/10] I0425 18:04:14.611521   662 hierarchical.cpp:1488] No resources available to allocate!
[18:04:14]W:	 [Step 10/10] I0425 18:04:14.611557   662 hierarchical.cpp:1139] Performed allocation for 1 agents in 82004ns
[18:04:15]W:	 [Step 10/10] I0425 18:04:15.345278   658 slave.cpp:4998] Querying resource estimator for oversubscribable resources
[18:04:15]W:	 [Step 10/10] I0425 18:04:15.345414   657 slave.cpp:5012] Received oversubscribable resources  from the resource estimator
[18:04:15]W:	 [Step 10/10] I0425 18:04:15.349479   659 slave.cpp:3701] Received ping from slave-observer(423)@172.30.2.229:48705
[18:04:15]W:	 [Step 10/10] I0425 18:04:15.612602   662 hierarchical.cpp:1488] No resources available to allocate!
[18:04:15]W:	 [Step 10/10] I0425 18:04:15.612638   662 hierarchical.cpp:1139] Performed allocation for 1 agents in 81616ns
[18:04:16]W:	 [Step 10/10] I0425 18:04:16.613032   656 hierarchical.cpp:1488] No resources available to allocate!
[18:04:16]W:	 [Step 10/10] I0425 18:04:16.613067   656 hierarchical.cpp:1139] Performed allocation for 1 agents in 80321ns
[18:04:17]W:	 [Step 10/10] I0425 18:04:17.613535   660 hierarchical.cpp:1488] No resources available to allocate!
[18:04:17]W:	 [Step 10/10] I0425 18:04:17.613569   660 hierarchical.cpp:1139] Performed allocation for 1 agents in 74160ns
[18:04:18]W:	 [Step 10/10] I0425 18:04:18.614146   657 hierarchical.cpp:1488] No resources available to allocate!
[18:04:18]W:	 [Step 10/10] I0425 18:04:18.614181   657 hierarchical.cpp:1139] Performed allocation for 1 agents in 84861ns
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.568646   642 slave.cpp:825] Agent terminating
[18:04:19] :	 [Step 10/10] ../../src/tests/cluster.cpp:500: Failure
[18:04:19] :	 [Step 10/10] Failed to wait 15secs for wait
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.568676   642 slave.cpp:2252] Asked to shut down framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 by @0.0.0.0:0
[18:04:19]W:	 [Step 10/10] W0425 18:04:19.568694   642 slave.cpp:2273] Ignoring shutdown framework 7e239aa6-d964-4f11-95a8-ba808ad23f4e-0000 because it is terminating
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.568778   658 master.cpp:1336] Agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io) disconnected
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.568796   658 master.cpp:2825] Disconnecting agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io)
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.568825   658 master.cpp:2844] Deactivating agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 at slave(458)@172.30.2.229:48705 (ip-172-30-2-229.mesosphere.io)
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.568877   659 hierarchical.cpp:560] Agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0 deactivated
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.614851   656 hierarchical.cpp:1488] No resources available to allocate!
[18:04:19]W:	 [Step 10/10] I0425 18:04:19.614899   656 hierarchical.cpp:1139] Performed allocation for 1 agents in 81590ns
[18:04:20]W:	 [Step 10/10] I0425 18:04:20.203956   642 master.cpp:1189] Master terminating
[18:04:20]W:	 [Step 10/10] I0425 18:04:20.204100   661 hierarchical.cpp:505] Removed agent 7e239aa6-d964-4f11-95a8-ba808ad23f4e-S0
[18:04:20] :	 [Step 10/10] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystemCommandExecutor (21627 ms)
{code}"	MESOS	Resolved	3	1	4582	isolation, mesosphere
12950138	Reduce the size of LinuxRootfs in tests.	Right now, LinuxRootfs copies files from the host filesystem to construct a chroot-able rootfs. We copy a lot of unnecessary files, making it very large. We can potentially strip a lot files.	MESOS	Resolved	3	4	4582	filesystem, isolation, mesosphere
12951645	Destroy a container while it's provisioning can lead to leaked provisioned directories.	"Here is the possible sequence of events:
1) containerizer->launch
2) provisioner->provision is called. it is fetching the image
3) executor registration timed out
4) containerizer->destroy is called
5) container->state is still in PREPARING
6) provisioner->destroy is called

So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes."	MESOS	Resolved	2	1	4582	mesosphere
13211119	ResourceOffersTest.ResourceOfferWithMultipleSlaves is flaky.	"{noformat}
09:48:57 I0114 09:48:57.153340  6468 credentials.hpp:86] Loading credential for authentication from '/tmp/4X6jRy/credential'
09:48:57 E0114 09:48:57.153373  6468 slave.cpp:296] EXIT with status 1: Empty credential file '/tmp/4X6jRy/credential' (see --credential flag)
{noformat}

caused by this commit https://github.com/apache/mesos/commit/07bccc6377a180267d4251897a765acba9fa0c4d"	MESOS	Resolved	3	1	4582	containerizer, flaky-test
13024523	Overlayfs backend may fail to mount the rootfs if both container image and image volume are specified.	"Depending on MESOS-6000, we use symlink to shorten the overlayfs mounting arguments. However, if more than one image need to be provisioned (e.g., a container image is specified while image volumes are specified for the same container), the symlink .../backends/overlay/links would fail to be created since it exists already.

Here is a simple log when we hard code overlayfs as our default backend:
{noformat}
[07:02:45] :	 [Step 10/10] [ RUN      ] Nesting/VolumeImageIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem/0
[07:02:46] :	 [Step 10/10] I1127 07:02:46.416021  2919 containerizer.cpp:207] Using isolation: filesystem/linux,volume/image,docker/runtime,network/cni
[07:02:46] :	 [Step 10/10] I1127 07:02:46.419312  2919 linux_launcher.cpp:150] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[07:02:46] :	 [Step 10/10] E1127 07:02:46.425336  2919 shell.hpp:107] Command 'hadoop version 2>&1' failed; this is the output:
[07:02:46] :	 [Step 10/10] sh: 1: hadoop: not found
[07:02:46] :	 [Step 10/10] I1127 07:02:46.425379  2919 fetcher.cpp:69] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[07:02:46] :	 [Step 10/10] I1127 07:02:46.425452  2919 local_puller.cpp:94] Creating local puller with docker registry '/tmp/R6OUei/registry'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427258  2934 containerizer.cpp:956] Starting container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 for executor 'test_executor' of framework 
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427592  2938 metadata_manager.cpp:167] Looking for image 'test_image_rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427774  2936 local_puller.cpp:147] Untarring image 'test_image_rootfs' from '/tmp/R6OUei/registry/test_image_rootfs.tar' to '/tmp/R6OUei/store/staging/9krDz2'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.512070  2933 local_puller.cpp:167] The repositories JSON file for image 'test_image_rootfs' is '{""test_image_rootfs"":{""latest"":""815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346""}}'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.512279  2933 local_puller.cpp:295] Extracting layer tar ball '/tmp/R6OUei/store/staging/9krDz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar to rootfs '/tmp/R6OUei/store/staging/9krDz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617442  2937 metadata_manager.cpp:155] Successfully cached image 'test_image_rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617908  2938 provisioner.cpp:286] Image layers: 1
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617925  2938 provisioner.cpp:296] Should hit here
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617949  2938 provisioner.cpp:315] !!!!: bind
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617959  2938 provisioner.cpp:315] !!!!: overlay
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617967  2938 provisioner.cpp:315] !!!!: copy
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617974  2938 provisioner.cpp:318] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7' for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 using overlay backend
[07:02:46] :	 [Step 10/10] I1127 07:02:46.618408  2936 overlay.cpp:175] Created symlink '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/links' -> '/tmp/DQ3blT'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.618472  2936 overlay.cpp:203] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/DQ3blT/0,upperdir=/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/upperdir,workdir=/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/workdir'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619098  2933 linux.cpp:451] Ignored an image volume for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619745  2938 metadata_manager.cpp:167] Looking for image 'test_image_volume'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619925  2937 local_puller.cpp:147] Untarring image 'test_image_volume' from '/tmp/R6OUei/registry/test_image_volume.tar' to '/tmp/R6OUei/store/staging/2GNlJO'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.713526  2935 local_puller.cpp:167] The repositories JSON file for image 'test_image_volume' is '{""test_image_volume"":{""latest"":""815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346""}}'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.713726  2935 local_puller.cpp:295] Extracting layer tar ball '/tmp/R6OUei/store/staging/2GNlJO/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar to rootfs '/tmp/R6OUei/store/staging/2GNlJO/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.818696  2937 metadata_manager.cpp:155] Successfully cached image 'test_image_volume'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819169  2934 provisioner.cpp:286] Image layers: 1
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819188  2934 provisioner.cpp:296] Should hit here
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819221  2934 provisioner.cpp:315] !!!!: bind
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819232  2934 provisioner.cpp:315] !!!!: overlay
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819236  2934 provisioner.cpp:315] !!!!: copy
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819241  2934 provisioner.cpp:318] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/baf632b3-29c5-45e4-9d2e-6f3a2bdd9759' for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 using overlay backend
[07:02:46] :	 [Step 10/10] ../../src/tests/containerizer/volume_image_isolator_tests.cpp:214: Failure
[07:02:46] :	 [Step 10/10] (launch).failure(): Failed to create symlink '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/links' -> '/tmp/6dj9IG'
[07:02:46] :	 [Step 10/10] [  FAILED  ] Nesting/VolumeImageIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem/0, where GetParam() = false (919 ms)
{noformat}

We should differenciate the links for different provisioned images."	MESOS	Resolved	2	1	4582	backend, containerizer, overlayfs
13039668	Introduce a new http::Headers abstraction.	Introduce a new http::Headers abstraction to replace the previous hashmap 'Headers'. The benefit is that it can be embedded with other header classes (e.g., WWW-Authenticate) to parse a header content, as well as doing validation.	MESOS	Resolved	3	4	4582	http, libprocess
13024526	Duplicate image layer ids may make the backend failed to mount rootfs.	"Some images (e.g., 'mesosphere/inky') may contain duplicate layer ids in manifest, which may cause some backends unable to mount the rootfs (e.g., 'aufs' backend). We should make sure that each layer path returned in 'ImageInfo' is unique.

Here is an example manifest from 'mesosphere/inky':
{noformat}
[20:13:08]W:	 [Step 10/10]    ""name"": ""mesosphere/inky"",
[20:13:08]W:	 [Step 10/10]    ""tag"": ""latest"",
[20:13:08]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[20:13:08]W:	 [Step 10/10]    ""fsLayers"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""history"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""parent\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""created\"":\""2014-08-15T00:31:36.247988044Z\"",\""container\"":\""ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [inky]\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""parent\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""created\"":\""2014-08-15T00:31:36.068514721Z\"",\""container\"":\""696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER support@mesosphere.io\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""parent\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""created\"":\""2014-06-05T00:05:35.990887725Z\"",\""container\"":\""bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [/bin/sh]\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""parent\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""created\"":\""2014-06-05T00:05:35.692528634Z\"",\""container\"":\""fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 in /\""],\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":2433303}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""parent\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""created\"":\""2014-06-05T00:05:35.589531476Z\"",\""container\"":\""f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER Jrme Petazzoni \\u003cjerome@docker.com\\u003e\""],\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""comment\"":\""Imported from -\"",\""created\"":\""2013-06-13T14:03:50.821769-07:00\"",\""container_config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":null,\""Labels\"":null},\""docker_version\"":\""0.4.0\"",\""architecture\"":\""x86_64\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""schemaVersion"": 1,
[20:13:08]W:	 [Step 10/10]    ""signatures"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""header"": {
[20:13:08]W:	 [Step 10/10]             ""jwk"": {
[20:13:08]W:	 [Step 10/10]                ""crv"": ""P-256"",
[20:13:08]W:	 [Step 10/10]                ""kid"": ""4AYN:KH32:GJJD:I6BX:SJAZ:A3EC:P7IC:7O7C:22ZQ:3Z5O:75VQ:3QOT"",
[20:13:08]W:	 [Step 10/10]                ""kty"": ""EC"",
[20:13:08]W:	 [Step 10/10]                ""x"": ""o8bvrUwNpXKZdgoo2wQ7EHQzCVYhVuoOvjqGEXtRylU"",
[20:13:08]W:	 [Step 10/10]                ""y"": ""DCHyGr0Cbi-fZzqypQm16qKfefUMqCTk0rQME-q5GmA""
[20:13:08]W:	 [Step 10/10]             },
[20:13:08]W:	 [Step 10/10]             ""alg"": ""ES256""
[20:13:08]W:	 [Step 10/10]          },
[20:13:08]W:	 [Step 10/10]          ""signature"": ""f3fAob4XPT0pUW9TiPtxAE_zPAe0PdM2imxAeaCmJbBf6Lb-SuFPVGE4iqz1CO0VOijeYVuB1G1lv_a5Nnj5zg"",
[20:13:08]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNzA3LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTYtMDgtMDVUMjA6MTM6MDdaIn0""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ]
[20:13:08]W:	 [Step 10/10] }'
{noformat}

These two layer ids are totally identical:
{noformat}
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
{noformat}

It would make the backend (e.g., aufs) failed to mount the rootfs due to invalid arguments.
{noformat}
[20:13:08]W:	 [Step 10/10] E0805 20:13:08.614994 23432 slave.cpp:4029] Container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 failed to start: Failed to mount rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' with aufs: Invalid argument
{noformat}

We should make sure the vector of layer paths that is passed to the backend contains only unique layer path."	MESOS	Resolved	2	1	4582	aufs, backend, containerizer
12984166	Command executor health check does not work when the task specifies container image.	"Since we launch the task after pivot_root, we no longer has the access to the mesos-health-check binary. The solution is to refactor health check to be a library (libprocess) so that it does not depend on the underlying filesystem.

One note here is that we should strive to keep both the command executor and the task in the same mount namespace so that Mesos CLI tooling does not need to find the mount namespace for the task. It just need to find the corresponding pid for the executor. This statement is *arguable*, see the comment below."	MESOS	Resolved	3	1	4582	containerizer, health-check, mesosphere
12958046	The filesystem/linux isolator does not set the permissions of the host_path.	"The {{filesystem/linux}} isolator is not a drop in replacement for the {{filesystem/shared}} isolator. This should be considered before the latter is deprecated.

We are currently using the {{filesystem/shared}} isolator together with the following slave option. This provides us with a private {{/tmp}} and {{/var/tmp}} folder for each task.

{code}
    --default_container_info='{
            ""type"": ""MESOS"",
            ""volumes"": [
                {""host_path"": ""system/tmp"",     ""container_path"": ""/tmp"",        ""mode"": ""RW""},
                {""host_path"": ""system/vartmp"",  ""container_path"": ""/var/tmp"",    ""mode"": ""RW""}
            ]
        }'
{code}

When browsing the Mesos sandbox, one can see the following permissions:
{code}
mode	nlink	uid	gid	size	mtime		
drwxrwxrwx	3	root	root	4 KB	Apr 11 18:16	 tmp	
drwxrwxrwx	2	root	root	4 KB	Apr 11 18:15	 vartmp	
{code}

However, when running with the new {{filesystem/linux}} isolator, the permissions are different:
{code}
mode	nlink	uid	gid	size	mtime		
drwxr-xr-x	 2	root	root	4 KB	Apr 12 10:34	 tmp	
drwxr-xr-x	 2	root	root	4 KB	Apr 12 10:34	 vartmp
{code}

This prevents user code (running as a non-root user) from writing to those folders, i.e. every write attempt fails with permission denied. 

*Context*:
* We are using Apache Aurora. Aurora is running its custom executor as root but then switches to a non-privileged user before running the actual user code. 
* The follow code seems to have enabled our usecase in the existing {{filesystem/shared}} isolator: https://github.com/apache/mesos/blob/4d2b1b793e07a9c90b984ca330a3d7bc9e1404cc/src/slave/containerizer/mesos/isolators/filesystem/shared.cpp#L175-L198 "	MESOS	Resolved	3	1	4582	mesosphere, volumes
13056535	Support pulling images from AliCloud private registry.	"The image puller via curl doesn't work when I'm specifying the image name as:
registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75
400 BAD REQUEST

But the docker pulls it successfully 
bq. docker pull registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75"	MESOS	Resolved	1	1	4582	docker, fetcher, provisioner
13211147	CniIsolatorTest.ROOT_CleanupAfterReboot is flaky.	"{noformat}
Error Message
../../src/tests/containerizer/cni_isolator_tests.cpp:2685
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffc7c05aa0, @0x7fe637918430 136-byte object <80-24 29-45 E6-7F 00-00 00-00 00-00 00-00 00-00 3E-E8 00-00 00-00 00-00 00-B8 0E-20 F0-55 00-00 C0-03 07-18 E6-7F 00-00 20-17 05-18 E6-7F 00-00 10-50 05-18 E6-7F 00-00 50-D1 04-18 E6-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 F0-89 16-E9 58-2B D7-41 00-00 00-00 01-00 00-00 18-00 00-00 0B-00 00-00>)
         Expected: to be called 3 times
           Actual: called 4 times - over-saturated and active
Stacktrace
../../src/tests/containerizer/cni_isolator_tests.cpp:2685
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffc7c05aa0, @0x7fe637918430 136-byte object <80-24 29-45 E6-7F 00-00 00-00 00-00 00-00 00-00 3E-E8 00-00 00-00 00-00 00-B8 0E-20 F0-55 00-00 C0-03 07-18 E6-7F 00-00 20-17 05-18 E6-7F 00-00 10-50 05-18 E6-7F 00-00 50-D1 04-18 E6-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 F0-89 16-E9 58-2B D7-41 00-00 00-00 01-00 00-00 18-00 00-00 0B-00 00-00>)
         Expected: to be called 3 times
           Actual: called 4 times - over-saturated and active
{noformat}

It was from this commit https://github.com/apache/mesos/commit/c338f5ada0123c0558658c6452ac3402d9fbec29"	MESOS	Resolved	3	1	4582	cni, flaky-test
12905008	Support docker local store pull same image simultaneously 	The current local store implements get() using the local puller. For all requests of pulling same docker image at the same time, the local puller just untar the image tarball as many times as those requests are, and cp all of them to the same directory, which wastes time and bear high demand of computation. We should be able to support the local store/puller only do these for the first time, and the simultaneous pulling request should wait for the promised future and get it once the first pulling finishes. 	MESOS	Resolved	3	4	4582	mesosphere
12981161	Add CGROUP namespace to linux ns helper.	"Since linux kernel 4.6, CGROUP namespace is added. we need to support the handle for the cgroup namespace of the process.

This also relates to two test failures on Ubuntu 16:
{noformat}
[22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_setns
[22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:75: Failure
[22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'
[22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_setns (1 ms)
{noformat}

{noformat}
[22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_getns
[22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:160: Failure
[22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'
[22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_getns (0 ms)
{noformat}"	MESOS	Resolved	3	1	4582	cgroups, linux, mesosphere, namespace
12939421	Document docker runtime isolator.	"Should include the following information:

*What features are currently supported in docker runtime isolator.
*How to use the docker runtime isolator (user manual).
*Compare the different semantics v.s. docker containerizer, and explain why."	MESOS	Resolved	3	1	4582	containerizer, documentation
12995375	Aufs backend cannot support the image with numerous layers.	"This issue was exposed in this unit test `ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller` by manually specifying the `bind` backend. Most likely mounting the aufs with specific options is limited by string length.

{noformat}
[20:13:07] :	 [Step 10/10] [ RUN      ] DockerRuntimeIsolatorTest.ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.615844 23416 cluster.cpp:155] Creating default 'local' authorizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.624106 23416 leveldb.cpp:174] Opened db in 8.148813ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627252 23416 leveldb.cpp:181] Compacted db in 3.126629ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627275 23416 leveldb.cpp:196] Created db iterator in 4410ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627282 23416 leveldb.cpp:202] Seeked to beginning of db in 763ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627287 23416 leveldb.cpp:271] Iterated through 0 keys in the db in 491ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627301 23416 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627563 23434 recover.cpp:451] Starting replica recovery
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627800 23437 recover.cpp:477] Replica is in EMPTY status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628113 23431 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(5852)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628243 23430 recover.cpp:197] Received a recover response from a replica in EMPTY status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628365 23437 recover.cpp:568] Updating replica status to STARTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628744 23432 master.cpp:375] Master dd755a55-0dd1-4d2d-9a49-812a666015cb (ip-172-30-2-138.mesosphere.io) started on 172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628758 23432 master.cpp:377] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/OZHDIQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/OZHDIQ/master"" --zk_session_timeout=""10secs""
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628893 23432 master.cpp:427] Master only allowing authenticated frameworks to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628900 23432 master.cpp:441] Master only allowing authenticated agents to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628902 23432 master.cpp:454] Master only allowing authenticated HTTP frameworks to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628906 23432 credentials.hpp:37] Loading credentials for authentication from '/tmp/OZHDIQ/credentials'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628999 23432 master.cpp:499] Using default 'crammd5' authenticator
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629041 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629114 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629166 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629231 23432 master.cpp:579] Authorization enabled
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629290 23434 whitelist_watcher.cpp:77] No whitelist given
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629302 23430 hierarchical.cpp:151] Initialized hierarchical allocator process
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629921 23433 master.cpp:1851] Elected as the leading master!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629933 23433 master.cpp:1547] Recovering from registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629992 23436 registrar.cpp:332] Recovering registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630861 23435 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.358536ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630877 23435 replica.cpp:320] Persisted replica status to STARTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630924 23435 recover.cpp:477] Replica is in STARTING status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631178 23435 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(5853)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631285 23435 recover.cpp:197] Received a recover response from a replica in STARTING status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631433 23436 recover.cpp:568] Updating replica status to VOTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633391 23433 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.912156ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633409 23433 replica.cpp:320] Persisted replica status to VOTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633438 23433 recover.cpp:582] Successfully joined the Paxos group
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633479 23433 recover.cpp:466] Recover process terminated
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633635 23435 log.cpp:553] Attempting to start the writer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.634021 23432 replica.cpp:493] Replica received implicit promise request from __req_res__(5854)@172.30.2.138:44256 with proposal 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636034 23432 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.995908ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636049 23432 replica.cpp:342] Persisted promised to 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636239 23432 coordinator.cpp:238] Coordinator attempting to fill missing positions
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636672 23432 replica.cpp:388] Replica received explicit promise request from __req_res__(5855)@172.30.2.138:44256 for position 0 with proposal 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637307 23432 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 614745ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637318 23432 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637668 23432 replica.cpp:537] Replica received write request for position 0 from __req_res__(5856)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637692 23432 leveldb.cpp:436] Reading position from leveldb took 10680ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638314 23432 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 610038ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638325 23432 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638569 23436 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640446 23436 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.856131ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640461 23436 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640645 23437 log.cpp:569] Writer started with ending position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640940 23430 leveldb.cpp:436] Reading position from leveldb took 11341ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641152 23430 registrar.cpp:365] Successfully fetched the registry (0B) in 11.14496ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641185 23430 registrar.cpp:464] Applied 1 operations in 5010ns; attempting to update the registry
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641381 23434 log.cpp:577] Attempting to append 209 bytes to the log
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641425 23430 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641706 23434 replica.cpp:537] Replica received write request for position 1 from __req_res__(5857)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642320 23434 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 596016ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642333 23434 replica.cpp:708] Persisted action APPEND at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642608 23435 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644492 23435 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.868216ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644507 23435 replica.cpp:708] Persisted action APPEND at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644716 23432 registrar.cpp:509] Successfully updated the registry in 3.512064ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644759 23432 registrar.cpp:395] Successfully recovered registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644811 23431 log.cpp:596] Attempting to truncate the log to 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644879 23433 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644949 23430 master.cpp:1655] Recovered 0 agents from the registry (170B); allowing 10mins for agents to re-register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644959 23437 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645247 23431 replica.cpp:537] Replica received write request for position 2 from __req_res__(5858)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645884 23431 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 618643ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645896 23431 replica.cpp:708] Persisted action TRUNCATE at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.646080 23437 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648093 23437 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.995217ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648118 23437 leveldb.cpp:399] Deleting ~1 keys from leveldb took 10026ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648125 23437 replica.cpp:708] Persisted action TRUNCATE at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.649564 23416 containerizer.cpp:200] Using isolation: docker/runtime,filesystem/linux,network/cni
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.652878 23416 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[20:13:07]W:	 [Step 10/10] E0805 20:13:07.656265 23416 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[20:13:07]W:	 [Step 10/10] sh: 1: hadoop: not found
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.656286 23416 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.656338 23416 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.657330 23416 linux.cpp:148] Bind mounting '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn' and making it a shared mount
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663147 23416 cluster.cpp:434] Creating default 'local' authorizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663566 23436 slave.cpp:198] Mesos agent started on (506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663583 23436 slave.cpp:199] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/OZHDIQ/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/http_credentials"" --image_providers=""docker"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn""
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663796 23436 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/credential'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663868 23436 slave.cpp:336] Agent using credential for: test-principal
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663882 23436 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/http_credentials'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663969 23436 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664010 23436 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664225 23416 sched.cpp:226] Version: 1.1.0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664423 23435 sched.cpp:330] New master detected at master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664451 23435 sched.cpp:396] Authenticating with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664428 23436 slave.cpp:519] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664458 23435 sched.cpp:403] Using default CRAM-MD5 authenticatee
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664463 23436 slave.cpp:527] Agent attributes: [  ]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664470 23436 slave.cpp:532] Agent hostname: ip-172-30-2-138.mesosphere.io
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664588 23437 authenticatee.cpp:121] Creating new client SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664810 23437 master.cpp:5900] Authenticating scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664873 23437 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1028)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664939 23432 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/meta'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665006 23431 authenticator.cpp:98] Creating new server SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665024 23435 status_update_manager.cpp:203] Recovering status update manager
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665174 23434 containerizer.cpp:527] Recovering containerizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665201 23431 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665221 23431 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665266 23431 authenticator.cpp:204] Received SASL authentication start
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665303 23431 authenticator.cpp:326] Authentication requires more steps
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665347 23431 authenticatee.cpp:259] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665436 23431 authenticator.cpp:232] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665457 23431 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665465 23431 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665482 23431 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665494 23431 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665503 23431 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665510 23431 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665524 23431 authenticator.cpp:318] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665575 23436 authenticatee.cpp:299] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665596 23435 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1028)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665624 23431 master.cpp:5930] Successfully authenticated principal 'test-principal' at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665705 23436 sched.cpp:502] Successfully authenticated with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665715 23436 sched.cpp:820] Sending SUBSCRIBE call to master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665751 23436 sched.cpp:853] Will retry registration in 188.601026ms if necessary
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665796 23437 master.cpp:2425] Received SUBSCRIBE call for framework 'default' at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665817 23437 master.cpp:1887] Authorizing framework principal 'test-principal' to receive offers for role '*'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665998 23430 master.cpp:2501] Subscribing framework default with checkpointing disabled and capabilities [  ]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666132 23432 hierarchical.cpp:271] Added framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666148 23434 sched.cpp:743] Framework registered with dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666154 23432 hierarchical.cpp:1548] No allocations performed
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666173 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666177 23434 sched.cpp:757] Scheduler::registered took 11084ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666189 23432 hierarchical.cpp:1192] Performed allocation for 0 agents in 43102ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666486 23431 metadata_manager.cpp:205] No images to load from disk. Docker provisioner image storage path '/tmp/OZHDIQ/store/storedImages' does not exist
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666558 23436 provisioner.cpp:255] Provisioner recovery complete
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666677 23435 slave.cpp:4872] Finished recovery
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666831 23435 slave.cpp:5044] Querying resource estimator for oversubscribable resources
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666919 23435 slave.cpp:895] New master detected at master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666929 23435 slave.cpp:954] Authenticating with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666931 23436 status_update_manager.cpp:177] Pausing sending status updates
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666944 23435 slave.cpp:965] Using default CRAM-MD5 authenticatee
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666982 23435 slave.cpp:927] Detecting new master
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667006 23431 authenticatee.cpp:121] Creating new client SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667014 23435 slave.cpp:5058] Received oversubscribable resources  from the resource estimator
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667162 23431 master.cpp:5900] Authenticating slave(506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667225 23434 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1029)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667275 23434 authenticator.cpp:98] Creating new server SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667418 23434 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667436 23434 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667492 23436 authenticator.cpp:204] Received SASL authentication start
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667515 23436 authenticator.cpp:326] Authentication requires more steps
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667546 23436 authenticatee.cpp:259] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667592 23436 authenticator.cpp:232] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667603 23436 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667610 23436 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667619 23436 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667630 23436 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667639 23436 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667642 23436 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667652 23436 authenticator.cpp:318] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667688 23436 authenticatee.cpp:299] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667713 23432 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1029)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667733 23434 master.cpp:5930] Successfully authenticated principal 'test-principal' at slave(506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667783 23437 slave.cpp:1049] Successfully authenticated with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667836 23437 slave.cpp:1455] Will retry registration in 4.197236ms if necessary
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667901 23436 master.cpp:4554] Registering agent at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with id dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668021 23430 registrar.cpp:464] Applied 1 operations in 13306ns; attempting to update the registry
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668269 23433 log.cpp:577] Attempting to append 395 bytes to the log
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668329 23434 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668622 23433 replica.cpp:537] Replica received write request for position 3 from __req_res__(5859)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669297 23433 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 658552ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669309 23433 replica.cpp:708] Persisted action APPEND at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669589 23432 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672566 23432 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 2.962622ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672580 23432 replica.cpp:708] Persisted action APPEND at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672866 23435 registrar.cpp:509] Successfully updated the registry in 4.822784ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672936 23434 log.cpp:596] Attempting to truncate the log to 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673001 23437 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673110 23436 master.cpp:4623] Registered agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673152 23432 hierarchical.cpp:478] Added agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673174 23430 slave.cpp:3739] Received ping from slave-observer(465)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673254 23430 slave.cpp:1095] Registered with master master@172.30.2.138:44256; given agent ID dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673266 23430 fetcher.cpp:86] Clearing fetcher cache
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673288 23433 replica.cpp:537] Replica received write request for position 4 from __req_res__(5860)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673317 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673333 23432 hierarchical.cpp:1215] Performed allocation for agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 in 160981ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673358 23432 status_update_manager.cpp:184] Resuming sending status updates
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673435 23437 master.cpp:5729] Sending 1 offers to framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673467 23430 slave.cpp:1118] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/meta/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/slave.info'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673566 23437 sched.cpp:917] Scheduler::resourceOffers took 40919ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673607 23430 slave.cpp:1155] Forwarding total oversubscribed resources 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673710 23437 master.cpp:5006] Received update of agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with total oversubscribed resources 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673781 23437 hierarchical.cpp:542] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673823 23437 hierarchical.cpp:1548] No allocations performed
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673830 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673838 23437 hierarchical.cpp:1215] Performed allocation for agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 in 31940ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674163 23435 master.cpp:3346] Processing ACCEPT call for offers: [ dd755a55-0dd1-4d2d-9a49-812a666015cb-O0 ] on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674186 23435 master.cpp:2981] Authorizing framework principal 'test-principal' to launch task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674538 23437 master.cpp:7451] Adding task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 with resources cpus(*):1; mem(*):128 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io)
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674564 23437 master.cpp:3835] Launching task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 with resources cpus(*):1; mem(*):128 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674665 23430 slave.cpp:1495] Got assigned task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674713 23436 hierarchical.cpp:924] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674736 23436 hierarchical.cpp:961] Framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 filtered agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 for 5secs
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674866 23430 slave.cpp:1614] Launching task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.675107 23430 paths.cpp:536] Trying to chown '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' to user 'root'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678246 23433 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 4.916164ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678267 23433 replica.cpp:708] Persisted action TRUNCATE at position 4
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678629 23436 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679050 23430 slave.cpp:5764] Launching executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679200 23430 slave.cpp:1840] Queuing task 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679219 23437 containerizer.cpp:786] Starting container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679234 23430 slave.cpp:848] Successfully attached file '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679435 23430 metadata_manager.cpp:167] Looking for image 'mesosphere/inky'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679572 23430 registry_puller.cpp:236] Pulling image 'mesosphere/inky' from 'docker-manifest://registry-1.docker.io:443mesosphere/inky?latest#https' to '/tmp/OZHDIQ/store/staging/HbsybX'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.680943 23436 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.14361ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.681073 23436 leveldb.cpp:399] Deleting ~2 keys from leveldb took 60273ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.681112 23436 replica.cpp:708] Persisted action TRUNCATE at position 4
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104004 23431 registry_puller.cpp:259] The manifest for image 'mesosphere/inky' is '{
[20:13:08]W:	 [Step 10/10]    ""name"": ""mesosphere/inky"",
[20:13:08]W:	 [Step 10/10]    ""tag"": ""latest"",
[20:13:08]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[20:13:08]W:	 [Step 10/10]    ""fsLayers"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""history"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""parent\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""created\"":\""2014-08-15T00:31:36.247988044Z\"",\""container\"":\""ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [inky]\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""parent\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""created\"":\""2014-08-15T00:31:36.068514721Z\"",\""container\"":\""696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER support@mesosphere.io\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""parent\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""created\"":\""2014-06-05T00:05:35.990887725Z\"",\""container\"":\""bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [/bin/sh]\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""parent\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""created\"":\""2014-06-05T00:05:35.692528634Z\"",\""container\"":\""fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 in /\""],\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":2433303}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""parent\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""created\"":\""2014-06-05T00:05:35.589531476Z\"",\""container\"":\""f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER Jrme Petazzoni \\u003cjerome@docker.com\\u003e\""],\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""comment\"":\""Imported from -\"",\""created\"":\""2013-06-13T14:03:50.821769-07:00\"",\""container_config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":null,\""Labels\"":null},\""docker_version\"":\""0.4.0\"",\""architecture\"":\""x86_64\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""schemaVersion"": 1,
[20:13:08]W:	 [Step 10/10]    ""signatures"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""header"": {
[20:13:08]W:	 [Step 10/10]             ""jwk"": {
[20:13:08]W:	 [Step 10/10]                ""crv"": ""P-256"",
[20:13:08]W:	 [Step 10/10]                ""kid"": ""4AYN:KH32:GJJD:I6BX:SJAZ:A3EC:P7IC:7O7C:22ZQ:3Z5O:75VQ:3QOT"",
[20:13:08]W:	 [Step 10/10]                ""kty"": ""EC"",
[20:13:08]W:	 [Step 10/10]                ""x"": ""o8bvrUwNpXKZdgoo2wQ7EHQzCVYhVuoOvjqGEXtRylU"",
[20:13:08]W:	 [Step 10/10]                ""y"": ""DCHyGr0Cbi-fZzqypQm16qKfefUMqCTk0rQME-q5GmA""
[20:13:08]W:	 [Step 10/10]             },
[20:13:08]W:	 [Step 10/10]             ""alg"": ""ES256""
[20:13:08]W:	 [Step 10/10]          },
[20:13:08]W:	 [Step 10/10]          ""signature"": ""f3fAob4XPT0pUW9TiPtxAE_zPAe0PdM2imxAeaCmJbBf6Lb-SuFPVGE4iqz1CO0VOijeYVuB1G1lv_a5Nnj5zg"",
[20:13:08]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNzA3LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTYtMDgtMDVUMjA6MTM6MDdaIn0""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ]
[20:13:08]W:	 [Step 10/10] }'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104116 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104130 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104138 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104146 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104151 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104158 23431 registry_puller.cpp:369] Fetching blob 'sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66' for layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104164 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104171 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.504564 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.507129 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.508962 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.510915 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.512848 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.515400 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.517390 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.519486 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.606955 23434 metadata_manager.cpp:155] Successfully cached image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.607501 23436 provisioner.cpp:312] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' for container f2c1fd6d-4d11-45cd-a916-e4d73d226451 using the 'aufs' backend
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.607787 23434 aufs.cpp:152] Provisioning image rootfs with aufs: 'dirs=/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/scratch/427b7851-bf82-4553-80f3-da2d42cede77/workdir:/tmp/OZHDIQ/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs:/tmp/OZHDIQ/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs:/tmp/OZHDIQ/store/layers/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs:/tmp/OZHDIQ/store/layers/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs:/tmp/OZHDIQ/store/layers/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs:/tmp/OZHDIQ/store/layers/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs:/tmp/OZHDIQ/store/layers/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs:/tmp/OZHDIQ/store/layers/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs'
[20:13:08]W:	 [Step 10/10] E0805 20:13:08.614994 23432 slave.cpp:4029] Container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 failed to start: Failed to mount rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' with aufs: Invalid argument
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615058 23436 containerizer.cpp:1637] Destroying container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615072 23436 containerizer.cpp:1640] Waiting for the provisioner to complete for container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615279 23435 provisioner.cpp:455] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' for container f2c1fd6d-4d11-45cd-a916-e4d73d226451
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616097 23430 slave.cpp:4135] Executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 has terminated with unknown status
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616173 23430 slave.cpp:3264] Handling status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 from @0.0.0.0:0
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616320 23435 slave.cpp:6104] Terminating task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:08]W:	 [Step 10/10] W0805 20:13:08.616402 23432 containerizer.cpp:1466] Ignoring update for unknown container: f2c1fd6d-4d11-45cd-a916-e4d73d226451
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616528 23433 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616545 23433 status_update_manager.cpp:500] Creating StatusUpdate stream for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616750 23433 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 to the agent
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616827 23431 slave.cpp:3657] Forwarding the update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 to master@172.30.2.138:44256
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616936 23431 slave.cpp:3551] Status update manager successfully handled status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617010 23433 master.cpp:5141] Status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 from agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617032 23433 master.cpp:5203] Forwarding status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617079 23433 master.cpp:6845] Updating the state of task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617187 23435 sched.cpp:1025] Scheduler::statusUpdate took 57204ns
[20:13:08] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:309: Failure
[20:13:08] :	 [Step 10/10] Value of: statusRunning->state()
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617234 23436 hierarchical.cpp:924] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08] :	 [Step 10/10]   Actual: TASK_FAILED
[20:13:08] :	 [Step 10/10] Expected: TASK_RUNNING
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617281 23432 master.cpp:4266] Processing ACKNOWLEDGE call 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617311 23432 master.cpp:6911] Removing task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 with resources cpus(*):1; mem(*):128 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617450 23430 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617480 23430 status_update_manager.cpp:531] Cleaning up status update stream for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617545 23430 slave.cpp:2650] Status update manager successfully handled status update acknowledgement (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617561 23430 slave.cpp:6145] Completing task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617575 23430 slave.cpp:4246] Cleaning up executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617660 23435 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' for gc 6.99999285160889days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617688 23430 slave.cpp:4334] Cleaning up framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617708 23435 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12' for gc 6.9999928509363days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617748 23434 status_update_manager.cpp:285] Closing status update streams for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617772 23434 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000' for gc 6.99999285021926days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630481 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630504 23432 hierarchical.cpp:1192] Performed allocation for 1 agents in 155186ns
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630609 23430 master.cpp:5729] Sending 1 offers to framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630728 23430 sched.cpp:917] Scheduler::resourceOffers took 13371ns
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631413 23437 hierarchical.cpp:1548] No allocations performed
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631450 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631465 23437 hierarchical.cpp:1192] Performed allocation for 1 agents in 202676ns
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631609 23435 hierarchical.cpp:1548] No allocations performed
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631640 23435 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631655 23435 hierarchical.cpp:1192] Performed allocation for 1 agents in 102058ns
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632261 23431 hierarchical.cpp:1548] No allocations performed
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632294 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632308 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 112653ns
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632477 23433 hierarchical.cpp:1548] No allocations performed
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632510 23433 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632525 23433 hierarchical.cpp:1192] Performed allocation for 1 agents in 144467ns
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633517 23430 hierarchical.cpp:1548] No allocations performed
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633549 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633563 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 111395ns
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.633985 23436 hierarchical.cpp:1548] No allocations performed
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.634018 23436 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.634048 23436 hierarchical.cpp:1192] Performed allocation for 1 agents in 132707ns
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634266 23430 hierarchical.cpp:1548] No allocations performed
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634299 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634313 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 103933ns
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635295 23431 hierarchical.cpp:1548] No allocations performed
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635330 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635346 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 115517ns
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635922 23436 hierarchical.cpp:1548] No allocations performed
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635958 23436 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635973 23436 hierarchical.cpp:1192] Performed allocation for 1 agents in 109700ns
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636693 23437 hierarchical.cpp:1548] No allocations performed
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636728 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636744 23437 hierarchical.cpp:1192] Performed allocation for 1 agents in 123133ns
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637589 23432 hierarchical.cpp:1548] No allocations performed
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637624 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637639 23432 hierarchical.cpp:1192] Performed allocation for 1 agents in 118581ns
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638517 23431 hierarchical.cpp:1548] No allocations performed
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638550 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638566 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 107979ns
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639577 23435 hierarchical.cpp:1548] No allocations performed
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639612 23435 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639628 23435 hierarchical.cpp:1192] Performed allocation for 1 agents in 126299ns
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640533 23430 hierarchical.cpp:1548] No allocations performed
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640566 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640581 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 106384ns
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.667985 23437 slave.cpp:5044] Querying resource estimator for oversubscribable resources
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.668124 23434 slave.cpp:5058] Received oversubscribable resources  from the resource estimator
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.674278 23433 slave.cpp:3739] Received ping from slave-observer(465)@172.30.2.138:44256
[20:13:23] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:311: Failure
[20:13:23] :	 [Step 10/10] Failed to wait 15secs for statusFinished
[20:13:23] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:301: Failure
[20:13:23] :	 [Step 10/10] Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
[20:13:23] :	 [Step 10/10]          Expected: to be called twice
[20:13:23] :	 [Step 10/10]            Actual: called once - unsatisfied and active
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618680 23433 master.cpp:1284] Framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 disconnected
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618721 23433 master.cpp:2726] Disconnecting framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618737 23433 master.cpp:2750] Deactivating framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618883 23434 hierarchical.cpp:382] Deactivated framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] W0805 20:13:23.618918 23433 master.hpp:2131] Master attempted to send message to disconnected framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618963 23433 master.cpp:1297] Giving framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 0ns to failover
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619046 23434 hierarchical.cpp:924] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619258 23416 slave.cpp:767] Agent terminating
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619321 23432 master.cpp:1245] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) disconnected
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619336 23432 master.cpp:2785] Disconnecting agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619371 23432 master.cpp:2804] Deactivating agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619431 23432 hierarchical.cpp:571] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 deactivated
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620216 23435 master.cpp:5581] Framework failover timeout, removing framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620232 23435 master.cpp:6316] Removing framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620357 23433 hierarchical.cpp:333] Removed framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.621464 23416 master.cpp:1092] Master terminating
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.621561 23433 hierarchical.cpp:510] Removed agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:23] :	 [Step 10/10] [  FAILED  ] DockerRuntimeIsolatorTest.ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller (16012 ms)
{noformat}"	MESOS	Resolved	2	1	4582	aufs, backend, containerizer
13078820	Docker image with universal containerizer does not work if WORKDIR is missing in the rootfs.	"hello,
used the following docker image recently

quay.io/spinnaker/front50:master
https://quay.io/repository/spinnaker/front50

Here the link to the Dockerfile
https://github.com/spinnaker/front50/blob/master/Dockerfile

and here the source
{color:blue}FROM java:8

MAINTAINER delivery-engineering@netflix.com

COPY . workdir/

WORKDIR workdir

RUN GRADLE_USER_HOME=cache ./gradlew buildDeb -x test && \
  dpkg -i ./front50-web/build/distributions/*.deb && \
  cd .. && \
  rm -rf workdir

CMD [""/opt/front50/bin/front50""]{color}


The image works fine with the docker containerizer, but the universal containerizer shows the following in stderr.

""Failed to chdir into current working directory '/workdir': No such file or directory""

The problem comes from the fact that the Dockerfile creates a workdir but then later removes the created dir as part of a RUN. The docker containerizer has no problem with it if you do

docker run -ti --rm quay.io/spinnaker/front50:master bash

you get into the working dir, but the universal containerizer fails with the error.

thanks for your help,
Michael"	MESOS	Resolved	2	1	4582	mesosphere
12917658	Enable `Env` specified in docker image can be returned from docker pull	Currently docker pull only return an image structure, which only contains entrypoint info. We have docker inspect as a subprocess inside docker pull, which contains many other useful information of a docker image. We should be able to support returning environment variables information from the image.	MESOS	Resolved	3	4	4582	mesosphere
12993604	Support auto backend in Unified Containerizer.	"Currently in Unified Containerizer, copy backend will be selected by default. This is not ideal, especially for production environment. It would take a long time to prepare an huge container image to copy it from the store to provisioner.

Ideally, we should support `auto backend`, which would automatically/intelligently select the best/optimal backend for image provisioner if user does not specify one from the agent flag.

We should have a logic design first in this ticket, to determine how we want to choose the right backend (e.g., overlayfs or aufs should be preferred if available from the kernel)."	MESOS	Resolved	1	4	4582	backend, containerizer, mesosphere
12939422	Create base docker image for test suite.	"This should be widely used for unified containerizer testing. Should basically include:

*at least one layer.
*repositories.

For each layer:
*root file system as a layer tar ball.
*docker image json (manifest).
*docker version."	MESOS	Resolved	3	1	4582	containerizer
13045006	Document provisioner auto backend support.	Document the provisioner auto backend semantic in container-image.md	MESOS	Resolved	3	20	4582	document, provisioner
12954666	Support docker private registry default docker config.	For docker private registry with authentication, docker containerizer should support using a default .docker/config.json file (or the old .dockercfg file) locally, which is pre-handled by operators. The default docker config file should be exposed by a new agent flag `--docker_config`. 	MESOS	Resolved	3	3	4582	mesosphere
13016262	Use 'geteuid()' for the root privileges check.	"Currently, parts of code in Mesos check the root privileges using os::user() to compare to ""root"", which is not sufficient, since it compares the real user. When people change the mesos binary by 'setuid root', the process may not have the right permission to execute.

We should check the effective user id instead in our code. "	MESOS	Resolved	3	1	4582	backend, isolator, mesosphere, user
12945924	Bind docker runtime isolator with docker image provider.	If image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. A check should be added to make sure docker runtime isolator is on if using docker as image provider.	MESOS	Resolved	3	1	4582	containerizer, mesosphere
12981159	CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask fails on CentOS 7.	"{noformat}
[22:41:54] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.348641 30896 cluster.cpp:155] Creating default 'local' authorizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.353384 30896 leveldb.cpp:174] Opened db in 4.634552ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354763 30896 leveldb.cpp:181] Compacted db in 1.360201ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354784 30896 leveldb.cpp:196] Created db iterator in 3421ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354790 30896 leveldb.cpp:202] Seeked to beginning of db in 633ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354797 30896 leveldb.cpp:271] Iterated through 0 keys in the db in 401ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354811 30896 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354990 30913 recover.cpp:451] Starting replica recovery
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355123 30915 recover.cpp:477] Replica is in EMPTY status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355391 30915 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18695)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355479 30912 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355581 30914 recover.cpp:568] Updating replica status to STARTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356091 30910 master.cpp:382] Master 27c796db-6f98-4d61-96c0-f583f22787ff (ip-172-30-2-105.mesosphere.io) started on 172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356104 30910 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/KhgYrQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/KhgYrQ/master"" --zk_session_timeout=""10secs""
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356237 30910 master.cpp:434] Master only allowing authenticated frameworks to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356245 30910 master.cpp:448] Master only allowing authenticated agents to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356247 30910 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356251 30910 credentials.hpp:37] Loading credentials for authentication from '/tmp/KhgYrQ/credentials'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356351 30910 master.cpp:506] Using default 'crammd5' authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356389 30910 master.cpp:578] Using default 'basic' HTTP authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356439 30910 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356467 30910 master.cpp:705] Authorization enabled
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356531 30913 whitelist_watcher.cpp:77] No whitelist given
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356549 30912 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356868 30916 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.232816ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356884 30916 replica.cpp:320] Persisted replica status to STARTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356945 30916 recover.cpp:477] Replica is in STARTING status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357100 30917 master.cpp:1969] The newly elected leader is master@172.30.2.105:40724 with id 27c796db-6f98-4d61-96c0-f583f22787ff
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357115 30917 master.cpp:1982] Elected as the leading master!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357122 30917 master.cpp:1669] Recovering from registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357213 30910 registrar.cpp:332] Recovering registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357429 30913 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18698)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357549 30914 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357728 30913 recover.cpp:568] Updating replica status to VOTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358937 30913 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.14792ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358952 30913 replica.cpp:320] Persisted replica status to VOTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358986 30913 recover.cpp:582] Successfully joined the Paxos group
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359041 30913 recover.cpp:466] Recover process terminated
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359180 30916 log.cpp:553] Attempting to start the writer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359578 30917 replica.cpp:493] Replica received implicit promise request from (18699)@172.30.2.105:40724 with proposal 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360752 30917 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.157449ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360767 30917 replica.cpp:342] Persisted promised to 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360982 30914 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.361426 30910 replica.cpp:388] Replica received explicit promise request from (18700)@172.30.2.105:40724 for position 0 with proposal 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362571 30910 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.124969ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362587 30910 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362999 30911 replica.cpp:537] Replica received write request for position 0 from (18701)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.363030 30911 leveldb.cpp:436] Reading position from leveldb took 14967ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364264 30911 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.214497ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364279 30911 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364470 30910 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365622 30910 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.131398ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365636 30910 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365643 30910 replica.cpp:697] Replica learned NOP action at position 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365769 30915 log.cpp:569] Writer started with ending position 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366080 30913 leveldb.cpp:436] Reading position from leveldb took 8794ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366284 30915 registrar.cpp:365] Successfully fetched the registry (0B) in 9.053952ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366315 30915 registrar.cpp:464] Applied 1 operations in 3436ns; attempting to update the 'registry'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366487 30911 log.cpp:577] Attempting to append 209 bytes to the log
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366539 30917 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366839 30917 replica.cpp:537] Replica received write request for position 1 from (18702)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.367966 30917 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.106053ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.367982 30917 replica.cpp:712] Persisted action at 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.368201 30915 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371786 30915 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 3.566076ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371803 30915 replica.cpp:712] Persisted action at 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371809 30915 replica.cpp:697] Replica learned APPEND action at position 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372032 30910 registrar.cpp:509] Successfully updated the 'registry' in 5.693952ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372097 30910 registrar.cpp:395] Successfully recovered registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372107 30911 log.cpp:596] Attempting to truncate the log to 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372151 30910 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372218 30911 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372242 30915 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372467 30914 replica.cpp:537] Replica received write request for position 2 from (18703)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373693 30914 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.207676ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373708 30914 replica.cpp:712] Persisted action at 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373920 30913 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375115 30913 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.17978ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375145 30913 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14216ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375154 30913 replica.cpp:712] Persisted action at 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375159 30913 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.383839 30896 containerizer.cpp:201] Using isolation: docker/runtime,filesystem/linux,network/cni
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.388789 30896 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:41:54]W:	 [Step 10/10] E0619 22:41:54.393234 30896 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[22:41:54]W:	 [Step 10/10] sh: hadoop: command not found
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.393265 30896 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.393316 30896 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.395668 30896 cluster.cpp:432] Creating default 'local' authorizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396100 30914 slave.cpp:203] Agent started on 469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396116 30914 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/KhgYrQ/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux,network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/KhgYrQ/configs"" --network_cni_plugins_dir=""/tmp/KhgYrQ/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI""
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396380 30914 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396495 30914 slave.cpp:341] Agent using credential for: test-principal
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396509 30914 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396586 30914 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396698 30914 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396780 30896 sched.cpp:224] Version: 1.0.0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396991 30914 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397020 30914 slave.cpp:600] Agent attributes: [  ]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397029 30914 slave.cpp:605] Agent hostname: ip-172-30-2-105.mesosphere.io
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397040 30916 sched.cpp:328] New master detected at master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397068 30916 sched.cpp:394] Authenticating with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397078 30916 sched.cpp:401] Using default CRAM-MD5 authenticatee
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397188 30916 authenticatee.cpp:121] Creating new client SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397467 30914 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/meta'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397476 30912 master.cpp:5943] Authenticating scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397544 30913 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(953)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397614 30915 status_update_manager.cpp:200] Recovering status update manager
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397668 30912 authenticator.cpp:98] Creating new server SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397709 30915 containerizer.cpp:514] Recovering containerizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397869 30912 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397886 30912 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397927 30912 authenticator.cpp:204] Received SASL authentication start
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397964 30912 authenticator.cpp:326] Authentication requires more steps
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398000 30912 authenticatee.cpp:259] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398052 30912 authenticator.cpp:232] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398066 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398073 30912 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398087 30912 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398098 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398103 30912 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398108 30912 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398116 30912 authenticator.cpp:318] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398162 30914 authenticatee.cpp:299] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398181 30913 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(953)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398200 30912 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398270 30914 sched.cpp:484] Successfully authenticated with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398280 30914 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398342 30914 sched.cpp:833] Will retry registration in 869.123866ms if necessary
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398381 30916 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398398 30916 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398483 30916 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398679 30916 sched.cpp:723] Framework registered with 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398701 30916 sched.cpp:737] Scheduler::registered took 10291ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398784 30910 hierarchical.cpp:264] Added framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398802 30910 hierarchical.cpp:1488] No allocations performed
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398808 30910 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398818 30910 hierarchical.cpp:1139] Performed allocation for 0 agents in 22451ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399222 30916 metadata_manager.cpp:205] No images to load from disk. Docker provisioner image storage path '/tmp/KhgYrQ/store/storedImages' does not exist
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399318 30910 provisioner.cpp:253] Provisioner recovery complete
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399453 30913 slave.cpp:4845] Finished recovery
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399690 30913 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399796 30911 slave.cpp:967] New master detected at master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399811 30911 slave.cpp:1029] Authenticating with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399801 30914 status_update_manager.cpp:174] Pausing sending status updates
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399821 30911 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399855 30911 slave.cpp:1002] Detecting new master
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399879 30915 authenticatee.cpp:121] Creating new client SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399910 30911 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400044 30915 master.cpp:5943] Authenticating slave(469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400099 30910 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(954)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400151 30910 authenticator.cpp:98] Creating new server SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400316 30910 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400329 30910 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400367 30910 authenticator.cpp:204] Received SASL authentication start
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400398 30910 authenticator.cpp:326] Authentication requires more steps
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400431 30910 authenticatee.cpp:259] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400516 30917 authenticator.cpp:232] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400530 30917 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400537 30917 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400544 30917 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400550 30917 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400554 30917 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400558 30917 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400566 30917 authenticator.cpp:318] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400609 30914 authenticatee.cpp:299] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400640 30912 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(954)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400682 30917 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400738 30911 slave.cpp:1108] Successfully authenticated with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400790 30911 slave.cpp:1511] Will retry registration in 13.364855ms if necessary
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400848 30913 master.cpp:4653] Registering agent at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with id 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400950 30914 registrar.cpp:464] Applied 1 operations in 16921ns; attempting to update the 'registry'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401154 30915 log.cpp:577] Attempting to append 395 bytes to the log
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401213 30914 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401515 30914 replica.cpp:537] Replica received write request for position 3 from (18725)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.402851 30914 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 1.317458ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.402866 30914 replica.cpp:712] Persisted action at 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.403101 30917 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404217 30917 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 1.100393ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404233 30917 replica.cpp:712] Persisted action at 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404239 30917 replica.cpp:697] Replica learned APPEND action at position 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404495 30915 registrar.cpp:509] Successfully updated the 'registry' in 3.521792ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404561 30913 log.cpp:596] Attempting to truncate the log to 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404621 30915 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404690 30910 master.cpp:4721] Registered agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404726 30915 slave.cpp:3747] Received ping from slave-observer(429)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404747 30916 hierarchical.cpp:473] Added agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404825 30915 slave.cpp:1152] Registered with master master@172.30.2.105:40724; given agent ID 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404840 30915 fetcher.cpp:86] Clearing fetcher cache
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404880 30910 replica.cpp:537] Replica received write request for position 4 from (18726)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404911 30916 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404932 30913 status_update_manager.cpp:181] Resuming sending status updates
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404942 30916 hierarchical.cpp:1162] Performed allocation for agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 in 168147ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405025 30911 master.cpp:5772] Sending 1 offers to framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405082 30915 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/meta/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/slave.info'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405177 30911 sched.cpp:897] Scheduler::resourceOffers took 55063ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405239 30915 slave.cpp:1212] Forwarding total oversubscribed resources 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405299 30911 master.cpp:5066] Received update of agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with total oversubscribed resources 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405318 30896 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405387 30911 hierarchical.cpp:531] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405421 30911 hierarchical.cpp:1488] No allocations performed
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405431 30911 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405447 30911 hierarchical.cpp:1162] Performed allocation for agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 in 40224ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405643 30914 master.cpp:3457] Processing ACCEPT call for offers: [ 27c796db-6f98-4d61-96c0-f583f22787ff-O0 ] on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405668 30914 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406030 30912 master.hpp:177] Adding task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 with resources cpus(*):1; mem(*):128 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io)
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406056 30912 master.cpp:3946] Launching task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 with resources cpus(*):1; mem(*):128 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406158 30916 slave.cpp:1551] Got assigned task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406193 30912 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 from framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406214 30912 hierarchical.cpp:928] Framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 filtered agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 for 5secs
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406250 30916 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406347 30910 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.44747ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406359 30910 replica.cpp:712] Persisted action at 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406381 30916 slave.cpp:1670] Launching task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406420 30916 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406555 30914 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406793 30916 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' to user 'root'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408360 30914 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.635458ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408453 30914 leveldb.cpp:399] Deleting ~2 keys from leveldb took 53370ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408469 30914 replica.cpp:712] Persisted action at 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408480 30914 replica.cpp:697] Replica learned TRUNCATE action at position 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411355 30916 slave.cpp:5734] Launching executor d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411485 30916 slave.cpp:1896] Queuing task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411516 30915 containerizer.cpp:773] Starting container '548370b5-05f2-4e33-8f6f-015aa3fd1af4' for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411521 30916 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411733 30914 metadata_manager.cpp:167] Looking for image 'alpine'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.412009 30911 registry_puller.cpp:235] Pulling image 'library/alpine' from 'docker-manifest://registry-1.docker.io:443library/alpine?latest#https' to '/tmp/KhgYrQ/store/staging/0cVlJm'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.870712 30915 registry_puller.cpp:258] The manifest for image 'library/alpine' is '{
[22:41:54]W:	 [Step 10/10]    ""schemaVersion"": 1,
[22:41:54]W:	 [Step 10/10]    ""name"": ""library/alpine"",
[22:41:54]W:	 [Step 10/10]    ""tag"": ""latest"",
[22:41:54]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[22:41:54]W:	 [Step 10/10]    ""fsLayers"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""blobSum"": ""sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ],
[22:41:54]W:	 [Step 10/10]    ""history"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""Hostname\"":\""571cde9b03ce\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""container\"":\""571cde9b03ce6f46b78b8e9c5089d03034863a4ab9f05d3e4997d0e5e80a2a6e\"",\""container_config\"":{\""Hostname\"":\""571cde9b03ce\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:701fd33a2f463fd4bd459779276897ef01dcf998dd47f6c8eae34fa5e0886046 in /\""],\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""created\"":\""2016-06-02T21:43:31.291506236Z\"",\""docker_version\"":\""1.9.1\"",\""id\"":\""e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b\"",\""os\"":\""linux\""}""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ],
[22:41:54]W:	 [Step 10/10]    ""signatures"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""header"": {
[22:41:54]W:	 [Step 10/10]             ""jwk"": {
[22:41:54]W:	 [Step 10/10]                ""crv"": ""P-256"",
[22:41:54]W:	 [Step 10/10]                ""kid"": ""IZ4C:AKG6:LLBK:4Y62:6YWU:OI2G:K2EN:ZOJH:GHRY:5PKA:PFEE:WZWD"",
[22:41:54]W:	 [Step 10/10]                ""kty"": ""EC"",
[22:41:54]W:	 [Step 10/10]                ""x"": ""hU3h5pMhA0tgT3mF41BH5EbsLy9Tv3O-bla53S8-25g"",
[22:41:54]W:	 [Step 10/10]                ""y"": ""Y9sM4tXh_3KKKeEhikWEGgTUlQLYJxPWCXcs_bVP4Pc""
[22:41:54]W:	 [Step 10/10]             },
[22:41:54]W:	 [Step 10/10]             ""alg"": ""ES256""
[22:41:54]W:	 [Step 10/10]          },
[22:41:54]W:	 [Step 10/10]          ""signature"": ""8SZVGFKd_Ovz9FtfNMoLRWkwayOY9zaTq4bgPnKPuFPK-48nhDTMlkMz52Nqm2SHCk2xtYYkhzLtE6wUctrjqA"",
[22:41:54]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNTgsImZvcm1hdFRhaWwiOiJDbjAiLCJ0aW1lIjoiMjAxNi0wNi0xOVQyMjo0MTo1NFoifQ""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ]
[22:41:54]W:	 [Step 10/10] }'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.870767 30915 registry_puller.cpp:368] Fetching blob 'sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957' for layer 'e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b' of image 'library/alpine'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357898 30910 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357965 30910 hierarchical.cpp:1488] No allocations performed
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357980 30910 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.358002 30910 hierarchical.cpp:1139] Performed allocation for 1 agents in 238814ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.474309 30911 registry_puller.cpp:305] Extracting layer tar ball '/tmp/KhgYrQ/store/staging/0cVlJm/sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957 to rootfs '/tmp/KhgYrQ/store/staging/0cVlJm/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootfs'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.575764 30910 metadata_manager.cpp:155] Successfully cached image 'alpine'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.576198 30911 provisioner.cpp:294] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.576556 30910 copy.cpp:128] Copying layer path '/tmp/KhgYrQ/store/layers/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootfs' to rootfs '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.676825 30916 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--sandbox_directory=\/mnt\/mesos\/sandbox"",""--user=root"",""--rootfs=\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f""],""shell"":false,""user"":""root"",""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[{""shell"":true,""value"":""#!\/bin\/sh\nset -x -e\n\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-containerizer mount --help=\""false\"" --operation=\""make-rslave\"" --path=\""\/\""\nmount -n --rbind '\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/slaves\/27c796db-6f98-4d61-96c0-f583f22787ff-S0\/frameworks\/27c796db-6f98-4d61-96c0-f583f22787ff-0000\/executors\/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2\/runs\/548370b5-05f2-4e33-8f6f-015aa3fd1af4' '\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f\/mnt\/mesos\/sandbox'\n""}]}"" --help=""false"" --pipe_read=""17"" --pipe_write=""20"" --sandbox=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4"" --user=""root""'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.676923 30916 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWUTS | CLONE_NEWNS
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.681491 30913 cni.cpp:683] Bind mounted '/proc/13484/ns/net' to '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/ns' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.681712 30913 cni.cpp:977] Invoking CNI plugin 'mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.776078 30916 cni.cpp:1066] Got assigned IPv4 address '172.17.0.1/16' from CNI network '__MESOS_TEST__' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.776463 30913 cni.cpp:808] DNS nameservers for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4 are:
[22:41:55]W:	 [Step 10/10] nameserver 172.30.0.2
[22:41:55]W:	 [Step 10/10] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
[22:41:55]W:	 [Step 10/10] + mount -n --rbind /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4 /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f/mnt/mesos/sandbox
[22:41:55]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.944355 13484 process.cpp:1060] libprocess is initialized on 172.17.0.1:60396 with 8 worker threads
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.946605 13484 logging.cpp:199] Logging to STDERR
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947335 13484 exec.cpp:161] Version: 1.0.0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947404 13541 exec.cpp:211] Executor started at: executor(1)@172.17.0.1:60396 with pid 13484
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947883 30917 slave.cpp:2884] Got registration for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.948427 13543 exec.cpp:236] Executor registered on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.948524 30914 slave.cpp:2061] Sending queued task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' to executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949061 13543 exec.cpp:248] Executor::registered took 75489ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949213 13543 exec.cpp:323] Executor asked to run task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949246 13543 exec.cpp:332] Executor::launchTask took 21245ns
[22:41:55] :	 [Step 10/10] Received SUBSCRIBED event
[22:41:55] :	 [Step 10/10] Subscribed executor on ip-172-30-2-105.mesosphere.io
[22:41:55] :	 [Step 10/10] Received LAUNCH event
[22:41:55] :	 [Step 10/10] Starting task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:55] :	 [Step 10/10] Forked command at 13550
[22:41:55] :	 [Step 10/10] sh -c 'ifconfig'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.953589 13547 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] Failed to exec: No such file or directory
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.953891 30917 slave.cpp:3267] Handling status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954368 30910 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954385 30910 status_update_manager.cpp:497] Creating StatusUpdate stream for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954545 30910 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to the agent
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954637 30911 slave.cpp:3665] Forwarding the update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to master@172.30.2.105:40724
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954711 30911 slave.cpp:3559] Status update manager successfully handled status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954732 30911 slave.cpp:3575] Sending acknowledgement for status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954761 30914 master.cpp:5211] Status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954788 30914 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954843 30914 master.cpp:6871] Updating the state of task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954934 13548 exec.cpp:369] Executor received status update acknowledgement 5caccf6c-9e1e-44cc-93d4-6851987802cd for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954967 30910 sched.cpp:1005] Scheduler::statusUpdate took 57021ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955070 30914 master.cpp:4365] Processing ACKNOWLEDGE call 5caccf6c-9e1e-44cc-93d4-6851987802cd for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955150 30911 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955219 30911 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56] :	 [Step 10/10] Command terminated with signal Aborted (pid: 13550)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054153 13541 exec.cpp:546] Executor sending status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054498 30913 slave.cpp:3267] Handling status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054955 30917 slave.cpp:6074] Terminating task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055366 30912 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055409 30912 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to the agent
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055485 30916 slave.cpp:3665] Forwarding the update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to master@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055558 30916 slave.cpp:3559] Status update manager successfully handled status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:216: Failure
[22:41:56] :	 [Step 10/10] Value of: statusFinished->state()
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055572 30916 slave.cpp:3575] Sending acknowledgement for status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to executor(1)@172.17.0.1:60396
[22:41:56] :	 [Step 10/10]   Actual: TASK_FAILED
[22:41:56] :	 [Step 10/10] Expected: TASK_FINISHED
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055613 30914 master.cpp:5211] Status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055640 30914 master.cpp:5259] Forwarding status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055696 30914 master.cpp:6871] Updating the state of task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055773 30912 sched.cpp:1005] Scheduler::statusUpdate took 29145ns
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055780 13546 exec.cpp:369] Executor received status update acknowledgement 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055816 30916 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 from framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055887 30911 master.cpp:4365] Processing ACKNOWLEDGE call 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055907 30911 master.cpp:6937] Removing task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 with resources cpus(*):1; mem(*):128 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055971 30896 sched.cpp:1964] Asked to stop the driver
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056030 30913 sched.cpp:1167] Stopping framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056040 30916 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056073 30916 status_update_manager.cpp:528] Cleaning up status update stream for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056151 30915 master.cpp:6342] Processing TEARDOWN call for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056172 30915 master.cpp:6354] Removing framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056197 30916 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056216 30916 slave.cpp:6115] Completing task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056218 30913 hierarchical.cpp:375] Deactivated framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056248 30916 slave.cpp:2274] Asked to shut down framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 by master@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056265 30916 slave.cpp:2299] Shutting down framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056277 30916 slave.cpp:4470] Shutting down executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056468 30914 hierarchical.cpp:326] Removed framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056634 30911 containerizer.cpp:1576] Destroying container '548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057258 13543 exec.cpp:410] Executor asked to shutdown
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057303 13543 exec.cpp:425] Executor::shutdown took 6363ns
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057324 13547 exec.cpp:92] Scheduling shutdown of the executor in 5secs
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.058279 30910 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.059762 30912 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 after 1.460736ms
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.061364 30910 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.062861 30915 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 after 1.478912ms
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.064016 30910 slave.cpp:3793] executor(1)@172.17.0.1:60396 exited
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.078352 30915 containerizer.cpp:1812] Executor for container '548370b5-05f2-4e33-8f6f-015aa3fd1af4' has exited
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.179833 30916 cni.cpp:1217] Unmounted the network namespace handle '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/ns' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.179924 30916 cni.cpp:1228] Removed the container directory '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.180981 30913 provisioner.cpp:434] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280364 30912 slave.cpp:4152] Executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 terminated with signal Killed
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280406 30912 slave.cpp:4256] Cleaning up executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280545 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' for gc 6.99999675365926days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280575 30912 slave.cpp:4344] Cleaning up framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280647 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2' for gc 6.99999675293037days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280654 30914 status_update_manager.cpp:282] Closing status update streams for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280710 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000' for gc 6.99999675200296days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280745 30915 slave.cpp:839] Agent terminating
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280810 30912 master.cpp:1367] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) disconnected
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280827 30912 master.cpp:2899] Disconnecting agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280844 30912 master.cpp:2918] Deactivating agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280912 30912 hierarchical.cpp:560] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 deactivated
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.283011 30896 master.cpp:1214] Master terminating
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.283140 30916 hierarchical.cpp:505] Removed agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:56] :	 [Step 10/10] [  FAILED  ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask (1945 ms)
{noformat}"	MESOS	Resolved	3	1	4582	containerizer, isolator, mesosphere
12962872	LinuxFilesystemIsolatorTest.ROOT_SandboxEnvironmentVariable is flaky.	"Observed on the internal Mesosphere CI:
{code}
[03:05:29] :	 [Step 10/10] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_SandboxEnvironmentVariable
[03:05:30]W:	 [Step 10/10] I0427 03:05:30.880957 32158 linux.cpp:81] Making '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900' a shared mount
[03:05:30]W:	 [Step 10/10] I0427 03:05:30.893527 32158 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[03:05:30]W:	 [Step 10/10] I0427 03:05:30.893870 32176 containerizer.cpp:703] Starting container '5c2eecfd-75a4-416a-b8e9-fba620274c97' for executor 'test_executor' of framework ''
[03:05:30]W:	 [Step 10/10] I0427 03:05:30.894084 32172 provisioner.cpp:285] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900/provisioner/containers/5c2eecfd-75a4-416a-b8e9-fba620274c97/backends/copy/rootfses/0e8c0218-6551-4dea-abf5-20e9f729dcf9' for container 5c2eecfd-75a4-416a-b8e9-fba620274c97
[03:05:30]W:	 [Step 10/10] I0427 03:05:30.894450 32177 copy.cpp:128] Copying layer path '/tmp/dKE6qq/test_image' to rootfs '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900/provisioner/containers/5c2eecfd-75a4-416a-b8e9-fba620274c97/backends/copy/rootfses/0e8c0218-6551-4dea-abf5-20e9f729dcf9'
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.358980 32172 linux.cpp:355] Bind mounting work directory from '/tmp/dKE6qq/sandbox' to '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900/provisioner/containers/5c2eecfd-75a4-416a-b8e9-fba620274c97/backends/copy/rootfses/0e8c0218-6551-4dea-abf5-20e9f729dcf9/mnt/mesos/sandbox' for container 5c2eecfd-75a4-416a-b8e9-fba620274c97
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.359519 32174 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWNS
[03:05:35]W:	 [Step 10/10] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
[03:05:35]W:	 [Step 10/10] + grep -E /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900/.+ /proc/self/mountinfo
[03:05:35]W:	 [Step 10/10] + grep -v 5c2eecfd-75a4-416a-b8e9-fba620274c97
[03:05:35]W:	 [Step 10/10] + cut '-d ' -f5
[03:05:35]W:	 [Step 10/10] + xargs --no-run-if-empty umount -l
[03:05:35] :	 [Step 10/10] Changing root to /mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900/provisioner/containers/5c2eecfd-75a4-416a-b8e9-fba620274c97/backends/copy/rootfses/0e8c0218-6551-4dea-abf5-20e9f729dcf9
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.459079 32173 containerizer.cpp:1717] Executor for container '5c2eecfd-75a4-416a-b8e9-fba620274c97' has exited
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.459105 32173 containerizer.cpp:1481] Destroying container '5c2eecfd-75a4-416a-b8e9-fba620274c97'
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.460752 32173 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos/5c2eecfd-75a4-416a-b8e9-fba620274c97
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.462360 32172 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/5c2eecfd-75a4-416a-b8e9-fba620274c97 after 1.578752ms
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.463892 32176 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos/5c2eecfd-75a4-416a-b8e9-fba620274c97
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.465405 32173 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/5c2eecfd-75a4-416a-b8e9-fba620274c97 after 1.494272ms
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.466672 32177 linux.cpp:825] Unmounting sandbox/work directory '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900/provisioner/containers/5c2eecfd-75a4-416a-b8e9-fba620274c97/backends/copy/rootfses/0e8c0218-6551-4dea-abf5-20e9f729dcf9/mnt/mesos/sandbox' for container 5c2eecfd-75a4-416a-b8e9-fba620274c97
[03:05:35]W:	 [Step 10/10] I0427 03:05:35.466837 32176 provisioner.cpp:338] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/LinuxFilesystemIsolatorTest_ROOT_SandboxEnvironmentVariable_TU5900/provisioner/containers/5c2eecfd-75a4-416a-b8e9-fba620274c97/backends/copy/rootfses/0e8c0218-6551-4dea-abf5-20e9f729dcf9' for container 5c2eecfd-75a4-416a-b8e9-fba620274c97
[03:05:50] :	 [Step 10/10] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1386: Failure
[03:05:50] :	 [Step 10/10] Failed to wait 15secs for wait
[03:06:13] :	 [Step 10/10] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_SandboxEnvironmentVariable (21793 ms)
{code}"	MESOS	Resolved	3	1	4582	isolation, mesosphere
13032093	Nested container's launch command is not set correctly in docker/runtime isolator.	"We in-correctly append executor's command to the default entrypoint of the image:
https://github.com/apache/mesos/blob/1.1.0/src/slave/containerizer/mesos/isolators/docker/runtime.cpp#L323-L327
https://github.com/apache/mesos/blob/1.1.0/src/slave/containerizer/mesos/isolators/docker/runtime.cpp#L350-L354

This is probably due to the negligence while adding nested container support."	MESOS	Resolved	1	1	4582	runtime-isolator
12922572	Serialize docker v1 image spec as protobuf	Currently we only support v2 docker manifest serialization method. When we read docker image spec locally from disk, we should be able to parse v1 docker manifest as protobuf, which will make it easier to gather runtime config and other necessary info.	MESOS	Resolved	3	4	4582	mesosphere
12999364	Update the streaming function for ContainerID to be nesting aware.	We need to print the hierarchical structure of the nested container. For instance: xxxxx/yyyyy/zzzzzz	MESOS	Resolved	3	3	4582	mesosphere
12991989	Volume container_path should be forbidden to be the container sandbox.	For either local volumes or docker external volumes, the container path should not be identical to the the container sandbox. Otherwise, persistent volumes and log inside of the sandbox will be overwritten.	MESOS	Open	3	1	4582	containerizer, mesosphere, volumes
13011201	`NestedMesosContainerizerTest.ROOT_CGROUPS_ParentExit` is flaky in Debian 8.	"{noformat}
[00:21:51] :	 [Step 10/10] [ RUN      ] NestedMesosContainerizerTest.ROOT_CGROUPS_ParentExit
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.357839 23530 containerizer.cpp:202] Using isolation: cgroups/cpu,filesystem/linux,namespaces/pid,network/cni,volume/image
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.361143 23530 linux_launcher.cpp:150] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.366930 23547 containerizer.cpp:557] Recovering containerizer
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.367962 23551 provisioner.cpp:253] Provisioner recovery complete
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.368253 23549 containerizer.cpp:954] Starting container 42589936-56b2-4e41-86d8-447bfaba4666 for executor 'executor' of framework 
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.368577 23548 cgroups.cpp:404] Creating cgroup at '/sys/fs/cgroup/cpu,cpuacct/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666' for container 42589936-56b2-4e41-86d8-447bfaba4666
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.369863 23544 cpu.cpp:103] Updated 'cpu.shares' to 1024 (cpus 1) for container 42589936-56b2-4e41-86d8-447bfaba4666
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.370384 23545 containerizer.cpp:1443] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""read key <&30""}"" --help=""false"" --pipe_read=""30"" --pipe_write=""34"" --pre_exec_commands=""[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-containerizer""},{""shell"":true,""value"":""mount -n -t proc proc \/proc -o nosuid,noexec,nodev""}]"" --runtime_directory=""/mnt/teamcity/temp/buildTmp/NestedMesosContainerizerTest_ROOT_CGROUPS_ParentExit_sEbtvQ/containers/42589936-56b2-4e41-86d8-447bfaba4666"" --unshare_namespace_mnt=""false"" --working_directory=""/mnt/teamcity/temp/buildTmp/NestedMesosContainerizerTest_ROOT_CGROUPS_ParentExit_MqjHi0""'
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.370483 23544 linux_launcher.cpp:421] Launching container 42589936-56b2-4e41-86d8-447bfaba4666 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.374867 23545 containerizer.cpp:1480] Checkpointing container's forked pid 14139 to '/mnt/teamcity/temp/buildTmp/NestedMesosContainerizerTest_ROOT_CGROUPS_ParentExit_gzjeKG/meta/slaves/frameworks/executors/executor/runs/42589936-56b2-4e41-86d8-447bfaba4666/pids/forked.pid'
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.376519 23551 containerizer.cpp:1648] Starting nested container 42589936-56b2-4e41-86d8-447bfaba4666.a5bc9913-c32c-40c6-ab78-2b08910847f8
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.377296 23549 containerizer.cpp:1443] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""sleep 1000""}"" --help=""false"" --pipe_read=""30"" --pipe_write=""34"" --pre_exec_commands=""[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-containerizer""},{""shell"":true,""value"":""mount -n -t proc proc \/proc -o nosuid,noexec,nodev""}]"" --runtime_directory=""/mnt/teamcity/temp/buildTmp/NestedMesosContainerizerTest_ROOT_CGROUPS_ParentExit_sEbtvQ/containers/42589936-56b2-4e41-86d8-447bfaba4666/containers/a5bc9913-c32c-40c6-ab78-2b08910847f8"" --unshare_namespace_mnt=""false"" --working_directory=""/mnt/teamcity/temp/buildTmp/NestedMesosContainerizerTest_ROOT_CGROUPS_ParentExit_MqjHi0/containers/a5bc9913-c32c-40c6-ab78-2b08910847f8""'
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.377424 23548 linux_launcher.cpp:421] Launching nested container 42589936-56b2-4e41-86d8-447bfaba4666.a5bc9913-c32c-40c6-ab78-2b08910847f8 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
[00:21:51] :	 [Step 10/10] Executing pre-exec command '{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-containerizer""}'
[00:21:51] :	 [Step 10/10] Executing pre-exec command '{""shell"":true,""value"":""mount -n -t proc proc \/proc -o nosuid,noexec,nodev""}'
[00:21:51]W:	 [Step 10/10] sh: 1: Syntax error: Bad fd number
[00:21:51] :	 [Step 10/10] ../../src/tests/containerizer/nested_mesos_containerizer_tests.cpp:451: Failure
[00:21:51] :	 [Step 10/10] (launch).failure(): Failed to fork: Failed to clone child process: Failed to synchronize child process
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.506002 23544 cgroups.cpp:2705] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos/a5bc9913-c32c-40c6-ab78-2b08910847f8
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.507055 23546 cgroups.cpp:1439] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos/a5bc9913-c32c-40c6-ab78-2b08910847f8 after 1.019904ms
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.507967 23549 cgroups.cpp:2723] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos/a5bc9913-c32c-40c6-ab78-2b08910847f8
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.508783 23549 cgroups.cpp:1468] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos/a5bc9913-c32c-40c6-ab78-2b08910847f8 after 797952ns
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.510030 23551 cgroups.cpp:2705] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.612181 23551 cgroups.cpp:1439] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos after 102.128896ms
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.613134 23548 cgroups.cpp:2723] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.614034 23547 cgroups.cpp:1468] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666/mesos after 878080ns
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.615409 23550 cgroups.cpp:2705] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.717521 23550 cgroups.cpp:1439] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666 after 102.08512ms
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.718426 23545 cgroups.cpp:2723] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.719254 23547 cgroups.cpp:1468] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d/42589936-56b2-4e41-86d8-447bfaba4666 after 806912ns
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.720582 23551 cgroups.cpp:2705] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.822980 23546 cgroups.cpp:1439] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d after 102.373888ms
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.824023 23544 cgroups.cpp:2723] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d
[00:21:51]W:	 [Step 10/10] I1008 00:21:51.824843 23546 cgroups.cpp:1468] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_458f8018-67e7-4cc6-8126-a535974db35d after 801024ns
[00:21:51] :	 [Step 10/10] [  FAILED  ] NestedMesosContainerizerTest.ROOT_CGROUPS_ParentExit (480 ms)
{noformat}"	MESOS	Resolved	3	1	4582	flaky-test
13033160	Support backend per container.	Currently, the container backend is determined by the agent flag and all containers are using the same backend. It is possible to achieve backend per container by introducing a user facing API, which fulfills more robust use cases (e.g., imagine that a group of container/nested container running an application, while some containers only read from huge images and some others only write to pluggable volumes).	MESOS	Accepted	3	4	4582	backend, containerizer
12982426	Support file volume in mesos containerizer.	Currently in mesos containerizer, the host_path volume (to be bind mounted from a host path) specified in ContainerInfo can only be a directory. We should also support the volume type as a file.	MESOS	Resolved	3	4	4582	containerizer, isolation, mesosphere, volumes
12843276	Support mounting in default configuration files/volumes into every new container	"Most container images leave out system configuration (e.g: /etc/*) and expect the container runtimes to mount in specific configurations as needed such as /etc/resolv.conf from the host into the container when needed.

We need to support mounting in specific configuration files for command executor to work, and also allow the user to optionally define other configuration files to mount in as well via flags.
"	MESOS	Resolved	3	4	4582	mesosphere, unified-containerizer-mvp
12961770	Isolator cleanup should not be invoked if they are not prepared yet.	"If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet. 

In this case, there no need to clean up any isolator, call provisioner destroy directly."	MESOS	Resolved	3	1	4582	containerizer
12930147	Enhance the performance of launching Docker container	"When we tested Swarm on Mesos, we found it has some some performance downgrade compared with Swarm standalone, see the test result for details:
https://github.com/Open-I-Beam/containers-os/tree/master/swarm-mesos/151201-paolo-swarm-on-mesos

Some potential root causes are:
1. In Swarm on Mesos, there will be one Docker executor for each Docker container.
2. In Swarm on Mesos, Docker CLI is used rather than Docker API.
"	MESOS	Accepted	3	15	4582	mesosphere
13189612	Nested container launch could fail if the agent upgrade with new cgroup subsystems.	Nested container launch could fail if the agent upgrade with new cgroup subsystems, because the new cgroup subsystems do not exist on parent container's cgroup hierarchy.	MESOS	Resolved	3	1	4582	containerizer
12952538	Enforce that DiskInfo principal is equal to framework/operator principal	Currently, we require that {{ReservationInfo.principal}} be equal to the principal provided for authentication, which means that when HTTP authentication is disabled this field cannot be set. Based on comments in 'mesos.proto', the original intention was to enforce this same constraint for {{Persistence.principal}}, but it seems that we don't enforce it. This should be changed to make the two fields equivalent, with one exception: when the framework/operator principal is {{None}}, we should allow the principal in {{DiskInfo}} to take any value, along the same lines as MESOS-5212.	MESOS	Resolved	3	1	4664	mesosphere, persistent-volumes, reservations
12946482	Add agent flags for HTTP authentication	"Flags should be added to the agent to:
1. Enable HTTP authentication ({{--authenticate_http}})
2. Specify credentials ({{--http_credentials}})
3. Specify HTTP authenticators ({{--authenticators}})"	MESOS	Resolved	3	3	4664	mesosphere, security
12953665	Extend example persistent volume framework: reservations and lifetime	"The existing example persistent volume test framework would benefit from two improvements:
* It relies on statically reserved resources. To more accurately represent real-world use cases, and to remove the dependency on slaves with statically reserved resources, the framework should be updated to dynamically reserve the disk resources that it uses for persistent volumes.
* The framework currently runs quickly and then exits. It would be useful to have a command-line flag that allows the framework lifetime to be specified, so that it could run continuously for a specified length of time, or indefinitely. This would facilitate testing, including the testing of realistic live-upgrade scenarios."	MESOS	Open	3	4	4664	examples, mesosphere, persistent-volumes
12918922	ReservationTest.ACLMultipleOperations is flaky	Observed from the CI: https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1319/changes	MESOS	Resolved	3	1	4664	flaky, mesosphere
12993579	Agent's '--version' flag doesn't work	With the removal of the agent's default {{work_dir}}, the {{--version}} flag no longer works. Instead, the agent complains about the lack of a {{work_dir}} and prints the usage instructions.	MESOS	Resolved	3	1	4664	mesosphere
13038059	Add executor authentication documentation	"Documentation should be added regarding executor authentication. This will include updating:
1) the configuration docs to include new agent flags
2) the authentication documentation
3) the authorization documentation
4) the upgrade documentation
5) the CHANGELOG"	MESOS	Resolved	3	20	4664	documentation, mesosphere
12982107	The /files/download endpoint's authorization can be compromised	"If a forward slash is appended to the path of a file a user wishes to download via {{/files/download}}, the authorization logic for that path will be bypassed and the file will be downloaded regardless of permissions. This is because we store the authorization callbacks for these paths in a map which is keyed by the path name, so a request to {{/master/log/}} fails to find the callback which is installed for {{/master/log}}. When the master fails to find the callback, it assumes authorization is not required for that path and authorizes the action.

Consider the following excerpt:
{code}
gmann@gmac:~/src/mesos/build  http GET http://127.0.0.1:5050/files/download\?path\=/master/log -a foo:bar
HTTP/1.1 403 Forbidden
Content-Length: 0
Date: Wed, 22 Jun 2016 21:28:53 GMT

gmann@gmac:~/src/mesos/build  http GET http://127.0.0.1:5050/files/download\?path\=/master/log/ -a foo:bar
HTTP/1.1 200 OK
Content-Disposition: attachment; filename=mesos-master.gmac.gmann.log.INFO.20160622-142843.65615
Content-Length: 14432
Content-Type: application/octet-stream
Date: Wed, 22 Jun 2016 21:28:56 GMT

Log file created at: 2016/06/22 14:28:43
Running on machine: gmac
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0622 14:28:43.476925 2080764672 logging.cpp:194] INFO level logging started!
I0622 14:28:43.477522 2080764672 main.cpp:367] Using 'HierarchicalDRF' allocator
I0622 14:28:43.480650 2080764672 leveldb.cpp:174] Opened db in 2961us
I0622 14:28:43.481046 2080764672 leveldb.cpp:181] Compacted db in 372us
I0622 14:28:43.481078 2080764672 leveldb.cpp:196] Created db iterator in 13us
I0622 14:28:43.481096 2080764672 leveldb.cpp:202] Seeked to beginning of db in 9us
I0622 14:28:43.481111 2080764672 leveldb.cpp:271] Iterated through 0 keys in the db in 8us
I0622 14:28:43.481165 2080764672 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0622 14:28:43.481967 219914240 recover.cpp:451] Starting replica recovery
I0622 14:28:43.482193 219914240 recover.cpp:477] Replica is in EMPTY status
I0622 14:28:43.482589 2080764672 main.cpp:488] Creating default 'local' authorizer
I0622 14:28:43.482719 2080764672 main.cpp:545] Starting Mesos master
I0622 14:28:43.483085 218841088 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@127.0.0.1:5050
I0622 14:28:43.487284 218304512 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0622 14:28:43.487694 219914240 recover.cpp:568] Updating replica status to STARTING
{code}

We could consider disallowing paths which end in trailing slashes."	MESOS	Resolved	1	1	4664	mesosphere
13038196	Add a 'secret' field to the 'Environment' message	A new field of type {{Secret}} should be added to the {{Environment}} message to enable the inclusion of secrets in executor and task environments.	MESOS	Resolved	3	3	4664	mesosphere, security
12780472	Write documentation for all the LIBPROCESS_* environment variables.	"libprocess uses a set of environment variables to modify its behaviour; however, these variables are not documented anywhere, nor it is defined where the documentation should be.

What would be needed is a decision whether the environment variables should be documented (a new doc file or reusing an existing one), and then add the documentation there.

After searching in the code, these are the variables which need to be documented:

# {{LIBPROCESS_IP}}
# {{LIBPROCESS_PORT}}
# {{LIBPROCESS_ADVERTISE_IP}}
# {{LIBPROCESS_ADVERTISE_PORT}}"	MESOS	Resolved	3	20	4664	documentation, mesosphere
13117165	Implement explicit offer operation reconciliation between the master, agent and RPs.	"Upon receiving an {{UpdateSlave}} message the master should compare its list of pending operations for the agent/LRPs to the list of pending operations contained in the message. It should then build a {{ReconcileOfferOperations}} message with all the operations missing in the {{UpdateSlave}} message and send it to the agent.

The agent will receive these messages and should handle them by itself if the operations affect the default resources, or forward them to the RP manager otherwise.

The agent/RP handler should check if the operations are pending. If an operation is not pending, then an {{ApplyOfferOperation}} message got dropped, and the agent/LRP should send an {{OFFER_OPERATION_DROPPED}} status update to the master."	MESOS	Resolved	3	3	4664	mesosphere
13131380	Add missing fields to agent v1 operator API	Some fields which are available via the agent {{/state}} endpoint are not accessible via the v1 API.	MESOS	Accepted	3	4	4664	mesosphere
13191711	Consider providing better operation status updates while an RP is recovering	"Consider the following scenario:

1. A framework accepts an offer with an operation affecting SLRP resources.
2. The master forwards it to the corresponding agent.
3. The agent forwards it to the corresponding RP.
4. The agent and the master fail over.
5. The master recovers.
6. The agent recovers while the RP is still recovering, so it doesn't include the pending operation on the {{RegisterMessage}}.
7. A framework performs an explicit operation status reconciliation.

In this case the master will currently respond with {{OPERATION_UNKNOWN}}, but it should be possible to respond with a more fine-grained and useful state, such as {{OPERATION_RECOVERING}}."	MESOS	Resolved	3	3	4664	foundations, mesosphere, operation-feedback
12857999	Remove remnants of LIBPROCESS_STATISTICS_WINDOW	"As seen in MESOS-1283, LIBPROCESS_STATISTICS_WINDOW is no longer needed since metrics now require specification of a window size, and default to no history if not provided.

Some commented-out code remnants associated with this environment variable still remain and should be removed."	MESOS	Resolved	5	4	4664	easyfix, mesosphere
12923241	PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky	"{noformat}
[ RUN      ] PersistentVolumeTest.BadACLDropCreateAndDestroy
I1219 09:51:32.623245 31878 leveldb.cpp:174] Opened db in 4.393596ms
I1219 09:51:32.624084 31878 leveldb.cpp:181] Compacted db in 709447ns
I1219 09:51:32.624186 31878 leveldb.cpp:196] Created db iterator in 21252ns
I1219 09:51:32.624290 31878 leveldb.cpp:202] Seeked to beginning of db in 11391ns
I1219 09:51:32.624378 31878 leveldb.cpp:271] Iterated through 0 keys in the db in 611ns
I1219 09:51:32.624505 31878 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1219 09:51:32.625195 31904 recover.cpp:447] Starting replica recovery
I1219 09:51:32.625641 31904 recover.cpp:473] Replica is in EMPTY status
I1219 09:51:32.627305 31904 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (6740)@172.17.0.3:36408
I1219 09:51:32.627749 31904 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1219 09:51:32.628330 31904 recover.cpp:564] Updating replica status to STARTING
I1219 09:51:32.629068 31906 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 410494ns
I1219 09:51:32.629169 31906 replica.cpp:320] Persisted replica status to STARTING
I1219 09:51:32.629598 31906 recover.cpp:473] Replica is in STARTING status
I1219 09:51:32.630782 31912 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6741)@172.17.0.3:36408
I1219 09:51:32.631166 31901 recover.cpp:193] Received a recover response from a replica in STARTING status
I1219 09:51:32.632467 31902 recover.cpp:564] Updating replica status to VOTING
I1219 09:51:32.633600 31907 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 311370ns
I1219 09:51:32.633627 31907 replica.cpp:320] Persisted replica status to VOTING
I1219 09:51:32.633719 31907 recover.cpp:578] Successfully joined the Paxos group
I1219 09:51:32.633874 31907 recover.cpp:462] Recover process terminated
I1219 09:51:32.636409 31909 master.cpp:365] Master bded856d-1c7f-4fad-a8bc-3629ba8c59d3 (60ab6e727501) started on 172.17.0.3:36408
I1219 09:51:32.636593 31909 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""creator-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SpPF7B/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SpPF7B/master"" --zk_session_timeout=""10secs""
I1219 09:51:32.637055 31909 master.cpp:414] Master allowing unauthenticated frameworks to register
I1219 09:51:32.637068 31909 master.cpp:417] Master only allowing authenticated slaves to register
I1219 09:51:32.637094 31909 credentials.hpp:35] Loading credentials for authentication from '/tmp/SpPF7B/credentials'
I1219 09:51:32.637403 31909 master.cpp:456] Using default 'crammd5' authenticator
I1219 09:51:32.637555 31909 master.cpp:493] Authorization enabled
W1219 09:51:32.637575 31909 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I1219 09:51:32.637806 31897 whitelist_watcher.cpp:77] No whitelist given
I1219 09:51:32.637820 31910 hierarchical.cpp:147] Initialized hierarchical allocator process
I1219 09:51:32.639677 31909 master.cpp:1629] The newly elected leader is master@172.17.0.3:36408 with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3
I1219 09:51:32.639768 31909 master.cpp:1642] Elected as the leading master!
I1219 09:51:32.639892 31909 master.cpp:1387] Recovering from registrar
I1219 09:51:32.640136 31907 registrar.cpp:307] Recovering registrar
I1219 09:51:32.640929 31901 log.cpp:659] Attempting to start the writer
I1219 09:51:32.642199 31912 replica.cpp:493] Replica received implicit promise request from (6742)@172.17.0.3:36408 with proposal 1
I1219 09:51:32.642719 31912 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 445876ns
I1219 09:51:32.642755 31912 replica.cpp:342] Persisted promised to 1
I1219 09:51:32.643478 31904 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1219 09:51:32.645009 31909 replica.cpp:388] Replica received explicit promise request from (6743)@172.17.0.3:36408 for position 0 with proposal 2
I1219 09:51:32.645356 31909 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 310064ns
I1219 09:51:32.645382 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.646662 31909 replica.cpp:537] Replica received write request for position 0 from (6744)@172.17.0.3:36408
I1219 09:51:32.646721 31909 leveldb.cpp:436] Reading position from leveldb took 29298ns
I1219 09:51:32.647047 31909 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 283424ns
I1219 09:51:32.647073 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.647722 31909 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1219 09:51:32.648052 31909 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 300825ns
I1219 09:51:32.648077 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.648095 31909 replica.cpp:697] Replica learned NOP action at position 0
I1219 09:51:32.655295 31899 log.cpp:675] Writer started with ending position 0
I1219 09:51:32.656543 31905 leveldb.cpp:436] Reading position from leveldb took 32788ns
I1219 09:51:32.658164 31905 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I1219 09:51:32.658604 31905 registrar.cpp:439] Applied 1 operations in 38183ns; attempting to update the 'registry'
I1219 09:51:32.660102 31905 log.cpp:683] Attempting to append 170 bytes to the log
I1219 09:51:32.660538 31906 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1219 09:51:32.661872 31906 replica.cpp:537] Replica received write request for position 1 from (6745)@172.17.0.3:36408
I1219 09:51:32.662719 31906 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 483018ns
I1219 09:51:32.663054 31906 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664008 31902 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1219 09:51:32.664330 31902 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 287310ns
I1219 09:51:32.664355 31902 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664376 31902 replica.cpp:697] Replica learned APPEND action at position 1
I1219 09:51:32.665365 31902 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.665493 31902 registrar.cpp:370] Successfully recovered registrar
I1219 09:51:32.665894 31902 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1219 09:51:32.665990 31902 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1219 09:51:32.666266 31902 log.cpp:702] Attempting to truncate the log to 1
I1219 09:51:32.666424 31902 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1219 09:51:32.667181 31907 replica.cpp:537] Replica received write request for position 2 from (6746)@172.17.0.3:36408
I1219 09:51:32.667768 31907 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 335947ns
I1219 09:51:32.668067 31907 replica.cpp:712] Persisted action at 2
I1219 09:51:32.668942 31906 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1219 09:51:32.669240 31906 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 266566ns
I1219 09:51:32.669292 31906 leveldb.cpp:399] Deleting ~1 keys from leveldb took 27852ns
I1219 09:51:32.669314 31906 replica.cpp:712] Persisted action at 2
I1219 09:51:32.669334 31906 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1219 09:51:32.691251 31878 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1219 09:51:32.691759 31878 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1219 09:51:32.697428 31901 slave.cpp:191] Slave started on 228)@172.17.0.3:36408
I1219 09:51:32.697459 31901 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc""
I1219 09:51:32.697963 31901 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential'
I1219 09:51:32.698210 31901 slave.cpp:322] Slave using credential for: test-principal
I1219 09:51:32.698449 31901 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I1219 09:51:32.699065 31901 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.699137 31901 slave.cpp:400] Slave attributes: [  ]
I1219 09:51:32.699151 31901 slave.cpp:405] Slave hostname: 60ab6e727501
I1219 09:51:32.699161 31901 slave.cpp:410] Slave checkpoint: true
I1219 09:51:32.699364 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.700614 31911 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.700703 31911 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.700724 31911 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.700839 31911 sched.cpp:747] Will retry registration in 620.399428ms if necessary
I1219 09:51:32.701244 31903 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.701313 31903 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I1219 09:51:32.701625 31903 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1219 09:51:32.702308 31903 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702386 31903 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.702422 31903 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.702448 31903 hierarchical.cpp:1079] Performed allocation for 0 slaves in 114358ns
I1219 09:51:32.702638 31903 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702688 31903 sched.cpp:655] Scheduler::registered took 25558ns
I1219 09:51:32.703553 31901 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta'
I1219 09:51:32.704118 31897 status_update_manager.cpp:200] Recovering status update manager
I1219 09:51:32.704407 31907 containerizer.cpp:383] Recovering containerizer
I1219 09:51:32.705373 31907 slave.cpp:4427] Finished recovery
I1219 09:51:32.705991 31907 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1219 09:51:32.706277 31907 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1219 09:51:32.706666 31907 slave.cpp:729] New master detected at master@172.17.0.3:36408
I1219 09:51:32.706738 31907 slave.cpp:792] Authenticating with master master@172.17.0.3:36408
I1219 09:51:32.706760 31907 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1219 09:51:32.706886 31899 status_update_manager.cpp:174] Pausing sending status updates
I1219 09:51:32.706941 31907 slave.cpp:765] Detecting new master
I1219 09:51:32.707036 31899 authenticatee.cpp:121] Creating new client SASL connection
I1219 09:51:32.707291 31910 master.cpp:5423] Authenticating slave(228)@172.17.0.3:36408
I1219 09:51:32.707479 31910 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.707849 31910 authenticator.cpp:98] Creating new server SASL connection
I1219 09:51:32.708082 31910 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1219 09:51:32.708112 31910 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1219 09:51:32.708196 31910 authenticator.cpp:203] Received SASL authentication start
I1219 09:51:32.708395 31910 authenticator.cpp:325] Authentication requires more steps
I1219 09:51:32.708611 31902 authenticatee.cpp:258] Received SASL authentication step
I1219 09:51:32.708773 31910 authenticator.cpp:231] Received SASL authentication step
I1219 09:51:32.708889 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1219 09:51:32.708976 31910 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1219 09:51:32.709096 31910 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1219 09:51:32.709200 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1219 09:51:32.709285 31910 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709363 31910 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709452 31910 authenticator.cpp:317] Authentication success
I1219 09:51:32.709707 31910 authenticatee.cpp:298] Authentication success
I1219 09:51:32.710252 31910 slave.cpp:860] Successfully authenticated with master master@172.17.0.3:36408
I1219 09:51:32.710525 31910 slave.cpp:1254] Will retry registration in 17.44437ms if necessary
I1219 09:51:32.709839 31908 master.cpp:5453] Successfully authenticated principal 'test-principal' at slave(228)@172.17.0.3:36408
I1219 09:51:32.710985 31908 master.cpp:4132] Registering slave at slave(228)@172.17.0.3:36408 (60ab6e727501) with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.711645 31908 registrar.cpp:439] Applied 1 operations in 83191ns; attempting to update the 'registry'
I1219 09:51:32.709908 31912 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.713407 31908 log.cpp:683] Attempting to append 343 bytes to the log
I1219 09:51:32.713646 31912 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1219 09:51:32.714884 31911 replica.cpp:537] Replica received write request for position 3 from (6758)@172.17.0.3:36408
I1219 09:51:32.715221 31911 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 288909ns
I1219 09:51:32.715250 31911 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716145 31912 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1219 09:51:32.716689 31912 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 512217ns
I1219 09:51:32.716716 31912 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716737 31912 replica.cpp:697] Replica learned APPEND action at position 3
I1219 09:51:32.718426 31911 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.719441 31902 slave.cpp:3371] Received ping from slave-observer(228)@172.17.0.3:36408
I1219 09:51:32.719843 31909 log.cpp:702] Attempting to truncate the log to 3
I1219 09:51:32.719908 31911 master.cpp:4200] Registered slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.720064 31911 slave.cpp:904] Registered with master master@172.17.0.3:36408; given slave ID bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.720088 31911 fetcher.cpp:81] Clearing fetcher cache
I1219 09:51:32.720491 31911 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta/slaves/bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0/slave.info'
I1219 09:51:32.720844 31909 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1219 09:51:32.720929 31911 slave.cpp:963] Forwarding total oversubscribed resources 
I1219 09:51:32.721017 31903 status_update_manager.cpp:181] Resuming sending status updates
I1219 09:51:32.721099 31911 master.cpp:4542] Received update of slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with total oversubscribed resources 
I1219 09:51:32.721141 31905 hierarchical.cpp:465] Added slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I1219 09:51:32.721879 31911 replica.cpp:537] Replica received write request for position 4 from (6759)@172.17.0.3:36408
I1219 09:51:32.722293 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.722337 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 1.155563ms
I1219 09:51:32.722681 31905 hierarchical.cpp:521] Slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I1219 09:51:32.722713 31909 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.723031 31905 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.723073 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.723095 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 368889ns
I1219 09:51:32.723191 31909 sched.cpp:811] Scheduler::resourceOffers took 113921ns
I1219 09:51:32.723410 31911 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.418243ms
I1219 09:51:32.723497 31911 replica.cpp:712] Persisted action at 4
I1219 09:51:32.724326 31907 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1219 09:51:32.724758 31907 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 329678ns
I1219 09:51:32.724917 31907 leveldb.cpp:399] Deleting ~2 keys from leveldb took 58317ns
I1219 09:51:32.725025 31907 replica.cpp:712] Persisted action at 4
I1219 09:51:32.725127 31907 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1219 09:51:32.731515 31910 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.731564 31910 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.731591 31910 hierarchical.cpp:1079] Performed allocation for 1 slaves in 239271ns
I1219 09:51:32.741710 31910 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O0 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.741770 31910 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
E1219 09:51:32.742707 31910 master.cpp:1737] Dropping CREATE offer operation from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408: Not authorized to create persistent volumes as 'test-principal'
I1219 09:51:32.743219 31910 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.752542 31908 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.752590 31908 hierarchical.cpp:1079] Performed allocation for 1 slaves in 888401ns
I1219 09:51:32.753018 31908 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.753435 31908 sched.cpp:811] Scheduler::resourceOffers took 92252ns
I1219 09:51:32.761533 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.761931 31897 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O1 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.762373 31897 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.762451 31897 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.762470 31897 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.762543 31897 sched.cpp:747] Will retry registration in 465.481193ms if necessary
I1219 09:51:32.762572 31898 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.762722 31898 master.cpp:2197] Received SUBSCRIBE call for framework 'creator-framework' at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.762785 31898 master.cpp:1668] Authorizing framework principal 'creator-principal' to receive offers for role 'role1'
I1219 09:51:32.763036 31897 master.cpp:2268] Subscribing framework creator-framework with checkpointing disabled and capabilities [  ]
I1219 09:51:32.763464 31898 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763562 31897 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763605 31897 sched.cpp:655] Scheduler::registered took 20669ns
I1219 09:51:32.763804 31908 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.764343 31898 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.764382 31898 hierarchical.cpp:1079] Performed allocation for 1 slaves in 893765ns
I1219 09:51:32.764428 31898 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.764746 31898 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.765127 31898 sched.cpp:811] Scheduler::resourceOffers took 83608ns
I1219 09:51:32.773298 31900 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.773339 31900 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.773365 31900 hierarchical.cpp:1079] Performed allocation for 1 slaves in 201759ns
I1219 09:51:32.782901 31898 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O2 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.782961 31898 master.cpp:2843] Authorizing principal 'creator-principal' to create volumes
I1219 09:51:32.784190 31904 master.cpp:3362] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.784548 31904 master.cpp:6486] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.786471 31904 hierarchical.cpp:642] Updated allocation of framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I1219 09:51:32.786929 31904 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.788035 31904 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I1219 09:51:32.795177 31902 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.795250 31902 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.357898ms
I1219 09:51:32.795897 31902 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.796540 31897 sched.cpp:811] Scheduler::resourceOffers took 138880ns
I1219 09:51:32.803026 31902 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O3 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804143 31902 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.804622 31907 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804729 31907 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.805140 31897 master.cpp:3649] Processing REVIVE call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.805250 31897 hierarchical.cpp:973] Removed offer filters for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.806507 31897 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.806562 31897 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.284779ms
I1219 09:51:32.807067 31897 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
../../src/tests/persistent_volume_tests.cpp:1336: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7ffff9edb3a0, @0x7f71079798f0 { 144-byte object <F0-1B 42-14 71-7F 00-00 00-00 00-00 00-00 00-00 D0-96 02-F0 70-7F 00-00 50-97 02-F0 70-7F 00-00 20-A1 02-F0 70-7F 00-00 50-E0 01-F0 70-7F 00-00 B0-9F 02-F0 70-7F 00-00 00-32 01-F0 70-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 70-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I1219 09:51:32.807899 31897 sched.cpp:811] Scheduler::resourceOffers took 406435ns
I1219 09:51:32.820523 31909 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.820611 31909 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.820642 31909 hierarchical.cpp:1079] Performed allocation for 1 slaves in 448034ns
2015-12-19 09:51:33,146:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:36,482:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:39,818:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:43,155:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:46,490:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1411: Failure
Failed to wait 15secs for offers
I1219 09:51:47.829073 31909 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 disconnected
I1219 09:51:47.829169 31909 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829200 31909 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829366 31909 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 0ns to failover
I1219 09:51:47.829720 31909 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.831614 31907 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.831748 31907 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.833314 31897 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 by master@172.17.0.3:36408
W1219 09:51:47.833421 31897 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.834002 31897 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.843332 31908 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 disconnected
I1219 09:51:47.843521 31908 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.843663 31908 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
W1219 09:51:47.844665 31908 master.hpp:1758] Master attempted to send message to disconnected framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.845077 31908 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 0ns to failover
I1219 09:51:47.844887 31903 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.845728 31903 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
../../src/tests/persistent_volume_tests.cpp:1404: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1219 09:51:47.847968 31902 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848068 31902 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848553 31902 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 by master@172.17.0.3:36408
W1219 09:51:47.848644 31902 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.848999 31902 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.849782 31912 master.cpp:930] Master terminating
I1219 09:51:47.851934 31899 hierarchical.cpp:496] Removed slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:47.855919 31907 slave.cpp:3417] master@172.17.0.3:36408 exited
W1219 09:51:47.856021 31907 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1219 09:51:47.908278 31878 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLDropCreateAndDestroy (15298 ms)
{noformat}"	MESOS	Resolved	3	1	4664	flaky-test, mesosphere, persistent-volumes
13117156	Update the master to accept OfferOperationIDs from frameworks.	Masters {{ACCEPT}} handler should send failed operation updates when a framework sets the {{OfferOperationID}} on an operation destined for an agent without the {{RESOURCE_PROVIDER}} capability.	MESOS	Resolved	3	3	4664	mesosphere
13139160	Add infra to test a hung Docker daemon	"We should add infrastructure to our tests which enables us to test the behavior of the Docker executor and containerizer in the presence of a hung Docker daemon.

One possible first-order solution is to build a simple binary which never returns. We could initialize the agent/executor with this binary instead of the Docker CLI in order to simulate a Docker daemon which hangs on every call."	MESOS	Resolved	3	4	4664	mesosphere
13038058	Enable multiple HTTP authenticator modules	"To accommodate executor authentication, we will add support for the loading of multiple authenticator modules. The {{--http_authenticators}} flag is already set up for this, but we must relax the constraint in Mesos which enforces just a single authenticator.

In order to load multiple authenticators for a realm, a new Mesos-level authenticator, the {{CombinedAuthenticator}}, will be added. This class will call multiple authenticators and combine their results if necessary."	MESOS	Resolved	3	3	4664	libprocess, module, security
12928023	Sync up configuration.md and flags.cpp	The https://reviews.apache.org/r/39923/ made some clean up for configuration.md but the related flags.cpp was not updated, we should update those files as well.	MESOS	Resolved	3	1	4664	documentation, mesosphere, newbie
12680397	ExamplesTest.JavaFramework is flaky	"Identify the cause of the following test failure:

[ RUN      ] ExamplesTest.JavaFramework
Using temporary directory '/tmp/ExamplesTest_JavaFramework_wSc7u8'
Enabling authentication for the framework
I1120 15:13:39.820032 1681264640 master.cpp:285] Master started on 172.25.133.171:52576
I1120 15:13:39.820180 1681264640 master.cpp:299] Master ID: 201311201513-2877626796-52576-3234
I1120 15:13:39.820194 1681264640 master.cpp:302] Master only allowing authenticated frameworks to register!
I1120 15:13:39.821197 1679654912 slave.cpp:112] Slave started on 1)@172.25.133.171:52576
I1120 15:13:39.821795 1679654912 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.822855 1682337792 slave.cpp:112] Slave started on 2)@172.25.133.171:52576
I1120 15:13:39.823652 1682337792 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.825330 1679118336 master.cpp:744] The newly elected leader is master@172.25.133.171:52576
I1120 15:13:39.825445 1679118336 master.cpp:748] Elected as the leading master!
I1120 15:13:39.825907 1681264640 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta'
I1120 15:13:39.826127 1681264640 status_update_manager.cpp:180] Recovering status update manager
I1120 15:13:39.826331 1681801216 process_isolator.cpp:317] Recovering isolator
I1120 15:13:39.826738 1682874368 slave.cpp:2743] Finished recovery
I1120 15:13:39.827747 1682337792 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta'
I1120 15:13:39.827945 1680191488 slave.cpp:112] Slave started on 3)@172.25.133.171:52576
I1120 15:13:39.828415 1682337792 status_update_manager.cpp:180] Recovering status update manager
I1120 15:13:39.828608 1680728064 sched.cpp:260] Authenticating with master master@172.25.133.171:52576
I1120 15:13:39.828606 1680191488 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.828680 1682874368 slave.cpp:497] New master detected at master@172.25.133.171:52576
I1120 15:13:39.828765 1682337792 process_isolator.cpp:317] Recovering isolator
I1120 15:13:39.829828 1680728064 sched.cpp:229] Detecting new master
I1120 15:13:39.830288 1679654912 authenticatee.hpp:100] Initializing client SASL
I1120 15:13:39.831635 1680191488 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta'
I1120 15:13:39.831991 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576
I1120 15:13:39.832042 1682874368 slave.cpp:524] Detecting new master
I1120 15:13:39.832314 1682337792 slave.cpp:2743] Finished recovery
I1120 15:13:39.832309 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(1)@172.25.133.171:52576
I1120 15:13:39.832929 1680728064 status_update_manager.cpp:180] Recovering status update manager
I1120 15:13:39.833371 1681801216 slave.cpp:497] New master detected at master@172.25.133.171:52576
I1120 15:13:39.833273 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-0 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.833595 1680728064 process_isolator.cpp:317] Recovering isolator
I1120 15:13:39.833859 1681801216 slave.cpp:524] Detecting new master
I1120 15:13:39.833861 1682874368 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576
I1120 15:13:39.834092 1680191488 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-0
I1120 15:13:39.834486 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(2)@172.25.133.171:52576
I1120 15:13:39.834549 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-1 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.834750 1680191488 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta/slaves/201311201513-2877626796-52576-3234-0/slave.info'
I1120 15:13:39.834875 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-0 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available)
I1120 15:13:39.835155 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-1
I1120 15:13:39.835458 1679118336 slave.cpp:2743] Finished recovery
I1120 15:13:39.835739 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta/slaves/201311201513-2877626796-52576-3234-1/slave.info'
I1120 15:13:39.835922 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-1 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available)
I1120 15:13:39.836120 1681264640 slave.cpp:497] New master detected at master@172.25.133.171:52576
I1120 15:13:39.836340 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576
I1120 15:13:39.836436 1681264640 slave.cpp:524] Detecting new master
I1120 15:13:39.836629 1682874368 master.cpp:1266] Attempting to register slave on vkone.local at slave(3)@172.25.133.171:52576
I1120 15:13:39.836653 1682874368 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-2 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.836804 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-2
I1120 15:13:39.837190 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta/slaves/201311201513-2877626796-52576-3234-2/slave.info'
I1120 15:13:39.837569 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-2 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available)
I1120 15:13:39.852011 1679654912 authenticatee.hpp:124] Creating new client SASL connection
I1120 15:13:39.852219 1680191488 master.cpp:1734] Authenticating framework at scheduler(1)@172.25.133.171:52576
I1120 15:13:39.852577 1682337792 authenticator.hpp:83] Initializing server SASL
I1120 15:13:39.856160 1682337792 authenticator.hpp:140] Creating new server SASL connection
I1120 15:13:39.856334 1681264640 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1120 15:13:39.856360 1681264640 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1120 15:13:39.856421 1681264640 authenticator.hpp:243] Received SASL authentication start
I1120 15:13:39.856487 1681264640 authenticator.hpp:325] Authentication requires more steps
I1120 15:13:39.856531 1681264640 authenticatee.hpp:258] Received SASL authentication step
I1120 15:13:39.856576 1681264640 authenticator.hpp:271] Received SASL authentication step
I1120 15:13:39.856643 1681264640 authenticator.hpp:317] Authentication success
I1120 15:13:39.856724 1681264640 authenticatee.hpp:298] Authentication success
I1120 15:13:39.856768 1681264640 master.cpp:1774] Successfully authenticated framework at scheduler(1)@172.25.133.171:52576
I1120 15:13:39.857028 1681264640 sched.cpp:334] Successfully authenticated with master master@172.25.133.171:52576
I1120 15:13:39.857139 1681264640 master.cpp:798] Received registration request from scheduler(1)@172.25.133.171:52576
I1120 15:13:39.857306 1681264640 master.cpp:816] Registering framework 201311201513-2877626796-52576-3234-0000 at scheduler(1)@172.25.133.171:52576
I1120 15:13:39.862296 1680191488 hierarchical_allocator_process.hpp:332] Added framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.863867 1680191488 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000
Registered! ID = 201311201513-2877626796-52576-3234-0000
Launching task 0
Launching task 1
Launching task 2
I1120 15:13:39.905390 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-0 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.905825 1680191488 master.hpp:400] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:39.905886 1680191488 master.cpp:2150] Launching task 0 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:39.906422 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-1 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.906664 1680191488 master.hpp:400] Adding task 1 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:39.906721 1680191488 master.cpp:2150] Launching task 1 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:39.907171 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-2 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.907419 1680191488 master.hpp:400] Adding task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local)
I1120 15:13:39.907480 1680191488 master.cpp:2150] Launching task 2 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local)
I1120 15:13:39.907938 1680191488 slave.cpp:722] Got assigned task 0 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.908473 1680191488 slave.cpp:833] Launching task 0 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.914427 1682874368 slave.cpp:722] Got assigned task 1 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.914594 1680728064 slave.cpp:722] Got assigned task 2 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.914844 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs
I1120 15:13:39.915292 1682874368 slave.cpp:833] Launching task 1 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.915424 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs
I1120 15:13:39.915685 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs
I1120 15:13:39.915828 1680728064 slave.cpp:833] Launching task 2 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.917840 1680191488 slave.cpp:943] Queuing task '0' for executor default of framework '201311201513-2877626796-52576-3234-0000
I1120 15:13:39.917935 1679118336 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4 with resources ' for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.922019 1679118336 process_isolator.cpp:163] Forked executor at 3268
I1120 15:13:39.922703 1679118336 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3268
I1120 15:13:39.929134 1682874368 slave.cpp:943] Queuing task '1' for executor default of framework '201311201513-2877626796-52576-3234-0000
I1120 15:13:39.929323 1682874368 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5 with resources ' for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.931243 1682874368 process_isolator.cpp:163] Forked executor at 3269
I1120 15:13:39.931612 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3269
E1120 15:13:39.931836 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched
I1120 15:13:39.936460 1680728064 slave.cpp:943] Queuing task '2' for executor default of framework '201311201513-2877626796-52576-3234-0000
I1120 15:13:39.936619 1681801216 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534 with resources ' for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.941299 1681801216 process_isolator.cpp:163] Forked executor at 3270
I1120 15:13:39.942179 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3270
E1120 15:13:39.942395 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched
Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5'
Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4'
Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534'
I1120 15:13:40.372573 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.373258 1681801216 slave.cpp:1527] Flushing queued task 1 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.388317 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.388983 1681801216 slave.cpp:1527] Flushing queued task 0 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.398084 1679654912 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.399344 1679654912 slave.cpp:1527] Flushing queued task 2 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
Registered executor on vkone.local
I1120 15:13:40.491843 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.492202 1679654912 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.492424 1679654912 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
Registered executor on vkone.local
I1120 15:13:40.492671 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.492735 1682337792 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
Status update: task 1 is in state TASK_RUNNING
I1120 15:13:40.502235 1679654912 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000
Registered executor on vkone.local
I1120 15:13:40.531292 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579
I1120 15:13:40.532091 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.532305 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.532776 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579
I1120 15:13:40.532951 1681801216 master.cpp:1452] Status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576
Status update: task 2 is in state TASK_RUNNING
I1120 15:13:40.538895 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.541267 1682874368 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.541555 1682874368 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.541725 1682874368 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.542196 1682874368 master.cpp:1452] Status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
I1120 15:13:40.542251 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
Status update: task 0 is in state TASK_RUNNING
I1120 15:13:40.545537 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000
Running task value: ""1""

I1120 15:13:40.764219 1682337792 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.764629 1682337792 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.764698 1682337792 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.765043 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.765192 1682337792 master.hpp:418] Removing task 1 with resources cpus(*):1; mem(*):128 on slave 2Status update: task 1 is in state TASK_FINISHED
Finished tasks: 1
01311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.765363 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
I1120 15:13:40.772738 1682337792 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-2 from framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.773190 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000
Running task value: ""0""

Running task value: ""2""

I1120 15:13:40.790068 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.790411 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.790493 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.790674 1679118336 master.cpp:1452] Status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
I1120 15:13:40.790798 1679118336 master.hpp:418] Removing task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.790928 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
Status update: task 0 is in state TASK_FINISHED
Finished tasks: 2
I1120 15:13:40.791225 1680191488 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.794234 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.795830 1681801216 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579
I1120 15:13:40.796111 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.796182 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.796352 1680728064 master.cpp:1452] Status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576
I1120 15:13:40.796398 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579
I1120 15:13:40.796466 1680728064 master.hpp:418] Removing task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local)
I1120 15:13:40.796707 1679118336 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-0 from framework 201311201513-2877626796-52576-3234-0000
Status update: task 2 is in state TASK_FINISHED
Finished tasks: 3
I1120 15:13:40.797384 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.824383 1681801216 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000
Launching task 3
Launching task 4
I1120 15:13:40.826971 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-3 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827268 1679118336 master.hpp:400] Adding task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.827348 1679118336 master.cpp:2150] Launching task 3 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.827487 1680728064 slave.cpp:722] Got assigned task 3 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827857 1680728064 slave.cpp:833] Launching task 3 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827913 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-4 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827986 1680728064 slave.cpp:968] Sending task '3' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.828126 1679118336 master.hpp:400] Adding task 4 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.828187 1679118336 master.cpp:2150] Launching task 4 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.828632 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-5 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.828655 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs
I1120 15:13:40.829005 1679118336 slave.cpp:722] Got assigned task 4 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.829027 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs
I1120 15:13:40.829260 1679118336 slave.cpp:833] Launching task 4 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.829273 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs
I1120 15:13:40.829390 1679118336 slave.cpp:968] Sending task '4' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000
Running task value: ""3""

Running task value: ""4""

I1120 15:13:40.839279 1682337792 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.839534 1679118336 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.839705 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.839944 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
Status update: task 3 is in state TASK_RUNNING
I1120 15:13:40.839947 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
I1120 15:13:40.856334 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.856650 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.856818 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.856875 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.857105 1679118336 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.857369 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
I1120 15:13:40.857498 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.857518 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
I1120 15:13:40.857635 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.857630 1682337792 master.hpp:418] Removing task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.857843 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.858043 1680728064 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.858098 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
Status update: task 3 is in state TASK_FINISHED
Finished tasks: 4
Status update: task 4 is in state TASK_RUNNING
I1120 15:13:40.858896 1682337792 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.858957 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.859905 1679654912 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.860174 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.860245 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.860437 1679654912 master.cpp:1452] Status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.860486 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
I1120 15:13:40.860550 1679654912 master.hpp:418] Removing task 4 with resources cpus(*):1; mem(Status update: task 4 is in state TASK_FINISHED
Finished tasks: 5
*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.863689 1679654912 master.cpp:996] Asked to unregister framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.863750 1679654912 master.cpp:2385] Removing framework 201311201513-2877626796-52576-3234-0000
../../src/tests/script.cpp:81: Failure
Failed
java_framework_test.sh terminated with signal 'Abort trap: 6'
[  FAILED  ] ExamplesTest.JavaFramework (2688 ms)
[----------] 1 test from ExamplesTest (2688 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (2692 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ExamplesTest.JavaFramework
"	MESOS	Resolved	3	1	4664	flaky, mesosphere
13100957	Design Doc for Extended KillPolicy	"After introducing the {{KillPolicy}} in MESOS-4909, some interactions with framework developers have led to the suggestion of a couple possible improvements to this interface. Namely,
* Allowing the framework to specify a command to be run to initiate termination, rather than a signal to be sent, would allow some developers to avoid wrapping their application in a signal handler. This is useful because a signal handler wrapper modifies the application's process tree, which may make introspection and debugging more difficult in the case of well-known services with standard debugging procedures.
* In the case of terminations which do begin with a signal, it would be useful to allow the framework to specify the signal to be sent, rather than assuming SIGTERM. PostgreSQL, for example, permits several shutdown types, each initiated with a [different signal|https://www.postgresql.org/docs/9.3/static/server-shutdown.html]."	MESOS	Resolved	3	4	4664	mesosphere
12958633	Update the documentation for '/reserve' and '/create-volumes'	There are a couple issues related to the {{principal}} field in {{DiskInfo}} and {{ReservationInfo}} (see linked JIRAs) that should be better documented. We need to help users understand the purpose of these fields and how they interact with the principal provided in the HTTP authentication header. See linked tickets for background.	MESOS	Resolved	3	20	4664	documentation, mesosphere
13151971	Expose Check and HealthCheck information on Mesos HTTP endpoints.	"Is the information about task health check definition not exposed on Mesos HTTP endpoints ({{/master/tasks}} or {{/slave/state}} ) for some specific reason? I'm working on integration with Hashicorp Consul and it would allow me to synchronize the definitions of health checks only by using HTTP API. If this information is not exposed by accident, I will gladly make a pull request.

Thisis related to both {{HealthCheck}} and {{CheckInfo}} in both {{v0}} and {{v1}} APIs."	MESOS	Accepted	4	4	4664	api, mesosphere
13138086	Make Docker executor/containerizer resilient to Docker daemon failures.	Experience has shown that the Docker CLI can hang indefinitely at times. There are many variations of this behavior, and it occurs across many versions of Docker. For these reasons, and since many users of Mesos still make heavy use of the Docker containerizer and the Docker executor, it will improve the user experience to make the Docker containerizer/executor resilient to such Docker daemon failures.	MESOS	Accepted	3	15	4664	mesosphere
13158621	Master crash when removing quota.	"The allocator can crash when quota is removed due to a race between the removal and the quota_allocated metric computation. If the metric dispatches before the quota removal can remove the metric, then this crash occurs:

{noformat}
May 10 20:15:28 int-master1.sanitized.mesosphe.re mesos-master[7189]: F0510 20:15:28.821099  7205 sorter.cpp:395] Check failed: 'find(clientPath)' Must be non NULL
May 10 20:15:28 int-master1.sanitized.mesosphe.re mesos-master[7189]: *** Check failure stack trace: ***
May 10 20:15:28 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d843bdcd  google::LogMessage::Fail()
May 10 20:15:28 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d843dbfd  google::LogMessage::SendToLog()
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d843b9bc  google::LogMessage::Flush()
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d843e4f9  google::LogMessageFatal::~LogMessageFatal()
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d791f79d  google::CheckNotNull<>()
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d791a3c4  mesos::internal::master::allocator::DRFSorter::allocationScalarQuantities()
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d7900bc9  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::_quota_allocated()
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d79182f9  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEESt5_BindIFZNS0_8dispatchIdN5mesos8internal6master9allocator8internal28HierarchicalAllocatorProcessERKSsSD_SD_SD_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSI_FSF_T1_T2_EOT3_OT4_EUlRSsSU_S2_E_SsSsSt12_PlaceholderILi1EEEEE9_M_invokeERKSt9_Any_dataS2_
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d83a6eac  process::ProcessManager::resume()
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d83ac826  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d63d12b0  (unknown)
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d5befe25  start_thread
May 10 20:15:29 int-master1.sanitized.mesosphe.re mesos-master[7189]: @     0x7fd7d591d34d  __clone
{noformat}"	MESOS	Resolved	1	1	4664	mesosphere, mesosphere-oncall
12923184	Add an example bug due to a lack of defer() to the defer() documentation	In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation.	MESOS	Resolved	4	20	4664	documentation, libprocess, mesosphere
12912395	Add authorization for '/create-volume' and '/destroy-volume' HTTP endpoints	"This is the fourth in a series of tickets that adds authorization support for persistent volumes.

We need to add ACL authorization for the '/create-volume' and '/destroy-volume' HTTP endpoints. In other complementary work, authorization for frameworks performing {{CREATE}} and {{DESTROY}} operations is being added by MESOS-3065.

This will consist of adding authorization calls into the HTTP endpoint code in {{src/master/http.cpp}}, as well as tests for both failed & successful calls to '/create-volumes' and '/destroy-volumes' with authorization. We also must ensure that the {{principal}} field of {{Resource.DiskInfo.Persistence}} is being populated correctly."	MESOS	Resolved	3	4	4664	mesosphere, persistent-volumes
12914813	LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8.	"sudo ./bin/mesos-test.sh --gtest_filter=""LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs""

{noformat}
...
F1119 14:34:52.514742 30706 isolator_tests.cpp:455] CHECK_SOME(isolator): Failed to find 'cpu.cfs_quota_us'. Your kernel might be too old to use the CFS cgroups feature.
{noformat}
"	MESOS	Resolved	1	1	4664	mesosphere
13038043	Add authentication support to agent's '/v1/executor' endpoint	The new agent flag {{--authenticate_http_executors}} must be added. When set, it will require that requests received on the {{/v1/executor}} endpoint be authenticated, and the default JWT authenticator will be loaded. Note that this will require the addition of a new authentication realm for that endpoint.	MESOS	Resolved	3	3	4664	agent, executor, mesosphere, security
13129474	RP manager incorrectly setting framework ID leads to CHECK failure	The resource provider manager [unconditionally sets the framework ID|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/manager.cpp#L637] when forwarding operation status updates to the agent. This is incorrect, for example, when the resource provider [generates OPERATION_DROPPED updates during reconciliation|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/storage/provider.cpp#L1653-L1657], and leads to protobuf errors in this case since the framework ID's required {{value}} field is left unset.	MESOS	Resolved	1	1	4664	mesosphere
13237760	Agent kills all tasks when draining	The agent's {{DrainSlaveMessage}} handler should kill all tasks when draining is initiated, specifying a kill policy with a grace period equal to the minimum of the task's grace period and the min_grace_period specified in the drain message.	MESOS	Resolved	3	3	4664	foundations, mesosphere
12980658	Creating a persistent volume through the operator endpoints fail and doesn't produce meaningful logs.	"When attempting to create a persistent volume via the /create-volumes operator endpoint. I get a HTTP 200  from the master and in the logs on the master I see:

{noformat}
http.cpp:312] HTTP POST for /master/create-volumes from ""172.16.10.11:40686 with User-Agent='curl/7.29.0' ""
{noformat}

then next line I see on the master is:
{noformat}
""master.cpp:6560] Sending checkpointed resources  to slave 0ef7d2e1-8b0d-44d4-8db0-cc58ac2058af-S0 at slave(1)@172.16.10.4:5051""
{noformat}

Now if I look in the logs on the slave that was specified in the request to create a persistent volume I see:

then on the slave I see:
{noformat}
 ""1572 slave.cpp:2327] Updated checkpointed resources from  to   ""
{noformat}

Notice that from destination and a to destination are both missing specifically, they should be the valueos of:

checkpointedResources and newCheckpointedResources, from here:
https://github.com/apache/mesos/blob/master/src/slave/slave.cpp#L2582

I am currently running only one slave for troubleshooting purposes, the resource file on the slave with the disk resource looks like the following:

#resources=file:///etc/default/mesos.resources.json

{noformat}
[
   {
    ""name"": ""disk"",
    ""type"": ""SCALAR"",
    ""scalar"": {
      ""value"": 50000
    }
  },
   {
      ""name"":""disk"",
      ""type"":""SCALAR"",
      ""scalar"":{
         ""value"":1000000
      },
      ""role"":""testing"",
      ""disk"":{
         ""source"":{
            ""type"":""MOUNT"",
            ""mount"":{
               ""root"":""/data""
            }
         }
      }
   },
   {
      ""name"":""cpus"",
      ""type"":""SCALAR"",
      ""scalar"":{
         ""value"":16
      },
      ""role"":""testing""
   },
   {
      ""name"":""mem"",
      ""type"":""SCALAR"",
      ""scalar"":{
         ""value"":128000
      },
      ""role"":""testing""
   },
   {
      ""name"":""ports"",
      ""type"":""RANGES"",
      ""ranges"":{
         ""range"":[
            {
               ""begin"":31000,
               ""end"":32000
            }
         ]
      },
      ""role"":""testing""
   }
]
{noformat}

When I {{curl master:5050/slaves | jq '.'}} and look under the key {{reserved_resources_full}}, I see the above resources on that slave. 

Here is my request to via the operator endpoint {{/create-resources}}, I am trying to create a persistent volume on the disk of type MOUNT above, which is in {{/proc/mounts}} as {{/data}}:

{noformat}
curl -i  -d slaveId=0ee7d2e7-8b0d-44d4-8d80-cc58ac2058ae-S4     \      
          -d volumes='[
          {
            ""name"": ""testvol"",
            ""type"": ""SCALAR"",
            ""scalar"": { ""value"": 10000 },
            ""role"": ""testing"",
            ""disk"": {
             ""source"": {
               ""type"" : ""MOUNT"",
                ""path"" : { ""root"" : ""/data"" }
             },
              ""persistence"": {
               ""id"" : ""cliff""
             },
              ""volume"": {
               ""mode"": ""RW"",
               ""container_path"": ""/data""
              }
            }
          }
        ]' -X POST http://master:5050/master/create-volumes
{noformat}
        
{noformat}
HTTP/1.1 200 OK
Date: Sun, 19 Jun 2016 04:38:45 GMT
{noformat}

If look at the slave specified with slaveID above via:

{noformat}
curl - http://slave1:5051/state  
{noformat}

I will not see the volume created. Also here are no errors in the INFO logs on either the master or slave relating to this request. The only log entries are those that I have provided. 

The same problem/behavior seems to exist when trying creating persistent volumes on dynamically reserved resources as well.

My steps were:
systemctl stop meso-slave
cd /var/mesos
rm -rf meta
systemctl start mesos-slave

then I issued the following to the /reserve operator endpoint:

{noformat}

curl -i \
      -d slaveId=0ee7d2b7-7b0d-44d4-8d80-cc51ac2058ae-S0 \
      -d resources='[

        {
            ""name"": ""disk"",
            ""type"": ""SCALAR"",
            ""scalar"": { ""value"": 10000 },
            ""disk"": {
             ""source"": {
               ""type"" : ""MOUNT"",
                ""path"" : { ""root"" : ""/data"" }
             },
              ""persistence"": {
               ""id"" : ""testing""
             },
              ""volume"": {
               ""mode"": ""RW"",
               ""container_path"": ""/data""
              }
            }
          }
          ]' \
          -X POST http://master:5050/master/reserve
{noformat}

The volume will never get created, there will be no error logged anywhere on the master or slave and I will only see the following on the slave, the same as when attempting to create a persistent volume on statically defined resources:

{noformat}
5558 slave.cpp:2327] Updated checkpointed resources from  to
{noformat}

I also tried enabling auth to rule out that possibly being a factor. Steps taken:

{noformat}
/etc/default/mesos-master:
 
export authenticate_http=true
export credentials=""/etc/default/credentials.json""

/etc/default/credentials.json
{
   ""credentials"" : [
     {
       ""principal"": ""test"",
       ""secret"": ""test""
     }
   ]
 }   
{noformat}

restart masters with ""systemctl restart mesos-master""

{noformat}
# curl -i \
>      -u test:test \
>      -d slaveId=af6e2f17-3d53-4656-a6ce-49658b6b4db3-S0 \
>      -d resources='[
>         {
>           ""name"": ""disk"",
>           ""type"": ""SCALAR"",
>           ""scalar"": { ""value"": 1024 },
>           ""reservation"": {
>             ""principal"": ""test""
>            }
>         }
>       ]' \
>            -X POST http://master:5050/master/reserve
HTTP/1.1 200 OK

{noformat}

The result is the same, if look at the output of""
{noformat}
http://master:5050/slaves 
{noformat}

I won't see anything reserved:

{noformat}
reserved_resources_full"": {},
{noformat}

and again in the logs on the one slave that is currently active I will see:

{noformat}
slave.cpp:2327] Updated checkpointed resources from  to
{noformat}

and no further information either on the slave agent or the master.

Whether or not I specify  a role doesn't have any effect:

{noformat}

curl -i \
     -u test:test \
       -d slaveId=af6e2f17-3d53-4656-a6ce-49658b6b4db3-S0 \
       -d resources='[

         {
             ""name"": ""disk"",
             ""type"": ""SCALAR"",
             ""scalar"": { ""value"": 10000 },
             ""role"": ""test"",
             ""disk"": {
              ""source"": {
                ""type"" : ""MOUNT"",
                 ""path"" : { ""root"" : ""/data"" }
              },
               ""persistence"": {
                ""id"" : ""testing""
              },
               ""volume"": {
                ""mode"": ""RW"",
                ""container_path"": ""/data""
               }
             }
           }
           ]' \
           -X POST http://master:5050/master/reserve

HTTP/1.1 200 OK
Date: Mon, 20 Jun 2016 21:32:17 GMT
Content-Length: 0


curl http://master:5050/slaves | jq '.' | grep full
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   590  100   590    0     0   158k      0 --:--:-- --:--:-- --:--:--  192k

      ""reserved_resources_full"": {},
      ""used_resources_full"": [],
      ""offered_resources_full"": []

{noformat}"	MESOS	Resolved	3	1	4664	persistent-volumes
12928743	PersistentVolumeTest.BadACLNoPrincipal is flaky	"https://builds.apache.org/job/Mesos/1457/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull

{noformat}
[ RUN      ] PersistentVolumeTest.BadACLNoPrincipal
I0108 01:13:16.117883  1325 leveldb.cpp:174] Opened db in 2.614722ms
I0108 01:13:16.118650  1325 leveldb.cpp:181] Compacted db in 706567ns
I0108 01:13:16.118702  1325 leveldb.cpp:196] Created db iterator in 24489ns
I0108 01:13:16.118723  1325 leveldb.cpp:202] Seeked to beginning of db in 2436ns
I0108 01:13:16.118738  1325 leveldb.cpp:271] Iterated through 0 keys in the db in 397ns
I0108 01:13:16.118793  1325 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0108 01:13:16.119627  1348 recover.cpp:447] Starting replica recovery
I0108 01:13:16.120352  1348 recover.cpp:473] Replica is in EMPTY status
I0108 01:13:16.121750  1357 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (7084)@172.17.0.2:32801
I0108 01:13:16.122297  1353 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0108 01:13:16.122747  1350 recover.cpp:564] Updating replica status to STARTING
I0108 01:13:16.123625  1354 master.cpp:365] Master 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2 (d9632dd1c41e) started on 172.17.0.2:32801
I0108 01:13:16.123946  1347 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 728242ns
I0108 01:13:16.123999  1347 replica.cpp:320] Persisted replica status to STARTING
I0108 01:13:16.123708  1354 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""test-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/f2rA75/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/f2rA75/master"" --zk_session_timeout=""10secs""
I0108 01:13:16.124219  1354 master.cpp:414] Master allowing unauthenticated frameworks to register
I0108 01:13:16.124236  1354 master.cpp:417] Master only allowing authenticated slaves to register
I0108 01:13:16.124248  1354 credentials.hpp:35] Loading credentials for authentication from '/tmp/f2rA75/credentials'
I0108 01:13:16.124294  1358 recover.cpp:473] Replica is in STARTING status
I0108 01:13:16.124644  1354 master.cpp:456] Using default 'crammd5' authenticator
I0108 01:13:16.124820  1354 master.cpp:493] Authorization enabled
W0108 01:13:16.124843  1354 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0108 01:13:16.125154  1348 hierarchical.cpp:147] Initialized hierarchical allocator process
I0108 01:13:16.125334  1345 whitelist_watcher.cpp:77] No whitelist given
I0108 01:13:16.126065  1346 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (7085)@172.17.0.2:32801
I0108 01:13:16.126806  1348 recover.cpp:193] Received a recover response from a replica in STARTING status
I0108 01:13:16.128237  1354 recover.cpp:564] Updating replica status to VOTING
I0108 01:13:16.128402  1359 master.cpp:1629] The newly elected leader is master@172.17.0.2:32801 with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2
I0108 01:13:16.128489  1359 master.cpp:1642] Elected as the leading master!
I0108 01:13:16.128523  1359 master.cpp:1387] Recovering from registrar
I0108 01:13:16.128756  1355 registrar.cpp:307] Recovering registrar
I0108 01:13:16.129259  1344 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 531437ns
I0108 01:13:16.129292  1344 replica.cpp:320] Persisted replica status to VOTING
I0108 01:13:16.129425  1358 recover.cpp:578] Successfully joined the Paxos group
I0108 01:13:16.129680  1358 recover.cpp:462] Recover process terminated
I0108 01:13:16.130187  1358 log.cpp:659] Attempting to start the writer
I0108 01:13:16.131613  1352 replica.cpp:493] Replica received implicit promise request from (7086)@172.17.0.2:32801 with proposal 1
I0108 01:13:16.131983  1352 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 333646ns
I0108 01:13:16.132004  1352 replica.cpp:342] Persisted promised to 1
I0108 01:13:16.132627  1348 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0108 01:13:16.133896  1349 replica.cpp:388] Replica received explicit promise request from (7087)@172.17.0.2:32801 for position 0 with proposal 2
I0108 01:13:16.134289  1349 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 349652ns
I0108 01:13:16.134317  1349 replica.cpp:712] Persisted action at 0
I0108 01:13:16.135470  1351 replica.cpp:537] Replica received write request for position 0 from (7088)@172.17.0.2:32801
I0108 01:13:16.135537  1351 leveldb.cpp:436] Reading position from leveldb took 36181ns
I0108 01:13:16.135901  1351 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 308752ns
I0108 01:13:16.135924  1351 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136529  1347 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0108 01:13:16.136889  1347 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 327106ns
I0108 01:13:16.136916  1347 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136943  1347 replica.cpp:697] Replica learned NOP action at position 0
I0108 01:13:16.137707  1359 log.cpp:675] Writer started with ending position 0
I0108 01:13:16.138844  1348 leveldb.cpp:436] Reading position from leveldb took 31371ns
I0108 01:13:16.139878  1356 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I0108 01:13:16.140012  1356 registrar.cpp:439] Applied 1 operations in 42063ns; attempting to update the 'registry'
I0108 01:13:16.140797  1355 log.cpp:683] Attempting to append 170 bytes to the log
I0108 01:13:16.140974  1345 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0108 01:13:16.141744  1354 replica.cpp:537] Replica received write request for position 1 from (7089)@172.17.0.2:32801
I0108 01:13:16.142226  1354 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 441971ns
I0108 01:13:16.142251  1354 replica.cpp:712] Persisted action at 1
I0108 01:13:16.142860  1351 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0108 01:13:16.143198  1351 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 305928ns
I0108 01:13:16.143223  1351 replica.cpp:712] Persisted action at 1
I0108 01:13:16.143241  1351 replica.cpp:697] Replica learned APPEND action at position 1
I0108 01:13:16.144271  1354 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.144435  1354 registrar.cpp:370] Successfully recovered registrar
I0108 01:13:16.144567  1359 log.cpp:702] Attempting to truncate the log to 1
I0108 01:13:16.144780  1359 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0108 01:13:16.144989  1348 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I0108 01:13:16.144928  1354 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0108 01:13:16.145690  1357 replica.cpp:537] Replica received write request for position 2 from (7090)@172.17.0.2:32801
I0108 01:13:16.146072  1357 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 345113ns
I0108 01:13:16.146097  1357 replica.cpp:712] Persisted action at 2
I0108 01:13:16.146667  1358 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0108 01:13:16.147060  1358 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 283648ns
I0108 01:13:16.147116  1358 leveldb.cpp:399] Deleting ~1 keys from leveldb took 32174ns
I0108 01:13:16.147135  1358 replica.cpp:712] Persisted action at 2
I0108 01:13:16.147153  1358 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0108 01:13:16.166832  1325 containerizer.cpp:139] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0108 01:13:16.167556  1325 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0108 01:13:16.170526  1349 slave.cpp:191] Slave started on 231)@172.17.0.2:32801
I0108 01:13:16.170718  1349 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY""
I0108 01:13:16.171269  1349 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential'
I0108 01:13:16.171505  1349 slave.cpp:322] Slave using credential for: test-principal
I0108 01:13:16.171747  1349 resources.cpp:481] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I0108 01:13:16.172266  1349 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.172327  1349 slave.cpp:400] Slave attributes: [  ]
I0108 01:13:16.172340  1349 slave.cpp:405] Slave hostname: d9632dd1c41e
I0108 01:13:16.172353  1349 slave.cpp:410] Slave checkpoint: true
I0108 01:13:16.173418  1353 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta'
I0108 01:13:16.173521  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.174054  1345 status_update_manager.cpp:200] Recovering status update manager
I0108 01:13:16.174289  1353 containerizer.cpp:387] Recovering containerizer
I0108 01:13:16.174295  1356 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.174387  1356 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.174409  1356 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.174515  1356 sched.cpp:755] Will retry registration in 1.699889272secs if necessary
I0108 01:13:16.174653  1349 master.cpp:2197] Received SUBSCRIBE call for framework 'no-principal' at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.174823  1349 master.cpp:1668] Authorizing framework principal '' to receive offers for role 'role1'
I0108 01:13:16.175250  1347 master.cpp:2268] Subscribing framework no-principal with checkpointing disabled and capabilities [  ]
I0108 01:13:16.175359  1353 slave.cpp:4429] Finished recovery
I0108 01:13:16.175715  1345 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175734  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175792  1345 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.175833  1345 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.175853  1353 slave.cpp:4601] Querying resource estimator for oversubscribable resources
I0108 01:13:16.175869  1345 hierarchical.cpp:1079] Performed allocation for 0 slaves in 127881ns
I0108 01:13:16.175923  1351 sched.cpp:663] Scheduler::registered took 27956ns
I0108 01:13:16.176110  1353 slave.cpp:729] New master detected at master@172.17.0.2:32801
I0108 01:13:16.176187  1353 slave.cpp:792] Authenticating with master master@172.17.0.2:32801
I0108 01:13:16.176216  1353 slave.cpp:797] Using default CRAM-MD5 authenticatee
I0108 01:13:16.176398  1357 status_update_manager.cpp:174] Pausing sending status updates
I0108 01:13:16.176404  1353 slave.cpp:765] Detecting new master
I0108 01:13:16.176463  1358 authenticatee.cpp:121] Creating new client SASL connection
I0108 01:13:16.176553  1353 slave.cpp:4615] Received oversubscribable resources  from the resource estimator
I0108 01:13:16.176709  1353 master.cpp:5445] Authenticating slave(231)@172.17.0.2:32801
I0108 01:13:16.176823  1359 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.177135  1348 authenticator.cpp:98] Creating new server SASL connection
I0108 01:13:16.177373  1356 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0108 01:13:16.177399  1356 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0108 01:13:16.177502  1344 authenticator.cpp:203] Received SASL authentication start
I0108 01:13:16.177563  1344 authenticator.cpp:325] Authentication requires more steps
I0108 01:13:16.177680  1346 authenticatee.cpp:258] Received SASL authentication step
I0108 01:13:16.177848  1354 authenticator.cpp:231] Received SASL authentication step
I0108 01:13:16.177883  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0108 01:13:16.177894  1354 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0108 01:13:16.177944  1354 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0108 01:13:16.177994  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0108 01:13:16.178014  1354 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178040  1354 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178066  1354 authenticator.cpp:317] Authentication success
I0108 01:13:16.178256  1355 authenticatee.cpp:298] Authentication success
I0108 01:13:16.178315  1354 master.cpp:5475] Successfully authenticated principal 'test-principal' at slave(231)@172.17.0.2:32801
I0108 01:13:16.178356  1355 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.178710  1354 slave.cpp:860] Successfully authenticated with master master@172.17.0.2:32801
I0108 01:13:16.178865  1354 slave.cpp:1254] Will retry registration in 13.009431ms if necessary
I0108 01:13:16.179138  1350 master.cpp:4154] Registering slave at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.179628  1345 registrar.cpp:439] Applied 1 operations in 71663ns; attempting to update the 'registry'
I0108 01:13:16.180505  1356 log.cpp:683] Attempting to append 343 bytes to the log
I0108 01:13:16.180711  1352 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0108 01:13:16.181499  1350 replica.cpp:537] Replica received write request for position 3 from (7103)@172.17.0.2:32801
I0108 01:13:16.182080  1350 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 537757ns
I0108 01:13:16.182112  1350 replica.cpp:712] Persisted action at 3
I0108 01:13:16.182749  1351 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0108 01:13:16.183120  1351 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 340999ns
I0108 01:13:16.183151  1351 replica.cpp:712] Persisted action at 3
I0108 01:13:16.183177  1351 replica.cpp:697] Replica learned APPEND action at position 3
I0108 01:13:16.184787  1348 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.185287  1348 log.cpp:702] Attempting to truncate the log to 3
I0108 01:13:16.185484  1349 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0108 01:13:16.186043  1353 slave.cpp:3371] Received ping from slave-observer(230)@172.17.0.2:32801
I0108 01:13:16.186074  1345 master.cpp:4222] Registered slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.186224  1353 slave.cpp:904] Registered with master master@172.17.0.2:32801; given slave ID 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.186441  1353 fetcher.cpp:81] Clearing fetcher cache
I0108 01:13:16.186486  1349 hierarchical.cpp:465] Added slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I0108 01:13:16.186658  1346 status_update_manager.cpp:181] Resuming sending status updates
I0108 01:13:16.186885  1353 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta/slaves/773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0/slave.info'
I0108 01:13:16.186905  1350 replica.cpp:537] Replica received write request for position 4 from (7104)@172.17.0.2:32801
I0108 01:13:16.187595  1350 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 645704ns
I0108 01:13:16.187628  1350 replica.cpp:712] Persisted action at 4
I0108 01:13:16.188347  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.188475  1349 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 1.861833ms
I0108 01:13:16.188560  1348 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0108 01:13:16.188385  1353 slave.cpp:963] Forwarding total oversubscribed resources 
I0108 01:13:16.189275  1344 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.189792  1344 master.cpp:4564] Received update of slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with total oversubscribed resources 
I0108 01:13:16.189851  1348 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.204958ms
I0108 01:13:16.190150  1348 leveldb.cpp:399] Deleting ~2 keys from leveldb took 62381ns
I0108 01:13:16.190265  1348 replica.cpp:712] Persisted action at 4
I0108 01:13:16.190402  1348 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0108 01:13:16.191192  1349 sched.cpp:819] Scheduler::resourceOffers took 126783ns
I0108 01:13:16.191253  1359 hierarchical.cpp:521] Slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I0108 01:13:16.191529  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.191591  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.191627  1359 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 310808ns
I0108 01:13:16.195103  1349 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.195171  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.195205  1349 hierarchical.cpp:1079] Performed allocation for 1 slaves in 368834ns
I0108 01:13:16.205402  1351 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O0 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.205471  1351 master.cpp:2843] Authorizing principal 'ANY' to create volumes
E0108 01:13:16.206641  1351 master.cpp:1737] Dropping CREATE offer operation from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801: Not authorized to create persistent volumes as ''
I0108 01:13:16.207283  1351 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.216485  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.216562  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 983574ns
I0108 01:13:16.216915  1345 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.217514  1345 sched.cpp:819] Scheduler::resourceOffers took 82354ns
I0108 01:13:16.227466  1348 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O1 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.227843  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.228489  1344 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.228989  1346 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.229118  1346 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.229143  1346 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.229277  1346 sched.cpp:755] Will retry registration in 1.383902465secs if necessary
I0108 01:13:16.229912  1348 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.230171  1346 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.230262  1348 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.230370  1348 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0108 01:13:16.230788  1348 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0108 01:13:16.231477  1346 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.232698  1346 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.232795  1346 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.282992ms
I0108 01:13:16.233512  1348 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.233728  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.233800  1351 sched.cpp:663] Scheduler::registered took 29498ns
I0108 01:13:16.234381  1359 sched.cpp:819] Scheduler::resourceOffers took 113212ns
I0108 01:13:16.239941  1348 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.240223  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.240275  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 633949ns
I0108 01:13:16.251688  1357 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O2 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.251785  1357 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
I0108 01:13:16.253445  1352 master.cpp:3384] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.253911  1352 master.cpp:6508] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.255210  1352 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I0108 01:13:16.257128  1356 hierarchical.cpp:642] Updated allocation of framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I0108 01:13:16.257844  1356 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.262976  1344 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.263068  1344 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.435723ms
I0108 01:13:16.263535  1353 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.264181  1356 sched.cpp:819] Scheduler::resourceOffers took 139353ns
I0108 01:13:16.271931  1355 master.cpp:3671] Processing REVIVE call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.272141  1359 hierarchical.cpp:973] Removed offer filters for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.272177  1355 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O3 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272423  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.272483  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.272514  1359 hierarchical.cpp:1079] Performed allocation for 1 slaves in 344563ns
I0108 01:13:16.272924  1355 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272989  1359 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.273309  1359 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
2016-01-08 01:13:18,959:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:22,295:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:25,631:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:28,968:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1211: Failure
Failed to wait 15secs for offers
I0108 01:13:31.277577  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 disconnected
I../../src/tests/persistent_volume_tests.cpp:1204: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
0108 01:13:31.277909  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279088  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279496  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280046  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 disconnected
I0108 01:13:31.280603  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280644  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280863  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280563  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.281056  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.281097  1354 master.cpp:930] Master terminating
I0108 01:13:31.281910  1355 hierarchical.cpp:496] Removed slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:31.282516  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.282817  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.282985  1352 slave.cpp:3417] master@172.17.0.2:32801 exited
W0108 01:13:31.283144  1352 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I0108 01:13:31.313812  1346 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLNoPrincipal (15203 ms)
{noformat}"	MESOS	Resolved	3	1	4664	flaky-test
13112027	Add new protobuf messages for offer operation feedback	We should add the necessary protobuf messages for offer operation feedback as detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#].	MESOS	Resolved	3	3	4664	mesosphere
13242621	Mesos did not respond correctly when operations should fail	"For testing persistent volumes with {{OPERATION_FAILED/ERROR}} feedbacks, we sshed into the mesos-agent and made it unable to create subdirectories in {{/srv/mesos/work/volumes}}, however, mesos did not respond any operation failed response. Instead, we received {{OPERATION_FINISHED}} feedback.

Steps to recreate the issue:

1. Ssh into a magent.
 2. Make it impossible to create a persistent volume (we expect the agent to crash and reregister, and the master to release that the operation is {{OPERATION_DROPPED}}):
 * cd /srv/mesos/work (if it doesn't exist mkdir /srv/mesos/work/volumes)
 * chattr -RV +i volumes (then no subdirectories can be created)

3. Launch a service with persistent volumes with the constraint of only using the magent modified above.





Logs for the scheduler for receiving `OPERATION_FINISHED`:

(Also see screenshot)



2019-06-27 21:57:11.879 [12768651|rdar://12768651] [Jarvis-mesos-dispatcher-105] INFO c.a.j.s.ServicePodInstance - Stored operation=4g3k02s1gjb0q_5f912b59-a32d-462c-9c46-8401eba4d2c1 and feedback=OPERATION_FINISHED in podInstanceID=4g3k02s1gjb0q on serviceID=yifan-badagents-1



* 2019-06-27 21:55:23: task reached state TASK_FAILED for mesos reason: REASON_CONTAINER_LAUNCH_FAILED with mesos message: Failed to launch container: Failed to change the ownership of the persistent volume at '/srv/mesos/work/volumes/roles/test-2/19b564e8-3a90-4f2f-981d-b3dd2a5d9f90' with uid 264 and gid 264: No such file or directory"	MESOS	Resolved	1	1	4664	foundations, mesosphere
13024618	Nested containers can become unkillable	"An incident occurred recently in a cluster running a build of Mesos based on commit {{757319357471227c0a1e906076eae8f9aa2fdbd6}} from master. A task group of five tasks was launched via Marathon. After the tasks were launched, one of the containers quickly exited and was successfully destroyed. A couple minutes later, the task group was killed manually via Marathon, and the agent can then be seen repeatedly attempting to kill the tasks for hours. No calls to {{WAIT_NESTED_CONTAINER}} are visible in the agent logs, and the executor logs do not indicate at any point that the nested containers were launched successfully.

Agent logs:
{code}
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.890911  6406 slave.cpp:1539] Got assigned task group containing tasks [ dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5 ] for framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.892299  6406 gc.cpp:83] Unscheduling '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000' from gc
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.892379  6406 gc.cpp:83] Unscheduling '/var/lib/mesos/slave/meta/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000' from gc
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.893131  6405 slave.cpp:1701] Launching task group containing tasks [ dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5 ] for framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.893435  6405 paths.cpp:536] Trying to chown '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91' to user 'root'
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.898026  6405 slave.cpp:6179] Launching executor 'instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001' of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000 with resources cpus(*):0.1; mem(*):32; disk(*):10; ports(*):[21421-21425] in work directory '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91'
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.898731  6407 docker.cpp:1000] Skipping non-docker container
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.899050  6407 containerizer.cpp:938] Starting container 8750c2a7-8bef-4a69-8ef2-b873f884bf91 for executor 'instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001' of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.899909  6405 slave.cpp:1987] Queued task group containing tasks [ dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5 ] for executor 'instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001' of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.907033  6405 memory.cpp:451] Started listening for OOM events for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.908336  6405 memory.cpp:562] Started listening on 'low' memory pressure events for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.909638  6405 memory.cpp:562] Started listening on 'medium' memory pressure events for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.910923  6405 memory.cpp:562] Started listening on 'critical' memory pressure events for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.912464  6405 memory.cpp:199] Updated 'memory.soft_limit_in_bytes' to 32MB for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.914640  6405 memory.cpp:251] Updated 'memory.limit_in_bytes' to 32MB for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.915769  6405 cpu.cpp:101] Updated 'cpu.shares' to 102 (cpus 0.1) for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.917944  6405 cpu.cpp:121] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 10ms (cpus 0.1) for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.918515  6404 isolator_module.cpp:55] Container prepare: container_id[value: ""8750c2a7-8bef-4a69-8ef2-b873f884bf91""] container_config[directory: ""/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91"" user: ""root"" executor_info { executor_id { value: ""instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001"" } resources { name: ""cpus"" type: SCALAR scalar { value: 0.1 } } resources { name: ""mem"" type: SCALAR scalar { value: 32 } } resources { name: ""disk"" type: SCALAR scalar { value: 10 } } resources { name: ""ports"" type: RANGES ranges { range { begin: 21421 end: 21425 } } role: ""*"" } command { value: ""/opt/mesosphere/packages/mesos--a00089c894f82b6455d1db7c4267a742458020b3/libexec/mesos/mesos-default-executor"" user: ""root"" shell: false arguments: ""mesos-default-executor"" } framework_id { value: ""ce4bd8be-1198-4819-81d4-9a8439439741-0000"" } container { type: MESOS network_infos { labels { labels { key: ""rcluster.location"" value: ""globalqa.las2.test0"" } labels { key: ""rcluster.group"" value: ""dat"" } labels { key: ""rcluster.application"" value: ""scout"" } } name: ""rcluster-cni"" port_mappings { host_port: 21421 container_port: 8080 protocol: ""tcp"" } port_mappings { host_port: 21422 container_port: 8090 protocol: ""tcp"" } port_mappings { host_port: 21423 container_port: 8093 protocol: ""tcp"" } port_mappings { host_port: 21424 container_port: 8090 protocol: ""tcp"" } port_mappings { host_port: 21425 container_port: 8090 protocol: ""tcp"" } } } labels { } type: DEFAULT } command_info { value: ""/opt/mesosphere/packages/mesos--a00089c894f82b6455d1db7c4267a742458020b3/libexec/mesos/mesos-default-executor"" user: ""root"" shell: false arguments: ""mesos-default-executor"" } container_info { type: MESOS network_infos { labels { labels { key: ""rcluster.location"" value: ""globalqa.las2.test0"" } labels { key: ""rcluster.group"" value: ""dat"" } labels { key: ""rc
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: luster.application"" value: ""scout"" } } name: ""rcluster-cni"" port_mappings { host_port: 21421 container_port: 8080 protocol: ""tcp"" } port_mappings { host_port: 21422 container_port: 8090 protocol: ""tcp"" } port_mappings { host_port: 21423 container_port: 8093 protocol: ""tcp"" } port_mappings { host_port: 21424 container_port: 8090 protocol: ""tcp"" } port_mappings { host_port: 21425 container_port: 8090 protocol: ""tcp"" } } } resources { name: ""cpus"" type: SCALAR scalar { value: 0.1 } } resources { name: ""mem"" type: SCALAR scalar { value: 32 } } resources { name: ""disk"" type: SCALAR scalar { value: 10 } } resources { name: ""ports"" type: RANGES ranges { range { begin: 21421 end: 21425 } } role: ""*"" }]
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.918647  6404 container_assigner.cpp:64] Registering and retrieving endpoint for container_id[value: ""8750c2a7-8bef-4a69-8ef2-b873f884bf91""] executor_info[executor_id { value: ""instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001"" } resources { name: ""cpus"" type: SCALAR scalar { value: 0.1 } } resources { name: ""mem"" type: SCALAR scalar { value: 32 } } resources { name: ""disk"" type: SCALAR scalar { value: 10 } } resources { name: ""ports"" type: RANGES ranges { range { begin: 21421 end: 21425 } } role: ""*"" } command { value: ""/opt/mesosphere/packages/mesos--a00089c894f82b6455d1db7c4267a742458020b3/libexec/mesos/mesos-default-executor"" user: ""root"" shell: false arguments: ""mesos-default-executor"" } framework_id { value: ""ce4bd8be-1198-4819-81d4-9a8439439741-0000"" } container { type: MESOS network_infos { labels { labels { key: ""rcluster.location"" value: ""globalqa.las2.test0"" } labels { key: ""rcluster.group"" value: ""dat"" } labels { key: ""rcluster.application"" value: ""scout"" } } name: ""rcluster-cni"" port_mappings { host_port: 21421 container_port: 8080 protocol: ""tcp"" } port_mappings { host_port: 21422 container_port: 8090 protocol: ""tcp"" } port_mappings { host_port: 21423 container_port: 8093 protocol: ""tcp"" } port_mappings { host_port: 21424 container_port: 8090 protocol: ""tcp"" } port_mappings { host_port: 21425 container_port: 8090 protocol: ""tcp"" } } } labels { } type: DEFAULT].
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.918715  6404 sync_util.hpp:39] Dispatching and waiting <=5s for ticket 431: register_and_update_cache
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.918865  6488 container_reader_impl.cpp:34] Reader constructed for 198.51.100.1:0
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.919052  6488 container_reader_impl.cpp:103] Reader listening on 198.51.100.1:51243
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.919108  6488 container_assigner_strategy.cpp:156] New ephemeral-port reader for container[value: ""8750c2a7-8bef-4a69-8ef2-b873f884bf91""] created at endpoint[198.51.100.1:51243].
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.919144  6488 container_state_cache_impl.cpp:134] Writing container file[/var/run/mesos/isolators/com_mesosphere_MetricsIsolatorModule/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91] with endpoint[198.51.100.1:51243]
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.919220  6488 sync_util.hpp:136] Result for ticket 431 complete, returning value.
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.919275  6404 sync_util.hpp:83] Dispatch result obtained for ticket 431 after waiting <=5s: register_and_update_cache
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.924211  6404 systemd.cpp:96] Assigned child process '2701' to 'mesos_executors.slice'
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.927724  6404 systemd.cpp:96] Assigned child process '2702' to 'mesos_executors.slice'
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.929244  6403 linux_launcher.cpp:421] Launching container 8750c2a7-8bef-4a69-8ef2-b873f884bf91 and cloning with namespaces CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWNET
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.935976  6403 systemd.cpp:96] Assigned child process '2705' to 'mesos_executors.slice'
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.943640  6409 containerizer.cpp:1489] Checkpointing container's forked pid 2705 to '/var/lib/mesos/slave/meta/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91/pids/forked.pid'
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:16.945247  6410 cni.cpp:814] Bind mounted '/proc/2705/ns/net' to '/run/mesos/isolators/network/cni/8750c2a7-8bef-4a69-8ef2-b873f884bf91/ns' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:16.961654  2701 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:16 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:16.963691  2702 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.250937  6410 cni.cpp:1230] Got assigned IPv4 address '10.47.11.252/32' from CNI network 'rcluster-cni' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.251171  6410 cni.cpp:939] DNS nameservers for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91 are:
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: nameserver 10.190.4.22
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: nameserver 10.190.4.23
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.942303  6404 http.cpp:277] HTTP POST for /slave(1)/api/v1/executor from 10.47.11.252:38416
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.942446  6404 slave.cpp:3022] Received Subscribe request for HTTP executor 'instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001' of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.942533  6404 slave.cpp:3085] Creating a marker file for HTTP based executor 'instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001' of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000 (via HTTP) at path '/var/lib/mesos/slave/meta/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91/http.marker'
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.944046  6404 disk.cpp:207] Updating the disk resources for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91 to cpus(*):0.6; mem(*):192; disk(*):170; ports(*):[21421-21425]
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.945669  6407 memory.cpp:199] Updated 'memory.soft_limit_in_bytes' to 192MB for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.947779  6407 memory.cpp:251] Updated 'memory.limit_in_bytes' to 192MB for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.948951  6407 cpu.cpp:101] Updated 'cpu.shares' to 614 (cpus 0.6) for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.951117  6407 cpu.cpp:121] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 60ms (cpus 0.6) for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91
Nov 29 04:04:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:17.952219  6404 slave.cpp:2220] Sending queued task group task group containing tasks [ dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4, dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5 ] to executor 'instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001' of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000 (via HTTP)
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.002063  6408 http.cpp:277] HTTP POST for /slave(1)/api/v1 from 10.47.11.252:38420
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.002223  6408 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.003235  6409 containerizer.cpp:1657] Starting nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.063dbcd6-1a0a-4058-a615-7e6deaf17707
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.003327  6407 http.cpp:277] HTTP POST for /slave(1)/api/v1 from 10.47.11.252:38420
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.003432  6409 containerizer.cpp:1681] Trying to chown '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/063dbcd6-1a0a-4058-a615-7e6deaf17707' to user 'nobody'
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.003465  6407 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.005939  6407 http.cpp:277] HTTP POST for /slave(1)/api/v1 from 10.47.11.252:38420
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.006057  6407 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.006265  6407 http.cpp:277] HTTP POST for /slave(1)/api/v1 from 10.47.11.252:38420
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.006384  6407 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.008982  6409 containerizer.cpp:1657] Starting nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.a3b19108-d56c-44ea-b20a-912d539dde7b
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.009096  6409 containerizer.cpp:1681] Trying to chown '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/a3b19108-d56c-44ea-b20a-912d539dde7b' to user 'nobody'
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.013618  6409 containerizer.cpp:1657] Starting nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.02c64ae4-ccdd-415d-946a-3f665acbb668
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.013708  6409 containerizer.cpp:1681] Trying to chown '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/02c64ae4-ccdd-415d-946a-3f665acbb668' to user 'nobody'
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.018230  6409 containerizer.cpp:1657] Starting nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c5c10cff-a6af-4e51-911e-61d04e5c1d47
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.018399  6409 containerizer.cpp:1681] Trying to chown '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/c5c10cff-a6af-4e51-911e-61d04e5c1d47' to user 'nobody'
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.022992  6403 http.cpp:277] HTTP POST for /slave(1)/api/v1 from 10.47.11.252:38420
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.023133  6403 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.023598  6406 containerizer.cpp:1657] Starting nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.023705  6406 containerizer.cpp:1681] Trying to chown '/var/lib/mesos/slave/slaves/ce4bd8be-1198-4819-81d4-9a8439439741-S1/frameworks/ce4bd8be-1198-4819-81d4-9a8439439741-0000/executors/instance-dat_scout.e57be1fe-b5e8-11e6-995b-70b3d5800001/runs/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/c8f6dc94-baba-44be-ab65-49538645a31c' to user 'nobody'
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.462172  6405 provisioner.cpp:294] Provisioning image rootfs '/var/lib/mesos/slave/provisioner/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/063dbcd6-1a0a-4058-a615-7e6deaf17707/backends/copy/rootfses/97f1a1df-1eec-46e3-8a93-8a2b80d48dd2' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.063dbcd6-1a0a-4058-a615-7e6deaf17707
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.462401  6405 provisioner.cpp:294] Provisioning image rootfs '/var/lib/mesos/slave/provisioner/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/a3b19108-d56c-44ea-b20a-912d539dde7b/backends/copy/rootfses/b93ffe07-6f64-42e1-b8c4-77bb224fc9e0' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.a3b19108-d56c-44ea-b20a-912d539dde7b
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.465819  6410 provisioner.cpp:294] Provisioning image rootfs '/var/lib/mesos/slave/provisioner/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/02c64ae4-ccdd-415d-946a-3f665acbb668/backends/copy/rootfses/5d388325-8b4d-496f-b520-4a96584a5971' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.02c64ae4-ccdd-415d-946a-3f665acbb668
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.470854  6409 provisioner.cpp:294] Provisioning image rootfs '/var/lib/mesos/slave/provisioner/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/c5c10cff-a6af-4e51-911e-61d04e5c1d47/backends/copy/rootfses/5b4d32da-7d82-45d2-ad28-85a48456a18b' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c5c10cff-a6af-4e51-911e-61d04e5c1d47
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.471132  6409 provisioner.cpp:294] Provisioning image rootfs '/var/lib/mesos/slave/provisioner/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/c8f6dc94-baba-44be-ab65-49538645a31c/backends/copy/rootfses/244e36f5-ecca-444f-9bf6-05aa77cb7263' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:18.893782  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.164958  6405 runtime.cpp:109] Container user 'nobody' is not supported yet for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.063dbcd6-1a0a-4058-a615-7e6deaf17707
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.171018  6405 systemd.cpp:96] Assigned child process '2880' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.176822  6405 systemd.cpp:96] Assigned child process '2881' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.179283  6405 linux_launcher.cpp:421] Launching nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.063dbcd6-1a0a-4058-a615-7e6deaf17707 and cloning with namespaces CLONE_NEWNS
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.185796  6409 runtime.cpp:109] Container user 'nobody' is not supported yet for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c5c10cff-a6af-4e51-911e-61d04e5c1d47
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.200559  6405 systemd.cpp:96] Assigned child process '2884' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.209010  2881 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.211642  2880 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.215493  6405 runtime.cpp:109] Container user 'nobody' is not supported yet for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.a3b19108-d56c-44ea-b20a-912d539dde7b
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.228160  6407 systemd.cpp:96] Assigned child process '2896' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.234012  6407 systemd.cpp:96] Assigned child process '2897' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.239496  6409 linux_launcher.cpp:421] Launching nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c5c10cff-a6af-4e51-911e-61d04e5c1d47 and cloning with namespaces CLONE_NEWNS
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.242473  6405 runtime.cpp:109] Container user 'nobody' is not supported yet for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.02c64ae4-ccdd-415d-946a-3f665acbb668
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.243351  6405 runtime.cpp:109] Container user 'nobody' is not supported yet for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.254703  6409 systemd.cpp:96] Assigned child process '2900' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.268317  2896 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.274961  6405 systemd.cpp:96] Assigned child process '2902' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.279268  6405 systemd.cpp:96] Assigned child process '2911' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.284328  6405 systemd.cpp:96] Assigned child process '2912' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.286672  6404 linux_launcher.cpp:421] Launching nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.a3b19108-d56c-44ea-b20a-912d539dde7b and cloning with namespaces CLONE_NEWNS
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.299939  6405 systemd.cpp:96] Assigned child process '2914' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.304103  6405 systemd.cpp:96] Assigned child process '2917' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.307452  6405 systemd.cpp:96] Assigned child process '2918' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.309020  2897 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.311029  6404 systemd.cpp:96] Assigned child process '2916' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.331065  2911 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.333595  6405 linux_launcher.cpp:421] Launching nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.02c64ae4-ccdd-415d-946a-3f665acbb668 and cloning with namespaces CLONE_NEWNS
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.342017  2902 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.372247  6405 systemd.cpp:96] Assigned child process '2937' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.372742  2912 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.378247  2914 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.388391  2917 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.388517  6410 linux_launcher.cpp:421] Launching nested container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c and cloning with namespaces CLONE_NEWNS
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: WARNING: Logging before InitGoogleLogging() is written to STDERR
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: W1129 04:04:20.396700  2918 process.cpp:882] Failed SSL connections will be downgraded to a non-SSL socket
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.400269  6410 systemd.cpp:96] Assigned child process '2969' to 'mesos_executors.slice'
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.772186  6403 containerizer.cpp:2313] Container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c has exited
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.772228  6403 containerizer.cpp:1950] Destroying container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c in RUNNING state
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.772330  6403 linux_launcher.cpp:498] Asked to destroy container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.773825  6403 linux_launcher.cpp:541] Using freezer to destroy cgroup mesos/8750c2a7-8bef-4a69-8ef2-b873f884bf91/mesos/c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.776217  6407 cgroups.cpp:2705] Freezing cgroup /sys/fs/cgroup/freezer/mesos/8750c2a7-8bef-4a69-8ef2-b873f884bf91/mesos/c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.780053  6410 cgroups.cpp:1439] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/8750c2a7-8bef-4a69-8ef2-b873f884bf91/mesos/c8f6dc94-baba-44be-ab65-49538645a31c after 3.798016ms
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.783502  6407 cgroups.cpp:2723] Thawing cgroup /sys/fs/cgroup/freezer/mesos/8750c2a7-8bef-4a69-8ef2-b873f884bf91/mesos/c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.786717  6407 cgroups.cpp:1468] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/8750c2a7-8bef-4a69-8ef2-b873f884bf91/mesos/c8f6dc94-baba-44be-ab65-49538645a31c after 3.186176ms
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.791746  6404 provisioner.cpp:488] Destroying container rootfs at '/var/lib/mesos/slave/provisioner/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/c8f6dc94-baba-44be-ab65-49538645a31c/backends/copy/rootfses/244e36f5-ecca-444f-9bf6-05aa77cb7263' for container 8750c2a7-8bef-4a69-8ef2-b873f884bf91.c8f6dc94-baba-44be-ab65-49538645a31c
Nov 29 04:04:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:20.873241  6405 containerizer.cpp:2229] Checkpointing termination state to nested container's runtime directory '/var/run/mesos/containers/8750c2a7-8bef-4a69-8ef2-b873f884bf91/containers/c8f6dc94-baba-44be-ab65-49538645a31c/termination'
Nov 29 04:04:21 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:21.153594  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:23 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:23.410384  6404 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:25 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:25.667546  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:27 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:27.925597  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:30 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:30.198536  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:32 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:32.456645  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:34 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:34.716507  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:36 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:36.974459  6409 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:39 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:39.229652  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:41 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:41.482465  6404 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:43 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:43.736668  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:45 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:45.994588  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:45 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:45.999001  6488 metrics_tcp_sender.cpp:252] TCP Throughput (bytes): sent=0, dropped=0, failed=0, pending=0 (state CONNECT_PENDING)
Nov 29 04:04:46 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:46.187460  6408 slave.cpp:5044] Current disk usage 10.89%. Max allowed age: 1.582110178819861days
Nov 29 04:04:48 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:48.161314  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57362 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:04:48 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:48.250435  6409 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:49.033789  6488 metrics_tcp_sender.cpp:127] Timed out when opening metrics connection to 127.0.0.1:8124. This is expected if no Metrics Collector is running on this agent. (state CONNECT_PENDING)
Nov 29 04:04:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:49.033946  6488 metrics_tcp_sender.cpp:112] Attempting to open connection to 127.0.0.1:8124
Nov 29 04:04:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:49.034059  6488 metrics_tcp_sender.cpp:140] Got error 'Connection refused'(system:111) when connecting to metrics service at 127.0.0.1:8124. This is expected if no Metrics Collector is running on this agent. (state CONNECT_IN_PROGRESS)
Nov 29 04:04:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:49.034081  6488 metrics_tcp_sender.cpp:94] Scheduling reconnect to 127.0.0.1:8124 in 60s...
Nov 29 04:04:50 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:50.161448  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57426 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:04:50 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:50.503459  6404 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:52 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:52.161912  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57486 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:04:52 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:52.765455  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:54 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:54.161412  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57546 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:04:55 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:55.022529  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:56 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:56.163820  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57606 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:04:57 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:57.279559  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:04:58 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:58.162578  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57666 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:04:59 ip-10-190-112-199 mesos-agent[6397]: I1129 04:04:59.538532  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:00 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:00.161715  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57726 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:01 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:01.801764  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:02 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:02.160838  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57790 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:04 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:04.060504  6409 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:04 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:04.162878  6409 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57908 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:06 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:06.161875  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:57962 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:06 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:06.318395  6409 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:08 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:08.160797  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:58016 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:08 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:08.572476  6404 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:10 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:10.162160  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:58070 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:10 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:10.828313  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:12 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:12.161572  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:58124 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:13 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:13.084312  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:14 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:14.164269  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:58182 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:15 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:15.339344  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:16.165223  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:58238 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:16.919145  6488 container_reader_impl.cpp:146] Throughput from container at port 51243 (bytes): received=0, throttled=0
Nov 29 04:05:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:17.476977  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.74:58298 with User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36' with X-Forwarded-For='10.192.128.26'
Nov 29 04:05:17 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:17.594532  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:19 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:19.856494  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:22 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:22.112598  6404 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:24 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:24.371556  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:26 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:26.628479  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:28 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:28.885504  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:31 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:31.143674  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:33 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:33.434650  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:35 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:35.694718  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:37 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:37.952450  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:40 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:40.209468  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:42 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:42.466473  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:44 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:44.724468  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:45 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:45.999135  6488 metrics_tcp_sender.cpp:252] TCP Throughput (bytes): sent=0, dropped=373, failed=0, pending=0 (state CONNECT_PENDING)
Nov 29 04:05:46 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:46.188647  6408 slave.cpp:5044] Current disk usage 10.83%. Max allowed age: 1.583336138877014days
Nov 29 04:05:46 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:46.986548  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:49.034080  6488 metrics_tcp_sender.cpp:127] Timed out when opening metrics connection to 127.0.0.1:8124. This is expected if no Metrics Collector is running on this agent. (state CONNECT_PENDING)
Nov 29 04:05:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:49.034231  6488 metrics_tcp_sender.cpp:112] Attempting to open connection to 127.0.0.1:8124
Nov 29 04:05:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:49.034332  6488 metrics_tcp_sender.cpp:140] Got error 'Connection refused'(system:111) when connecting to metrics service at 127.0.0.1:8124. This is expected if no Metrics Collector is running on this agent. (state CONNECT_IN_PROGRESS)
Nov 29 04:05:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:49.034358  6488 metrics_tcp_sender.cpp:94] Scheduling reconnect to 127.0.0.1:8124 in 60s...
Nov 29 04:05:49 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:49.243544  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:51 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:51.500578  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:53 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:53.755667  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:56 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:56.010568  6409 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:05:58 ip-10-190-112-199 mesos-agent[6397]: I1129 04:05:58.269441  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:00 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:00.526623  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:02 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:02.784615  6406 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:05 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:05.043455  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:07 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:07.299536  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:09 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:09.556404  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:11 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:11.813442  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:14 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:14.068482  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:16.325479  6410 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:16 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:16.919404  6488 container_reader_impl.cpp:146] Throughput from container at port 51243 (bytes): received=0, throttled=0
Nov 29 04:06:18 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:18.581571  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:20 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:20.841563  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:23 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:23.098562  6403 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:25 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:25.358496  6404 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:26 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:26.886557  6406 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:26 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:26.886762  6405 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:26 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:26.886831  6405 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:26 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:26.886884  6405 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:26 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:26.886924  6405 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:27 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:27.615567  6404 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:29 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:29.877619  6405 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:32 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:32.135524  6408 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:34 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:34.402644  6409 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:36 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:36.664479  6407 http.cpp:277] HTTP GET for /slave(1)/state from 10.190.112.199:36156
Nov 29 04:06:36 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:36.903115  6405 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:36 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:36.903259  6405 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:36 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:36.903317  6405 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:36 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:36.903436  6408 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
Nov 29 04:06:36 ip-10-190-112-199 mesos-agent[6397]: I1129 04:06:36.903501  6408 slave.cpp:2288] Asked to kill task dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2 of framework ce4bd8be-1198-4819-81d4-9a8439439741-0000
{code}

Executor log:
{code}
I1129 04:04:17.832391  2734 executor.cpp:189] Version: 1.1.0
I1129 04:04:17.956600  2738 default_executor.cpp:123] Received SUBSCRIBED event
I1129 04:04:17.956832  2738 default_executor.cpp:127] Subscribed executor on 10.190.112.199
I1129 04:04:17.968544  2735 default_executor.cpp:123] Received LAUNCH_GROUP event
I1129 04:06:26.899070  2735 default_executor.cpp:123] Received KILL event
I1129 04:06:26.899101  2735 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5'
W1129 04:06:26.899122  2735 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5' as it is no longer active
I1129 04:06:26.899235  2742 default_executor.cpp:123] Received KILL event
I1129 04:06:26.899251  2742 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1'
W1129 04:06:26.899260  2742 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1' as it is no longer active
I1129 04:06:26.899425  2741 default_executor.cpp:123] Received KILL event
I1129 04:06:26.899443  2741 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3'
W1129 04:06:26.899448  2741 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3' as it is no longer active
I1129 04:06:26.899591  2740 default_executor.cpp:123] Received KILL event
I1129 04:06:26.899616  2740 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4'
W1129 04:06:26.899627  2740 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4' as it is no longer active
I1129 04:06:26.899699  2735 default_executor.cpp:123] Received KILL event
I1129 04:06:26.899713  2735 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2'
W1129 04:06:26.899721  2735 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2' as it is no longer active
I1129 04:06:36.915755  2740 default_executor.cpp:123] Received KILL event
I1129 04:06:36.915786  2740 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5'
W1129 04:06:36.915796  2740 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server5' as it is no longer active
I1129 04:06:36.915915  2736 default_executor.cpp:123] Received KILL event
I1129 04:06:36.915931  2736 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1'
W1129 04:06:36.915940  2736 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server1' as it is no longer active
I1129 04:06:36.915962  2736 default_executor.cpp:123] Received KILL event
I1129 04:06:36.915972  2736 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3'
W1129 04:06:36.915979  2736 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server3' as it is no longer active
I1129 04:06:36.915997  2736 default_executor.cpp:123] Received KILL event
I1129 04:06:36.916007  2736 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4'
W1129 04:06:36.916013  2736 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server4' as it is no longer active
I1129 04:06:36.916031  2736 default_executor.cpp:123] Received KILL event
I1129 04:06:36.916040  2736 default_executor.cpp:802] Received kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2'
W1129 04:06:36.916048  2736 default_executor.cpp:813] Ignoring kill for task 'dat_scout.instance-e57be1fe-b5e8-11e6-995b-70b3d5800001.scout-server2' as it is no longer active

continuing for hours ...
{code}

Meanwhile, the tasks show up as {{STAGING}} in the Mesos web UI, and their sandboxes are empty."	MESOS	Resolved	3	1	4664	nested
12946483	Add authentication to agent endpoints /state and /flags	"The {{/state}} and {{/flags}} endpoints are installed in {{src/slave/slave.cpp}}, and thus are straightforward to make authenticated. Other agent endpoints require a bit more consideration, and are tracked in MESOS-4902.

For more information on agent endpoints, see http://mesos.apache.org/documentation/latest/endpoints/
or search for `route(` in the source code:
{code}
$ grep -rn ""route("" src/ |grep -v master |grep -v tests |grep -v json
src/version/version.cpp:75:  route(""/"", VERSION_HELP(), &VersionProcess::version);
src/files/files.cpp:150:  route(""/browse"",
src/files/files.cpp:153:  route(""/read"",
src/files/files.cpp:156:  route(""/download"",
src/files/files.cpp:159:  route(""/debug"",
src/slave/slave.cpp:580:  route(""/api/v1/executor"",
src/slave/slave.cpp:595:  route(""/state"",
src/slave/slave.cpp:601:  route(""/flags"",
src/slave/slave.cpp:607:  route(""/health"",
src/slave/monitor.cpp:100:    route(""/statistics"",
$ grep -rn ""route("" 3rdparty/ |grep -v tests |grep -v README |grep -v examples |grep -v help |grep -v ""process..pp""
3rdparty/libprocess/include/process/profiler.hpp:34:    route(""/start"", START_HELP(), &Profiler::start);
3rdparty/libprocess/include/process/profiler.hpp:35:    route(""/stop"", STOP_HELP(), &Profiler::stop);
3rdparty/libprocess/include/process/system.hpp:70:    route(""/stats.json"", statsHelp(), &System::stats);
3rdparty/libprocess/include/process/logging.hpp:44:    route(""/toggle"", TOGGLE_HELP(), &This::toggle);
{code}"	MESOS	Resolved	3	3	4664	mesosphere, security
13237587	Add draining state information to master state endpoints	"The response for {{GET_STATE}} and {{GET_AGENTS}} should include the new fields indicating deactivation or draining states:
{code}
message Response {
  . . .

  message GetAgents {
    message Agent {
      . . .

      optional bool deactivated = 12;
      optional DrainInfo drain_info = 13;

      . . .
    }
  }
  . . .
}
{code}

The {{/state}} and {{/state-summary}} handlers should also expose this information."	MESOS	Resolved	3	3	4664	foundations, mesosphere
12845642	Add 'principal' field to 'Resource.DiskInfo.Persistence'	In order to support authorization for persistent volumes, we should add the {{principal}} to {{Resource.DiskInfo}}, analogous to {{Resource.ReservationInfo.principal}}.	MESOS	Resolved	3	3	4664	mesosphere, persistent-volumes
12922136	Add persistent volume support to the Authorizer	"This ticket is the first in a series that adds authorization support for persistent volume creation and destruction.

Persistent volumes should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Create}} and {{Destroy}} into the ACL.

{code}
  message Create {
    // Subjects.
    required Entity principals = 1;

    // Objects? Perhaps the kind of volume? allowed permissions?
  }

  message Destroy {
    // Subjects.
    required Entity principals = 1;

    // Objects.
    required Entity creator_principals = 2;
  }
{code}

ACLs for volume creation and destruction must be added to {{authorizer.proto}}, and the appropriate function overloads must be added to the Authorizer."	MESOS	Resolved	3	1	4664	mesosphere, persistent-volumes
13108956	Some fields went missing with no replacement in api/v1.	"Hi friends, 

These fields are available via the state.json but went missing in the v1 of the API:
-leader_info- -> available via GET_MASTER which should always return leading master info
start_time
elected_time

As we're showing them on the Overview page of the DC/OS UI, yet would like not be using state.json, it would be great to have them somewhere in V1."	MESOS	Resolved	2	4	4664	mesosphere
13220014	OperationReconciliationTest.AgentPendingOperationAfterMasterFailover is flaky again (3x) due to orphan operations	"This test fails consistently when run while the system is stressed:
{code}
[ RUN      ] ContentType/OperationReconciliationTest.AgentPendingOperationAfterMasterFailover/0
F0305 08:10:07.670622  3982 hierarchical.cpp:1259] Check failed: slave.getAllocated().contains(resources) {} does not contain disk(allocated: default-role)[RAW(,,profile)]:200
*** Check failure stack trace: ***
    @     0x7f1120b0ce5e  google::LogMessage::Fail()
    @     0x7f1120b0cdbb  google::LogMessage::SendToLog()
    @     0x7f1120b0c7b5  google::LogMessage::Flush()
    @     0x7f1120b0f578  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f111e536f2a  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::recoverResources()
    @     0x5580c2651c26  _ZZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS1_11FrameworkIDERKNS1_7SlaveIDERKNS1_9ResourcesERK6OptionINS1_7FiltersEES8_SB_SE_SJ_EEvRKNS_3PIDIT_EEMSL_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_ENKUlOS6_OS9_OSC_OSH_PNS_11ProcessBaseEE_clES13_S14_S15_S16_S18_
    @     0x5580c26c7e02  _ZN5cpp176invokeIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS3_11FrameworkIDERKNS3_7SlaveIDERKNS3_9ResourcesERK6OptionINS3_7FiltersEESA_SD_SG_SL_EEvRKNS1_3PIDIT_EEMSN_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOS8_OSB_OSE_OSJ_PNS1_11ProcessBaseEE_JS8_SB_SE_SJ_S1A_EEEDTclcl7forwardISN_Efp_Espcl7forwardIT0_Efp0_EEEOSN_DpOS1C_
    @     0x5580c26c5b1e  _ZN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS4_11FrameworkIDERKNS4_7SlaveIDERKNS4_9ResourcesERK6OptionINS4_7FiltersEESB_SE_SH_SM_EEvRKNS2_3PIDIT_EEMSO_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOS9_OSC_OSF_OSK_PNS2_11ProcessBaseEE_JS9_SC_SF_SK_St12_PlaceholderILi1EEEE13invoke_expandIS1C_St5tupleIJS9_SC_SF_SK_S1E_EES1H_IJOS1B_EEJLm0ELm1ELm2ELm3ELm4EEEEDTcl6invokecl7forwardISO_Efp_Espcl6expandcl3getIXT2_EEcl7forwardISS_Efp0_EEcl7forwardIST_Efp2_EEEEOSO_OSS_N5cpp1416integer_sequenceImJXspT2_EEEEOST_
    @     0x5580c26c47ac  _ZNO6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS4_11FrameworkIDERKNS4_7SlaveIDERKNS4_9ResourcesERK6OptionINS4_7FiltersEESB_SE_SH_SM_EEvRKNS2_3PIDIT_EEMSO_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOS9_OSC_OSF_OSK_PNS2_11ProcessBaseEE_JS9_SC_SF_SK_St12_PlaceholderILi1EEEEclIJS1B_EEEDTcl13invoke_expandcl4movedtdefpT1fEcl4movedtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0ELm1ELm2ELm3ELm4EEEE_Ecl16forward_as_tuplespcl7forwardIT_Efp_EEEEDpOS1K_
    @     0x5580c26c3ad7  _ZN5cpp176invokeIN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS6_11FrameworkIDERKNS6_7SlaveIDERKNS6_9ResourcesERK6OptionINS6_7FiltersEESD_SG_SJ_SO_EEvRKNS4_3PIDIT_EEMSQ_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOSB_OSE_OSH_OSM_PNS4_11ProcessBaseEE_JSB_SE_SH_SM_St12_PlaceholderILi1EEEEEJS1D_EEEDTclcl7forwardISQ_Efp_Espcl7forwardIT0_Efp0_EEEOSQ_DpOS1I_
    @     0x5580c26c32ad  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS7_11FrameworkIDERKNS7_7SlaveIDERKNS7_9ResourcesERK6OptionINS7_7FiltersEESE_SH_SK_SP_EEvRKNS5_3PIDIT_EEMSR_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOSC_OSF_OSI_OSN_PNS5_11ProcessBaseEE_JSC_SF_SI_SN_St12_PlaceholderILi1EEEEEJS1E_EEEvOSR_DpOT0_
    @     0x5580c26c0a5e  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNSA_11FrameworkIDERKNSA_7SlaveIDERKNSA_9ResourcesERK6OptionINSA_7FiltersEESH_SK_SN_SS_EEvRKNS1_3PIDIT_EEMSU_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOSF_OSI_OSL_OSQ_S3_E_JSF_SI_SL_SQ_St12_PlaceholderILi1EEEEEEclEOS3_
    @     0x7f1120a51c60  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEEclES3_
    @     0x7f1120a16a4e  process::ProcessBase::consume()
    @     0x7f1120a3d9d8  _ZNO7process13DispatchEvent7consumeEPNS_13EventConsumerE
    @     0x5580c2284afa  process::ProcessBase::serve()
    @     0x7f1120a138db  process::ProcessManager::resume()
    @     0x7f1120a0fc28  _ZZN7process14ProcessManager12init_threadsEvENKUlvE_clEv
    @     0x7f1120a375d0  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7f1120a36734  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEclEv
    @     0x7f1120a3569c  _ZNSt6thread11_State_implISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
    @     0x7f111499276f  (unknown)
    @     0x7f111507273a  start_thread
    @     0x7f11140f8e7f  __GI___clone
{code}"	MESOS	Resolved	1	1	4664	foundations, mesosphere
13188758	Docker containerizer actor can get backlogged with large number of containers.	"We observed during some scale testing that we do internally.

When launching 300+ Docker containers on a single agent box, it's possible that the Docker containerizer actor gets backlogged. As a result, API processing like `GET_CONTAINERS` will become unresponsive. It'll also block Mesos containerizer from launching containers if one specified `--containers=docker,mesos` because Docker containerizer launch will be invoked first by the composing containerizer (and queued).

Profiling results show that the bottleneck is `os::killtree`, which will be invoked when the Docker commands are discarded (e.g., client disconnect, etc.).

For this particular case, killtree is not really necessary because the docker command does not fork additional subprocesses. If we use the argv version of `subprocess` to launch docker commands, we can simply use os::kill instead. We confirmed that, by switching to os::kill, the performance issues goes away, and the agent can easily scale up to 300+ containers."	MESOS	Resolved	1	1	4664	perfomance
12936498	`/reserve` and `/create-volumes` endpoints allow operations for any role	"When frameworks reserve resources, the validation of the operation ensures that the {{role}} of the reservation matches the {{role}} of the framework. For the case of the {{/reserve}} operator endpoint, however, the operator has no role to validate, so this check isn't performed.

This means that if an ACL exists which authorizes a framework's principal to reserve resources, that same principal can be used to reserve resources for _any_ role through the operator endpoint.

We should restrict reservations made through the operator endpoint to specified roles. A few possibilities:
* The {{object}} of the {{reserve_resources}} ACL could be changed from {{resources}} to {{roles}}
* A second ACL could be added for authorization of {{reserve}} operations, with an {{object}} of {{role}}
* Our conception of the {{resources}} object in the {{reserve_resources}} ACL could be expanded to include role information, i.e., {{disk(role1);mem(role1)}}"	MESOS	Resolved	3	1	4664	mesosphere, reservations
12695583	Python extension build is broken if gflags-dev is installed	"In my environment mesos build from master results in broken python api module {{_mesos.so}}:
{noformat}
nekto0n@ya-darkstar ~/workspace/mesos/src/python $ PYTHONPATH=build/lib.linux-x86_64-2.7/ python -c ""import _mesos""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: /home/nekto0n/workspace/mesos/src/python/build/lib.linux-x86_64-2.7/_mesos.so: undefined symbol: _ZN6google14FlagRegistererC1EPKcS2_S2_S2_PvS3_
{noformat}
Unmangled version of symbol looks like this:
{noformat}
google::FlagRegisterer::FlagRegisterer(char const*, char const*, char const*, char const*, void*, void*)
{noformat}
During {{./configure}} step {{glog}} finds {{gflags}} development files and starts using them, thus *implicitly* adding dependency on {{libgflags.so}}. This breaks Python extensions module and perhaps can break other mesos subsystems when moved to hosts without {{gflags}} installed.

This task is done when the ExamplesTest.PythonFramework test will pass on a system with gflags installed."	MESOS	Resolved	3	1	4664	flaky-test, mesosphere
12930252	Change the `principal` in `ReservationInfo` to optional	With the addition of HTTP endpoints for {{/reserve}} and {{/unreserve}}, it is now desirable to allow dynamic reservations without a principal, in the case where HTTP authentication is disabled. To allow for this, we will change the {{principal}} field in {{ReservationInfo}} from required to optional. For backwards-compatibility, however, the master should currently invalidate any {{ReservationInfo}} messages that do not have this field set.	MESOS	Resolved	3	4	4664	mesosphere, reservations
13188255	Add an operation status update manager to the agent	Review here: https://reviews.apache.org/r/69505/	MESOS	Resolved	3	3	4664	foundations, mesosphere
12901474	LIBPROCESS_IP not passed when executor's environment is specified	When the executor's environment is specified explicitly via {{\-\-executor_environment_variables}}, {{LIBPROCESS_IP}} will not be passed, leading to errors in some cases - for example, when no DNS is available.	MESOS	Resolved	3	1	4664	mesosphere
12695795	ExamplesTest.JavaLog is flaky	"The {{ExamplesTest.JavaLog}} test framework is flaky, possibly related to a race condition between mutexes.
{noformat}
[ RUN      ] ExamplesTest.JavaLog
Using temporary directory '/tmp/ExamplesTest_JavaLog_WBWEb9'
Feb 18, 2014 12:10:57 PM TestLog main
INFO: Starting a local ZooKeeper server
...
F0218 12:10:58.575036 17450 coordinator.cpp:394] Check failed: !missing Not expecting local replica to be missing position 3 after the writing is done
*** Check failure stack trace: ***
tests/script.cpp:81: Failure
Failed
java_log_test.sh terminated with signal 'Aborted'
[  FAILED  ] ExamplesTest.JavaLog (2166 ms)
{noformat}

Full logs attached."	MESOS	Resolved	3	1	4664	flaky, mesosphere
13038206	Add authorization actions for V1 executor calls	"Authorization actions should be added for the V1 executor calls:
* Subscribe
* Update
* Message"	MESOS	Resolved	3	3	4664	authorization, executor, mesosphere, protobuf, security
12845645	Add framework authorization for persistent volume	"This is the third in a series of tickets that adds authorization support to persistent volumes.

When a framework creates a persistent volume, ""create"" ACLs are checked to see if the framework (FrameworkInfo.principal) or the operator (Credential.user) is authorized to create persistent volumes. If not authorized, the create operation is rejected.

When a framework destroys a persistent volume, ""destroy"" ACLs are checked to see if the framework (FrameworkInfo.principal) or the operator (Credential.user) is authorized to destroy the persistent volume created by a framework or operator (Resource.DiskInfo.principal). If not authorized, the destroy operation is rejected.

A separate ticket will use the structures created here to enable authorization of the ""/create"" and ""/destroy"" HTTP endpoints: https://issues.apache.org/jira/browse/MESOS-3903"	MESOS	Resolved	3	3	4664	mesosphere, persistent-volumes
12902463	Introduce stream IDs in HTTP Scheduler API	"Currently, the HTTP Scheduler API has no concept of Sessions aka {{SessionID}} or a {{TokenID}}. This is useful in some failure scenarios. As of now, if a framework fails over and then subscribes again with the same {{FrameworkID}} with the {{force}} option set, the Mesos master would subscribe it.

If the previous instance of the framework/scheduler tries to send a Call , e.g. {{Call::KILL}} with the same previous {{FrameworkID}} set, it would be still accepted by the master leading to erroneously killing a task.

This is possible because we do not have a way currently of distinguishing connections. It used to work in the previous driver implementation due to the master also performing a {{UPID}} check to verify if they matched and only then allowing the call. Following the design process, we will implemented ""stream IDs"" for Mesos HTTP schedulers; each ID will be associated with a single subscription connection, and the scheduler must include it as a header in all non-subscribe calls sent to the master."	MESOS	Resolved	3	3	4664	mesosphere, tech-debt
12994955	SSL Socket CHECK can fail after socket receives EOF	"While writing a test for MESOS-3753, I encountered a bug where [this check|https://github.com/apache/mesos/blob/853821cafcca3550b9c7bdaba5262d73869e2ee1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L708] fails at the very end of the test body, while objects in the stack frame are being destroyed. After adding some debug logging output, I produced the following:
{code}
I0804 08:32:33.263211 273793024 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.263209 273256448 process.cpp:2970] Cleaning up __limiter__(3)@127.0.0.1:55688
I0804 08:32:33.263263 275939328 libevent_ssl_socket.cpp:152] *** in initialize(): 14
I0804 08:32:33.263206 272719872 process.cpp:2865] Resuming (61)@127.0.0.1:55688 at 2016-08-04 15:32:33.263261952+00:00
I0804 08:32:33.263327 275939328 libevent_ssl_socket.cpp:584] *** in recv()14
I0804 08:32:33.263337 272719872 hierarchical.cpp:571] Agent e2a49340-34ec-403f-a5a4-15e29c4a2434-S0 deactivated
I0804 08:32:33.263322 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263343104+00:00
I0804 08:32:33.263510 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263536 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 19
I0804 08:32:33.263592 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 19
I0804 08:32:33.263622 1985901312 process.cpp:3170] Donating thread to (87)@127.0.0.1:55688 while waiting
I0804 08:32:33.263639 274329600 process.cpp:2865] Resuming __http__(12)@127.0.0.1:55688 at 2016-08-04 15:32:33.263653888+00:00
I0804 08:32:33.263659 1985901312 process.cpp:2865] Resuming (87)@127.0.0.1:55688 at 2016-08-04 15:32:33.263671040+00:00
I0804 08:32:33.263730 1985901312 process.cpp:2970] Cleaning up (87)@127.0.0.1:55688
I0804 08:32:33.263741 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263736 274329600 process.cpp:2970] Cleaning up __http__(12)@127.0.0.1:55688
I0804 08:32:33.263778 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 17
I0804 08:32:33.263818 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 17
I0804 08:32:33.263839 272183296 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263857920+00:00
I0804 08:32:33.263933 273793024 process.cpp:2865] Resuming __gc__@127.0.0.1:55688 at 2016-08-04 15:32:33.263951104+00:00
I0804 08:32:33.264034 275939328 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.264020 272719872 process.cpp:2865] Resuming __http__(11)@127.0.0.1:55688 at 2016-08-04 15:32:33.264041984+00:00
I0804 08:32:33.264036 274329600 process.cpp:2865] Resuming status-update-manager(3)@127.0.0.1:55688 at 2016-08-04 15:32:33.264056064+00:00
I0804 08:32:33.264071 272719872 process.cpp:2970] Cleaning up __http__(11)@127.0.0.1:55688
I0804 08:32:33.264088 274329600 process.cpp:2970] Cleaning up status-update-manager(3)@127.0.0.1:55688
I0804 08:32:33.264086 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.264112 272183296 process.cpp:2865] Resuming (89)@127.0.0.1:55688 at 2016-08-04 15:32:33.264126976+00:00
I0804 08:32:33.264118 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.264144896+00:00
I0804 08:32:33.264149 272183296 process.cpp:2970] Cleaning up (89)@127.0.0.1:55688
I0804 08:32:33.264202 275939328 libevent_ssl_socket.cpp:281] *** in send_callback(bev)
I0804 08:32:33.264400 273793024 process.cpp:3170] Donating thread to (86)@127.0.0.1:55688 while waiting
I0804 08:32:33.264413 273256448 process.cpp:2865] Resuming (76)@127.0.0.1:55688 at 2016-08-04 15:32:33.264428032+00:00
I0804 08:32:33.296268 275939328 libevent_ssl_socket.cpp:300] *** in send_callback(): 17
I0804 08:32:33.296419 273256448 process.cpp:2970] Cleaning up (76)@127.0.0.1:55688
I0804 08:32:33.296357 273793024 process.cpp:2865] Resuming (86)@127.0.0.1:55688 at 2016-08-04 15:32:33.296414976+00:00
I0804 08:32:33.296464 273793024 process.cpp:2970] Cleaning up (86)@127.0.0.1:55688
I0804 08:32:33.296497 275939328 libevent_ssl_socket.cpp:104] *** releasing SSL socket
I0804 08:32:33.296517 275939328 libevent_ssl_socket.cpp:106] *** released SSL socket: 19
I0804 08:32:33.296515 274329600 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.296532992+00:00
I0804 08:32:33.296550 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.296583 273793024 process.cpp:2865] Resuming (77)@127.0.0.1:55688 at 2016-08-04 15:32:33.296616960+00:00
F0804 08:32:33.296623 275939328 libevent_ssl_socket.cpp:723] Check failed: 'self->send_request.get()' Must be non NULL
*** Check failure stack trace: ***
{code}

The {{in send()17}} line indicates the beginning of {{send()}} for the SSL socket using FD 17. {{in shutdown(): 17}} indicates the beginning of {{shutdown()}} for the same socket, while {{sending on socket: 17}} indicates the execution of the lambda from {{send()}} on the event loop. Since {{shutdown()}} was called in between the call to {{send()}} and the execution of its lambda, it looks like the {{Socket}} was destroyed before the lambda could run. It's unclear why this would happen, since {{send()}}'s lambda captures a shared copy of the socket's {{this}} pointer in order to keep it alive."	MESOS	Resolved	1	1	4664	mesosphere
12857069	Failing ROOT_ tests on CentOS 7.1 - MesosContainerizerLaunchTest	"h2. MesosContainerizerLaunchTest

This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:

* fix;
* remove; OR
* redesign.

(full verbose logs attached)

h2. Steps to Reproduce

Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:

{noformat}
[==========] 751 tests from 114 test cases ran. (231218 ms total)
[  PASSED  ] 742 tests.
[  FAILED  ] 9 tests, listed below:
[  FAILED  ] LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
[  FAILED  ] ContainerizerTest.ROOT_CGROUPS_BalloonFramework
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystem
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromSandbox
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHost
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
[  FAILED  ] MesosContainerizerLaunchTest.ROOT_ChangeRootfs

 9 FAILED TESTS
  YOU HAVE 10 DISABLED TESTS
{noformat}"	MESOS	Resolved	1	1	4664	mesosphere, tech-debt
13013469	Introduce an extra 'unknown' health check state.	"There are three logical states regarding health checks:
1) no health checks;
2) a health check is defined, but no result is available yet;
3) a health check is defined, it is either healthy or not.

Currently, we do not distinguish between 1) and 2), which can be problematic for framework authors."	MESOS	Accepted	3	4	4664	health-check, mesosphere
13176287	Port mapper CNI plugin should use '-n' option with 'iptables --list'	"Without the {{-n}} option, [this iptables command|https://github.com/apache/mesos/blob/9457dce1d99b5616d1b5eeb9a344733f6320d7b5/src/slave/containerizer/mesos/isolators/network/cni/plugins/port_mapper/port_mapper.cpp#L313] could result in a large number of reverse hostname lookups, which could take a while.

We should use the -n option there to avoid this issue."	MESOS	Resolved	3	4	4664	mesosphere
12989298	The fetcher can access any local file as root	The Mesos fetcher currently runs as root and does a blind cp+chown of any file:// URI into the task's sandbox, to be owned by the task user. Even if frameworks are restricted from running tasks as root, it seems they can still access root-protected files in this way. We should secure the fetcher so that it has the filesystem permissions of the user its associated task is being run as. One option would be to run the fetcher as the same user that the task will run as.	MESOS	Resolved	3	1	4664	mesosphere, security
13063233	Add authentication to the checker and health checker libraries	The health checker library in {{src/checks/health_checker.cpp}} must be updated to authenticate with the agent's HTTP operator API.	MESOS	Resolved	3	3	4664	check, executor, health-check, mesosphere
12946481	Agent Authn Research Spike	"Research the master authentication flags to see what changes will be necessary for agent http authentication.
Write up a 1-2 page summary/design doc."	MESOS	Resolved	3	3	4664	mesosphere, security
13009312	Add authentication support to the default executor	"The V1 executors should be updated to authenticate with the agent when HTTP executor authentication is enabled. This will be hard-coded into the executor library for the MVP, and it can be refactored into an {{HttpAuthenticatee}} module later. The executor must:
* load a JWT from its environment, if present
* decorate its requests with an {{Authorization}} header containing the JWT"	MESOS	Resolved	3	4	4664	executor, mesosphere, module, security
13220951	Make operation reconciliation send asynchronous updates	Following discussion amongst the community, it seems better for frameworks to perform operation reconciliation the same way we do task state reconciliation: a reconciliation request from the scheduler should trigger an asynchronous stream of operation status updates on the scheduler's event stream, rather than a synchronous HTTP response which contains all updates.	MESOS	Resolved	3	3	4664	foundations, mesosphere
12950657	Enable actors to pass an authentication realm to libprocess	To prepare for MESOS-4902, the Mesos master and agent need a way to pass the desired authentication realm to libprocess. Since some endpoints (like {{/profiler/*}}) get installed in libprocess, the master/agent should be able to specify during initialization what authentication realm the libprocess-level endpoints will be authenticated under.	MESOS	Resolved	3	4	4664	authentication, http, mesosphere, security
12950805	Add authentication to /files endpoints	"To protect access (authz) to master/agent logs as well as executor sandboxes, we need authentication on the /files endpoints.

Adding HTTP authentication to these endpoints is a bit complicated since they are defined in code that is shared by the master and agent.

While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.

We should establish a mechanism for making an endpoint authenticated that allows us to:
1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent
2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess

Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other."	MESOS	Resolved	3	4	4664	authentication, http, mesosphere, security
13038204	Add an '--executor_secret_key' flag to the agent	A new {{\-\-executor_secret_key}} flag should be added to the agent to allow the operator to specify a secret file to be loaded into the default executor JWT authenticator and SecretGenerator modules. This secret will be used to generate default executor secrets when {{\-\-generate_executor_secrets}} is set, and will be used to verify those secrets when {{\-\-authenticate_http_executors}} is set.	MESOS	Resolved	3	3	4664	agent, flags, mesosphere, security
13073978	Add an agent flag for executor re-registration timeout.	Currently, the executor re-register timeout is hard-coded at 2 seconds. It would be beneficial to allow operators to specify this value.	MESOS	Resolved	3	4	4664	mesosphere
13211339	Master should clean up operations from downgraded agents	"If a Mesos agent is upgraded to provide reliable feedback for operations on agent default resources and then later downgraded, the master may possess in-memory state related to operations requesting feedback which should be cleaned up. We should update the master to detect downgraded agents and clean up appropriately.

This ticket does not include sending best-effort feedback to schedulers for operations on downgraded agents.

The upgrade documentation should also be updated with a note about the impact of downgrades on operation feedback."	MESOS	Resolved	3	3	4664	foundations, mesosphere
13003060	Make the disk usage isolator nesting-aware	With the addition of task groups, the disk usage isolator must be updated. Since sub-container sandboxes are nested within the parent container's sandbox, the isolator must exclude these folders from its usage calculation when examining the parent container's disk usage.	MESOS	Resolved	3	3	4664	mesosphere
12711995	reintroduce LIBPROCESS_STATISTICS_WINDOW	LIBPROCESS_STATISTICS_WINDOW used to be used to override the default history window duration for statistics - it should be reintroduced and plumbed through to metrics.	MESOS	Resolved	4	3	4664	mesosphere
13038045	Add agent support for generating and passing executor secrets	"The agent must generate and pass executor secrets to all executors using the V1 API. For MVP, the agent will have this behavior by default when compiled with SSL support. To accomplish this, the agent must:
* load the default {{SecretGenerator}} module
* call the secret generator when launching an executor
* pass the generated secret into the executor's environment"	MESOS	Resolved	3	3	4664	agent, executor, flags, mesosphere, security
12780525	Allow --resources flag to take JSON.	"Currently, we used a customized format for --resources flag. As we introduce more and more stuffs (e.g., persistence, reservation) in Resource object, we need a more generic way to specify --resources.

For backward compatibility, we can scan the first character. If it is '[', then we invoke the JSON parser. Otherwise, we use the existing parser."	MESOS	Resolved	3	4	4664	mesosphere
12913936	/reserve and /unreserve should be permissive under a master without authentication.	Currently, the {{/reserve}} and {{/unreserve}} endpoints do not work without authentication enabled on the master. When authentication is disabled on the master, these endpoints should just be permissive.	MESOS	Resolved	3	1	4664	authentication, mesosphere, reservations
12849471	CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseRSS Flaky	"Test will occasionally with:

[ RUN      ] CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseUnlockedRSS
../../src/tests/containerizer/cgroups_tests.cpp:1103: Failure
helper.increaseRSS(getpagesize()): Failed to sync with the subprocess
../../src/tests/containerizer/cgroups_tests.cpp:1103: Failure
helper.increaseRSS(getpagesize()): The subprocess has not been spawned yet
[  FAILED  ] CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseUnlockedRSS (223 ms)"	MESOS	Open	3	1	4664	cgroups, disabled-test, flaky-test, mesosphere
12838512	Random recursive_mutex errors in when running make check	"While running make check on OS X, from time to time {{recursive_mutex}} errors appear after running all the test successfully. Just one of the experience messages actually stops {{make check}} reporting an error.

The following error messages have been experienced:

{code}
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument


*** Aborted at 1434553937 (unix time) try ""date -d @1434553937"" if you are using GNU date ***
{code}

{code}
libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument
*** Aborted at 1434557001 (unix time) try ""date -d @1434557001"" if you are using GNU date ***
libc++abi.dylib: PC: @     0x7fff93855286 __pthread_kill
libc++abi.dylib: *** SIGABRT (@0x7fff93855286) received by PID 88060 (TID 0x10fc40000) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
libc++abi.dylib:     @        0x10fc3f1a8 (unknown)
libc++abi.dylib:     @     0x7fff979deb53 abort
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentMaking check in include
{code}

{code}
Assertion failed: (e == 0), function ~recursive_mutex, file /SourceCache/libcxx/libcxx-120/src/mutex.cpp, line 82.
*** Aborted at 1434555685 (unix time) try ""date -d @1434555685"" if you are using GNU date ***
PC: @     0x7fff93855286 __pthread_kill
*** SIGABRT (@0x7fff93855286) received by PID 60235 (TID 0x7fff7ebdc300) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
    @        0x10b512350 google::CheckNotNull<>()
    @     0x7fff979deb53 abort
    @     0x7fff979a6c39 __assert_rtn
    @     0x7fff9bffdcc9 std::__1::recursive_mutex::~recursive_mutex()
    @        0x10b881928 process::ProcessManager::~ProcessManager()
    @        0x10b874445 process::ProcessManager::~ProcessManager()
    @        0x10b874418 process::finalize()
    @        0x10b2f7aec main
    @     0x7fff98edc5c9 start
make[5]: *** [check-local] Abort trap: 6
make[4]: *** [check-am] Error 2
make[3]: *** [check-recursive] Error 1
make[2]: *** [check-recursive] Error 1
make[1]: *** [check] Error 2
make: *** [check-recursive] Error 1
{code}"	MESOS	Resolved	3	1	4664	mesosphere, tech-debt
12981093	Invalid resources sent to '/reserve' are silently dropped	"If an invalid resource is passed to the master's {{/reserve}} endpoint, it will be silently dropped and not cause an error. This can lead, for example, to a {{/reserve}} request containing a single invalid resource receiving a 200 OK response, despite the fact that no resources were reserved as a result of the request.

This is due to the fact that the {{+=}} operator for {{Resources}} silently drops invalid resources, and this operator is used when parsing the resources in the HTTP request. This could be addressed by validating the resource objects one at a time as they are parsed."	MESOS	Resolved	3	1	4664	mesosphere
12982040	Master captures `this` when creating authorization callback	When exposing its log file, the master currently installs an authorization callback for the log file which captures the master's {{this}} pointer. Such captures have previously caused bugs (MESOS-5629), and this one should be fixed as well. The callback should be dispatched to the master process, and it should be dispatched via the {{self()}} PID.	MESOS	Resolved	1	1	4664	mesosphere
12895936	Build instructions for CentOS 6.6 should include `sudo yum update`	Neglecting to run {{sudo yum update}} on CentOS 6.6 currently causes the build to break when building {{mesos-0.25.0.jar}}. The build instructions for this platform on the Getting Started page should be changed accordingly.	MESOS	Resolved	3	1	4664	documentation, mesosphere
13027940	SSL socket's 'shutdown()' method is broken	"We recently uncovered two issues with the {{LibeventSSLSocketImpl::shutdown}} method:
* The introduction of a shutdown method parameter with [this commit|https://reviews.apache.org/r/54113/] means that the implementation's method is no longer overriding the default implementation. In addition to fixing the implementation method's signature, we should add the {{override}} specifier to all of our socket implementations' methods to ensure that this doesn't happen in the future.
* The {{LibeventSSLSocketImpl::shutdown}} function does not actually shutdown the SSL socket. The proper function to shutdown an SSL socket is {{SSL_shutdown}}, which is called in the implementation's destructor. We should move this into {{shutdown()}} so that by the time that method returns, the socket has actually been shutdown."	MESOS	Resolved	3	1	4664	encryption, libprocess, ssl
12931512	Add persistent volume endpoint tests with no principal	There are currently no persistent volume endpoint tests that do not use a principal; they should be added.	MESOS	Resolved	3	1	4664	mesosphere, persistent-volumes, tests
12922137	Extend `Master` to authorize persistent volumes	"This ticket is the second in a series that adds authorization support for persistent volumes.

Methods {{Master::authorizeCreateVolume()}} and {{Master::authorizeDestroyVolume}} must be added to allow the Master to authorize these operations."	MESOS	Resolved	3	1	4664	persistent-volumes
12775976	Test script for verifying compatibility between Mesos components	"While our current unit/integration test suite catches functional bugs, it doesn't catch compatibility bugs (e.g, MESOS-2371). This is really crucial to provide operators the ability to do seamless upgrades on live clusters.

We should have a test suite / framework (ideally running on CI vetting each review on RB) that tests upgrade paths between master, slave, scheduler and executor."	MESOS	Resolved	3	4	4664	mesosphere, tests, upgrade
12906835	Backticks are not mentioned in Mesos C++ Style Guide	"As far as I can tell, current practice is to quote code excerpts and object names with backticks when writing comments. For example:

{code}
// You know, `sadPanda` seems extra sad lately.
std::string sadPanda;
sadPanda = ""   :'(   "";
{code}

However, I don't see this documented in our C++ style guide at all. It should be added."	MESOS	Resolved	4	20	4664	documentation, mesosphere
12977078	Masters may drop the first message they send between masters after a network partition	"We observed the following situation in a cluster of five masters:
|| Time || Master 1 || Master 2 || Master 3 || Master 4 || Master 5 ||
| 0 | Follower | Follower | Follower | Follower | Leader |
| 1 | Follower | Follower | Follower | Follower || Partitioned from cluster by downing this VM's network ||
| 2 || Elected Leader by ZK | Voting | Voting | Voting | Suicides due to lost leadership |
| 3 | Performs consensus | Replies to leader | Replies to leader | Replies to leader | Still down |
| 4 | Performs writing | Acks to leader | Acks to leader | Acks to leader | Still down |
| 5 | Leader | Follower | Follower | Follower | Still down |
| 6 | Leader | Follower | Follower | Follower | Comes back up |
| 7 | Leader | Follower | Follower | Follower | Follower |
| 8 || Partitioned in the same way as Master 5 | Follower | Follower | Follower | Follower |
| 9 | Suicides due to lost leadership || Elected Leader by ZK | Follower | Follower | Follower |
| 10 | Still down | Performs consensus | Replies to leader | Replies to leader || Doesn't get the message! ||
| 11 | Still down | Performs writing | Acks to leader | Acks to leader || Acks to leader ||
| 12 | Still down | Leader | Follower | Follower | Follower |

Master 2 sends a series of messages to the recently-restarted Master 5.  The first message is dropped, but subsequent messages are not dropped.

This appears to be due to a stale link between the masters.  Before leader election, the replicated log actors create a network watcher, which adds links to masters that join the ZK group:
https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/network.hpp#L157-L159

This link does not appear to break (Master 2 -> 5) when Master 5 goes down, perhaps due to how the network partition was induced (in the hypervisor layer, rather than in the VM itself).

When Master 2 tries to send an {{PromiseRequest}} to Master 5, we do not observe the [expected log message|https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/replica.cpp#L493-L494]

Instead, we see a log line in Master 2:
{code}
process.cpp:2040] Failed to shutdown socket with fd 27: Transport endpoint is not connected
{code}

The broken link is removed by the libprocess {{socket_manager}} and the following {{WriteRequest}} from Master 2 to Master 5 succeeds via a new socket."	MESOS	Resolved	3	4	6057	mesosphere
13074700	Introduce a heartbeat mechanism for v1 HTTP executor <-> agent communication.	"Currently, we do not have heartbeats for executor <-> agent communication. This is especially problematic in scenarios when IPFilters are enabled since the default conntrack keep alive timeout is 5 days. When that timeout elapses, the executor doesn't get notified via a socket disconnection when the agent process restarts. The executor would then get killed if it doesn't re-register when the agent recovery process is completed.

Enabling application level heartbeats or TCP KeepAlive's can be a possible way for fixing this issue.

We should also update executor API documentation to explain the new behavior."	MESOS	Resolved	2	1	6057	api, foundations, mesosphere, v1_api
12919831	Containerizer logging modularization	"Executors and tasks are configured (via the various containerizers) to write their output (stdout/stderr) to files (""stdout"" and ""stderr"") on an agent's disk.

Unlike Master/Agent logs, executor/task logs are not attached to any formal logging system, like {{glog}}.  As such, there is significant scope for improvement.

By introducing a module for logging, we can provide a common/programmatic way to access and manage executor/task logs.  Modules could implement additional sinks for logs, such as:
* to the sandbox (the status quo),
* to syslog,
* to journald

This would also provide the hooks to deal with logging related problems, such as:
* the (current) lack of log rotation,
* searching through executor/task logs (i.e. via aggregation)"	MESOS	In Progress	3	15	6057	logging, mesosphere
12930269	Offers and InverseOffers cannot be accepted in the same ACCEPT call	"*Problem*
* In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call.
* If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)

Here's a regression test:
https://reviews.apache.org/r/42092/

*Proprosal*
The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.

Arguments for mixing:
* The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers.
* Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.

Arguments against mixing:
* Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a ""reason""?
* What happens if we presumably add a third type of offer?
* Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?"	MESOS	Resolved	3	1	6057	maintenance, mesosphere
12861115	Scope out approaches to deal with logging to finite disks (i.e. log rotation|capped-size logging).	"For the background, see the parent story [MESOS-3348].

For the work/design/discussion, see the linked design document (below).

"	MESOS	Resolved	3	3	6057	design, mesosphere
12918067	Investigate remaining flakiness in MasterMaintenanceTest.InverseOffersFilters	"Per comments in MESOS-3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur -- should be investigated.

*Flakiness in task acknowledgment*
{code}
I1203 18:25:04.609817 28732 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
W1203 18:25:04.610076 28732 status_update_manager.cpp:762] Unexpected status update acknowledgement (received 6afd012e-8e88-41b2-8239-a9b852d07ca1, expecting 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for update TASK_RUNNING (UUID: 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
E1203 18:25:04.610339 28736 slave.cpp:2339] Failed to handle status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000: Duplicate acknowledgemen
{code}

This is a race between [launching and acknowledging two tasks|https://github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/master_maintenance_tests.cpp#L1486-L1517].  The status updates for each task are not necessarily received in the same order as launching the tasks.

*Flakiness in first inverse offer filter*
See [this comment in MESOS-3916|https://issues.apache.org/jira/browse/MESOS-3916?focusedCommentId=15027478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15027478] for the explanation.  The related logs are above the comment."	MESOS	Resolved	4	1	6057	flaky-test, mesosphere
13231030	Agent V1 GET_STATE response may report a complete executor's tasks as non-terminal after a graceful agent shutdown	"When the following steps occur:
1) A graceful shutdown is initiated on the agent (i.e. SIGUSR1 or /master/machine/down).
2) The executor is sent a kill, and the agent counts down on {{executor_shutdown_grace_period}}.
3) The executor exits, before all terminal status updates reach the agent. This is more likely if {{executor_shutdown_grace_period}} passes.

This results in a completed executor, with non-terminal tasks (according to status updates).

When the agent starts back up, the completed executor will be recovered and shows up correctly  as a completed executor in {{/state}}.  However, if you fetch the V1 {{GET_STATE}} result, there will be an entry in {{launched_tasks}} even though nothing is running.
{code}
get_tasks {
  launched_tasks {
    name: ""test-task""
    task_id {
      value: ""dff5a155-47f1-4a71-9b92-30ca059ab456""
    }
    framework_id {
      value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-0000""
    }
    executor_id {
      value: ""default""
    }
    agent_id {
      value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-S0""
    }
    state: TASK_RUNNING
    resources { ... }
    resources { ... }
    resources { ... }
    resources { ... }
    statuses {
      task_id {
        value: ""dff5a155-47f1-4a71-9b92-30ca059ab456""
      }
      state: TASK_RUNNING
      agent_id {
        value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-S0""
      }
      timestamp: 1556674758.2175469
      executor_id {
        value: ""default""
      }
      source: SOURCE_EXECUTOR
      uuid: ""xPmn\234\236F&\235\\d\364\326\323\222\224""
      container_status { ... }
    }
  }
}
get_executors {
  completed_executors {
    executor_info {
      executor_id {
        value: ""default""
      }
      command {
        value: """"
      }
      framework_id {
        value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-0000""
      }
    }
  }
}
get_frameworks {
  completed_frameworks {
    framework_info {
      user: ""user""
      name: ""default""
      id {
        value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-0000""
      }
      checkpoint: true
      hostname: ""localhost""
      principal: ""test-principal""
      capabilities {
        type: MULTI_ROLE
      }
      capabilities {
        type: RESERVATION_REFINEMENT
      }
      roles: ""*""
    }
  }
}
{code}

This happens because we combine executors and completed executors when constructing the response.  The terminal task(s) with non-terminal updates appear under completed executors.
https://github.com/apache/mesos/blob/89c3dd95a421e14044bc91ceb1998ff4ae3883b4/src/slave/http.cpp#L1734-L1756"	MESOS	Resolved	3	1	6057	foundations
12986030	ProcessRemoteLinkTest.RemoteUseStaleLink and RemoteStaleLinkRelink are flaky	"{{ProcessRemoteLinkTest.RemoteUseStaleLink}} and {{ProcessRemoteLinkTest.RemoteStaleLinkRelink}} are failing occasionally with the error:
{code}
[ RUN      ] ProcessRemoteLinkTest.RemoteStaleLinkRelink
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0630 07:42:34.661110 18888 process.cpp:1066] libprocess is initialized on 172.17.0.2:56294 with 16 worker threads
E0630 07:42:34.666393 18765 process.cpp:2104] Failed to shutdown socket with fd 7: Transport endpoint is not connected
/mesos/3rdparty/libprocess/src/tests/process_tests.cpp:1059: Failure
Value of: exitedPid.isPending()
  Actual: false
Expected: true
[  FAILED  ] ProcessRemoteLinkTest.RemoteStaleLinkRelink (56 ms)
{code}

There appears to be a race between establishing a socket connection and the test calling {{::shutdown}} on the socket.  Under some circumstances, the {{::shutdown}} may actually result in failing the future in {{SocketManager::link_connect}} error and thereby trigger {{SocketManager::close}}."	MESOS	Resolved	3	1	6057	libprocess, mesosphere
12841797	Add implicit cast to string operator to Path.	"For example:

{code}inline Try<Nothing> rm(const std::string& path){code} does not have an overload for {code}inline Try<Nothing> rm(const Path& path){code}

The implementation should be something like: 
{code}
inline Try<Nothing> rm(const Path& path)
{
  rm(path.value);
}
{code}"	MESOS	Resolved	4	4	6057	beginner, mesosphere, newbie, stout
12983892	SSL-enabled libprocess will leak incoming links to forks	"Encountered two different buggy behaviors that can be tracked down to the same underlying problem.

Repro #1 (non-crashy):
(1) Start a master.  Doesn't matter if SSL is enabled or not.
(2) Start an agent, with SSL enabled.  Downgrade support has the same problem.  The master/agent {{link}} to one another.
(3) Run a sleep task.  Keep this alive.  If you inspect FDs at this point, you'll notice the task has inherited the {{link}} FD (master -> agent).
(4) Restart the agent.  Due to (3), the master's {{link}} stays open.
(5) Check master's logs for the agent's re-registration message.
(6) Check the agent's logs for re-registration.  The message will not appear.  The master is actually using the old {{link}} which is not connected to the agent.

----

Repro #2 (crashy):
(1) Start a master.  Doesn't matter if SSL is enabled or not.
(2) Start an agent, with SSL enabled.  Downgrade support has the same problem.
(3) Run ~100 sleep task one after the other, keep them all alive.  Each task links back to the agent.  Due to an FD leak, each task will inherit the incoming links from all other actors...
(4) At some point, the agent will run out of FDs and kernel panic.

----

It appears that the SSL socket {{accept}} call is missing {{os::nonblock}} and {{os::cloexec}} calls:
https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L794-L806

For reference, here's {{poll}} socket's {{accept}}:
https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/poll_socket.cpp#L53-L75
"	MESOS	Resolved	1	1	6057	libprocess, mesosphere, ssl
13024733	Mesos tests generated with cmake build fail to unload libraries properly	"A default cmake build created from {{ec0546e}} creates a {{mesos-tests}} which cannot unload dependency without an error,
{code}
$ ./src/mesos-tests  --gtest_filter=''
Source directory: /vagrant
Build directory: /home/vagrant/mesos
Note: Google Test filter =
[==========] Running 0 tests from 0 test cases.
[==========] 0 tests from 0 test cases ran. (0 ms total)
[  PASSED  ] 0 tests.
Inconsistency detected by ld.so: dl-close.c: 762: _dl_close: Assertion `map->l_init_called' failed!
{code}
This problem appears e.g., ubuntu-14.04 with cmake-2.8.12, but also on debian-8, or ubuntu-16."	MESOS	Resolved	3	1	6057	mesosphere
12846395	Standardize separation of Windows/Linux-specific OS code	"There are 50+ files that must be touched to separate OS-specific code.

First, we will standardize the changes by using stout/abort.hpp as an example.
The review/discussion can be found here:
https://reviews.apache.org/r/36625/"	MESOS	Resolved	3	3	6057	mesosphere
13006985	Potential socket leak during Zookeeper network changes	"There is a potential leak when using the version of {{link}} with {{RemoteConnection::RECONNECT}}.  This was originally implemented to refresh links during master recovery. 

The leak occurs here:
https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1592-L1597
^ The comment here is not correct, as that is *not* the last reference to the {{existing}} socket.

At this point, the {{existing}} socket may be a perfectly valid link.  Valid links will all have a reference inside a callback loop created here:
https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1503-L1509

-----

We need to stop the callback loop but prevent any resulting {{ExitedEvents}} from being sent due to stopping the callback loop.  This means discarding the callback loop's future after we have called {{swap_implementing_socket}}."	MESOS	Resolved	3	1	6057	libprocess, mesosphere
12944005	MasterMaintenanceTest.InverseOffers is flaky	"[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].

Verbose logs from ASF Centos7 build:
{code}
[ RUN      ] MasterMaintenanceTest.InverseOffers
I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms
I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns
I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns
I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns
I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns
I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery
I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status
I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678
I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING
I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns
I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING
I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status
I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678
I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status
I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING
I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns
I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING
I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group
I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated
I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678
I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs""
I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register
I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register
I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials'
I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator
I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator
I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled
I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process
I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given
I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880
I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master!
I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar
I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar
I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer
I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1
I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns
I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1
I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2
I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns
I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678
I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns
I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms
I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms
I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0
I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0
I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0
I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns
I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms
I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry'
I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log
I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678
I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns
I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns
I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1
I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms
I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar
I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1
I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678
I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns
I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2
I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns
I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns
I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2
I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678
I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF""
I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential'
I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal
I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ]
I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host
I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta'
I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager
I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery
I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678
I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates
I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678
I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master
I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection
I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678
I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection
I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start
I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps
I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step
I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step
I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success
I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success
I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678
I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678
I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary
I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry'
I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'
I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095
I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log
I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678
I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns
I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3
I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary
I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns
I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3
I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3
I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress
I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms
I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry'
I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3
I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache
I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns
I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info'
I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates
I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678
I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources 
I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources 
I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns
I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4
I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678
I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns
I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns
I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns
I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4
I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log
I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678
I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms
I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0224 22:35:53.812348  1979 leveldb.cpp:341] Persisting action (486 bytes) to leveldb took 318928ns
I0224 22:35:53.812376  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.812397  1979 replica.cpp:697] Replica learned APPEND action at position 5
I0224 22:35:53.815132  1973 registrar.cpp:484] Successfully updated the 'registry' in 15.437312ms
I0224 22:35:53.815491  1976 log.cpp:702] Attempting to truncate the log to 5
I0224 22:35:53.815610  1973 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0224 22:35:53.815661  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.815845  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.816069  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816103  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 175822ns
I0224 22:35:53.816272  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816303  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 110913ns
I0224 22:35:53.817291  1972 replica.cpp:537] Replica received write request for position 6 from (4550)@172.17.0.1:36678
I0224 22:35:53.817908  1972 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 576032ns
I0224 22:35:53.817932  1972 replica.cpp:712] Persisted action at 6
I0224 22:35:53.818686  1980 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0224 22:35:53.819021  1980 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 305298ns
I0224 22:35:53.819095  1980 leveldb.cpp:399] Deleting ~2 keys from leveldb took 44332ns
I0224 22:35:53.819120  1980 replica.cpp:712] Persisted action at 6
I0224 22:35:53.819162  1980 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0224 22:35:53.820662  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/status'
I0224 22:35:53.821190  1976 http.cpp:501] HTTP GET for /master/maintenance/status from 172.17.0.1:45096
I0224 22:35:53.823709  1948 scheduler.cpp:154] Version: 0.28.0
I0224 22:35:53.824424  1972 scheduler.cpp:236] New master detected at master@172.17.0.1:36678
I0224 22:35:53.825402  1982 scheduler.cpp:298] Sending SUBSCRIBE call to master@172.17.0.1:36678
I0224 22:35:53.827201  1978 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.827636  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45097
I0224 22:35:53.827922  1978 master.cpp:1974] Received subscription request for HTTP framework 'default'
I0224 22:35:53.827991  1978 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 22:35:53.828418  1982 master.cpp:2065] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0224 22:35:53.828943  1968 hierarchical.cpp:265] Added framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829124  1982 master.hpp:1657] Sending heartbeat to aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829987  1968 hierarchical.cpp:1127] Performed allocation for 1 slaves in 1.011356ms
I0224 22:35:53.830204  1982 master.cpp:5355] Sending 1 offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.830801  1982 master.cpp:5445] Sending 1 inverse offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.831132  1969 scheduler.cpp:457] Enqueuing event SUBSCRIBED received from master@172.17.0.1:36678
I0224 22:35:53.832396  1968 scheduler.cpp:457] Enqueuing event HEARTBEAT received from master@172.17.0.1:36678
I0224 22:35:53.833050  1976 master_maintenance_tests.cpp:177] Ignoring HEARTBEAT event
I0224 22:35:53.833256  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.833775  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.835662  1980 scheduler.cpp:298] Sending ACCEPT call to master@172.17.0.1:36678
I0224 22:35:53.837591  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.838021  1967 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45098
I0224 22:35:53.838851  1967 master.cpp:3138] Processing ACCEPT call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O0 ] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.838946  1967 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 as user 'mesos'
W0224 22:35:53.841048  1967 validation.cpp:404] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 22:35:53.841101  1967 validation.cpp:416] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 22:35:53.841624  1967 master.hpp:176] Adding task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host)
I0224 22:35:53.842157  1967 master.cpp:3623] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.842571  1980 slave.cpp:1361] Got assigned task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843122  1980 slave.cpp:1480] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843718  1980 paths.cpp:474] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' to user 'mesos'
I0224 22:35:53.852052  1980 slave.cpp:5367] Launching executor default of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.854452  1980 exec.cpp:143] Version: 0.28.0
I0224 22:35:53.854812  1967 exec.cpp:193] Executor started at: executor(47)@172.17.0.1:36678 with pid 1948
I0224 22:35:53.855108  1980 slave.cpp:1698] Queuing task '90bcae0c-9d40-40b7-9537-dae7e83479f6' for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.855264  1980 slave.cpp:749] Successfully attached file '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.855362  1980 slave.cpp:2643] Got registration for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.855785  1974 exec.cpp:217] Executor registered on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.855857  1974 exec.cpp:229] Executor::registered took 42512ns
I0224 22:35:53.856391  1980 slave.cpp:1863] Sending queued task '90bcae0c-9d40-40b7-9537-dae7e83479f6' to executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:35:53.856720  1974 exec.cpp:304] Executor asked to run task '90bcae0c-9d40-40b7-9537-dae7e83479f6'
I0224 22:35:53.856812  1974 exec.cpp:313] Executor::launchTask took 65703ns
I0224 22:35:53.856922  1974 exec.cpp:526] Executor sending status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.857378  1980 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.858175  1980 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858222  1980 status_update_manager.cpp:497] Creating StatusUpdate stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858687  1980 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:35:53.859210  1980 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:35:53.859390  1980 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859436  1980 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to executor(47)@172.17.0.1:36678
I0224 22:35:53.859663  1980 exec.cpp:350] Executor received status update acknowledgement 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859657  1967 master.cpp:4794] Status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.859851  1967 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.860587  1967 master.cpp:6450] Updating the state of task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0224 22:35:53.862711  1967 scheduler.cpp:457] Enqueuing event UPDATE received from master@172.17.0.1:36678
I0224 22:35:53.866711  1976 scheduler.cpp:298] Sending ACKNOWLEDGE call to master@172.17.0.1:36678
I0224 22:35:53.870667  1972 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.871269  1972 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45099
I0224 22:35:53.871459  1972 master.cpp:3952] Processing ACKNOWLEDGE call 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.872184  1972 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.872537  1972 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.874407  1975 scheduler.cpp:298] Sending DECLINE call to master@172.17.0.1:36678
I0224 22:35:53.877537  1979 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.877795  1979 hierarchical.cpp:1127] Performed allocation for 1 slaves in 482441ns
I0224 22:35:53.878082  1981 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.878675  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45100
I0224 22:35:53.878931  1978 master.cpp:3675] Processing DECLINE call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O1 ] for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
../../src/tests/master_maintenance_tests.cpp:1222: Failure
Failed to wait 15secs for event
I0224 22:36:08.881649  1948 master.cpp:1027] Master terminating
W0224 22:36:08.881925  1948 master.cpp:6502] Removing task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) in non-terminal state TASK_RUNNING
I0224 22:36:08.882961  1948 master.cpp:6545] Removing executor 'default' with resources  of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:36:08.884789  1969 hierarchical.cpp:505] Removed slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:36:08.887261  1969 hierarchical.cpp:326] Removed framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.916983  1976 slave.cpp:3528] master@172.17.0.1:36678 exited
W0224 22:36:08.917191  1976 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0224 22:36:08.934546  1975 slave.cpp:3528] executor(47)@172.17.0.1:36678 exited
I0224 22:36:08.934806  1974 slave.cpp:3886] Executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 exited with status 0
I0224 22:36:08.935024  1974 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from @0.0.0.0:0
I0224 22:36:08.935505  1974 slave.cpp:5677] Terminating task 90bcae0c-9d40-40b7-9537-dae7e83479f6
I0224 22:36:08.936190  1967 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.936368  1967 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:36:08.936606  1974 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:36:08.936779  1974 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955370  1967 slave.cpp:668] Slave terminating
I0224 22:36:08.955499  1967 slave.cpp:2079] Asked to shut down framework aab18b61-7811-4c43-a672-d1a63818c880-0000 by @0.0.0.0:0
I0224 22:36:08.955538  1967 slave.cpp:2104] Shutting down framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955606  1967 slave.cpp:3990] Cleaning up executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:36:08.956053  1967 slave.cpp:4078] Cleaning up framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956327  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956495  1973 status_update_manager.cpp:282] Closing status update streams for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956524  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956549  1973 status_update_manager.cpp:528] Cleaning up status update stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956619  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000' for gc 1.00002336880296weeks in the future
[  FAILED  ] MasterMaintenanceTest.InverseOffers (15258 ms)
{code}"	MESOS	Resolved	3	1	6057	mesosphere, test
13214218	Operations are leaked in Framework struct when agents are removed	Currently, when agents are removed from the master, their operations are not removed from the {{Framework}} structs. We should ensure that this occurs in all cases.	MESOS	Resolved	3	1	6057	foundations, mesosphere
13036034	SchedulerTest.MasterFailover is flaky	"This was observed in a CentOS 7 VM, with libevent and SSL enabled:
{code}
W0118 22:38:33.789465  3407 scheduler.cpp:513] Dropping SUBSCRIBE: Scheduler is in state DISCONNECTED
I0118 22:38:33.811820  3408 scheduler.cpp:361] Connected with the master at http://127.0.0.1:43211/master/api/v1/scheduler
../../src/tests/scheduler_tests.cpp:315: Failure
Mock function called more times than expected - returning directly.
    Function call: connected(0x7fff97227550)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{code}

Find attached the entire log from a failed run."	MESOS	Resolved	3	1	6057	flaky-test, tests
12936993	ROOT_DOCKER_DockerHealthyTask is flaky.	"Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:
{noformat}
[18:27:14][Step 8/8] [----------] 8 tests from HealthCheckTest
[18:27:14][Step 8/8] [ RUN      ] HealthCheckTest.HealthyTask
[18:27:17][Step 8/8] [       OK ] HealthCheckTest.HealthyTask (2222 ms)
[18:27:17][Step 8/8] [ RUN      ] HealthCheckTest.ROOT_DOCKER_DockerHealthyTask
[18:27:36][Step 8/8] ../../src/tests/health_check_tests.cpp:388: Failure
[18:27:36][Step 8/8] Failed to wait 15secs for termination
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
[18:27:36][Step 8/8]     @          0x16eb7b2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e61a9  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16c56aa  testing::Test::Run()
[18:27:36][Step 8/8]     @          0x16c5e89  testing::TestInfo::Run()
[18:27:36][Step 8/8]     @          0x16c650a  testing::TestCase::Run()
[18:27:36][Step 8/8]     @          0x16cd1f6  testing::internal::UnitTestImpl::RunAllTests()
[18:27:36][Step 8/8]     @          0x16ec513  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e6df1  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16cbe26  testing::UnitTest::Run()
[18:27:36][Step 8/8]     @           0xe54c84  RUN_ALL_TESTS()
[18:27:36][Step 8/8]     @           0xe54867  main
[18:27:36][Step 8/8]     @     0x7f7071560a40  (unknown)
[18:27:36][Step 8/8]     @           0x9b52d9  _start
[18:27:36][Step 8/8] Aborted (core dumped)
[18:27:36][Step 8/8] Process exited with code 134
{noformat}
Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. "	MESOS	Resolved	3	1	6057	flaky-test, health-check, mesosphere, test
12936838	Logrotate ContainerLogger should not remove IP from environment.	"The {{LogrotateContainerLogger}} starts libprocess-using subprocesses.  Libprocess initialization will attempt to resolve the IP from the hostname.  If a DNS service is not available, this step will fail, which terminates the logger subprocess prematurely.

Since the logger subprocesses live on the agent, they should use the same {{LIBPROCESS_IP}} supplied to the agent."	MESOS	Resolved	3	1	6057	mesosphere
12875727	Segfault when accepting or declining inverse offers	"Discovered while writing a test for filters (in regards to inverse offers).

Fix here: https://reviews.apache.org/r/38470/"	MESOS	Resolved	1	1	6057	mesosphere
13207524	Benchmark command health checks in default executor	"TCP/HTTP health checks were extensively scale tested as part of https://mesosphere.com/blog/introducing-mesos-native-health-checks-apache-mesos-part-2/. 

We should do the same for command checks by default executor because it uses a very different mechanism (agent fork/execs the check command as a nested container) and will have very different scalability characteristics.

We should also use these benchmarks as an opportunity to produce perf traces of the Mesos agent (both with and without process inheritance) so that a thorough analysis of the performance can be done as part of MESOS-9513."	MESOS	Resolved	3	3	6057	default-executor, foundations, mesosphere, perfomance
13164451	SlaveRecoveryTest/0.PingTimeoutDuringRecovery is flaky	"During an unrelated change in a PR, the apache build bot sent the following error:


{noformat}
    @   00007FF71117D888  std::invoke<<lambda_9f5bb6c728b761604e288ae85a7b250c>,process::Future<Option<mesos::MasterInfo> >,process::ProcessBase *>
    @   00007FF71119257B  lambda::internal::Partial<<lambda_9f5bb6c728b761604e288ae85a7b250c>,process::Future<Option<mesos::MasterInfo> >,std::_Ph<1> >::invoke_expand<<lambda_9f5bb6c728b761604e288ae85a7b250c>,std::tuple<process::Future<Option<mesos::MasterInfo> >,std::_Ph<1> >,st
    @   00007FF7110C08BA  )<process::ProcessBase *
    @   00007FF7110F058C  std::_Invoker_functor::_Call<lambda::internal::Partial<<lambda_9f5bb6c728b761604e288ae85a7b250c>,process::Future<Option<mesos::MasterInfo> >,std::_Ph<1> >,process::ProcessBase *>
    @   00007FF711183EBC  std::invoke<lambda::internal::Partial<<lambda_9f5bb6c728b761604e288ae85a7b250c>,process::Future<Option<mesos::MasterInfo> >,std::_Ph<1> >,process::ProcessBase *>
    @   00007FF7110C9F21  )<lambda::internal::Partial<<lambda_9f5bb6c728b761604e288ae85a7b250c>,process::Future<Option<mesos::MasterInfo> >,std::_Ph<1> >,process::ProcessBase *
    @   00007FF711236416  process::ProcessBase *)>::CallableFn<lambda::internal::Partial<<lambda_9f5bb6c728b761604e288ae85a7b250c>,process::Future<Option<mesos::MasterInfo> >,std::_Ph<1> > >::operator(
    @   00007FF712C1A25D  process::ProcessBase *)>::operator(
    @   00007FF712ACB2F9  process::ProcessBase::consume
    @   00007FF712C738CA  process::DispatchEvent::consume
    @   00007FF70ECE7B07  process::ProcessBase::serve
    @   00007FF712AD93B0  process::ProcessManager::resume
    @   00007FF712C07371   ?? 
    @   00007FF712B2B130  std::_Invoker_functor::_Call<<lambda_124422ac022fa041208b80c1460630d7> >
    @   00007FF712B8B8E0  std::invoke<<lambda_124422ac022fa041208b80c1460630d7> >
    @   00007FF712B4076C  std::_LaunchPad<std::unique_ptr<std::tuple<<lambda_124422ac022fa041208b80c1460630d7> >,std::default_delete<std::tuple<<lambda_124422ac022fa041208b80c1460630d7> > > > >::_Execute<0>
    @   00007FF712C5A60A  std::_LaunchPad<std::unique_ptr<std::tuple<<lambda_124422ac022fa041208b80c1460630d7> >,std::default_delete<std::tuple<<lambda_124422ac022fa041208b80c1460630d7> > > > >::_Run
    @   00007FF712C45E78  std::_LaunchPad<std::unique_ptr<std::tuple<<lambda_124422ac022fa041208b80c1460630d7> >,std::default_delete<std::tuple<<lambda_124422ac022fa041208b80c1460630d7> > > > >::_Go
    @   00007FF712C2C3CD  std::_Pad::_Call_func
    @   00007FFF9BE53428  _register_onexit_function
    @   00007FFF9BE53071  _register_onexit_function
    @   00007FFFB6391FE4  BaseThreadInitThunk
    @   00007FFFB69FF061  RtlUserThreadStart
ll containerizers
I0606 10:25:26.680230 18356 slave.cpp:7158] Recovering executors
I0606 10:25:26.680230 18356 slave.cpp:7182] Sending reconnect request to executor '3f11d255-bb7b-4e99-967b-055fef95b595' of framework 62cf792a-dc69-4e3c-b54f-d83f98fb9451-0000 at executor(1)@192.10.1.5:55652
I0606 10:25:26.688225 22560 slave.cpp:4984] Received re-registration message from executor '3f11d255-bb7b-4e99-967b-055fef95b595' of framework 62cf792a-dc69-4e3c-b54f-d83f98fb9451-0000
I0606 10:25:26.691216 22888 slave.cpp:5901] No pings from master received within 75secs
F0606 10:25:26.692219 22888 slave.cpp:1249] Check failed: state == DISCONNECTED || state == RUNNING || state == TERMINATING RECOVERING
{noformat}"	MESOS	Accepted	3	1	6057	flaky-test, foundations
12754302	Add InverseOffer protobuf message.	"InverseOffer was defined as part of the maintenance work in MESOS-1474, design doc here: https://docs.google.com/document/d/16k0lVwpSGVOyxPSyXKmGC-gbNmRlisNEe4p-fAUSojk/edit?usp=sharing

{code}
/**
 * A request to return some resources occupied by a framework.
 */
message InverseOffer {
  required OfferID id = 1;
  required FrameworkID framework_id = 2;

  // A list of resources being requested back from the framework.
  repeated Resource resources = 3;

  // Specified if the resources need to be released from a particular slave.
  optional SlaveID slave_id = 4;

  // The resources in this InverseOffer are part of a planned maintenance
  // schedule in the specified window.  Any tasks running using these
  // resources may be killed when the window arrives.
  optional Interval unavailability = 5;
}
{code}

This ticket is to capture the addition of the InverseOffer protobuf to mesos.proto, the necessary API changes for Event/Call and the language bindings will be tracked separately."	MESOS	Resolved	3	3	6057	mesosphere
12911998	Libprocess: Implement process::Clock::finalize	"Tracks this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L974-L975].

The {{Clock}} is initialized with a callback that, among other things, will dereference the global {{process_manager}} object.

When libprocess is shutting down, the {{process_manager}} is cleaned up.  Between cleanup and termination of libprocess, there is some chance that a {{Timer}} will time out and result in dereferencing {{process_manager}}.

*Proposal* 
* Implement {{Clock::finalize}}.  This would clear:
** existing timers
** process-specific clocks
** ticks
* Change {{process::finalize}}.
*# Resume the clock.  (The clock is only paused during some tests.)  When the clock is not paused, the callback does not dereference {{process_manager}}.
*# Clean up {{process_manager}}.  This terminates all the processes that would potentially interact with {{Clock}}.
*# Call {{Clock::finalize}}."	MESOS	Resolved	3	3	6057	mesosphere
12860823	Expand the range of integer precision when converting into/out of json.	"For [MESOS-3299], we added some protobufs to represent time with integer precision.  However, this precision is not maintained through protobuf <-> JSON conversion, because of how our JSON encoders/decoders convert numbers to floating point.

To maintain precision, we can try one of the following:
* Try using a {{long double}} to represent a number.
* Add logic to stringify/parse numbers without loss when possible.
* Try representing {{int64_t}} as a string and parse it as such?
* Update PicoJson and add a compiler flag, i.e. {{-DPICOJSON_USE_INT64}} 

In all cases, we'll need to make sure that:
* Integers are properly stringified without loss.
* The JSON decoder parses the integer without loss.
* We have some unit tests for big (close to {{INT32_MAX}}/{{INT64_MAX}}) and small integers."	MESOS	Resolved	4	3	6057	json, mesosphere, protobuf
13041979	Test ContentTypeAndSSLConfig/SchedulerSSLTest.RunTaskAndTeardown/1 segfaults	"{{ContentTypeAndSSLConfig/SchedulerSSLTest.RunTaskAndTeardown/1}} segfaulted in our internal CI:
{noformat}
[ RUN      ] ContentTypeAndSSLConfig/SchedulerSSLTest.RunTaskAndTeardown/1
W0210 03:08:05.018744  1020 process.cpp:3029] Attempted to spawn a process (__http_connection__(1079)@10.168.212.35:42363) after finalizing libprocess!
*** Aborted at 1486696085 (unix time) try ""date -d @1486696085"" if you are using GNU date ***
I0210 03:08:05.023609  6019 process.cpp:1246] libprocess is initialized on 10.168.212.35:44850 with 8 worker threads
I0210 03:08:05.024163  6019 cluster.cpp:160] Creating default 'local' authorizer
I0210 03:08:05.025065  1025 master.cpp:383] Master 7adcbe15-38a9-4512-aa9c-8d5f7538e4ee (ip-10-168-212-35.ec2.internal) started on 10.168.212.35:44850
I0210 03:08:05.025089  1025 master.cpp:385] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/5DRa8u/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/5DRa8u/master"" --zk_session_timeout=""10secs""
I0210 03:08:05.025264  1025 master.cpp:435] Master only allowing authenticated frameworks to register
I0210 03:08:05.025276  1025 master.cpp:449] Master only allowing authenticated agents to register
I0210 03:08:05.025285  1025 master.cpp:462] Master only allowing authenticated HTTP frameworks to register
I0210 03:08:05.025293  1025 credentials.hpp:37] Loading credentials for authentication from '/tmp/5DRa8u/credentials'
I0210 03:08:05.025387  1025 master.cpp:507] Using default 'crammd5' authenticator
I0210 03:08:05.025441  1025 http.cpp:919] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0210 03:08:05.025512  1025 http.cpp:919] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0210 03:08:05.025560  1025 http.cpp:919] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0210 03:08:05.025619  1025 master.cpp:587] Authorization enabled
I0210 03:08:05.025728  1023 hierarchical.cpp:161] Initialized hierarchical allocator process
I0210 03:08:05.025754  1027 whitelist_watcher.cpp:77] No whitelist given
PC: @     0x7f69d2296012 process::ProcessManager::spawn()
*** SIGSEGV (@0x0) received by PID 6019 (TID 0x7f69c46d5700) from PID 0; stack trace: ***
    @     0x7f69c2408725 (unknown)
I0210 03:08:05.026340  1023 master.cpp:2124] Elected as the leading master!
I0210 03:08:05.026357  1023 master.cpp:1646] Recovering from registrar
I0210 03:08:05.026406  1025 registrar.cpp:329] Recovering registrar
    @     0x7f69c240d2f1 (unknown)
    @     0x7f69c24011e8 (unknown)
I0210 03:08:05.027294  1024 registrar.cpp:362] Successfully fetched the registry (0B) in 865024ns
I0210 03:08:05.027330  1024 registrar.cpp:461] Applied 1 operations in 2848ns; attempting to update the registry
    @     0x7f69d027b370 (unknown)
I0210 03:08:05.028261  1028 registrar.cpp:506] Successfully updated the registry in 916992ns
I0210 03:08:05.028313  1028 registrar.cpp:392] Successfully recovered registrar
I0210 03:08:05.028419  1028 master.cpp:1762] Recovered 0 agents from the registry (172B); allowing 10mins for agents to re-register
I0210 03:08:05.028448  1026 hierarchical.cpp:188] Skipping recovery of hierarchical allocator: nothing to recover
    @     0x7f69d2296012 process::ProcessManager::spawn()
I0210 03:08:05.030078  6019 cluster.cpp:446] Creating default 'local' authorizer
I0210 03:08:05.030418  1021 slave.cpp:211] Mesos agent started on (818)@10.168.212.35:44850
I0210 03:08:05.030581  6019 scheduler.cpp:184] Version: 1.3.0
I0210 03:08:05.030442  1021 slave.cpp:212] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_ZqeHXq/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_ZqeHXq/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_ZqeHXq/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-centos-7/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_ZqeHXq"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_FPCV2X""
I0210 03:08:05.030650  1021 credentials.hpp:86] Loading credential for authentication from '/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_ZqeHXq/credential'
I0210 03:08:05.030712  1021 slave.cpp:354] Agent using credential for: test-principal
I0210 03:08:05.030727  1021 credentials.hpp:37] Loading credentials for authentication from '/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_ZqeHXq/http_credentials'
I0210 03:08:05.030791  1021 http.cpp:919] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0210 03:08:05.030834  1021 http.cpp:919] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0210 03:08:05.031044  1025 scheduler.cpp:470] New master detected at master@10.168.212.35:44850
I0210 03:08:05.031404  1021 slave.cpp:541] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0210 03:08:05.031440  1021 slave.cpp:549] Agent attributes: [  ]
I0210 03:08:05.031445  1021 slave.cpp:554] Agent hostname: ip-10-168-212-35.ec2.internal
I0210 03:08:05.031496  1022 status_update_manager.cpp:177] Pausing sending status updates
I0210 03:08:05.031793  1021 state.cpp:62] Recovering state from '/tmp/ContentTypeAndSSLConfig_SchedulerSSLTest_RunTaskAndTeardown_1_FPCV2X/meta'
I0210 03:08:05.031877  1021 status_update_manager.cpp:203] Recovering status update manager
I0210 03:08:05.031976  1025 scheduler.cpp:479] Waiting for 0ns before initiating a re-(connection) attempt with the master
I0210 03:08:05.032043  1027 slave.cpp:5555] Finished recovery
I0210 03:08:05.032328  1027 slave.cpp:5729] Querying resource estimator for oversubscribable resources
    @     0x7f69d229a646 process::spawn()
I0210 03:08:05.032439  1027 slave.cpp:931] New master detected at master@10.168.212.35:44850
I0210 03:08:05.032445  1022 status_update_manager.cpp:177] Pausing sending status updates
I0210 03:08:05.032481  1027 slave.cpp:966] Detecting new master
I0210 03:08:05.032542  1027 slave.cpp:5743] Received oversubscribable resources {} from the resource estimator
    @     0x7f69d222ee99 process::spawn<>()
    @     0x7f69d2210634 process::http::Connection::Connection()
    @     0x7f69d222b72c _ZNSt17_Function_handlerIFN7process6FutureINS0_4http10ConnectionEEEvEZNS2_7connectERKNS0_7network7AddressENS2_6SchemeEEUlvE_E9_M_invokeERKSt9_Any_data
    @     0x7f69d1b53e14 std::_Function_handler<>::_M_invoke()
    @     0x7f69d1b6f6e6 process::internal::thenf<>()
    @     0x7f69d33bc2d6 process::internal::run<>()
    @     0x7f69d33bdfd7 process::Future<>::_set<>()
    @     0x7f69d22eb1d7 process::network::internal::LibeventSSLSocketImpl::event_callback()
    @     0x7f69d22eb627 process::network::internal::LibeventSSLSocketImpl::event_callback()
    @     0x7f69cd7a95c0 (unknown)
    @     0x7f69cd79fb05 (unknown)
    @     0x7f69d22ff4cd process::EventLoop::run()
    @     0x7f69cfc0c230 (unknown)
    @     0x7f69d0273dc5 start_thread
    @     0x7f69cf37573d __clone
{noformat}"	MESOS	Accepted	3	1	6057	flaky-test, mesosphere, test
12911691	Simplify and/or document the libprocess initialization synchronization logic	"Tracks this [TODO|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L749].

The [synchronization logic of libprocess|https://github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff-128d3b56fc8c9ec0176fdbadcfd11fc2] [predates abstractions|https://github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff-0eebc8689450916990abe080d86c2acb] like {{process::Once}}, which is used in almost all other one-time initialization blocks.  

The logic should be documented.  It can also be simplified (see the [review description|https://reviews.apache.org/r/39949/]).  Or it can be replaced with {{process::Once}}."	MESOS	Resolved	4	3	6057	mesosphere
13068244	Refactor containerizers to not depend on TaskInfo or ExecutorInfo	"The Containerizer interfaces should be refactored so that they do not depend on {{TaskInfo}} or {{ExecutorInfo}}, as a standalone container will have neither.

Currently, the {{launch}} interface depends on those fields.  Instead, we should consistently use {{ContainerInfo}} and {{CommandInfo}} in Containerizer and isolators."	MESOS	Resolved	3	3	6057	mesosphere, storage
12906330	Mesos JSON API creates invalid JSON due to lack of binary data / non-ASCII handling	"Spark encodes some binary data into the ExecutorInfo.data field.  This field is sent as a ""bytes"" Protobuf value, which can have arbitrary non-UTF8 data.

If you have such a field, it seems that it is splatted out into JSON without any regards to proper character encoding:

{code}
0006b0b0  2e 73 70 61 72 6b 2e 65  78 65 63 75 74 6f 72 2e  |.spark.executor.|
0006b0c0  4d 65 73 6f 73 45 78 65  63 75 74 6f 72 42 61 63  |MesosExecutorBac|
0006b0d0  6b 65 6e 64 22 7d 2c 22  64 61 74 61 22 3a 22 ac  |kend""},""data"":"".|
0006b0e0  ed 5c 75 30 30 30 30 5c  75 30 30 30 35 75 72 5c  |.\u0000\u0005ur\|
0006b0f0  75 30 30 30 30 5c 75 30  30 30 66 5b 4c 73 63 61  |u0000\u000f[Lsca|
0006b100  6c 61 2e 54 75 70 6c 65  32 3b 2e cc 5c 75 30 30  |la.Tuple2;..\u00|
{code}

I suspect this is because the HTTP api emits the executorInfo.data directly:

{code}
JSON::Object model(const ExecutorInfo& executorInfo)
{
  JSON::Object object;
  object.values[""executor_id""] = executorInfo.executor_id().value();
  object.values[""name""] = executorInfo.name();
  object.values[""data""] = executorInfo.data();
  object.values[""framework_id""] = executorInfo.framework_id().value();
  object.values[""command""] = model(executorInfo.command());
  object.values[""resources""] = model(executorInfo.resources());
  return object;
}
{code}

I think this may be because the custom JSON processing library in stout seems to not have any idea of what a byte array is.  I'm guessing that some implicit conversion makes it get written as a String instead, but:

{code}
inline std::ostream& operator<<(std::ostream& out, const String& string)
{
  // TODO(benh): This escaping DOES NOT handle unicode, it encodes as ASCII.
  // See RFC4627 for the JSON string specificiation.
  return out << picojson::value(string.value).serialize();
}
{code}

Thank you for any assistance here.  Our cluster is currently entirely down -- the frameworks cannot handle parsing the invalid JSON produced (it is not even valid utf-8)
"	MESOS	Resolved	2	1	6057	mesosphere
12875728	Change /machine/up and /machine/down endpoints to take an array	"With [MESOS-3312] committed, the {{/machine/up}} and {{/machine/down}} endpoints should also take an input as an array.

It is important to change this before maintenance primitives are released:
https://reviews.apache.org/r/38011/

Also, a minor change to the error message from these endpoints:
https://reviews.apache.org/r/37969/"	MESOS	Resolved	3	3	6057	mesosphere
12754336	Add HTTP API to the master for maintenance operations.	"Based on MESOS-1474, we'd like to provide an HTTP API on the master for the maintenance primitives in mesos.

For the MVP, we'll want something like this for manipulating the schedule:
{code}
/maintenance/schedule
  GET - returns the schedule, which will include the various maintenance windows.
  POST - create or update the schedule with a JSON blob (see below).

/maintenance/status
  GET - returns a list of machines and their maintenance mode.

/maintenance/start
  POST - Transition a set of machines from Draining into Deactivated mode.

/maintenance/stop
  POST - Transition a set of machines from Deactivated into Normal mode.

/maintenance/consensus <- (Not sure what the right name is.  matrix?  acceptance?)
  GET - Returns the latest info on which frameworks have accepted or declined the maintenance schedule.
{code}
(Note: The slashes in URLs might not be supported yet.)

A schedule might look like:
{code}
{
  ""windows"" : [
    {
      ""machines"" : [
          { ""ip"" : ""192.168.0.1"" },
          { ""hostname"" : ""localhost"" },
          ...
        ], 
      ""unavailability"" : {
        ""start"" : 12345, // Epoch seconds.
        ""duration"" : 1000 // Seconds.
      }
    },
    ...
  ]
}
{code}

There should be firewall settings such that only those with access to master can use these endpoints."	MESOS	Resolved	3	3	6057	mesosphere
12846398	Separate OS-specific code in the stout library	"This issue tracks changes for all files under {{3rdparty/libprocess/3rdparty/stout/}}

The changes will be based on this commit:
https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c"	MESOS	Resolved	3	3	6057	mesosphere
12853939	Implement FindMesos cmake module.	"[~alexr] Has floated the idea of making a FindMesos package that will make it easy for people to build projects that use the system installation of Mesos as a dependency.

A good start is here: https://github.com/rukletsov/mesos-modules/blob/master/cmake-modules/FindMesos.cmake"	MESOS	Accepted	3	3	6057	build, cmake, mesosphere
12938019	Tests will dereference stack allocated agent objects upon assertion/expectation failure.	"Tests that use the {{StartSlave}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartSlave}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartSlave}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests."	MESOS	Resolved	3	1	6057	flaky, mesosphere, tech-debt, test
13275112	Implement SSL downgrade on the native SSL socket	The new SSL socket implementation (the non-libevent one) does not currently implement the SSL downgrade hack.  We could probably use {{peek}} to achieve the same result, or modify our socket BIO to look at the first few bytes.	MESOS	Resolved	4	3	6057	foundations, ssl
13245134	Mesos failed to build due to fatal error C1083 on Windows using MSVC.	"Mesos failed to build due to fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory on Windows using MSVC. It can be first reproduced on 6a026e3 reversion on master branch. Could you please take a look at this isssue? Thanks a lot!

Reproduce steps:

1. git clone -c core.autocrlf=true https://github.com/apache/mesos D:\mesos\src
2. Open a VS 2017 x64 command prompt as admin and browse to D:\mesos
3. cd src
4. .\bootstrap.bat
5. cd ..
6. mkdir build_x64 && pushd build_x64
7. cmake ..\src -G ""Visual Studio 15 2017 Win64"" -DCMAKE_SYSTEM_VERSION=10.0.17134.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=""C:\gnuwin32\bin"" -T host=x64
8. msbuild Mesos.sln /p:Configuration=Debug /p:Platform=x64 /maxcpucount:4 /t:Rebuild



ErrorMessage:

D:\Mesos\src\include\mesos/docker/spec.hpp(29): fatal error C1083: Cannot open include file: 'mesos/docker/spec.pb.h': No such file or directory

D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory

D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory



"	MESOS	Resolved	3	1	6057	foundations, mesosphere
12687730	Introduce CMake as an alternative build system.	"This is a rather substantial undertaking, so I would want upstream debate+buy-in prior to full commitment.  The basic premise is: upstream rebundles several of its dependencies in part to tightly control its stack.  This is not out of the norm, but in order to be picked up by distribution channels it needs to built against system dependencies, and rebundling is strictly forbidden.  Given that the mesos primary target platform are data-center distributions such as RHEL/CENTOS/SL it makes sense to still have bundling support for those who do not have dependencies in their channels ""yet"".  This is where cmake can be win with it's uber macros (http://www.cmake.org/cmake/help/v2.8.8/cmake.html#module:ExternalProject).  I do not know of any equivalent in the autotools world, other then to brew your own solution.   I've done this type of work in the past, and completely transformed condor and would leverage a lot of the work that was done there. 

I currently have a tracking branch where I've started this work, but before I go off into the woods, it makes sense to have a debate in public. 

The primary benefits are: 
1. Enable downstream channels to easily distro without carrying a large patch sets. 
2. Still support existing ""non-proper"" distribution methods. 
3. Harden / future proof dependent interfaces. 

Side Benefits: 
Audit current build mechanics.  
 - Presently the language specific binding are not installed.  (.py & .jar)
 - make -jX currently fails 
 - optionally look in arm support. 

Costs:
1. Time
2. Potential temporary destabilization
3. Infrastructure around build+test may need to change."	MESOS	Resolved	3	15	6057	build, cmake, mesosphere
12921327	Modularize plain-file logging for executor/task logs launched with the Docker Containerizer	"Adding a hook inside the Docker containerizer is slightly more involved than the Mesos containerizer.

Docker executors/tasks perform plain-file logging in different places depending on whether the agent is in a Docker container itself
|| Agent || Code ||
| Not in container | {{DockerContainerizerProcess::launchExecutorProcess}} |
| In container | {{Docker::run}} in a {{mesos-docker-executor}} process |

This means a {{ContainerLogger}} will need to be loaded or hooked into the {{mesos-docker-executor}}.  Or we will need to change how piping in done in {{mesos-docker-executor}}."	MESOS	Resolved	3	3	6057	logging, mesosphere
13006966	Master CHECK fails during recovery while relinking to other masters	"Mesos Version: 1.0.1
OS: CoreOS 1068

{code}
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]: I0922 20:05:17.948004 104495 manager.cpp:795] overlay-master in `RECOVERING` state . Hence, not sending an update to agentoverlay-agent@10.4.4.1:5051
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]: F0922 20:05:17.948120 104529 process.cpp:2243] Check failed: sockets.count(from_fd) > 0
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]: *** Check failure stack trace: ***
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908829fd  google::LogMessage::Fail()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc19088482d  google::LogMessage::SendToLog()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908825ec  google::LogMessage::Flush()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc190885129  google::LogMessageFatal::~LogMessageFatal()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908171dd  process::SocketManager::swap_implementing_socket()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc19081aa90  process::SocketManager::link_connect()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908227f9  _ZNSt17_Function_handlerIFvRKN7process6FutureI7NothingEEEZNKS3_5onAnyISt5_BindIFSt7_Mem_fnIMNS0_13SocketManagerEFvS5_NS0_7network6SocketERKNS0_4UPIDEEEPSA_St12_PlaceholderILi1EESC_SD_EEvEES5_OT_NS3_6PreferEEUlS5_E_E9_M_invokeERKSt9_Any_dataS5_
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @           0x41eb26  _ZN7process8internal3runISt8functionIFvRKNS_6FutureI7NothingEEEEJRS5_EEEvRKSt6vectorIT_SaISC_EEDpOT0_
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @           0x42a36f  process::Future<>::fail()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc19085283c  process::network::LibeventSSLSocketImpl::event_callback()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc190852f17  process::network::LibeventSSLSocketImpl::event_callback()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18d616631  bufferevent_run_deferred_callbacks_locked
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18d60cc5d  event_base_loop
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc190865a1d  process::EventLoop::run()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18eeabd73  (unknown)
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18e6a852c  (unknown)
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18e3e61dd  (unknown)
Sep 22 20:05:18 node-44a84215535c systemd[1]: [0;1;39mdcos-mesos-master.service: Main process exited, code=killed, status=6/ABRT
{code}"	MESOS	Resolved	1	1	6057	mesosphere
12846417	Add autotools-style Mesos distributions to the CMake build system	"In the autoconf-based build system, we there is a notion of building a ""distribution"" of Mesos. Essentially, it is a version of Mesos that is configured for a specific platform (Ubuntu, say); so, if a consumer knows their platform, and there is a Mesos distribution, they need only run `make all` and Mesos builds. This allows the consumer to skip the configure step.

In CMake, it should be possible to do this (should be!), and we should explore making it work after we complete the MVP."	MESOS	Accepted	3	3	6057	build, cmake
12950869	ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky	"The logger subprocesses may exit before we reach the {{waitpid}} in the test.  If this happens, {{waitpid}} will return a {{-1}} as the process no longer exists.

Verbose logs:
{code}
[ RUN      ] ContainerLoggerTest.LOGROTATE_RotateInSandbox
I0316 14:28:51.329337  1242 cluster.cpp:139] Creating default 'local' authorizer
I0316 14:28:51.332823  1242 leveldb.cpp:174] Opened db in 3.079559ms
I0316 14:28:51.333916  1242 leveldb.cpp:181] Compacted db in 1.054247ms
I0316 14:28:51.333979  1242 leveldb.cpp:196] Created db iterator in 21450ns
I0316 14:28:51.334005  1242 leveldb.cpp:202] Seeked to beginning of db in 2205ns
I0316 14:28:51.334025  1242 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0316 14:28:51.334089  1242 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0316 14:28:51.334661  1275 recover.cpp:447] Starting replica recovery
I0316 14:28:51.335044  1275 recover.cpp:473] Replica is in EMPTY status
I0316 14:28:51.336207  1262 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (484)@172.17.0.3:45919
I0316 14:28:51.336730  1270 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0316 14:28:51.337257  1275 recover.cpp:564] Updating replica status to STARTING
I0316 14:28:51.338001  1267 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 537200ns
I0316 14:28:51.338032  1267 replica.cpp:320] Persisted replica status to STARTING
I0316 14:28:51.338183  1261 master.cpp:376] Master c7653f60-33e9-4406-9f62-dc74c906bf83 (2cbb23302fe5) started on 172.17.0.3:45919
I0316 14:28:51.338295  1263 recover.cpp:473] Replica is in STARTING status
I0316 14:28:51.338213  1261 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/XtqwkS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/XtqwkS/master"" --zk_session_timeout=""10secs""
I0316 14:28:51.338562  1261 master.cpp:423] Master only allowing authenticated frameworks to register
I0316 14:28:51.338572  1261 master.cpp:428] Master only allowing authenticated slaves to register
I0316 14:28:51.338580  1261 credentials.hpp:35] Loading credentials for authentication from '/tmp/XtqwkS/credentials'
I0316 14:28:51.338877  1261 master.cpp:468] Using default 'crammd5' authenticator
I0316 14:28:51.339030  1262 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (485)@172.17.0.3:45919
I0316 14:28:51.339246  1261 master.cpp:537] Using default 'basic' HTTP authenticator
I0316 14:28:51.339393  1261 master.cpp:571] Authorization enabled
I0316 14:28:51.339390  1266 recover.cpp:193] Received a recover response from a replica in STARTING status
I0316 14:28:51.339606  1271 whitelist_watcher.cpp:77] No whitelist given
I0316 14:28:51.339607  1275 hierarchical.cpp:144] Initialized hierarchical allocator process
I0316 14:28:51.340077  1268 recover.cpp:564] Updating replica status to VOTING
I0316 14:28:51.340533  1270 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 331558ns
I0316 14:28:51.340558  1270 replica.cpp:320] Persisted replica status to VOTING
I0316 14:28:51.340672  1270 recover.cpp:578] Successfully joined the Paxos group
I0316 14:28:51.340827  1270 recover.cpp:462] Recover process terminated
I0316 14:28:51.341684  1270 master.cpp:1806] The newly elected leader is master@172.17.0.3:45919 with id c7653f60-33e9-4406-9f62-dc74c906bf83
I0316 14:28:51.341717  1270 master.cpp:1819] Elected as the leading master!
I0316 14:28:51.341740  1270 master.cpp:1508] Recovering from registrar
I0316 14:28:51.341954  1263 registrar.cpp:307] Recovering registrar
I0316 14:28:51.342499  1273 log.cpp:659] Attempting to start the writer
I0316 14:28:51.343616  1266 replica.cpp:493] Replica received implicit promise request from (487)@172.17.0.3:45919 with proposal 1
I0316 14:28:51.344183  1266 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 536941ns
I0316 14:28:51.344208  1266 replica.cpp:342] Persisted promised to 1
I0316 14:28:51.344825  1267 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0316 14:28:51.346009  1276 replica.cpp:388] Replica received explicit promise request from (488)@172.17.0.3:45919 for position 0 with proposal 2
I0316 14:28:51.346371  1276 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 327890ns
I0316 14:28:51.346393  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.347363  1267 replica.cpp:537] Replica received write request for position 0 from (489)@172.17.0.3:45919
I0316 14:28:51.347414  1267 leveldb.cpp:436] Reading position from leveldb took 24861ns
I0316 14:28:51.347774  1267 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 323654ns
I0316 14:28:51.347796  1267 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348323  1276 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0316 14:28:51.348714  1276 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 361981ns
I0316 14:28:51.348738  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348760  1276 replica.cpp:697] Replica learned NOP action at position 0
I0316 14:28:51.349318  1274 log.cpp:675] Writer started with ending position 0
I0316 14:28:51.350275  1267 leveldb.cpp:436] Reading position from leveldb took 23849ns
I0316 14:28:51.351171  1271 registrar.cpp:340] Successfully fetched the registry (0B) in 9.173248ms
I0316 14:28:51.351300  1271 registrar.cpp:439] Applied 1 operations in 32119ns; attempting to update the 'registry'
I0316 14:28:51.351989  1272 log.cpp:683] Attempting to append 170 bytes to the log
I0316 14:28:51.352108  1266 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0316 14:28:51.352802  1263 replica.cpp:537] Replica received write request for position 1 from (490)@172.17.0.3:45919
I0316 14:28:51.353313  1263 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 474854ns
I0316 14:28:51.353338  1263 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354101  1273 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0316 14:28:51.354483  1273 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 338210ns
I0316 14:28:51.354507  1273 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354529  1273 replica.cpp:697] Replica learned APPEND action at position 1
I0316 14:28:51.355444  1275 registrar.cpp:484] Successfully updated the 'registry' in 4.084224ms
I0316 14:28:51.355569  1275 registrar.cpp:370] Successfully recovered registrar
I0316 14:28:51.355697  1268 log.cpp:702] Attempting to truncate the log to 1
I0316 14:28:51.355870  1269 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0316 14:28:51.356016  1274 master.cpp:1616] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0316 14:28:51.356032  1272 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0316 14:28:51.356761  1273 replica.cpp:537] Replica received write request for position 2 from (491)@172.17.0.3:45919
I0316 14:28:51.357203  1273 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406053ns
I0316 14:28:51.357226  1273 replica.cpp:712] Persisted action at 2
I0316 14:28:51.357718  1270 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0316 14:28:51.358093  1270 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 345370ns
I0316 14:28:51.358175  1270 leveldb.cpp:399] Deleting ~1 keys from leveldb took 57us
I0316 14:28:51.358201  1270 replica.cpp:712] Persisted action at 2
I0316 14:28:51.358220  1270 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0316 14:28:51.368399  1242 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0316 14:28:51.406371  1242 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0316 14:28:51.410480  1266 slave.cpp:193] Slave started on 12)@172.17.0.3:45919
I0316 14:28:51.410518  1266 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --container_logger=""org_apache_mesos_LogrotateContainerLogger"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy""
I0316 14:28:51.411118  1266 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential'
I0316 14:28:51.411381  1266 slave.cpp:324] Slave using credential for: test-principal
I0316 14:28:51.411696  1266 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0316 14:28:51.412075  1266 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.412148  1266 slave.cpp:472] Slave attributes: [  ]
I0316 14:28:51.412160  1266 slave.cpp:477] Slave hostname: 2cbb23302fe5
I0316 14:28:51.413516  1263 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta'
I0316 14:28:51.413774  1266 status_update_manager.cpp:200] Recovering status update manager
I0316 14:28:51.414029  1276 containerizer.cpp:407] Recovering containerizer
I0316 14:28:51.415222  1269 provisioner.cpp:245] Provisioner recovery complete
I0316 14:28:51.415650  1268 slave.cpp:4565] Finished recovery
I0316 14:28:51.416115  1268 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0316 14:28:51.416365  1268 slave.cpp:796] New master detected at master@172.17.0.3:45919
I0316 14:28:51.416448  1276 status_update_manager.cpp:174] Pausing sending status updates
I0316 14:28:51.416445  1268 slave.cpp:859] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.416522  1268 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0316 14:28:51.416671  1268 slave.cpp:832] Detecting new master
I0316 14:28:51.416731  1275 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.416807  1268 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0316 14:28:51.417006  1263 master.cpp:5659] Authenticating slave(12)@172.17.0.3:45919
I0316 14:28:51.417103  1262 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.417348  1273 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.417548  1266 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.417582  1266 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.417696  1264 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.417753  1264 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.417948  1265 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.418107  1267 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.418159  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.418180  1267 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.418233  1267 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.418270  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.418289  1267 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418300  1267 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418323  1267 authenticator.cpp:317] Authentication success
I0316 14:28:51.418414  1264 authenticatee.cpp:298] Authentication success
I0316 14:28:51.418473  1269 master.cpp:5689] Successfully authenticated principal 'test-principal' at slave(12)@172.17.0.3:45919
I0316 14:28:51.418514  1275 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.418781  1276 slave.cpp:927] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.418937  1276 slave.cpp:1321] Will retry registration in 1.983001ms if necessary
I0316 14:28:51.419108  1262 master.cpp:4370] Registering slave at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with id c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.419643  1266 registrar.cpp:439] Applied 1 operations in 75642ns; attempting to update the 'registry'
I0316 14:28:51.420670  1272 log.cpp:683] Attempting to append 339 bytes to the log
I0316 14:28:51.420820  1269 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0316 14:28:51.421495  1270 slave.cpp:1321] Will retry registration in 1.437257ms if necessary
I0316 14:28:51.421716  1275 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.422107  1267 replica.cpp:537] Replica received write request for position 3 from (505)@172.17.0.3:45919
I0316 14:28:51.423033  1267 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 762815ns
I0316 14:28:51.423066  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424069  1267 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0316 14:28:51.424232  1264 slave.cpp:1321] Will retry registration in 66.01292ms if necessary
I0316 14:28:51.424342  1269 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.424686  1267 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 574743ns
I0316 14:28:51.424757  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424792  1267 replica.cpp:697] Replica learned APPEND action at position 3
I0316 14:28:51.426441  1272 registrar.cpp:484] Successfully updated the 'registry' in 6.721024ms
I0316 14:28:51.426677  1262 log.cpp:702] Attempting to truncate the log to 3
I0316 14:28:51.426808  1264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0316 14:28:51.427584  1261 slave.cpp:3482] Received ping from slave-observer(11)@172.17.0.3:45919
I0316 14:28:51.428213  1262 hierarchical.cpp:473] Added slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0316 14:28:51.427865  1266 master.cpp:4438] Registered slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.428270  1267 slave.cpp:971] Registered with master master@172.17.0.3:45919; given slave ID c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.428412  1265 replica.cpp:537] Replica received write request for position 4 from (506)@172.17.0.3:45919
I0316 14:28:51.428443  1267 fetcher.cpp:81] Clearing fetcher cache
I0316 14:28:51.428503  1262 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.428535  1262 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 205421ns
I0316 14:28:51.428750  1273 status_update_manager.cpp:181] Resuming sending status updates
I0316 14:28:51.429157  1265 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 695258ns
I0316 14:28:51.429225  1267 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/slave.info'
I0316 14:28:51.429275  1265 replica.cpp:712] Persisted action at 4
I0316 14:28:51.429759  1267 slave.cpp:1030] Forwarding total oversubscribed resources 
I0316 14:28:51.430055  1265 master.cpp:4782] Received update of slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with total oversubscribed resources 
I0316 14:28:51.430614  1271 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0316 14:28:51.430891  1242 sched.cpp:222] Version: 0.29.0
I0316 14:28:51.431043  1265 hierarchical.cpp:531] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0316 14:28:51.431236  1271 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 536892ns
I0316 14:28:51.431267  1265 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.431584  1271 leveldb.cpp:399] Deleting ~2 keys from leveldb took 66904ns
I0316 14:28:51.431538  1273 sched.cpp:326] New master detected at master@172.17.0.3:45919
I0316 14:28:51.431622  1271 replica.cpp:712] Persisted action at 4
I0316 14:28:51.431623  1265 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 518588ns
I0316 14:28:51.431660  1271 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0316 14:28:51.431711  1273 sched.cpp:382] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.431737  1273 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0316 14:28:51.431982  1266 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.432369  1261 master.cpp:5659] Authenticating scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.432509  1263 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.432868  1267 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.433135  1276 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.433233  1276 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.433423  1276 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.433502  1276 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.433606  1274 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.433744  1273 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.433785  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.433801  1273 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.433861  1273 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.433897  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.433912  1273 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433924  1273 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433944  1273 authenticator.cpp:317] Authentication success
I0316 14:28:51.434037  1274 authenticatee.cpp:298] Authentication success
I0316 14:28:51.434108  1268 master.cpp:5689] Successfully authenticated principal 'test-principal' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434211  1272 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.434512  1274 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.434535  1274 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.3:45919
I0316 14:28:51.434648  1274 sched.cpp:809] Will retry registration in 356.547014ms if necessary
I0316 14:28:51.434819  1266 master.cpp:2326] Received SUBSCRIBE call for framework 'default' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434905  1266 master.cpp:1845] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0316 14:28:51.435464  1265 master.cpp:2397] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0316 14:28:51.435979  1269 hierarchical.cpp:265] Added framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436213  1272 sched.cpp:703] Framework registered with c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436316  1272 sched.cpp:717] Scheduler::registered took 73782ns
I0316 14:28:51.436928  1269 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:51.436978  1269 hierarchical.cpp:1130] Performed allocation for 1 slaves in 970638ns
I0316 14:28:51.437278  1272 master.cpp:5488] Sending 1 offers to framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.437782  1262 sched.cpp:873] Scheduler::resourceOffers took 129952ns
I0316 14:28:51.440006  1274 master.cpp:3268] Processing ACCEPT call for offers: [ c7653f60-33e9-4406-9f62-dc74c906bf83-O0 ] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.440094  1274 master.cpp:2871] Authorizing framework principal 'test-principal' to launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 as user 'mesos'
I0316 14:28:51.442152  1274 master.hpp:177] Adding task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5)
I0316 14:28:51.442348  1274 master.cpp:3753] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.442749  1265 slave.cpp:1361] Got assigned task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443006  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.443624  1265 slave.cpp:1480] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443730  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.444629  1265 paths.cpp:528] Trying to chown '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' to user 'mesos'
I0316 14:28:51.449493  1265 slave.cpp:5367] Launching executor 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.450256  1261 containerizer.cpp:666] Starting container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:51.450299  1265 slave.cpp:1698] Queuing task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.450428  1265 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.459421  1268 launcher.cpp:147] Forked child with pid '1453' for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.613296  1274 slave.cpp:2643] Got registration for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.615416  1271 slave.cpp:1863] Sending queued task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' to executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:51.622187  1272 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.623610  1275 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.623646  1275 status_update_manager.cpp:497] Creating StatusUpdate stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624053  1275 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:51.624423  1274 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:51.624621  1274 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624677  1274 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:51.624836  1270 master.cpp:4927] Status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.624881  1270 master.cpp:4975] Forwarding status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.625077  1270 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0316 14:28:51.625355  1269 sched.cpp:981] Scheduler::statusUpdate took 141149ns
I0316 14:28:51.625671  1266 master.cpp:4082] Processing ACKNOWLEDGE call aee0de1c-8acd-46eb-8723-d26cd203228f for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.625977  1267 status_update_manager.cpp:392] Received status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.626369  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:52.340801  1266 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:52.340884  1266 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:52.340922  1266 hierarchical.cpp:1130] Performed allocation for 1 slaves in 350313ns
I0316 14:28:53.342003  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:53.342077  1263 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:53.342110  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 332715ns
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.619144  1451 process.cpp:986] libprocess is initialized on 172.17.0.3:40885 for 16 cpus
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.790701  1452 process.cpp:986] libprocess is initialized on 172.17.0.3:50144 for 16 cpus
I0316 14:28:53.939643  1268 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:53.940950  1267 slave.cpp:5677] Terminating task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:53.942181  1275 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942358  1275 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:53.942715  1265 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:53.942919  1265 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942961  1265 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:53.943159  1273 master.cpp:4927] Status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.943218  1273 master.cpp:4975] Forwarding status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.943392  1273 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0316 14:28:53.944248  1275 sched.cpp:981] Scheduler::statusUpdate took 172957ns
I0316 14:28:53.944351  1262 hierarchical.cpp:890] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 from framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.944548  1242 sched.cpp:1903] Asked to stop the driver
I0316 14:28:53.944672  1275 sched.cpp:1143] Stopping framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:53.944736  1263 master.cpp:4082] Processing ACKNOWLEDGE call a873c6e2-442e-439e-a13f-54bb19df1881 for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:53.944795  1263 master.cpp:6654] Removing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.945226  1263 master.cpp:6061] Processing TEARDOWN call for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945253  1263 master.cpp:6073] Removing framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945324  1275 status_update_manager.cpp:392] Received status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945412  1274 hierarchical.cpp:375] Deactivated framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945462  1276 slave.cpp:2079] Asked to shut down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 by master@172.17.0.3:45919
I0316 14:28:53.945579  1276 slave.cpp:2104] Shutting down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945669  1276 slave.cpp:4198] Shutting down executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:53.945714  1275 status_update_manager.cpp:528] Cleaning up status update stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945818  1274 hierarchical.cpp:326] Removed framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946151  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946213  1265 slave.cpp:5718] Completing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:54.343000  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:54.343056  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 213036ns
I0316 14:28:54.943627  1261 slave.cpp:3528] executor(1)@172.17.0.3:56062 exited
I0316 14:28:54.944002  1274 containerizer.cpp:1608] Executor for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' has exited
I0316 14:28:54.944205  1274 containerizer.cpp:1392] Destroying container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:54.949076  1276 provisioner.cpp:306] Ignoring destroy request for unknown container 6e2770ca-32d3-47ad-b4fe-7d9f26489621
I0316 14:28:54.949502  1276 slave.cpp:3886] Executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 exited with status 0
I0316 14:28:54.949556  1276 slave.cpp:3990] Cleaning up executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:54.949807  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' for gc 6.99998900785778days in the future
I0316 14:28:54.949931  1276 slave.cpp:4078] Cleaning up framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950188  1276 status_update_manager.cpp:282] Closing status update streams for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950196  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917' for gc 6.99998900606519days in the future
I0316 14:28:54.950458  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000' for gc 6.99998900418963days in the future
../../src/tests/container_logger_tests.cpp:461: Failure
Value of: waitpid(pstree.process.pid, __null, 0)
  Actual: -1
Expected: pstree.process.pid
Which is: 1453
I0316 14:28:54.952739  1264 slave.cpp:668] Slave terminating
I0316 14:28:54.952980  1275 master.cpp:1212] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) disconnected
I0316 14:28:54.953069  1275 master.cpp:2681] Disconnecting slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953172  1275 master.cpp:2700] Deactivating slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953404  1269 hierarchical.cpp:560] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 deactivated
I0316 14:28:54.957495  1274 master.cpp:1065] Master terminating
I0316 14:28:54.958026  1276 hierarchical.cpp:505] Removed slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
[  FAILED  ] ContainerLoggerTest.LOGROTATE_RotateInSandbox (3635 ms)
{code}"	MESOS	Resolved	3	1	6057	flaky, mesosphere, test
13094026	Mesos master rescinds all the in-flight offers from all the registered agents when a new maintenance schedule is posted for a subset of slaves	"We are running mesos 1.1.0 in production. We use a custom autoscaler for scaling our mesos  cluster up and down. While scaling down the cluster, autoscaler makes a POST request to mesos master /maintenance/schedule endpoint with a set of slaves to move to maintenance mode. This forces mesos master to rescind all the in-flight offers from *all the slaves* in the cluster. If our scheduler accepts one of these offers, then we get a TASK_LOST status update back for that task. We also see such (https://gist.github.com/sagar8192/8858e7cb59a23e8e1762a27571824118) log lines in mesos master logs.

After reading the code(refs: https://github.com/apache/mesos/blob/master/src/master/master.cpp#L6772), it appears that offers are getting rescinded for all the slaves. I am not sure what is the expected behavior here, but it makes more sense if only resources from slaves marked for maintenance are reclaimed.

*Experiment:*
To verify that it is actually happening, I checked out the master branch(sha: a31dd52ab71d2a529b55cd9111ec54acf7550ded ) and added some log lines(https://gist.github.com/sagar8192/42ca055720549c5ff3067b1e6c7c68b3). Built the binary and started a mesos master and 2 agent processes. Used a basic python framework that launches docker containers on these slaves. Verified that there is no existing schedule for any slaves using `curl 10.40.19.239:5050/maintenance/status`. Posted maintenance schedule for one of the slaves(https://gist.github.com/sagar8192/fb65170240dd32a53f27e6985c549df0) after starting the mesos framework.

*Logs:*
mesos-master: https://gist.github.com/sagar8192/91888419fdf8284e33ebd58351131203
mesos-slave1: https://gist.github.com/sagar8192/3a83364b1f5ffc63902a80c728647f31
mesos-slave2: https://gist.github.com/sagar8192/1b341ef2271dde11d276974a27109426
Mesos framework: https://gist.github.com/sagar8192/bcd4b37dba03bde0a942b5b972004e8a

I think mesos should rescind offers and inverse offers only for those slaves that are marked for maintenance(draining mode)."	MESOS	Accepted	4	1	6057	maintenance, mesosphere
12913454	Libprocess: Unify the initialization of the MetricsProcess and ReaperProcess	"Related to this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L949-L950].

The {{MetricsProcess}} and {{ReaperProcess}} are global processes (singletons) which are initialized upon first use.  The two processes could be initialized alongside the {{gc}}, {{help}}, {{logging}}, {{profiler}}, and {{system}} (statistics) processes inside {{process::initialize}}.

This is also necessary for libprocess re-initialization."	MESOS	Resolved	3	3	6057	mesosphere
12941144	DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6	"This test passes consistently on other OS's, but fails consistently on CentOS 6.

Verbose logs from test failure:
{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes
I0222 18:16:12.327957 26681 leveldb.cpp:174] Opened db in 7.466102ms
I0222 18:16:12.330528 26681 leveldb.cpp:181] Compacted db in 2.540139ms
I0222 18:16:12.330580 26681 leveldb.cpp:196] Created db iterator in 16908ns
I0222 18:16:12.330592 26681 leveldb.cpp:202] Seeked to beginning of db in 1403ns
I0222 18:16:12.330600 26681 leveldb.cpp:271] Iterated through 0 keys in the db in 315ns
I0222 18:16:12.330634 26681 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0222 18:16:12.331082 26698 recover.cpp:447] Starting replica recovery
I0222 18:16:12.331289 26698 recover.cpp:473] Replica is in EMPTY status
I0222 18:16:12.332162 26703 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13761)@172.30.2.148:35274
I0222 18:16:12.332701 26701 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0222 18:16:12.333230 26699 recover.cpp:564] Updating replica status to STARTING
I0222 18:16:12.334102 26698 master.cpp:376] Master 652149b4-3932-4d8b-ba6f-8c9d9045be70 (ip-172-30-2-148.mesosphere.io) started on 172.30.2.148:35274
I0222 18:16:12.334116 26698 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/QEhLBS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/QEhLBS/master"" --zk_session_timeout=""10secs""
I0222 18:16:12.334354 26698 master.cpp:423] Master only allowing authenticated frameworks to register
I0222 18:16:12.334363 26698 master.cpp:428] Master only allowing authenticated slaves to register
I0222 18:16:12.334369 26698 credentials.hpp:35] Loading credentials for authentication from '/tmp/QEhLBS/credentials'
I0222 18:16:12.335366 26698 master.cpp:468] Using default 'crammd5' authenticator
I0222 18:16:12.335492 26698 master.cpp:537] Using default 'basic' HTTP authenticator
I0222 18:16:12.335623 26698 master.cpp:571] Authorization enabled
I0222 18:16:12.335752 26703 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.314693ms
I0222 18:16:12.335769 26700 whitelist_watcher.cpp:77] No whitelist given
I0222 18:16:12.335778 26703 replica.cpp:320] Persisted replica status to STARTING
I0222 18:16:12.335821 26697 hierarchical.cpp:144] Initialized hierarchical allocator process
I0222 18:16:12.335965 26701 recover.cpp:473] Replica is in STARTING status
I0222 18:16:12.336771 26703 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13763)@172.30.2.148:35274
I0222 18:16:12.337191 26696 recover.cpp:193] Received a recover response from a replica in STARTING status
I0222 18:16:12.337635 26700 recover.cpp:564] Updating replica status to VOTING
I0222 18:16:12.337671 26703 master.cpp:1712] The newly elected leader is master@172.30.2.148:35274 with id 652149b4-3932-4d8b-ba6f-8c9d9045be70
I0222 18:16:12.337698 26703 master.cpp:1725] Elected as the leading master!
I0222 18:16:12.337713 26703 master.cpp:1470] Recovering from registrar
I0222 18:16:12.337828 26696 registrar.cpp:307] Recovering registrar
I0222 18:16:12.339972 26702 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.06039ms
I0222 18:16:12.339994 26702 replica.cpp:320] Persisted replica status to VOTING
I0222 18:16:12.340082 26700 recover.cpp:578] Successfully joined the Paxos group
I0222 18:16:12.340267 26700 recover.cpp:462] Recover process terminated
I0222 18:16:12.340591 26699 log.cpp:659] Attempting to start the writer
I0222 18:16:12.341594 26698 replica.cpp:493] Replica received implicit promise request from (13764)@172.30.2.148:35274 with proposal 1
I0222 18:16:12.343598 26698 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.97941ms
I0222 18:16:12.343619 26698 replica.cpp:342] Persisted promised to 1
I0222 18:16:12.344182 26698 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0222 18:16:12.345285 26702 replica.cpp:388] Replica received explicit promise request from (13765)@172.30.2.148:35274 for position 0 with proposal 2
I0222 18:16:12.347275 26702 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.960198ms
I0222 18:16:12.347296 26702 replica.cpp:712] Persisted action at 0
I0222 18:16:12.348201 26703 replica.cpp:537] Replica received write request for position 0 from (13766)@172.30.2.148:35274
I0222 18:16:12.348247 26703 leveldb.cpp:436] Reading position from leveldb took 21399ns
I0222 18:16:12.350667 26703 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.39166ms
I0222 18:16:12.350690 26703 replica.cpp:712] Persisted action at 0
I0222 18:16:12.351191 26696 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0222 18:16:12.353152 26696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.935798ms
I0222 18:16:12.353173 26696 replica.cpp:712] Persisted action at 0
I0222 18:16:12.353188 26696 replica.cpp:697] Replica learned NOP action at position 0
I0222 18:16:12.353639 26696 log.cpp:675] Writer started with ending position 0
I0222 18:16:12.354508 26697 leveldb.cpp:436] Reading position from leveldb took 25625ns
I0222 18:16:12.355274 26696 registrar.cpp:340] Successfully fetched the registry (0B) in 17.406976ms
I0222 18:16:12.355357 26696 registrar.cpp:439] Applied 1 operations in 20977ns; attempting to update the 'registry'
I0222 18:16:12.355929 26697 log.cpp:683] Attempting to append 210 bytes to the log
I0222 18:16:12.356032 26703 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0222 18:16:12.356657 26698 replica.cpp:537] Replica received write request for position 1 from (13767)@172.30.2.148:35274
I0222 18:16:12.358566 26698 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.881945ms
I0222 18:16:12.358588 26698 replica.cpp:712] Persisted action at 1
I0222 18:16:12.359081 26697 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0222 18:16:12.361002 26697 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.894331ms
I0222 18:16:12.361023 26697 replica.cpp:712] Persisted action at 1
I0222 18:16:12.361038 26697 replica.cpp:697] Replica learned APPEND action at position 1
I0222 18:16:12.361883 26697 registrar.cpp:484] Successfully updated the 'registry' in 6.482944ms
I0222 18:16:12.361981 26697 registrar.cpp:370] Successfully recovered registrar
I0222 18:16:12.362052 26701 log.cpp:702] Attempting to truncate the log to 1
I0222 18:16:12.362167 26703 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0222 18:16:12.362421 26696 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
I0222 18:16:12.362447 26698 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0222 18:16:12.362911 26701 replica.cpp:537] Replica received write request for position 2 from (13768)@172.30.2.148:35274
I0222 18:16:12.364760 26701 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.819954ms
I0222 18:16:12.364783 26701 replica.cpp:712] Persisted action at 2
I0222 18:16:12.365384 26697 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0222 18:16:12.367961 26697 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.55143ms
I0222 18:16:12.368015 26697 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28196ns
I0222 18:16:12.368028 26697 replica.cpp:712] Persisted action at 2
I0222 18:16:12.368044 26697 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0222 18:16:12.376824 26703 slave.cpp:193] Slave started on 396)@172.30.2.148:35274
I0222 18:16:12.376838 26703 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1""
I0222 18:16:12.377109 26703 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential'
I0222 18:16:12.377300 26703 slave.cpp:324] Slave using credential for: test-principal
I0222 18:16:12.377439 26703 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0222 18:16:12.377804 26703 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.377881 26703 slave.cpp:472] Slave attributes: [  ]
I0222 18:16:12.377889 26703 slave.cpp:477] Slave hostname: ip-172-30-2-148.mesosphere.io
I0222 18:16:12.378779 26701 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta'
I0222 18:16:12.379092 26697 status_update_manager.cpp:200] Recovering status update manager
I0222 18:16:12.379156 26681 sched.cpp:222] Version: 0.28.0
I0222 18:16:12.379250 26697 docker.cpp:722] Recovering Docker containers
I0222 18:16:12.379421 26703 slave.cpp:4565] Finished recovery
I0222 18:16:12.379627 26700 sched.cpp:326] New master detected at master@172.30.2.148:35274
I0222 18:16:12.379735 26703 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0222 18:16:12.379765 26700 sched.cpp:382] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.379781 26700 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0222 18:16:12.379964 26696 status_update_manager.cpp:174] Pausing sending status updates
I0222 18:16:12.379992 26702 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380030 26697 slave.cpp:796] New master detected at master@172.30.2.148:35274
I0222 18:16:12.380106 26697 slave.cpp:859] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.380127 26697 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0222 18:16:12.380188 26699 master.cpp:5526] Authenticating scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.380269 26700 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.380280 26698 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380307 26697 slave.cpp:832] Detecting new master
I0222 18:16:12.380450 26697 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0222 18:16:12.380452 26699 master.cpp:5526] Authenticating slave(396)@172.30.2.148:35274
I0222 18:16:12.380506 26698 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380540 26697 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.380635 26700 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380659 26700 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.380762 26700 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.380765 26701 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380843 26700 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.380911 26698 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380931 26702 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.380936 26698 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.381036 26702 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381052 26698 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.381062 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381072 26702 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381104 26702 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381104 26698 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.381134 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381142 26702 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381147 26702 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381162 26702 authenticator.cpp:317] Authentication success
I0222 18:16:12.381184 26698 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.381247 26699 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381283 26696 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381311 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381325 26696 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381319 26701 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381345 26700 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.381361 26696 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381397 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381413 26696 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381422 26696 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381441 26696 authenticator.cpp:317] Authentication success
I0222 18:16:12.381548 26698 sched.cpp:471] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.381563 26698 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.148:35274
I0222 18:16:12.381634 26700 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381660 26698 sched.cpp:809] Will retry registration in 770.60771ms if necessary
I0222 18:16:12.381675 26697 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(396)@172.30.2.148:35274
I0222 18:16:12.381734 26702 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.381811 26697 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381882 26697 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0222 18:16:12.382004 26698 slave.cpp:927] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.382123 26698 slave.cpp:1321] Will retry registration in 8.1941ms if necessary
I0222 18:16:12.382282 26701 master.cpp:4240] Registering slave at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with id 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.382482 26701 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0222 18:16:12.382612 26703 registrar.cpp:439] Applied 1 operations in 46327ns; attempting to update the 'registry'
I0222 18:16:12.382829 26699 hierarchical.cpp:265] Added framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382910 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.382915 26701 sched.cpp:703] Framework registered with 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382936 26699 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.382953 26699 hierarchical.cpp:1127] Performed allocation for 0 slaves in 89949ns
I0222 18:16:12.382982 26701 sched.cpp:717] Scheduler::registered took 46498ns
I0222 18:16:12.383536 26698 log.cpp:683] Attempting to append 423 bytes to the log
I0222 18:16:12.383628 26699 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0222 18:16:12.384196 26700 replica.cpp:537] Replica received write request for position 3 from (13775)@172.30.2.148:35274
I0222 18:16:12.386602 26700 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 2.377119ms
I0222 18:16:12.386625 26700 replica.cpp:712] Persisted action at 3
I0222 18:16:12.387104 26698 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0222 18:16:12.389159 26698 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 2.032301ms
I0222 18:16:12.389181 26698 replica.cpp:712] Persisted action at 3
I0222 18:16:12.389196 26698 replica.cpp:697] Replica learned APPEND action at position 3
I0222 18:16:12.390281 26698 registrar.cpp:484] Successfully updated the 'registry' in 7.619072ms
I0222 18:16:12.390444 26702 log.cpp:702] Attempting to truncate the log to 3
I0222 18:16:12.390569 26701 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0222 18:16:12.390904 26701 slave.cpp:3482] Received ping from slave-observer(364)@172.30.2.148:35274
I0222 18:16:12.391054 26700 master.cpp:4308] Registered slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.391144 26703 slave.cpp:971] Registered with master master@172.30.2.148:35274; given slave ID 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.391168 26703 fetcher.cpp:81] Clearing fetcher cache
I0222 18:16:12.391238 26700 replica.cpp:537] Replica received write request for position 4 from (13776)@172.30.2.148:35274
I0222 18:16:12.391263 26701 status_update_manager.cpp:181] Resuming sending status updates
I0222 18:16:12.391304 26697 hierarchical.cpp:473] Added slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] (allocated: )
I0222 18:16:12.391388 26703 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/slave.info'
I0222 18:16:12.391636 26703 slave.cpp:1030] Forwarding total oversubscribed resources 
I0222 18:16:12.391772 26699 master.cpp:4649] Received update of slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with total oversubscribed resources 
I0222 18:16:12.392011 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392053 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 708377ns
I0222 18:16:12.392307 26703 master.cpp:5355] Sending 1 offers to framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.392374 26697 hierarchical.cpp:531] Slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) updated with oversubscribed resources  (total: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000], allocated: disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000])
I0222 18:16:12.392500 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.392531 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392556 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 136779ns
I0222 18:16:12.392704 26701 sched.cpp:873] Scheduler::resourceOffers took 94330ns
I0222 18:16:12.393086 26681 resources.cpp:576] Parsing resources as JSON failed: cpus:1;mem:64;
Trying semicolon-delimited string format instead
I0222 18:16:12.393600 26700 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 2.326382ms
I0222 18:16:12.393625 26700 replica.cpp:712] Persisted action at 4
I0222 18:16:12.394162 26696 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0222 18:16:12.394533 26701 master.cpp:3138] Processing ACCEPT call for offers: [ 652149b4-3932-4d8b-ba6f-8c9d9045be70-O0 ] on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.394567 26701 master.cpp:2926] Authorizing principal 'test-principal' to create volumes
I0222 18:16:12.394628 26701 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
I0222 18:16:12.395519 26701 master.cpp:3467] Applying CREATE operation for volumes disk(role1)[id1:path1]:64 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.395808 26701 master.cpp:6589] Sending checkpointed resources disk(role1)[id1:path1]:64 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396316 26696 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.130659ms
I0222 18:16:12.396317 26703 slave.cpp:2341] Updated checkpointed resources from  to disk(role1)[id1:path1]:64
I0222 18:16:12.396368 26696 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30004ns
I0222 18:16:12.396381 26696 replica.cpp:712] Persisted action at 4
I0222 18:16:12.396397 26696 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0222 18:16:12.396533 26701 master.hpp:176] Adding task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396680 26701 master.cpp:3623] Launching task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.397009 26696 slave.cpp:1361] Got assigned task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397143 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.397306 26699 hierarchical.cpp:653] Updated allocation of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000] to disk(role1):1984; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64
I0222 18:16:12.397625 26699 hierarchical.cpp:892] Recovered disk(role1):1984; cpu(*):2; mem(*):1984; cpus(*):7; ports(*):[31000-32000] (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: disk(role1)[id1:path1]:64; cpus(*):1; mem(*):64) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397857 26696 slave.cpp:1480] Launching task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397943 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.398560 26696 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' to user 'root'
I0222 18:16:12.403491 26696 slave.cpp:5367] Launching executor 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.404115 26696 slave.cpp:1698] Queuing task '1' for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.405709 26696 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.408308 26697 docker.cpp:1019] Starting container '207172a3-0ebd-4faa-946b-75a829fc75fc' for task '1' (and executor '1') of framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:12.408592 26697 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
I0222 18:16:12.520663 26702 docker.cpp:390] Docker pull alpine completed
I0222 18:16:12.520853 26702 docker.cpp:479] Changing the ownership of the persistent volume at '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' with uid 0 and gid 0
I0222 18:16:12.524782 26702 docker.cpp:500] Mounting '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc/path1' for persistent volume disk(role1)[id1:path1]:64 of container 207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:12.580834 26700 slave.cpp:2643] Got registration for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:12.581961 26699 docker.cpp:1299] Ignoring updating container '207172a3-0ebd-4faa-946b-75a829fc75fc' with resources passed to update is identical to existing resources
I0222 18:16:12.582307 26698 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.295573 26703 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.295940 26703 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.296381 26701 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26701 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26703 slave.cpp:5677] Terminating task 1
I0222 18:16:13.296839 26701 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.296902 26702 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:13.299427 26699 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.299921 26699 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.299969 26699 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.300130 26696 master.cpp:4794] Status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.300176 26696 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.300375 26696 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_RUNNING)
I0222 18:16:13.300765 26703 sched.cpp:981] Scheduler::statusUpdate took 164263ns
I0222 18:16:13.300962 26700 hierarchical.cpp:892] Recovered cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: ) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301178 26699 master.cpp:3952] Processing ACKNOWLEDGE call 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.301450 26699 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301697 26701 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327133 26697 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327280 26697 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.327481 26696 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.327621 26696 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327679 26696 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.327800 26698 master.cpp:4794] Status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.327850 26698 master.cpp:4842] Forwarding status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327977 26698 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0222 18:16:13.328248 26699 sched.cpp:981] Scheduler::statusUpdate took 100279ns
I0222 18:16:13.328588 26700 master.cpp:3952] Processing ACKNOWLEDGE call ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.328662 26681 sched.cpp:1903] Asked to stop the driver
I0222 18:16:13.328630 26700 master.cpp:6516] Removing task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.328747 26697 sched.cpp:1143] Stopping framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:13.329064 26696 status_update_manager.cpp:392] Received status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329069 26700 master.cpp:5926] Processing TEARDOWN call for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329100 26700 master.cpp:5938] Removing framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329200 26696 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329218 26703 hierarchical.cpp:375] Deactivated framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329309 26697 slave.cpp:2079] Asked to shut down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 by master@172.30.2.148:35274
I0222 18:16:13.329346 26697 slave.cpp:2104] Shutting down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329418 26697 slave.cpp:4198] Shutting down executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.329578 26699 hierarchical.cpp:326] Removed framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329684 26697 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329733 26697 slave.cpp:5718] Completing task 1
I0222 18:16:13.337236 26703 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:13.337266 26703 hierarchical.cpp:1127] Performed allocation for 1 slaves in 153077ns
I0222 18:16:14.297827 26702 slave.cpp:3528] executor(1)@172.30.2.148:56026 exited
I0222 18:16:14.332489 26697 docker.cpp:1915] Executor for container '207172a3-0ebd-4faa-946b-75a829fc75fc' has exited
I0222 18:16:14.332512 26697 docker.cpp:1679] Destroying container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.332600 26697 docker.cpp:1807] Running docker stop on container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333111 26697 docker.cpp:908] Unmounting volume for container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333288 26700 slave.cpp:3886] Executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 exited with status 0
I0222 18:16:14.333340 26700 slave.cpp:3990] Cleaning up executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:14.333603 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' for gc 6.99999614056593days in the future
I0222 18:16:14.333669 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.333704 26700 slave.cpp:4078] Cleaning up framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.333726 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1' for gc 6.99999613825185days in the future
I0222 18:16:14.336545 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000' for gc 6.9999961115763days in the future
I0222 18:16:14.336699 26701 status_update_manager.cpp:282] Closing status update streams for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.338240 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:14.338270 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 191822ns
I0222 18:16:14.635416 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.940042 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.245256 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.339015 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:15.339053 26697 hierarchical.cpp:1127] Performed allocation for 1 slaves in 265093ns
I0222 18:16:15.549804 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.854646 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.159210 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.339910 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:16.339951 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 255857ns
I0222 18:16:16.463809 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.768708 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.073479 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.340798 26696 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:17.340864 26696 hierarchical.cpp:1127] Performed allocation for 1 slaves in 260467ns
I0222 18:16:17.377902 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.683398 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.988231 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.292505 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.330112 26700 slave.cpp:4231] Framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 seems to have exited. Ignoring shutdown timeout for executor '1'
I0222 18:16:18.341600 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:18.341634 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 252012ns
I0222 18:16:18.596279 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.901157 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.204834 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.342326 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:19.342358 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 186829ns
I0222 18:16:19.508533 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.812255 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.116345 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.343556 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:20.343588 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 194704ns
I0222 18:16:20.420814 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.724819 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.029549 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.334319 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.344859 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:21.344892 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 241099ns
I0222 18:16:21.638164 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
../../src/tests/containerizer/docker_containerizer_tests.cpp:1434: Failure
os::read(path::join(volumePath, ""file"")): Failed to open file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1/file': No such file or directory
I0222 18:16:21.943008 26703 master.cpp:1027] Master terminating
I0222 18:16:21.943635 26696 hierarchical.cpp:505] Removed slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:21.943989 26702 slave.cpp:3528] master@172.30.2.148:35274 exited
W0222 18:16:21.944016 26702 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0222 18:16:21.948807 26699 slave.cpp:668] Slave terminating
I0222 18:16:21.951902 26681 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
I0222 18:16:22.044273 26698 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:22.148877 26681 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -f -v 422bfef31d51d2d3d2aafcf49b3e502654354bd98a98b076f4089b9a8e274d05
[  FAILED  ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes (10535 ms)
{code}"	MESOS	Resolved	3	1	6057	flaky, mesosphere, test
12906010	Document messages.proto	The messages we pass between Mesos components are largely undocumented.  See this [TODO|https://github.com/apache/mesos/blob/19f14d06bac269b635657960d8ea8b2928b7830c/src/messages/messages.proto#L23].	MESOS	Resolved	3	4	6057	docathon, documentation, mesosphere
13201109	ZooKeeperTest.Auth is flaky.	"{noformat}
05:37:27  [ RUN      ] ZooKeeperTest.Auth
05:37:27  I1127 21:37:44.361042 2668864320 zookeeper_test_server.cpp:156] Started ZooKeeperTestServer on port 53751
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@753: Client environment:zookeeper.version=zookeeper C client 3.4.8
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@757: Client environment:host.name=Jenkinss-Mac-mini.local
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@764: Client environment:os.name=Darwin
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@765: Client environment:os.arch=17.4.0
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@766: Client environment:os.version=Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@774: Client environment:user.name=jenkins
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@782: Client environment:user.home=/Users/jenkins
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@log_env@794: Client environment:user.dir=/Users/jenkins/workspace/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mac/mesos/build
05:37:27  2018-11-27 21:37:44,361:80931(0x700007bc3000):ZOO_INFO@zookeeper_init@827: Initiating client connection, host=127.0.0.1:53751 sessionTimeout=10000 watcher=0x10dbf7dc0 sessionId=0 sessionPasswd=<null> context=0x7f9c4b2d9600 flags=0
05:37:27  2018-11-27 21:37:44,361:80931(0x700008567000):ZOO_INFO@check_events@1764: initiated connection to server [127.0.0.1:53751]
05:37:27  2018-11-27 21:37:44,365:80931(0x700008567000):ZOO_INFO@check_events@1811: session establishment complete on server [127.0.0.1:53751], sessionId=0x16758d2b9a80000, negotiated timeout=10000
05:37:31  2018-11-27 21:37:47,699:80931(0x700008567000):ZOO_INFO@auth_completion_func@1327: Authentication scheme digest succeeded
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@753: Client environment:zookeeper.version=zookeeper C client 3.4.8
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@757: Client environment:host.name=Jenkinss-Mac-mini.local
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@764: Client environment:os.name=Darwin
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@765: Client environment:os.arch=17.4.0
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@766: Client environment:os.version=Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@774: Client environment:user.name=jenkins
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@782: Client environment:user.home=/Users/jenkins
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@log_env@794: Client environment:user.dir=/Users/jenkins/workspace/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mac/mesos/build
05:37:31  2018-11-27 21:37:47,702:80931(0x700007cc9000):ZOO_INFO@zookeeper_init@827: Initiating client connection, host=127.0.0.1:53751 sessionTimeout=10000 watcher=0x10dbf7dc0 sessionId=0 sessionPasswd=<null> context=0x7f9c487174f0 flags=0
05:37:31  2018-11-27 21:37:47,702:80931(0x700008d82000):ZOO_INFO@check_events@1764: initiated connection to server [127.0.0.1:53751]
05:37:31  2018-11-27 21:37:47,703:80931(0x700008d82000):ZOO_INFO@check_events@1811: session establishment complete on server [127.0.0.1:53751], sessionId=0x16758d2b9a80001, negotiated timeout=10000
05:37:31  2018-11-27 21:37:47,704:80931(0x700007b40000):ZOO_INFO@log_env@753: Client environment:zookeeper.version=zookeeper C client 3.4.8
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@log_env@757: Client environment:host.name=Jenkinss-Mac-mini.local
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@log_env@764: Client environment:os.name=Darwin
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@log_env@765: Client environment:os.arch=17.4.0
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@log_env@766: Client environment:os.version=Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@log_env@774: Client environment:user.name=jenkins
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@log_env@782: Client environment:user.home=/Users/jenkins
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@log_env@794: Client environment:user.dir=/Users/jenkins/workspace/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mac/mesos/build
05:37:31  2018-11-27 21:37:47,705:80931(0x700007b40000):ZOO_INFO@zookeeper_init@827: Initiating client connection, host=127.0.0.1:53751 sessionTimeout=10000 watcher=0x10dbf7dc0 sessionId=0 sessionPasswd=<null> context=0x7f9c4b37c170 flags=0
05:37:31  2018-11-27 21:37:47,705:80931(0x700008e88000):ZOO_INFO@check_events@1764: initiated connection to server [127.0.0.1:53751]
05:37:31  2018-11-27 21:37:47,706:80931(0x700008e88000):ZOO_INFO@check_events@1811: session establishment complete on server [127.0.0.1:53751], sessionId=0x16758d2b9a80002, negotiated timeout=10000
05:37:46  2018-11-27 21:38:01,187:80931(0x700008567000):ZOO_ERROR@handle_socket_error_msg@1782: Socket [127.0.0.1:53751] zk retcode=-4, errno=64(Host is down): failed while receiving a server response
05:37:46  2018-11-27 21:38:01,187:80931(0x700008567000):ZOO_WARN@zookeeper_interest@1597: Exceeded deadline by 10152ms
05:37:46  2018-11-27 21:38:01,187:80931(0x700008d82000):ZOO_ERROR@handle_socket_error_msg@1782: Socket [127.0.0.1:53751] zk retcode=-4, errno=64(Host is down): failed while receiving a server response
05:37:46  2018-11-27 21:38:01,187:80931(0x700008d82000):ZOO_WARN@zookeeper_interest@1597: Exceeded deadline by 10149ms
05:37:46  2018-11-27 21:38:01,187:80931(0x700008e88000):ZOO_ERROR@handle_socket_error_msg@1782: Socket [127.0.0.1:53751] zk retcode=-4, errno=64(Host is down): failed while receiving a server response
05:37:46  2018-11-27 21:38:01,188:80931(0x700008e88000):ZOO_WARN@zookeeper_interest@1597: Exceeded deadline by 10149ms
05:37:46  2018-11-27 21:38:01,188:80931(0x700008e88000):ZOO_WARN@zookeeper_interest@1597: Exceeded deadline by 10149ms
05:37:46  2018-11-27 21:38:01,188:80931(0x700008e88000):ZOO_INFO@check_events@1764: initiated connection to server [127.0.0.1:53751]
05:37:46  2018-11-27 21:38:01,188:80931(0x700008e88000):ZOO_ERROR@handle_socket_error_msg@1800: Socket [127.0.0.1:53751] zk retcode=-112, errno=70(Stale NFS file handle): sessionId=0x16758d2b9a80002 has expired.
05:37:46  ../../src/tests/zookeeper_tests.cpp:67: Failure
05:37:46  (&nonOwnerZk).get(""/test"", ...): session expired
05:37:46  2018-11-27 21:38:01,189:80931(0x7fff9f13a340):ZOO_INFO@zookeeper_close@2579: Freeing zookeeper resources for sessionId=0x16758d2b9a80002
05:37:46  
05:37:46  2018-11-27 21:38:01,189:80931(0x7fff9f13a340):ZOO_INFO@zookeeper_close@2579: Freeing zookeeper resources for sessionId=0x16758d2b9a80001
05:37:46  
05:37:46  2018-11-27 21:38:01,189:80931(0x7fff9f13a340):ZOO_INFO@zookeeper_close@2579: Freeing zookeeper resources for sessionId=0x16758d2b9a80000
05:37:46  
05:37:46  I1127 21:38:01.189885 2668864320 zookeeper_test_server.cpp:116] Shutting down ZooKeeperTestServer on port 53751
05:37:46  [  FAILED  ] ZooKeeperTest.Auth (16831 ms)
{noformat}"	MESOS	Open	3	1	6057	flaky, flaky-test, foundations
12954384	Update the long-lived-framework example to run on test clusters	"There are a couple of problems with the long-lived framework that prevent it from being deployed (easily) on an actual cluster:

* The framework will greedily accept all offers; it runs one executor per agent in the cluster.
* The framework assumes the {{long-lived-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.
* The framework does not specify an resources with the executor.  This is required by many isolators.
* The framework has no metrics."	MESOS	Resolved	3	4	6057	tech-debt
12728074	Create design document for Optimistic Offers	"As a first step toward Optimistic Offers, take the description from the epic and build an implementation design doc that can be shared for comments.

Note: the links to the working group notes and design doc are located in the [JIRA Epic|MESOS-1607]."	MESOS	Resolved	3	20	6057	mesosphere
13135442	When `UPDATE_SLAVE` messages are received, offers might not be rescinded due to a race 	"When an agent with enabled {{RESOURCE_PROVIDER}} capability (re-)registers with the master it sends a {{UPDATE_SLAVE}} after being (re-)registered. In the master, the agent is added (back) to the allocator, as soon as it's (re-)registered, i.e. before {{UPDATE_SLAVE}} is being send. This triggers an allocation and offers might get sent out to frameworks. When {{UPDATE_SLAVE}} is being handled in the master, these offers have to be rescinded, as they're based on an outdated agent state.
Internally, the allocator defers a offer callback in the master ({{Master::offer}}). In rare cases a {{UPDATE_SLAVE}} message might arrive at the same time and its handler in the master called before the offer callback (but after the actual allocation took place). In this case the (outdated) offer is still sent to frameworks and never rescinded.

Here's the relevant log lines, this was discovered while working on https://reviews.apache.org/r/65045/:
{noformat}
I0201 14:17:47.041093 242208768 hierarchical.cpp:1517] Performed allocation for 1 agents in 704915ns
I0201 14:17:47.041738 242745344 master.cpp:7235] Received update of agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 at slave(540)@172.18.8.20:60469 (172.18.8.20) with total oversubscribed resources {}
I0201 14:17:47.042778 242745344 master.cpp:8808] Sending 1 offers to framework 53c557e7-3161-449b-bacc-a4f8c02e78e7-0000 (default) at scheduler-798f476b-b099-443e-bd3b-9e7333f29672@172.18.8.20:60469
I0201 14:17:47.043102 243281920 sched.cpp:921] Scheduler::resourceOffers took 40444ns
I0201 14:17:47.043427 243818496 hierarchical.cpp:712] Grew agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 by disk[MOUNT]:200 (total), {  } (used)
I0201 14:17:47.043643 243818496 hierarchical.cpp:669] Agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 (172.18.8.20) updated with total resources disk[MOUNT]:200; cpus:2; mem:1024; disk:1024; ports:[31000-32000]
{noformat}"	MESOS	Open	3	1	6057	mesosphere
12845679	Registry operations do not exist for manipulating maintanence schedules	"In order to modify the maintenance schedule in the replicated registry, we will need Operations (src/master/registrar.hpp).

The operations will likely correspond to the HTTP API:
* UpdateMaintenanceSchedule: Given a blob representing a maintenance schedule, perform some verification on the blob.  Write the blob to the registry.  
* StartMaintenance:  Given a set of machines, verify then transition machines from Draining to Deactivated.
* StopMaintenance:  Given a set of machines, verify then transition machines from Deactivated to Normal.  Remove affected machines from the schedule(s)."	MESOS	Resolved	3	3	6057	mesosphere
12938029	Tests will dereference stack allocated master objects upon assertion/expectation failure.	"Tests that use the {{StartMaster}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartMaster}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartMaster}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests."	MESOS	Resolved	3	1	6057	flaky, mesosphere, tech-debt, test
12862592	Fully separate out libprocess and Stout CMake build system from the Mesos build system	"The official goal is to be able to put libprocess and stout into a call to `ExternalProject_Add`, rather than having them built in-tree as they are now. Since Libprocess and Stout depend on a few variables being defined by the project that is building against it (such as, e.g., the `LINUX` variable) this will involve, at minimum, figuring out which `-D` flags have to be passed through the `ExternalProject_Add` call.

NOTE: This goal may not be feasible. We will need to trigger a rebuild of many source files if we change a header in Libprocess or Stout, and a relink if we change a .cpp file in the source files of Libprocess. This might require a fair bit of effort.

Another complication is that `StoutConfigure` manages the dependencies of Stout, and Stout is built through `ExternalProject_Add`, we will need to make sure this is managed in roughly the same way it is now."	MESOS	Accepted	4	3	6057	cmake, mesosphere
12754637	Add documentation for maintenance primitives.	"We should provide some guiding documentation around the upcoming maintenance primitives in Mesos.

Specifically, we should ensure that general users, framework developers, and operators understand the notion of maintenance in Mesos. Some guidance and recommendations for the latter two audiences will be necessary."	MESOS	Resolved	3	20	6057	mesosphere
13007318	Libprocess links will not generate an ExitedEvent if the socket creation fails	"Noticed this while inspecting nearby code for potential races.

Normally, when a libprocess actor (the ""linkee"") links to a remote process, it does the following:
1) Create a socket.
2) Connect to the remote process (asynchronous).
3) Check the connection succeeded.

If (2) or (3) fail, the linkee will receive a {{ExitedEvent}}, which indicates that the link broke.  In case (1) fails, there is no {{ExitedEvent}}:
https://github.com/apache/mesos/blob/7c833abbec9c9e4eb51d67f7a8e7a8d0870825f8/3rdparty/libprocess/src/process.cpp#L1558-L1562"	MESOS	Resolved	3	1	6057	libprocess, mesosphere
12991975	/help endpoint does not set Content-Type to HTML	"This change added a default {{Content-Type}} to all responses:
https://github.com/apache/mesos/commit/b2c5d91addbae609af3791f128c53fb3a26c7d53

Unfortunately, this changed the {{/help}} endpoint from no {{Content-Type}} to {{text/plain}}.  For a browser to render this page correctly, we need an HTML content type."	MESOS	Resolved	2	1	6057	mesosphere
13193889	Data in persistent volume deleted accidentally when using Docker container and Persistent volume	"Using docker image w/ persistent volume to start a service, it will cause data in persistent volume deleted accidentally when task killed and restarted, also old mount pointsnot unmounted, even the service already deleted.

*The expected result should be data in persistent volume kept until task deleted completely, also dangling mount points should be unmounted correctly.*



*Step 1:* Use below JSON config to create a Mysql server using Docker image and Persistent Volume
{code:javascript}
{
  ""env"": {
    ""MYSQL_USER"": ""wordpress"",
    ""MYSQL_PASSWORD"": ""secret"",
    ""MYSQL_ROOT_PASSWORD"": ""supersecret"",
    ""MYSQL_DATABASE"": ""wordpress""
  },
  ""id"": ""/mysqlgc"",
  ""backoffFactor"": 1.15,
  ""backoffSeconds"": 1,
  ""constraints"": [
    [
      ""hostname"",
      ""IS"",
      ""172.27.12.216""
    ]
  ],
  ""container"": {
    ""portMappings"": [
      {
        ""containerPort"": 3306,
        ""hostPort"": 0,
        ""protocol"": ""tcp"",
        ""servicePort"": 10000
      }
    ],
    ""type"": ""DOCKER"",
    ""volumes"": [
      {
        ""persistent"": {
          ""type"": ""root"",
          ""size"": 1000,
          ""constraints"": []
        },
        ""mode"": ""RW"",
        ""containerPath"": ""mysqldata""
      },
      {
        ""containerPath"": ""/var/lib/mysql"",
        ""hostPath"": ""mysqldata"",
        ""mode"": ""RW""
      }
    ],
    ""docker"": {
      ""image"": ""mysql"",
      ""forcePullImage"": false,
      ""privileged"": false,
      ""parameters"": []
    }
  },
  ""cpus"": 1,
  ""disk"": 0,
  ""instances"": 1,
  ""maxLaunchDelaySeconds"": 3600,
  ""mem"": 512,
  ""gpus"": 0,
  ""networks"": [
    {
      ""mode"": ""container/bridge""
    }
  ],
  ""residency"": {
    ""relaunchEscalationTimeoutSeconds"": 3600,
    ""taskLostBehavior"": ""WAIT_FOREVER""
  },
  ""requirePorts"": false,
  ""upgradeStrategy"": {
    ""maximumOverCapacity"": 0,
    ""minimumHealthCapacity"": 0
  },
  ""killSelection"": ""YOUNGEST_FIRST"",
  ""unreachableStrategy"": ""disabled"",
  ""healthChecks"": [],
  ""fetch"": []
}
{code}
*Step 2:* Kill mysqld process to force rescheduling new Mysql task, but found 2 mount points to the same persistent volume, it means old mount point did not be unmounted immediately.

!image-2018-10-24-22-20-51-059.png!

*Step 3:* After GC, data in persistent volume was deleted accidentally, but mysqld (Mesos task) still running

!image-2018-10-24-22-21-13-399.png!

*Step 4:* Delete Mysql service from Marathon, all mount points unable to unmount, even the service already deleted."	MESOS	Resolved	2	1	6057	dcos, dcos-1.11.6, mesosphere, persistent-volumes
13022371	SSL downgrade path will CHECK-fail when using both temporary and persistent sockets	"The code path for downgrading sockets from SSL to non-SSL includes this code:
{code}
    // If this address is a temporary link.
    if (temps.count(addresses[to_fd]) > 0) {
      temps[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }

    // If this address is a persistent link.
    if (persists.count(addresses[to_fd]) > 0) {
      persists[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2311-L2321

It is possible for libprocess to hold both temporary and persistent sockets to the same address.  This can happen when a message is first sent ({{ProcessBase::send}}), and then a link is established ({{ProcessBase::link}}).  When the target of the message/link is a non-SSL socket, both temporary and persistent sockets go through the downgrade path.

If a temporary socket is present while a persistent socket is being created, the above code will remap both temporary and persistent sockets to the same address (it should only remap the persistent socket).  This leads to some CHECK failures if those sockets are used or closed later:
* {code}
    bool persist = persists.count(address) > 0;
    bool temp = temps.count(address) > 0;
    if (persist || temp) {
      int s = persist ? persists[address] : temps[address];
      CHECK(sockets.count(s) > 0);
socket = sockets.at(s);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L1942
* {code}
        if (dispose.count(s) > 0) {
          // This is either a temporary socket we created or it's a
          // socket that we were receiving data from and possibly
          // sending HTTP responses back on. Clean up either way.
          if (addresses.count(s) > 0) {
            const Address& address = addresses[s];
            CHECK(temps.count(address) > 0 && temps[address] == s);
temps.erase(address);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2044"	MESOS	Resolved	1	1	6057	mesosphere
12845992	Registry recovery does not recover the maintenance object.	"Persisted info is fetched from the registry when a master is elected or after failover.  Currently, this process involves 3 steps:
* Fetch the ""registry"".
* Start an operation to add the new master to the fetched registry.
* Check the success of the operation and finish recovering.
These methods can be found in src/master/registrar.cpp {code}RegistrarProcess::recover, ::_recover, ::__recover{code}

Since the maintenance schedule is stored in a separate key, the recover process must also fetch a new ""maintenance"" object.  This object needs to be passed along to the master along with the existing ""registry"" object.

Possible test(s):
* src/tests/registrar_tests.cpp
** Change the ""Recovery"" test to include checks for the new object."	MESOS	Resolved	3	3	6057	mesosphere
13036966	Fix BOOST random generator initialization on Windows	"seed_rng::seed_rng does not produced the expected result in Windows since is using `/dev/urandom` file.  

0:005> k
 # Child-SP          RetAddr           Call Site
00 00000049`22dfc108 00007ff6`5193822f kernel32!CreateFileW
...
0e 00000049`22dfc660 00007ff6`502228fd mesos_agent!boost::uuids::detail::seed_rng::seed_rng+0x3d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 80]
0f 00000049`22dfc690 00007ff6`502591e3 mesos_agent!boost::uuids::detail::seed<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0x4d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 246]
10 00000049`22dfc790 00007ff6`50395518 mesos_agent!boost::uuids::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0xd3 [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\random_generator.hpp @ 50]
11 00000049`22dfc800 00007ff6`500ad140 mesos_agent!id::UUID::random+0x78 [d:\repositories\mesoswin\3rdparty\stout\include\stout\uuid.hpp @ 49]
12 00000049`22dfc870 00007ff6`5007ff55 mesos_agent!mesos::internal::slave::Framework::launchExecutor+0x70 [d:\repositories\mesoswin\src\slave\slave.cpp @ 6301]
13 00000049`22dfd520 00007ff6`502a0a35 mesos_agent!mesos::internal::slave::Slave::_run+0x2455 [d:\repositories\mesoswin\src\slave\slave.cpp @ 1990]
...
0:005> du @rcx
000001d7`cc55fb60  ""/dev/urandom""
"	MESOS	Resolved	3	1	6057	Windows, microsoft
12921323	Add a ContainerLogger module that restrains log sizes	"One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).

We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.

This will be a non-default module which will also serve as an example for how to implement the module."	MESOS	Resolved	3	4	6057	logging, mesosphere
12992633	ExamplesTest.DiskFullFramework fails on Arch	This test fails consistently on recent Arch linux, running in a VM.	MESOS	Resolved	3	1	6057	mesosphere
12846411	Port flag generation logic from the autotools solution to CMake	"One major barrier to widespread adoption of the CMake-based build system (other than the fact that we haven't implemented it yet!) is that most of our institutional knowledge of the quirks of how to build Mesos across many platforms is tied up in files like `configure.ac`.

Therefore, a ""good"" CMake-based build system will require us to go through these files systematically and manually port this logic to CMake (as well as testing it)."	MESOS	Accepted	3	3	6057	autotools, build, cmake
13001158	Potential FD double close in libevent's implementation of `sendfile`.	"Repro copied from: https://reviews.apache.org/r/51509/

It is possible to make the master CHECK fail by repeatedly hitting the web UI and reloading the static assets:

1) Paste lots of text (16KB or more) of text into `src/webui/master/static/home.html`.  The more text, the more reliable the repro.

2) Start the master with SSL enabled:
{code}
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_KEY_FILE=key.pem LIBPROCESS_SSL_CERT_FILE=cert.pem bin/mesos-master.sh --work_dir=/tmp/master
{code}

3) Run two instances of this python script repeatedly:
{code}
import socket
import ssl

s = ssl.wrap_socket(socket.socket())
s.connect((""localhost"", 5050))

s.sendall(""""""GET /static/home.html HTTP/1.1
User-Agent: foobar
Host: localhost:5050
Accept: */*
Connection: Keep-Alive

"""""")

# The HTTP part of the response
print s.recv(1000)
{code}

i.e. 
{code}
while python test.py; do :; done & while python test.py; do :; done
{code}"	MESOS	Resolved	2	1	6057	mesosphere, ssl
13250640	Standalone container documentation	We should add documentation for standalone containers.	MESOS	Resolved	3	20	6057	foundations, mesosphere
13217979	Fetcher vulnerability - escaping from sandbox	"I have noticed that there is a possibility to exploit fetcher and overwrite any file on the agent host.

scenario to reproduce:

1) prepare a file with any content and name a file like ""../../../etc/test"" and archive it. We can use python and zipfile module to achieve that:
{code:java}
>>> import zipfile
>>> zip = zipfile.ZipFile(""exploit.zip"", ""w"")
>>> zip.writestr(""../../../../../../../../../../../../etc/mariusz_was_here.txt"", ""some content"")
>>> zip.close()

{code}
2) prepare a service that will use our artifact (exploit.zip)

3) run service

at the end in /etc we will get our file. As you can imagine there is a lot possibility how we can use it.



"	MESOS	Resolved	1	1	6057	bug, foundations, security-issue, vulnerabilities
12919841	Introduce a module for logging executor/task output	"Existing executor/task logs are logged to files in their sandbox directory, with some nuances based on which containerizer is used (see background section in linked document).

A logger for executor/task logs has the following requirements:
* The logger is given a command to run and must handle the stdout/stderr of the command.
* The handling of stdout/stderr must be resilient across agent failover.  Logging should not stop if the agent fails.
* Logs should be readable, presumably via the web UI, or via some other module-specific UI."	MESOS	Resolved	3	3	6057	logging, mesosphere
12921756	Implement container logger module metadata recovery	"The {{ContainerLoggers}} are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.

For default {{ContainerLogger}} s, like the {{SandboxContainerLogger}} and the (tentatively named) {{TruncatingSandboxContainerLogger}}, the log files are exposed during agent recovery regardless.

For non-default {{ContainerLogger}} s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  This can be implemented as part of {{Containerizer::recover}}."	MESOS	Resolved	3	3	6057	logging, mesosphere
12902531	MemoryPressureMesosTest.CGROUPS_ROOT_Statistics and CGROUPS_ROOT_SlaveRecovery are flaky	"I am install Mesos 0.24.0 on 4 servers which have very similar hardware and software configurations. 

After performing {{../configure}}, {{make}}, and {{make check}} some servers have completed successfully and other failed on test {{[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics}}.

Is there something I should check in this test? 

{code}
PERFORMED MAKE CHECK NODE-001
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
I1005 14:37:35.585067 38479 exec.cpp:133] Version: 0.24.0
I1005 14:37:35.593789 38497 exec.cpp:207] Executor registered on slave 20151005-143735-2393768202-35106-27900-S0
Registered executor on svdidac038.techlabs.accenture.com
Starting task 010b2fe9-4eac-4136-8a8a-6ce7665488b0
Forked command at 38510
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'


PERFORMED MAKE CHECK NODE-002
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
I1005 14:38:58.794112 36997 exec.cpp:133] Version: 0.24.0
I1005 14:38:58.802851 37022 exec.cpp:207] Executor registered on slave 20151005-143857-2360213770-50427-26325-S0
Registered executor on svdidac039.techlabs.accenture.com
Starting task 9bb317ba-41cb-44a4-b507-d1c85ceabc28
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 37028
../../src/tests/containerizer/memory_pressure_tests.cpp:145: Failure
Expected: (usage.get().mem_medium_pressure_counter()) >= (usage.get().mem_critical_pressure_counter()), actual: 5 vs 6
2015-10-05 14:39:00,130:26325(0x2af08cc78700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:37198] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics (4303 ms)
{code}"	MESOS	Resolved	3	1	6057	flaky, flaky-test
13093046	CMake build system improvements	"This is a followup Epic for the [CMake epic|https://issues.apache.org/jira/browse/MESOS-898].

This includes items that will be tackled after we officially switch the build system from Autotools to CMake."	MESOS	Open	3	15	6057	build, cmake, mesosphere
13100308	Support non-Win32 multiconfig generators	"The cmake build setup currently does not support non-Win32 multiconfig generators like Xcode,
{code}
clang: error: no such file or directory: '/Users/bbannier/src/mesos/_xcode/3rdparty/protobuf-3.3.0/src/protobuf-3.3.0-build/libprotobuf.dylib'
clang: error: no such file or directory: '/Users/bbannier/src/mesos/_xcode/3rdparty/googletest-1.8.0/src/googletest-1.8.0-build/googlemock/libgmock.a'
clang: error: no such file or directory: '/Users/bbannier/src/mesos/_xcode/3rdparty/googletest-1.8.0/src/googletest-1.8.0-build/googlemock/gtest/libgtest.a'
{code}

The issue here seems to be that the cmake setup assumes that only {{WIN32}} uses a multiconfig generator, e.g.,
{code}
if (WIN32)
  set_target_properties(
    protobuf PROPERTIES
    IMPORTED_LOCATION_DEBUG ${PROTOBUF_ROOT}-build/Debug/libprotobufd${LIBRARY_SUFFIX}
    IMPORTED_LOCATION_RELEASE ${PROTOBUF_ROOT}-build/Release/libprotobuf${LIBRARY_SUFFIX})

  set_target_properties(
    protoc PROPERTIES
    IMPORTED_LOCATION_DEBUG ${PROTOBUF_ROOT}-build/Debug/protoc.exe
    IMPORTED_LOCATION_RELEASE ${PROTOBUF_ROOT}-build/Release/protoc.exe)
else ()
  # This is for single-configuration generators such as GNU Make.
  if (CMAKE_BUILD_TYPE MATCHES DEBUG)
    set(PROTOBUF_SUFFIX d)
  endif ()

  set_target_properties(
    protobuf PROPERTIES
    IMPORTED_LOCATION ${PROTOBUF_ROOT}-build/libprotobuf${PROTOBUF_SUFFIX}${LIBRARY_SUFFIX})

  set_target_properties(
    protoc PROPERTIES
    IMPORTED_LOCATION ${PROTOBUF_ROOT}-build/protoc)
endif ()
{code}"	MESOS	Accepted	3	3	6057	cmake, mesosphere
13034704	Libprocess reinit code leaks SSL server socket FD	"After [this commit|https://github.com/apache/mesos/commit/789e9f7], it was discovered that tests which use {{process::reinitialize}} to switch between SSL and non-SSL modes will leak the file descriptor associated with the server socket {{\_\_s\_\_}}. This can be reproduced by running the following trivial test in repetition:
{code}
diff --git a/src/tests/scheduler_tests.cpp b/src/tests/scheduler_tests.cpp
index 1ff423f..d5fd575 100644
--- a/src/tests/scheduler_tests.cpp
+++ b/src/tests/scheduler_tests.cpp
@@ -1821,6 +1821,12 @@ INSTANTIATE_TEST_CASE_P(
 #endif // USE_SSL_SOCKET


+TEST_P(SchedulerSSLTest, LeakTest)
+{
+  ::sleep(1);
+}
+
+
 // Tests that a scheduler can subscribe, run a task, and then tear itself down.
 TEST_P(SchedulerSSLTest, RunTaskAndTeardown)
 {
{code}"	MESOS	Accepted	3	1	6057	foundations, libprocess, mesosphere, ssl
13059001	Support launching standalone containers.	Containerizer should support launching containers (both top level and nested) that are not tied to a particular Mesos task or executor. This is for the case where the agent wants to launch some system containers (e.g., for CSI plugin) that will be managed by Mesos.	MESOS	Resolved	3	15	6057	mesosphere, storage
12966450	Add asynchronous hook for validating docker containerizer tasks	"It is possible to plug in custom validation logic for the MesosContainerizer via an {{Isolator}} module, but the same is not true of the DockerContainerizer.

Basic logic can be plugged into the DockerContainerizer via {{Hooks}}, but this has some notable differences compared to isolators:
* Hooks are synchronous.
* Modifications to tasks via Hooks have lower priority compared to the task itself.  i.e. If both the {{TaskInfo}} and {{slaveExecutorEnvironmentDecorator}} define the same environment variable, the {{TaskInfo}} wins.
* Hooks have no effect if they fail (short of segfaulting)
i.e. The {{slavePreLaunchDockerHook}} has a return type of {{Try<Nothing>}}:
https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/include/mesos/hook.hpp#L90
But the effect of returning an {{Error}} is a log message:
https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/hook/manager.cpp#L227-L230

We should add a hook to the DockerContainerizer to narrow this gap.  This new hook would:
* Be called at roughly the same place as {{slavePreLaunchDockerHook}}
https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/slave/containerizer/docker.cpp#L1022
* Return a {{Future}} and require splitting up {{DockerContainerizer::launch}}.
* Prevent a task from launching if it returns a {{Failure}}."	MESOS	Resolved	4	4	6057	containerizer, hooks, mesosphere
12980890	ContainerizerTest.ROOT_CGROUPS_BalloonFramework fails because executor environment isn't inherited	A recent change forbits the executor to inherit environment variables from the agent's environment. As a regression this break {{ContainerizerTest.ROOT_CGROUPS_BalloonFramework}}.	MESOS	Resolved	3	1	6057	mesosphere
13261457	Implement an SSL socket for Windows, using OpenSSL directly	"{code}
class WindowsSSLSocketImpl : public SocketImpl
{
public:
  // This will be the entry point for Socket::create(SSL).
  static Try<std::shared_ptr<SocketImpl>> create(int_fd s);

  WindowsSSLSocketImpl(int_fd _s);
  ~WindowsSSLSocketImpl() override;

  // Overrides for the 'SocketImpl' interface below.

  // Unreachable.
  Future<Nothing> connect(const Address& address) override;

  // This will initialize SSL objects then call windows::connect()
  // and chain that onto the appropriate call to SSL_do_handshake.
  Future<Nothing> connect(
      const Address& address,
      const openssl::TLSClientConfig& config) override;

  // These will call SSL_read or SSL_write as appropriate.
  // As long as the SSL context is set up correctly, these will be
  // thin wrappers.  (More details after the code block.)
  Future<size_t> recv(char* data, size_t size) override;
  Future<size_t> send(const char* data, size_t size) override;
  Future<size_t> sendfile(int_fd fd, off_t offset, size_t size) override;

  // Nothing SSL here, just a plain old listener.
  Try<Nothing> listen(int backlog) override;

  // This will initialize SSL objects then call windows::accept()
  // and then perform handshaking.  Any downgrading will
  // happen here.  Since we control the event loop, we can
  // easily peek at the first few bytes to check SSL-ness.
  Future<std::shared_ptr<SocketImpl>> accept() override;

  SocketImpl::Kind kind() const override { return SocketImpl::Kind::SSL; }
}
{code}"	MESOS	Resolved	3	3	6057	foundations
12860861	Add either log rotation or capped-size logging (for tasks)	"Tasks currently log their output (i.e. stdout/stderr) to files (the ""sandbox"") on an agent's disk.  In some cases, the accumulation of these logs can completely fill up the agent's disk and thereby kill the task or machine.

To prevent this, we should either implement a log rotation mechanism or capped-size logging.  This would be used by executors to control the amount of logs they keep.  

Master/agent logs will not be affected.

We will first scope out several possible approaches for log rotation/capping in a design document (see [MESOS-3356]).  Once an approach is chosen, this story will be broken down into some corresponding issues."	MESOS	Resolved	3	16	6057	mesosphere
13227359	Heartbeat calls from executor to agent are reported as errors	"These HEARTBEAT calls and events were added in MESOS-7564.

HEARTBEAT calls are generated by the executor library, which does not have access to the executor's Framework/Executor IDs.  The library therefore uses some dummy values instead, because HEARTBEAT calls do not really require required fields.  When the agent receives these dummy values, it returns a 400 Bad Request.  It should return 202 Accepted instead."	MESOS	Resolved	4	1	6057	foundations
13183605	LongLivedDefaultExecutorRestart is flaky.	"{noformat}
03:52:07  [ RUN      ] GarbageCollectorIntegrationTest.LongLivedDefaultExecutorRestart
03:52:07  I0907 03:52:07.699676  2350 cluster.cpp:173] Creating default 'local' authorizer
03:52:07  I0907 03:52:07.700664  2374 master.cpp:413] Master 8e9d97f6-4dc4-490b-81f6-d2033e2109d3 (ip-172-16-10-27.ec2.internal) started on 172.16.10.27:45074
03:52:07  I0907 03:52:07.700690  2374 master.cpp:416] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""hierarchical"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authentication_v0_timeout=""15secs"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/cuUPYo/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --memory_profiling=""false"" --min_allocatable_resources=""cpus:0.01|mem:32"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --role_sorter=""drf"" --root_submissions=""true"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/cuUPYo/master"" --zk_session_timeout=""10secs""
03:52:07  I0907 03:52:07.700857  2374 master.cpp:465] Master only allowing authenticated frameworks to register
03:52:07  I0907 03:52:07.700870  2374 master.cpp:471] Master only allowing authenticated agents to register
03:52:07  I0907 03:52:07.700947  2374 master.cpp:477] Master only allowing authenticated HTTP frameworks to register
03:52:07  I0907 03:52:07.700958  2374 credentials.hpp:37] Loading credentials for authentication from '/tmp/cuUPYo/credentials'
03:52:07  I0907 03:52:07.701068  2374 master.cpp:521] Using default 'crammd5' authenticator
03:52:07  I0907 03:52:07.701151  2374 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
03:52:07  I0907 03:52:07.701254  2374 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
03:52:07  I0907 03:52:07.701352  2374 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
03:52:07  I0907 03:52:07.701445  2374 master.cpp:602] Authorization enabled
03:52:07  I0907 03:52:07.701566  2370 whitelist_watcher.cpp:77] No whitelist given
03:52:07  I0907 03:52:07.701695  2376 hierarchical.cpp:182] Initialized hierarchical allocator process
03:52:07  I0907 03:52:07.702237  2374 master.cpp:2083] Elected as the leading master!
03:52:07  I0907 03:52:07.702255  2374 master.cpp:1638] Recovering from registrar
03:52:07  I0907 03:52:07.702293  2375 registrar.cpp:339] Recovering registrar
03:52:07  I0907 03:52:07.706190  2375 registrar.cpp:383] Successfully fetched the registry (0B) in 3.884032ms
03:52:07  I0907 03:52:07.706233  2375 registrar.cpp:487] Applied 1 operations in 7967ns; attempting to update the registry
03:52:07  I0907 03:52:07.706378  2375 registrar.cpp:544] Successfully updated the registry in 126976ns
03:52:07  I0907 03:52:07.706413  2375 registrar.cpp:416] Successfully recovered registrar
03:52:07  I0907 03:52:07.706507  2375 master.cpp:1752] Recovered 0 agents from the registry (172B); allowing 10mins for agents to reregister
03:52:07  I0907 03:52:07.706548  2375 hierarchical.cpp:220] Skipping recovery of hierarchical allocator: nothing to recover
03:52:07  W0907 03:52:07.708107  2350 process.cpp:2810] Attempted to spawn already running process files@172.16.10.27:45074
03:52:07  I0907 03:52:07.708500  2350 containerizer.cpp:305] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }
03:52:07  I0907 03:52:07.710343  2350 linux_launcher.cpp:144] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
03:52:07  I0907 03:52:07.710748  2350 provisioner.cpp:298] Using default backend 'overlay'
03:52:07  I0907 03:52:07.711236  2350 cluster.cpp:485] Creating default 'local' authorizer
03:52:07  I0907 03:52:07.711629  2376 slave.cpp:267] Mesos agent started on (90)@172.16.10.27:45074
03:52:07  I0907 03:52:07.711647  2376 slave.cpp:268] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""true"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv"" --zk_session_timeout=""10secs""
03:52:07  I0907 03:52:07.711815  2376 credentials.hpp:86] Loading credential for authentication from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/credential'
03:52:07  I0907 03:52:07.711861  2376 slave.cpp:300] Agent using credential for: test-principal
03:52:07  I0907 03:52:07.711872  2376 credentials.hpp:37] Loading credentials for authentication from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/http_credentials'
03:52:07  I0907 03:52:07.711936  2376 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'
03:52:07  I0907 03:52:07.711989  2376 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'
03:52:07  I0907 03:52:07.712060  2376 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
03:52:07  I0907 03:52:07.712100  2376 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'
03:52:07  I0907 03:52:07.712137  2376 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
03:52:07  I0907 03:52:07.712158  2376 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'
03:52:07  I0907 03:52:07.712229  2376 disk_profile_adaptor.cpp:80] Creating default disk profile adaptor module
03:52:07  I0907 03:52:07.712790  2376 slave.cpp:615] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
03:52:07  I0907 03:52:07.712839  2376 slave.cpp:623] Agent attributes: [  ]
03:52:07  I0907 03:52:07.712848  2376 slave.cpp:632] Agent hostname: ip-172-16-10-27.ec2.internal
03:52:07  I0907 03:52:07.712952  2370 task_status_update_manager.cpp:181] Pausing sending task status updates
03:52:07  I0907 03:52:07.713099  2350 scheduler.cpp:189] Version: 1.8.0
03:52:07  I0907 03:52:07.713196  2376 state.cpp:66] Recovering state from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta'
03:52:07  I0907 03:52:07.713269  2373 slave.cpp:6909] Finished recovering checkpointed state from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta', beginning agent recovery
03:52:07  I0907 03:52:07.713306  2373 task_status_update_manager.cpp:207] Recovering task status update manager
03:52:07  I0907 03:52:07.713399  2373 containerizer.cpp:727] Recovering Mesos containers
03:52:07  I0907 03:52:07.713490  2373 linux_launcher.cpp:286] Recovering Linux launcher
03:52:07  I0907 03:52:07.713629  2373 containerizer.cpp:1053] Recovering isolators
03:52:07  I0907 03:52:07.713811  2374 scheduler.cpp:355] Using default 'basic' HTTP authenticatee
03:52:07  I0907 03:52:07.713838  2372 containerizer.cpp:1092] Recovering provisioner
03:52:07  I0907 03:52:07.713896  2374 scheduler.cpp:538] New master detected at master@172.16.10.27:45074
03:52:07  I0907 03:52:07.713909  2374 scheduler.cpp:547] Waiting for 0ns before initiating a re-(connection) attempt with the master
03:52:07  I0907 03:52:07.713941  2372 provisioner.cpp:494] Provisioner recovery complete
03:52:07  I0907 03:52:07.714123  2371 composing.cpp:339] Finished recovering all containerizers
03:52:07  I0907 03:52:07.714171  2371 slave.cpp:7138] Recovering executors
03:52:07  I0907 03:52:07.714198  2371 slave.cpp:7291] Finished recovery
03:52:07  I0907 03:52:07.714467  2371 slave.cpp:1254] New master detected at master@172.16.10.27:45074
03:52:07  I0907 03:52:07.714491  2371 slave.cpp:1319] Detecting new master
03:52:07  I0907 03:52:07.714519  2371 task_status_update_manager.cpp:181] Pausing sending task status updates
03:52:07  I0907 03:52:07.714651  2373 scheduler.cpp:429] Connected with the master at http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.714872  2373 scheduler.cpp:248] Sending SUBSCRIBE call to http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.715217  2376 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
03:52:07  I0907 03:52:07.715551  2374 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41612
03:52:07  I0907 03:52:07.715626  2374 master.cpp:2502] Received subscription request for HTTP framework 'default'
03:52:07  I0907 03:52:07.715656  2374 master.cpp:2155] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'
03:52:07  I0907 03:52:07.715811  2372 master.cpp:2637] Subscribing framework 'default' with checkpointing enabled and capabilities [ MULTI_ROLE, RESERVATION_REFINEMENT ]
03:52:07  I0907 03:52:07.716225  2372 master.cpp:9883] Adding framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) with roles {  } suppressed
03:52:07  I0907 03:52:07.716414  2374 hierarchical.cpp:306] Added framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.716454  2374 hierarchical.cpp:1564] Performed allocation for 0 agents in 6701ns
03:52:07  I0907 03:52:07.716715  2375 scheduler.cpp:845] Enqueuing event SUBSCRIBED received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.716856  2373 scheduler.cpp:845] Enqueuing event HEARTBEAT received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.719863  2373 slave.cpp:1346] Authenticating with master master@172.16.10.27:45074
03:52:07  I0907 03:52:07.719892  2373 slave.cpp:1355] Using default CRAM-MD5 authenticatee
03:52:07  I0907 03:52:07.719959  2375 authenticatee.cpp:121] Creating new client SASL connection
03:52:07  I0907 03:52:07.720403  2375 master.cpp:9653] Authenticating slave(90)@172.16.10.27:45074
03:52:07  I0907 03:52:07.720453  2375 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(200)@172.16.10.27:45074
03:52:07  I0907 03:52:07.720512  2375 authenticator.cpp:98] Creating new server SASL connection
03:52:07  I0907 03:52:07.720927  2375 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
03:52:07  I0907 03:52:07.720947  2375 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
03:52:07  I0907 03:52:07.720979  2375 authenticator.cpp:204] Received SASL authentication start
03:52:07  I0907 03:52:07.721015  2375 authenticator.cpp:326] Authentication requires more steps
03:52:07  I0907 03:52:07.721052  2375 authenticatee.cpp:259] Received SASL authentication step
03:52:07  I0907 03:52:07.721097  2375 authenticator.cpp:232] Received SASL authentication step
03:52:07  I0907 03:52:07.721117  2375 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-27.ec2.internal' server FQDN: 'ip-172-16-10-27.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
03:52:07  I0907 03:52:07.721127  2375 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
03:52:07  I0907 03:52:07.721137  2375 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
03:52:07  I0907 03:52:07.721144  2375 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-27.ec2.internal' server FQDN: 'ip-172-16-10-27.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
03:52:07  I0907 03:52:07.721151  2375 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
03:52:07  I0907 03:52:07.721158  2375 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
03:52:07  I0907 03:52:07.721169  2375 authenticator.cpp:318] Authentication success
03:52:07  I0907 03:52:07.721215  2370 authenticatee.cpp:299] Authentication success
03:52:07  I0907 03:52:07.721282  2370 slave.cpp:1446] Successfully authenticated with master master@172.16.10.27:45074
03:52:07  I0907 03:52:07.721366  2370 slave.cpp:1877] Will retry registration in 14.537761ms if necessary
03:52:07  I0907 03:52:07.721418  2375 master.cpp:9685] Successfully authenticated principal 'test-principal' at slave(90)@172.16.10.27:45074
03:52:07  I0907 03:52:07.721427  2374 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(200)@172.16.10.27:45074
03:52:07  I0907 03:52:07.721469  2375 master.cpp:6605] Received register agent message from slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.721518  2375 master.cpp:3964] Authorizing agent providing resources 'cpus:2; mem:1024; disk:1024; ports:[31000-32000]' with principal 'test-principal'
03:52:07  I0907 03:52:07.721632  2373 master.cpp:6672] Authorized registration of agent at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.721666  2373 master.cpp:6787] Registering agent at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) with id 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:07  I0907 03:52:07.721783  2373 registrar.cpp:487] Applied 1 operations in 28796ns; attempting to update the registry
03:52:07  I0907 03:52:07.721932  2370 registrar.cpp:544] Successfully updated the registry in 124928ns
03:52:07  I0907 03:52:07.721992  2370 master.cpp:6835] Admitted agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.722132  2370 master.cpp:6880] Registered agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) with cpus:2; mem:1024; disk:1024; ports:[31000-32000]
03:52:07  I0907 03:52:07.722203  2374 hierarchical.cpp:601] Added agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 (ip-172-16-10-27.ec2.internal) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})
03:52:07  I0907 03:52:07.722205  2370 slave.cpp:1479] Registered with master master@172.16.10.27:45074; given agent ID 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:07  I0907 03:52:07.722396  2370 slave.cpp:1499] Checkpointing SlaveInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/slave.info'
03:52:07  I0907 03:52:07.722447  2374 hierarchical.cpp:1564] Performed allocation for 1 agents in 173395ns
03:52:07  I0907 03:52:07.722482  2374 task_status_update_manager.cpp:188] Resuming sending task status updates
03:52:07  I0907 03:52:07.722573  2374 master.cpp:9468] Sending offers [ 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O0 ] to framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:07  I0907 03:52:07.722651  2370 slave.cpp:1548] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""V1oIFlhHRv2xxYxsF9hCkQ==""},""slave_id"":{""value"":""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0""},""update_oversubscribed_resources"":false}
03:52:07  I0907 03:52:07.722790  2374 master.cpp:7939] Ignoring update on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) as it reports no changes
03:52:07  I0907 03:52:07.723096  2375 scheduler.cpp:845] Enqueuing event OFFERS received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.723731  2369 scheduler.cpp:248] Sending ACCEPT call to http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.724097  2371 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
03:52:07  I0907 03:52:07.724397  2372 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41610
03:52:07  I0907 03:52:07.724557  2372 master.cpp:11462] Removing offer 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O0
03:52:07  I0907 03:52:07.724704  2372 master.cpp:4467] Processing ACCEPT call for offers: [ 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O0 ] on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:07  I0907 03:52:07.724750  2372 master.cpp:3541] Authorizing framework principal 'test-principal' to launch task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f
03:52:07  I0907 03:52:07.724932  2372 master.cpp:3541] Authorizing framework principal 'test-principal' to launch task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd
03:52:07  I0907 03:52:07.725594  2373 master.cpp:12209] Adding task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.725653  2373 master.cpp:5663] Launching task group { 2e8c13b6-fa45-4e3c-89cd-398a5abc192f } of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) on  new executor
03:52:07  I0907 03:52:07.725885  2374 slave.cpp:2014] Got assigned task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.725953  2374 slave.cpp:8908] Checkpointing FrameworkInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/framework.info'
03:52:07  I0907 03:52:07.726119  2373 master.cpp:12209] Adding task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.726174  2373 master.cpp:5663] Launching task group { c6c81339-65c6-4f86-b0ab-c5be60ea5fbd } of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) on  existing executor
03:52:07  I0907 03:52:07.726406  2372 hierarchical.cpp:1236] Recovered cpus(allocated: *):1.7; mem(allocated: *):928; disk(allocated: *):928; ports(allocated: *):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: cpus(allocated: *):0.3; mem(allocated: *):96; disk(allocated: *):96) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 from framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.726444  2372 hierarchical.cpp:1282] Framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 filtered agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 for 5secs
03:52:07  I0907 03:52:07.726583  2374 slave.cpp:8919] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/framework.pid'
03:52:07  I0907 03:52:07.727038  2374 slave.cpp:2014] Got assigned task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.727283  2374 slave.cpp:2388] Authorizing task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.727318  2374 slave.cpp:8466] Authorizing framework principal 'test-principal' to launch task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f
03:52:07  I0907 03:52:07.727609  2374 slave.cpp:2388] Authorizing task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.727641  2374 slave.cpp:8466] Authorizing framework principal 'test-principal' to launch task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd
03:52:07  I0907 03:52:07.728116  2374 slave.cpp:2831] Launching task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.728165  2374 paths.cpp:752] Creating sandbox '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' for user 'root'
03:52:07  I0907 03:52:07.728693  2374 slave.cpp:9694] Checkpointing ExecutorInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/executor.info'
03:52:07  I0907 03:52:07.728947  2374 paths.cpp:755] Creating sandbox '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f'
03:52:07  I0907 03:52:07.729185  2374 slave.cpp:8994] Launching executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":32.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":32.0},""type"":""SCALAR""}] in work directory '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f'
03:52:07  I0907 03:52:07.729526  2374 slave.cpp:9725] Checkpointing TaskInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/2e8c13b6-fa45-4e3c-89cd-398a5abc192f/task.info'
03:52:07  I0907 03:52:07.731062  2374 slave.cpp:3028] Queued task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.731325  2374 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/latest'
03:52:07  I0907 03:52:07.731355  2374 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/latest'
03:52:07  I0907 03:52:07.731370  2374 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f'
03:52:07  I0907 03:52:07.731529  2374 slave.cpp:3509] Launching container dbe02af2-3122-4f2e-9747-0c4343627c2f for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.731864  2374 slave.cpp:2831] Launching task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.731918  2374 slave.cpp:9725] Checkpointing TaskInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/c6c81339-65c6-4f86-b0ab-c5be60ea5fbd/task.info'
03:52:07  I0907 03:52:07.733171  2374 slave.cpp:3028] Queued task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.733458  2373 containerizer.cpp:1280] Starting container dbe02af2-3122-4f2e-9747-0c4343627c2f
03:52:07  I0907 03:52:07.733788  2373 containerizer.cpp:1446] Checkpointed ContainerConfig at '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f/config'
03:52:07  I0907 03:52:07.733808  2373 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f from PROVISIONING to PREPARING
03:52:07  I0907 03:52:07.734777  2369 containerizer.cpp:1939] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""arguments"":[""mesos-default-executor"",""--launcher_dir=/home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src""],""shell"":false,""value"":""/home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src/mesos-default-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.16.10.27:45074""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""1""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f""},{""name"":""MESOS_EXECUTOR_AUTHENTICATION_TOKEN"",""type"":""VALUE"",""value"":""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjaWQiOiJkYmUwMmFmMi0zMTIyLTRmMmUtOTc0Ny0wYzQzNDM2MjdjMmYiLCJlaWQiOiJkZWZhdWx0IiwiZmlkIjoiOGU5ZDk3ZjYtNGRjNC00OTBiLTgxZjYtZDIwMzNlMjEwOWQzLTAwMDAifQ.Ww__Iwo_c3fJl_ruqYdi_EePl81IKoIQJv74nq6pHl8""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""default""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_RECOVERY_TIMEOUT"",""type"":""VALUE"",""value"":""15mins""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(90)@172.16.10.27:45074""},{""name"":""MESOS_SUBSCRIPTION_BACKOFF_MAX"",""type"":""VALUE"",""value"":""2secs""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f""}]},""task_environment"":{},""user"":""root"",""working_directory"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f""}"" --pipe_read=""15"" --pipe_write=""18"" --runtime_directory=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f"" --unshare_namespace_mnt=""false""'
03:52:07  I0907 03:52:07.734957  2376 linux_launcher.cpp:492] Launching container dbe02af2-3122-4f2e-9747-0c4343627c2f and cloning with namespaces 
03:52:07  I0907 03:52:07.754221  2369 containerizer.cpp:2044] Checkpointing container's forked pid 10086 to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/pids/forked.pid'
03:52:07  I0907 03:52:07.754623  2369 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f from PREPARING to ISOLATING
03:52:07  I0907 03:52:07.755056  2369 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f from ISOLATING to FETCHING
03:52:07  I0907 03:52:07.755112  2369 fetcher.cpp:369] Starting to fetch URIs for container: dbe02af2-3122-4f2e-9747-0c4343627c2f, directory: /tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f
03:52:07  I0907 03:52:07.755334  2369 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f from FETCHING to RUNNING
03:52:07  I0907 03:52:07.856390 10100 executor.cpp:201] Version: 1.8.0
03:52:07  I0907 03:52:07.858984  2374 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1/executor'
03:52:07  I0907 03:52:07.859725  2373 http.cpp:1177] HTTP POST for /slave(90)/api/v1/executor from 172.16.10.27:41614
03:52:07  I0907 03:52:07.859807  2373 slave.cpp:4607] Received Subscribe request for HTTP executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.859856  2373 slave.cpp:4670] Creating a marker file for HTTP based executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (via HTTP) at path '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/http.marker'
03:52:07  I0907 03:52:07.860514  2373 slave.cpp:3282] Sending queued task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] to executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (via HTTP)
03:52:07  I0907 03:52:07.860643  2373 slave.cpp:3282] Sending queued task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] to executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (via HTTP)
03:52:07  I0907 03:52:07.861613 10117 default_executor.cpp:204] Received SUBSCRIBED event
03:52:07  I0907 03:52:07.861932 10117 default_executor.cpp:208] Subscribed executor on ip-172-16-10-27.ec2.internal
03:52:07  I0907 03:52:07.862021 10117 default_executor.cpp:204] Received LAUNCH_GROUP event
03:52:07  I0907 03:52:07.862232 10117 default_executor.cpp:204] Received LAUNCH_GROUP event
03:52:07  I0907 03:52:07.862555 10119 default_executor.cpp:428] Setting 'MESOS_CONTAINER_IP' to: 172.16.10.27
03:52:07  I0907 03:52:07.863678  2373 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1/executor'
03:52:07  I0907 03:52:07.863754  2373 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1'
03:52:07  I0907 03:52:07.864528  2373 http.cpp:1177] HTTP POST for /slave(90)/api/v1 from 172.16.10.27:41618
03:52:07  I0907 03:52:07.864616  2373 http.cpp:1177] HTTP POST for /slave(90)/api/v1/executor from 172.16.10.27:41616
03:52:07  I0907 03:52:07.864670 10121 default_executor.cpp:428] Setting 'MESOS_CONTAINER_IP' to: 172.16.10.27
03:52:07  I0907 03:52:07.864679  2373 slave.cpp:5269] Handling status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.864825  2373 http.cpp:2444] Processing LAUNCH_NESTED_CONTAINER call for container 'dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc'
03:52:07  I0907 03:52:07.865089  2376 task_status_update_manager.cpp:328] Received task status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.865116  2376 task_status_update_manager.cpp:507] Creating StatusUpdate stream for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.865145  2373 containerizer.cpp:1242] Creating sandbox '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/dbc72eae-9465-4bf7-a082-0bf8a055fecc' for user 'root'
03:52:07  I0907 03:52:07.865350  2373 containerizer.cpp:1280] Starting container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc
03:52:07  I0907 03:52:07.865348  2376 task_status_update_manager.cpp:842] Checkpointing UPDATE for task status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.865442  2376 task_status_update_manager.cpp:383] Forwarding task status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to the agent
03:52:07  I0907 03:52:07.865520  2376 slave.cpp:5761] Forwarding the update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to master@172.16.10.27:45074
03:52:07  I0907 03:52:07.865600  2376 slave.cpp:5654] Task status update manager successfully handled status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.865655  2369 master.cpp:8375] Status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 from agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.865679  2369 master.cpp:8432] Forwarding status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.865785  2369 master.cpp:10932] Updating the state of task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (latest state: TASK_STARTING, status update state: TASK_STARTING)
03:52:07  I0907 03:52:07.866125  2372 scheduler.cpp:845] Enqueuing event UPDATE received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.866276  2373 containerizer.cpp:1446] Checkpointed ContainerConfig at '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/dbc72eae-9465-4bf7-a082-0bf8a055fecc/config'
03:52:07  I0907 03:52:07.866367  2375 scheduler.cpp:248] Sending ACKNOWLEDGE call to http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.866674  2372 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
03:52:07  I0907 03:52:07.866292  2373 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc from PROVISIONING to PREPARING
03:52:07  I0907 03:52:07.867278  2375 containerizer.cpp:1939] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""sleep 1000""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/dbc72eae-9465-4bf7-a082-0bf8a055fecc""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""172.16.10.27""}]},""task_environment"":{},""user"":""root"",""working_directory"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/dbc72eae-9465-4bf7-a082-0bf8a055fecc""}"" --pipe_read=""22"" --pipe_write=""23"" --runtime_directory=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/dbc72eae-9465-4bf7-a082-0bf8a055fecc"" --unshare_namespace_mnt=""false""'
03:52:07  I0907 03:52:07.867460  2374 linux_launcher.cpp:492] Launching nested container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc and cloning with namespaces 
03:52:07  I0907 03:52:07.869093 10119 default_executor.cpp:204] Received ACKNOWLEDGED event
03:52:07  I0907 03:52:07.869769  2373 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1'
03:52:07  I0907 03:52:07.869877  2373 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1/executor'
03:52:07  I0907 03:52:07.870610  2370 http.cpp:1177] HTTP POST for /slave(90)/api/v1 from 172.16.10.27:41620
03:52:07  I0907 03:52:07.870734  2370 http.cpp:2444] Processing LAUNCH_NESTED_CONTAINER call for container 'dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c'
03:52:07  I0907 03:52:07.886466  2375 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc from PREPARING to ISOLATING
03:52:07  I0907 03:52:07.886638  2375 containerizer.cpp:1242] Creating sandbox '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c' for user 'root'
03:52:07  I0907 03:52:07.886797  2375 containerizer.cpp:1280] Starting container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c
03:52:07  I0907 03:52:07.887120  2375 containerizer.cpp:1446] Checkpointed ContainerConfig at '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c/config'
03:52:07  I0907 03:52:07.887145  2375 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c from PROVISIONING to PREPARING
03:52:07  I0907 03:52:07.887554  2375 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc from ISOLATING to FETCHING
03:52:07  I0907 03:52:07.887653  2375 fetcher.cpp:369] Starting to fetch URIs for container: dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc, directory: /tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/dbc72eae-9465-4bf7-a082-0bf8a055fecc
03:52:07  I0907 03:52:07.888100  2375 containerizer.cpp:1939] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""exit 0""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""172.16.10.27""}]},""task_environment"":{},""user"":""root"",""working_directory"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c""}"" --pipe_read=""24"" --pipe_write=""25"" --runtime_directory=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c"" --unshare_namespace_mnt=""false""'
03:52:07  I0907 03:52:07.888273  2371 linux_launcher.cpp:492] Launching nested container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c and cloning with namespaces 
03:52:07  I0907 03:52:07.893172  2375 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c from PREPARING to ISOLATING
03:52:07  I0907 03:52:07.893383  2375 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc from FETCHING to RUNNING
03:52:07  I0907 03:52:07.893857  2375 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c from ISOLATING to FETCHING
03:52:07  I0907 03:52:07.894094  2375 fetcher.cpp:369] Starting to fetch URIs for container: dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c, directory: /tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c
03:52:07  I0907 03:52:07.894352  2375 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c from FETCHING to RUNNING
03:52:07  I0907 03:52:07.896064 10116 default_executor.cpp:663] Finished launching tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] in child containers [ dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc ]
03:52:07  I0907 03:52:07.896095 10116 default_executor.cpp:687] Waiting on child containers of tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ]
03:52:07  I0907 03:52:07.896347 10116 default_executor.cpp:663] Finished launching tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] in child containers [ dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c ]
03:52:07  I0907 03:52:07.896368 10116 default_executor.cpp:687] Waiting on child containers of tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ]
03:52:07  I0907 03:52:07.896908 10118 default_executor.cpp:748] Waiting for child container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc of task '2e8c13b6-fa45-4e3c-89cd-398a5abc192f'
03:52:07  I0907 03:52:07.896988 10118 default_executor.cpp:748] Waiting for child container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c of task 'c6c81339-65c6-4f86-b0ab-c5be60ea5fbd'
03:52:07  I0907 03:52:07.897475  2374 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1'
03:52:07  I0907 03:52:07.897568  2374 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1'
03:52:07  I0907 03:52:07.898296  2374 http.cpp:1177] HTTP POST for /slave(90)/api/v1 from 172.16.10.27:41622
03:52:07  I0907 03:52:07.898389  2374 http.cpp:1177] HTTP POST for /slave(90)/api/v1 from 172.16.10.27:41624
03:52:07  I0907 03:52:07.898488  2374 http.cpp:2679] Processing WAIT_NESTED_CONTAINER call for container 'dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc'
03:52:07  I0907 03:52:07.898587  2374 http.cpp:2679] Processing WAIT_NESTED_CONTAINER call for container 'dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c'
03:52:07  I0907 03:52:07.906261  2374 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1/executor'
03:52:07  I0907 03:52:07.906337  2374 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1/executor'
03:52:07  I0907 03:52:07.906690  2374 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41610
03:52:07  I0907 03:52:07.906752  2374 master.cpp:6241] Processing ACKNOWLEDGE call for status e10442b0-02ac-4479-9810-b270714ce3c9 for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:07  I0907 03:52:07.906955  2374 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.906989  2374 task_status_update_manager.cpp:842] Checkpointing ACK for task status update TASK_STARTING (Status UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.907456  2374 slave.cpp:4505] Task status update manager successfully handled status update acknowledgement (UUID: e10442b0-02ac-4479-9810-b270714ce3c9) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.908149  2376 http.cpp:1177] HTTP POST for /slave(90)/api/v1/executor from 172.16.10.27:41616
03:52:07  I0907 03:52:07.908223  2376 slave.cpp:5269] Handling status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.908481  2376 http.cpp:1177] HTTP POST for /slave(90)/api/v1/executor from 172.16.10.27:41616
03:52:07  I0907 03:52:07.908550  2376 slave.cpp:5269] Handling status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.908641  2376 http.cpp:1177] HTTP POST for /slave(90)/api/v1/executor from 172.16.10.27:41616
03:52:07  I0907 03:52:07.908699  2376 slave.cpp:5269] Handling status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.909559  2376 task_status_update_manager.cpp:328] Received task status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.909593  2376 task_status_update_manager.cpp:507] Creating StatusUpdate stream for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.909816  2376 task_status_update_manager.cpp:842] Checkpointing UPDATE for task status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.909917  2376 task_status_update_manager.cpp:383] Forwarding task status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to the agent
03:52:07  I0907 03:52:07.910491  2369 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.910521  2369 task_status_update_manager.cpp:842] Checkpointing UPDATE for task status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.910598  2369 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to the agent
03:52:07  I0907 03:52:07.910706  2373 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.910732  2373 task_status_update_manager.cpp:842] Checkpointing UPDATE for task status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.910827  2376 slave.cpp:5761] Forwarding the update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to master@172.16.10.27:45074
03:52:07  I0907 03:52:07.910946  2372 master.cpp:8375] Status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 from agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.910979  2372 master.cpp:8432] Forwarding status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.911111  2372 master.cpp:10932] Updating the state of task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (latest state: TASK_RUNNING, status update state: TASK_STARTING)
03:52:07  I0907 03:52:07.911465  2375 scheduler.cpp:845] Enqueuing event UPDATE received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.911682  2375 scheduler.cpp:248] Sending ACKNOWLEDGE call to http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.912065  2371 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
03:52:07  I0907 03:52:07.912154  2376 slave.cpp:5654] Task status update manager successfully handled status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.912531 10119 default_executor.cpp:204] Received ACKNOWLEDGED event
03:52:07  I0907 03:52:07.912585  2376 slave.cpp:5761] Forwarding the update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to master@172.16.10.27:45074
03:52:07  I0907 03:52:07.912716  2373 master.cpp:8375] Status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 from agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.912746  2373 master.cpp:8432] Forwarding status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.912868  2373 master.cpp:10932] Updating the state of task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
03:52:07  I0907 03:52:07.913197  2370 scheduler.cpp:845] Enqueuing event UPDATE received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.913413  2370 scheduler.cpp:248] Sending ACKNOWLEDGE call to http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.913676  2376 slave.cpp:5654] Task status update manager successfully handled status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.914064 10119 default_executor.cpp:204] Received ACKNOWLEDGED event
03:52:07  I0907 03:52:07.914105  2376 slave.cpp:5654] Task status update manager successfully handled status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.914460 10120 default_executor.cpp:204] Received ACKNOWLEDGED event
03:52:07  I0907 03:52:07.950346  2371 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
03:52:07  I0907 03:52:07.950747  2371 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41610
03:52:07  I0907 03:52:07.950814  2371 master.cpp:6241] Processing ACKNOWLEDGE call for status 8dc5aae0-605b-42a0-a597-014404ae1bde for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:07  I0907 03:52:07.950927  2371 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41610
03:52:07  I0907 03:52:07.950971  2371 master.cpp:6241] Processing ACKNOWLEDGE call for status 898e5c8a-fb6a-47e6-8732-9c088c8986ea for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:07  I0907 03:52:07.951253  2371 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.951315  2371 task_status_update_manager.cpp:842] Checkpointing ACK for task status update TASK_STARTING (Status UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.951396  2371 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to the agent
03:52:07  I0907 03:52:07.951472  2371 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.951519  2371 task_status_update_manager.cpp:842] Checkpointing ACK for task status update TASK_RUNNING (Status UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.951596  2371 slave.cpp:5761] Forwarding the update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to master@172.16.10.27:45074
03:52:07  I0907 03:52:07.951683  2371 slave.cpp:4505] Task status update manager successfully handled status update acknowledgement (UUID: 8dc5aae0-605b-42a0-a597-014404ae1bde) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.951723  2371 slave.cpp:4505] Task status update manager successfully handled status update acknowledgement (UUID: 898e5c8a-fb6a-47e6-8732-9c088c8986ea) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.951797  2371 master.cpp:8375] Status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 from agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:07  I0907 03:52:07.951818  2371 master.cpp:8432] Forwarding status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:07  I0907 03:52:07.951963  2371 master.cpp:10932] Updating the state of task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
03:52:07  I0907 03:52:07.952499  2372 scheduler.cpp:845] Enqueuing event UPDATE received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.952716  2372 scheduler.cpp:248] Sending ACKNOWLEDGE call to http://172.16.10.27:45074/master/api/v1/scheduler
03:52:07  I0907 03:52:07.953107  2369 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
03:52:08  I0907 03:52:07.990520  2371 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41610
03:52:08  I0907 03:52:07.990584  2371 master.cpp:6241] Processing ACKNOWLEDGE call for status fa82b142-8506-416c-9f9a-5d5a0b0f5906 for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:08  I0907 03:52:07.990727  2374 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:07.990777  2374 task_status_update_manager.cpp:842] Checkpointing ACK for task status update TASK_RUNNING (Status UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:07.990979  2373 slave.cpp:4505] Task status update manager successfully handled status update acknowledgement (UUID: fa82b142-8506-416c-9f9a-5d5a0b0f5906) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.056339  2371 containerizer.cpp:2957] Container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c has exited
03:52:08  I0907 03:52:08.056361  2371 containerizer.cpp:2455] Destroying container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c in RUNNING state
03:52:08  I0907 03:52:08.056371  2371 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c from RUNNING to DESTROYING
03:52:08  I0907 03:52:08.056445  2371 linux_launcher.cpp:580] Asked to destroy container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c
03:52:08  I0907 03:52:08.056494  2371 linux_launcher.cpp:622] Destroying cgroup '/sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/05a1238c-5190-476a-945f-4d8f1225e45c'
03:52:08  I0907 03:52:08.056720  2373 cgroups.cpp:2838] Freezing cgroup /sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/05a1238c-5190-476a-945f-4d8f1225e45c
03:52:08  I0907 03:52:08.056849  2373 cgroups.cpp:1229] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/05a1238c-5190-476a-945f-4d8f1225e45c after 104192ns
03:52:08  I0907 03:52:08.056985  2374 cgroups.cpp:2856] Thawing cgroup /sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/05a1238c-5190-476a-945f-4d8f1225e45c
03:52:08  I0907 03:52:08.057072  2374 cgroups.cpp:1258] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/05a1238c-5190-476a-945f-4d8f1225e45c after 59136ns
03:52:08  I0907 03:52:08.057595  2375 provisioner.cpp:597] Ignoring destroy request for unknown container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c
03:52:08  I0907 03:52:08.057662  2375 containerizer.cpp:2747] Checkpointing termination state to nested container's runtime directory '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c/termination'
03:52:08  I0907 03:52:08.057904  2376 gc.cpp:95] Scheduling '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c' for gc 6.99998775645926days in the future
03:52:08  I0907 03:52:08.058738  2369 process.cpp:3569] Handling HTTP event for process 'slave(90)' with path: '/slave(90)/api/v1/executor'
03:52:08  I0907 03:52:08.060922 10121 default_executor.cpp:955] Child container dbe02af2-3122-4f2e-9747-0c4343627c2f.05a1238c-5190-476a-945f-4d8f1225e45c of task 'c6c81339-65c6-4f86-b0ab-c5be60ea5fbd' completed in state TASK_FINISHED: Command exited with status 0
03:52:08  I0907 03:52:08.098583  2371 http.cpp:1177] HTTP POST for /slave(90)/api/v1/executor from 172.16.10.27:41616
03:52:08  I0907 03:52:08.098664  2371 slave.cpp:5269] Handling status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.099135  2373 task_status_update_manager.cpp:328] Received task status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.099171  2373 task_status_update_manager.cpp:842] Checkpointing UPDATE for task status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.099236  2373 task_status_update_manager.cpp:383] Forwarding task status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to the agent
03:52:08  I0907 03:52:08.099304  2373 slave.cpp:5761] Forwarding the update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to master@172.16.10.27:45074
03:52:08  I0907 03:52:08.099382  2373 slave.cpp:5654] Task status update manager successfully handled status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.099511  2373 master.cpp:8375] Status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 from agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:08  I0907 03:52:08.099537  2373 master.cpp:8432] Forwarding status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.099639  2373 master.cpp:10932] Updating the state of task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
03:52:08  I0907 03:52:08.099834  2373 hierarchical.cpp:1236] Recovered cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: cpus(allocated: *):0.2; mem(allocated: *):64; disk(allocated: *):64) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 from framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.100028 10119 default_executor.cpp:204] Received ACKNOWLEDGED event
03:52:08  I0907 03:52:08.100237  2369 scheduler.cpp:845] Enqueuing event UPDATE received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:08  I0907 03:52:08.100456  2370 scheduler.cpp:248] Sending ACKNOWLEDGE call to http://172.16.10.27:45074/master/api/v1/scheduler
03:52:08  I0907 03:52:08.100777  2369 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
03:52:08  I0907 03:52:08.138344  2370 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41610
03:52:08  I0907 03:52:08.138397  2370 master.cpp:6241] Processing ACKNOWLEDGE call for status 7459155f-590d-4e56-ad85-9cd2c2f4ba40 for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:08  I0907 03:52:08.138432  2370 master.cpp:11030] Removing task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:08  I0907 03:52:08.138602  2369 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.138638  2369 task_status_update_manager.cpp:842] Checkpointing ACK for task status update TASK_FINISHED (Status UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.138676  2369 task_status_update_manager.cpp:538] Cleaning up status update stream for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.138803  2369 slave.cpp:4505] Task status update manager successfully handled status update acknowledgement (UUID: 7459155f-590d-4e56-ad85-9cd2c2f4ba40) for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.138820  2369 slave.cpp:9651] Completing task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd
03:52:08  I0907 03:52:08.138900  2369 gc.cpp:95] Scheduling '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/c6c81339-65c6-4f86-b0ab-c5be60ea5fbd' for gc 6.99998681873778days in the future
03:52:08  I0907 03:52:08.139261  2350 slave.cpp:909] Agent terminating
03:52:08  I0907 03:52:08.139454  2371 master.cpp:1251] Agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) disconnected
03:52:08  I0907 03:52:08.139475  2371 master.cpp:3267] Disconnecting agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:08  I0907 03:52:08.139492  2371 master.cpp:3286] Deactivating agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:08  I0907 03:52:08.139536  2371 hierarchical.cpp:795] Agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 deactivated
03:52:08  W0907 03:52:08.140225  2350 process.cpp:2810] Attempted to spawn already running process files@172.16.10.27:45074
03:52:08  I0907 03:52:08.140589  2350 containerizer.cpp:305] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }
03:52:08  I0907 03:52:08.142592  2350 linux_launcher.cpp:144] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
03:52:08  I0907 03:52:08.143028  2350 provisioner.cpp:298] Using default backend 'overlay'
03:52:08  I0907 03:52:08.143687  2350 cluster.cpp:485] Creating default 'local' authorizer
03:52:08  I0907 03:52:08.144181  2370 slave.cpp:267] Mesos agent started on (91)@172.16.10.27:45074
03:52:08  I0907 03:52:08.144197  2370 slave.cpp:268] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""true"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv"" --zk_session_timeout=""10secs""
03:52:08  I0907 03:52:08.144366  2370 credentials.hpp:86] Loading credential for authentication from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/credential'
03:52:08  I0907 03:52:08.144414  2370 slave.cpp:300] Agent using credential for: test-principal
03:52:08  I0907 03:52:08.144425  2370 credentials.hpp:37] Loading credentials for authentication from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/http_credentials'
03:52:08  I0907 03:52:08.144500  2370 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'
03:52:08  I0907 03:52:08.144552  2370 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'
03:52:08  I0907 03:52:08.144613  2370 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
03:52:08  I0907 03:52:08.144652  2370 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'
03:52:08  I0907 03:52:08.144690  2370 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
03:52:08  I0907 03:52:08.144726  2370 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'
03:52:08  I0907 03:52:08.144798  2370 disk_profile_adaptor.cpp:80] Creating default disk profile adaptor module
03:52:08  I0907 03:52:08.145326  2370 slave.cpp:615] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
03:52:08  I0907 03:52:08.145390  2370 slave.cpp:623] Agent attributes: [  ]
03:52:08  I0907 03:52:08.145401  2370 slave.cpp:632] Agent hostname: ip-172-16-10-27.ec2.internal
03:52:08  I0907 03:52:08.145444  2373 task_status_update_manager.cpp:181] Pausing sending task status updates
03:52:08  I0907 03:52:08.145632  2370 state.cpp:66] Recovering state from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta'
03:52:08  I0907 03:52:08.145664  2370 state.cpp:711] No committed checkpointed resources found at '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/resources/resources.info'
03:52:08  E0907 03:52:08.145715 10118 executor.cpp:714] End-Of-File received from agent. The agent closed the event stream
03:52:08  I0907 03:52:08.147001 10118 default_executor.cpp:176] Disconnected from agent
03:52:08  I0907 03:52:08.147406  2370 slave.cpp:6909] Finished recovering checkpointed state from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta', beginning agent recovery
03:52:08  I0907 03:52:08.147666  2370 slave.cpp:7388] Recovering framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.147833  2370 slave.cpp:9112] Recovering executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.148141  2370 slave.cpp:9651] Completing task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd
03:52:08  I0907 03:52:08.148406  2376 gc.cpp:95] Scheduling '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/c6c81339-65c6-4f86-b0ab-c5be60ea5fbd' for gc 6.99998670912days in the future
03:52:08  I0907 03:52:08.148608  2373 task_status_update_manager.cpp:207] Recovering task status update manager
03:52:08  I0907 03:52:08.148630  2373 task_status_update_manager.cpp:215] Recovering executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.148672  2373 task_status_update_manager.cpp:507] Creating StatusUpdate stream for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.148811  2370 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/latest'
03:52:08  I0907 03:52:08.148908  2373 task_status_update_manager.cpp:818] Replaying task status update stream for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f
03:52:08  I0907 03:52:08.148990  2373 task_status_update_manager.cpp:507] Creating StatusUpdate stream for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.148998  2377 process.cpp:2735] Returning '404 Not Found' for '/slave(90)/api/v1/executor'
03:52:08  I0907 03:52:08.149178  2373 task_status_update_manager.cpp:818] Replaying task status update stream for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd
03:52:08  I0907 03:52:08.149216  2373 task_status_update_manager.cpp:538] Cleaning up status update stream for task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.149478  2370 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/latest'
03:52:08  I0907 03:52:08.149642  2370 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f'
03:52:08  W0907 03:52:08.149492 10114 executor.cpp:666] Received '404 Not Found' () for SUBSCRIBE
03:52:08  I0907 03:52:08.149968  2370 containerizer.cpp:727] Recovering Mesos containers
03:52:08  I0907 03:52:08.150168  2370 containerizer.cpp:784] Recovering container dbe02af2-3122-4f2e-9747-0c4343627c2f for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:08  I0907 03:52:08.154752  2370 gc.cpp:95] Scheduling '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c' for gc 6.99998663851852days in the future
03:52:08  I0907 03:52:08.154945  2370 linux_launcher.cpp:286] Recovering Linux launcher
03:52:08  I0907 03:52:08.155302  2370 linux_launcher.cpp:343] Recovered container dbe02af2-3122-4f2e-9747-0c4343627c2f
03:52:08  I0907 03:52:08.155437  2370 linux_launcher.cpp:331] Not recovering cgroup mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos
03:52:08  I0907 03:52:08.155575  2370 linux_launcher.cpp:343] Recovered container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc
03:52:08  I0907 03:52:08.155746  2370 containerizer.cpp:1053] Recovering isolators
03:52:08  I0907 03:52:08.157411  2370 containerizer.cpp:1092] Recovering provisioner
03:52:08  I0907 03:52:08.162133  2376 provisioner.cpp:494] Provisioner recovery complete
03:52:08  I0907 03:52:08.162586  2371 composing.cpp:339] Finished recovering all containerizers
03:52:08  I0907 03:52:08.162637  2376 slave.cpp:7138] Recovering executors
03:52:08  I0907 03:52:08.162654  2376 slave.cpp:7225] Waiting for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (via HTTP) to subscribe
03:52:08  I0907 03:52:08.702880  2370 hierarchical.cpp:1564] Performed allocation for 1 agents in 31079ns
03:52:08  I0907 03:52:08.859201  2377 process.cpp:2735] Returning '404 Not Found' for '/slave(90)/api/v1/executor'
03:52:08  W0907 03:52:08.859589 10114 executor.cpp:666] Received '404 Not Found' () for SUBSCRIBE
03:52:09  I0907 03:52:09.149554  2377 process.cpp:2735] Returning '404 Not Found' for '/slave(90)/api/v1/executor'
03:52:09  W0907 03:52:09.149917 10114 executor.cpp:666] Received '404 Not Found' () for SUBSCRIBE
03:52:09  I0907 03:52:09.703336  2376 hierarchical.cpp:1564] Performed allocation for 1 agents in 30173ns
03:52:09  I0907 03:52:09.859925  2377 process.cpp:2735] Returning '404 Not Found' for '/slave(90)/api/v1/executor'
03:52:09  W0907 03:52:09.860404 10117 executor.cpp:666] Received '404 Not Found' () for SUBSCRIBE
03:52:10  I0907 03:52:10.150609  2377 process.cpp:2735] Returning '404 Not Found' for '/slave(90)/api/v1/executor'
03:52:10  W0907 03:52:10.151021 10121 executor.cpp:666] Received '404 Not Found' () for SUBSCRIBE
03:52:10  I0907 03:52:10.162950  2373 slave.cpp:5197] Cleaning up un-reregistered executors
03:52:10  I0907 03:52:10.162973  2373 slave.cpp:5215] Killing un-reregistered executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (via HTTP)
03:52:10  I0907 03:52:10.163033  2373 slave.cpp:7291] Finished recovery
03:52:10  I0907 03:52:10.163075  2371 containerizer.cpp:2455] Destroying container dbe02af2-3122-4f2e-9747-0c4343627c2f in RUNNING state
03:52:10  I0907 03:52:10.163096  2371 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f from RUNNING to DESTROYING
03:52:10  I0907 03:52:10.163106  2371 containerizer.cpp:2455] Destroying container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc in RUNNING state
03:52:10  I0907 03:52:10.163115  2371 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc from RUNNING to DESTROYING
03:52:10  I0907 03:52:10.163275  2374 linux_launcher.cpp:580] Asked to destroy container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc
03:52:10  I0907 03:52:10.163326  2374 linux_launcher.cpp:622] Destroying cgroup '/sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/dbc72eae-9465-4bf7-a082-0bf8a055fecc'
03:52:10  I0907 03:52:10.163509  2374 cgroups.cpp:2838] Freezing cgroup /sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/dbc72eae-9465-4bf7-a082-0bf8a055fecc
03:52:10  I0907 03:52:10.164005  2373 slave.cpp:1254] New master detected at master@172.16.10.27:45074
03:52:10  I0907 03:52:10.164031  2373 slave.cpp:1319] Detecting new master
03:52:10  I0907 03:52:10.164106  2371 task_status_update_manager.cpp:181] Pausing sending task status updates
03:52:10  I0907 03:52:10.165093  2372 slave.cpp:1346] Authenticating with master master@172.16.10.27:45074
03:52:10  I0907 03:52:10.165128  2372 slave.cpp:1355] Using default CRAM-MD5 authenticatee
03:52:10  I0907 03:52:10.165200  2376 authenticatee.cpp:121] Creating new client SASL connection
03:52:10  I0907 03:52:10.165675  2376 master.cpp:9653] Authenticating slave(91)@172.16.10.27:45074
03:52:10  I0907 03:52:10.165731  2376 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(201)@172.16.10.27:45074
03:52:10  I0907 03:52:10.165791  2376 authenticator.cpp:98] Creating new server SASL connection
03:52:10  I0907 03:52:10.166316  2376 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
03:52:10  I0907 03:52:10.166338  2376 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
03:52:10  I0907 03:52:10.166371  2376 authenticator.cpp:204] Received SASL authentication start
03:52:10  I0907 03:52:10.166407  2376 authenticator.cpp:326] Authentication requires more steps
03:52:10  I0907 03:52:10.166440  2376 authenticatee.cpp:259] Received SASL authentication step
03:52:10  I0907 03:52:10.166481  2376 authenticator.cpp:232] Received SASL authentication step
03:52:10  I0907 03:52:10.166496  2376 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-27.ec2.internal' server FQDN: 'ip-172-16-10-27.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
03:52:10  I0907 03:52:10.166505  2376 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
03:52:10  I0907 03:52:10.166515  2376 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
03:52:10  I0907 03:52:10.166524  2376 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-27.ec2.internal' server FQDN: 'ip-172-16-10-27.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
03:52:10  I0907 03:52:10.166532  2376 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
03:52:10  I0907 03:52:10.166538  2376 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
03:52:10  I0907 03:52:10.166549  2376 authenticator.cpp:318] Authentication success
03:52:10  I0907 03:52:10.166592  2376 authenticatee.cpp:299] Authentication success
03:52:10  I0907 03:52:10.166638  2376 master.cpp:9685] Successfully authenticated principal 'test-principal' at slave(91)@172.16.10.27:45074
03:52:10  I0907 03:52:10.166651  2371 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(201)@172.16.10.27:45074
03:52:10  I0907 03:52:10.166708  2376 slave.cpp:1446] Successfully authenticated with master master@172.16.10.27:45074
03:52:10  I0907 03:52:10.166925  2376 slave.cpp:1877] Will retry registration in 4.802199ms if necessary
03:52:10  I0907 03:52:10.167089  2371 master.cpp:6959] Received reregister agent message from agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:10  I0907 03:52:10.167194  2371 master.cpp:3964] Authorizing agent providing resources 'cpus:2; mem:1024; disk:1024; ports:[31000-32000]' with principal 'test-principal'
03:52:10  I0907 03:52:10.167399  2372 master.cpp:7051] Authorized re-registration of agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:10  I0907 03:52:10.167444  2372 master.cpp:7135] Agent is already marked as registered: 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:10  I0907 03:52:10.167487  2372 master.cpp:7503] Registry updated for slave 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074(ip-172-16-10-27.ec2.internal)
03:52:10  I0907 03:52:10.167649  2376 hierarchical.cpp:697] Agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 (ip-172-16-10-27.ec2.internal) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
03:52:10  I0907 03:52:10.167691  2376 hierarchical.cpp:783] Agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 reactivated
03:52:10  I0907 03:52:10.167817  2372 slave.cpp:1585] Re-registered with master master@172.16.10.27:45074
03:52:10  I0907 03:52:10.168326  2371 hierarchical.cpp:1564] Performed allocation for 1 agents in 170435ns
03:52:10  I0907 03:52:10.170323  2376 task_status_update_manager.cpp:188] Resuming sending task status updates
03:52:10  I0907 03:52:10.170390  2372 slave.cpp:1630] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""j4ZjLDA/StuRQkYO2dDq+A==""},""slave_id"":{""value"":""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0""},""update_oversubscribed_resources"":false}
03:52:10  I0907 03:52:10.170478  2372 slave.cpp:4067] Updating info for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:10  I0907 03:52:10.170260  2369 scheduler.cpp:845] Enqueuing event HEARTBEAT received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:10  I0907 03:52:10.168475  2370 cgroups.cpp:2856] Thawing cgroup /sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/dbc72eae-9465-4bf7-a082-0bf8a055fecc
03:52:10  I0907 03:52:10.168576  2375 master.cpp:1827] Skipping periodic registry garbage collection: no agents qualify for removal
03:52:10  I0907 03:52:10.170827  2369 cgroups.cpp:1258] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/dbe02af2-3122-4f2e-9747-0c4343627c2f/mesos/dbc72eae-9465-4bf7-a082-0bf8a055fecc after 0ns
03:52:10  I0907 03:52:10.170676  2372 slave.cpp:8908] Checkpointing FrameworkInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/framework.info'
03:52:10  I0907 03:52:10.173640  2375 master.cpp:9468] Sending offers [ 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O1 ] to framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:10  I0907 03:52:10.169373  2373 gc.cpp:272] Deleting /tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/c6c81339-65c6-4f86-b0ab-c5be60ea5fbd
03:52:10  I0907 03:52:10.174196  2369 scheduler.cpp:845] Enqueuing event OFFERS received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:10  I0907 03:52:10.174222  2375 master.cpp:7939] Ignoring update on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) as it reports no changes
03:52:10  I0907 03:52:10.269464  2372 slave.cpp:8919] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/framework.pid'
03:52:10  E0907 03:52:10.269704  2372 slave.cpp:6267] Termination of executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 failed: Failed to destroy nested containers: Failed to kill all processes in the container: Timed out after 1mins
03:52:10  I0907 03:52:10.269757  2372 slave.cpp:5269] Handling status update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 from @0.0.0.0:0
03:52:10  I0907 03:52:10.269884  2372 slave.cpp:6824] Current disk usage 44.82%. Max allowed age: 3.162545109095440days
03:52:10  I0907 03:52:10.269927  2372 task_status_update_manager.cpp:188] Resuming sending task status updates
03:52:10  I0907 03:52:10.270087  2369 gc.cpp:331] Pruning directories with remaining removal time 0ns
03:52:10  I0907 03:52:10.270102  2369 gc.cpp:331] Pruning directories with remaining removal time 0ns
03:52:10  I0907 03:52:10.270117  2369 gc.cpp:188] Skipping deletion of '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/c6c81339-65c6-4f86-b0ab-c5be60ea5fbd'  as it is already in progress
03:52:10  E0907 03:52:10.270146  2369 slave.cpp:5600] Failed to update resources for container dbe02af2-3122-4f2e-9747-0c4343627c2f of executor 'default' running task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f on status update for terminal task, destroying container: Container not found
03:52:10  W0907 03:52:10.270192  2369 composing.cpp:609] Attempted to destroy unknown container dbe02af2-3122-4f2e-9747-0c4343627c2f
03:52:10  I0907 03:52:10.270216  2369 task_status_update_manager.cpp:328] Received task status update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:10  I0907 03:52:10.270236  2369 task_status_update_manager.cpp:842] Checkpointing UPDATE for task status update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:10  I0907 03:52:10.270299  2369 task_status_update_manager.cpp:383] Forwarding task status update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to the agent
03:52:10  I0907 03:52:10.270349  2369 slave.cpp:5761] Forwarding the update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 to master@172.16.10.27:45074
03:52:10  I0907 03:52:10.270387  2369 slave.cpp:5654] Task status update manager successfully handled status update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:10  I0907 03:52:10.270020  2372 master.cpp:8618] Executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal): wait status -1
03:52:10  I0907 03:52:10.270433  2372 master.cpp:11061] Removing executor 'default' with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":32.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":32.0},""type"":""SCALAR""}] of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:10  I0907 03:52:10.270642  2372 master.cpp:8375] Status update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 from agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:10  I0907 03:52:10.270660  2372 master.cpp:8432] Forwarding status update TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) for task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:10  I0907 03:52:10.270738  2372 master.cpp:10932] Updating the state of task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (latest state: TASK_LOST, status update state: TASK_LOST)
03:52:10  I0907 03:52:10.270931  2372 hierarchical.cpp:1236] Recovered cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: cpus(allocated: *):1.9; mem(allocated: *):992; disk(allocated: *):992; ports(allocated: *):[31000-32000]) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 from framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:10  I0907 03:52:10.271028  2372 hierarchical.cpp:1236] Recovered cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: cpus(allocated: *):1.8; mem(allocated: *):960; disk(allocated: *):960; ports(allocated: *):[31000-32000]) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 from framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:10  I0907 03:52:10.271473  2371 scheduler.cpp:845] Enqueuing event FAILURE received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:10  
03:52:10  GMOCK WARNING:
03:52:10  Uninteresting mock function call - returning directly.
03:52:10      Function call: failure(0x7ffcee44e920, @0x7ff40001de70 48-byte object <88-43 53-2C F4-7F 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 70-0A 00-00 F4-7F 00-00 30-FF 00-00 F4-7F 00-00 FF-FF FF-FF F4-7F 00-00>)
03:52:10  NOTE: You can safely ignore the above warning unless this call should not happen.  Do not suppress it by blindly adding an EXPECT_CALL() if you don't mean to enforce the call.  See https://github.com/google/googletest/blob/master/googlemock/docs/CookBook.md#knowing-when-to-expect for details.
03:52:10  I0907 03:52:10.271648  2371 scheduler.cpp:845] Enqueuing event UPDATE received from http://172.16.10.27:45074/master/api/v1/scheduler
03:52:10  unknown file: Failure
03:52:10  
03:52:10  Unexpected mock function call - returning directly.
03:52:10      Function call: update(0x7ffcee44e920, @0x7ff4000015a0 TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) Source: SOURCE_AGENT Reason: REASON_EXECUTOR_REREGISTRATION_TIMEOUT Message: 'Executor did not reregister within 2secs; Abnormal executor termination: Failed to destroy nested containers: Failed to kill all processes in the container: Timed out after 1mins' for task '2e8c13b6-fa45-4e3c-89cd-398a5abc192f' on agent: 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0)
03:52:10  Google Mock tried the following 5 expectations, but none matched:
03:52:10  
03:52:10  ../../src/tests/gc_tests.cpp:986: tried expectation #0: EXPECT_CALL(*scheduler, update(_, AllOf( TaskStatusUpdateTaskIdEq(longLivedTaskInfo), TaskStatusUpdateStateEq(v1::TASK_STARTING))))...
03:52:10           Expected: the expectation is active
03:52:10             Actual: it is retired
03:52:10           Expected: to be called once
03:52:10             Actual: called once - saturated and retired
03:52:10  ../../src/tests/gc_tests.cpp:996: tried expectation #1: EXPECT_CALL(*scheduler, update(_, AllOf( TaskStatusUpdateTaskIdEq(longLivedTaskInfo), TaskStatusUpdateStateEq(v1::TASK_RUNNING))))...
03:52:10    Expected arg #1: (task status update task id eq name: ""test-task""
03:52:10  task_id {
03:52:10    value: ""2e8c13b6-fa45-4e3c-89cd-398a5abc192f""
03:52:10  }
03:52:10  agent_id {
03:52:10    value: ""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0""
03:52:10  }
03:52:10  resources {
03:52:10    name: ""cpus""
03:52:10    type: SCALAR
03:52:10    scalar {
03:52:10      value: 0.1
03:52:10    }
03:52:10  }
03:52:10  resources {
03:52:10    name: ""mem""
03:52:10    type: SCALAR
03:52:10    scalar {
03:52:10      value: 32
03:52:10    }
03:52:10  }
03:52:10  resources {
03:52:10    name: ""disk""
03:52:10    type: SCALAR
03:52:10    scalar {
03:52:10      value: 32
03:52:10    }
03:52:10  }
03:52:10  command {
03:52:10    value: ""sleep 1000""
03:52:10  }
03:52:10  ) and (task status update state eq TASK_RUNNING)
03:52:10             Actual: TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) Source: SOURCE_AGENT Reason: REASON_EXECUTOR_REREGISTRATION_TIMEOUT Message: 'Executor did not reregister within 2secs; Abnormal executor termination: Failed to destroy nested containers: Failed to kill all processes in the container: Timed out after 1mins' for task '2e8c13b6-fa45-4e3c-89cd-398a5abc192f' on agent: 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:10           Expected: to be called once
03:52:10             Actual: called once - saturated and active
03:52:10  ../../src/tests/gc_tests.cpp:1011: tried expectation #2: EXPECT_CALL(*scheduler, update(_, AllOf( TaskStatusUpdateTaskIdEq(shortLivedTaskInfo), TaskStatusUpdateStateEq(v1::TASK_STARTING))))...
03:52:10           Expected: the expectation is active
03:52:10             Actual: it is retired
03:52:10           Expected: to be called once
03:52:10             Actual: called once - saturated and retired
03:52:10  ../../src/tests/gc_tests.cpp:1021: tried expectation #3: EXPECT_CALL(*scheduler, update(_, AllOf( TaskStatusUpdateTaskIdEq(shortLivedTaskInfo), TaskStatusUpdateStateEq(v1::TASK_RUNNING))))...
03:52:10           Expected: the expectation is active
03:52:10             Actual: it is retired
03:52:10           Expected: to be called once
03:52:10             Actual: called once - saturated and retired
03:52:10  ../../src/tests/gc_tests.cpp:1031: tried expectation #4: EXPECT_CALL(*scheduler, update(_, AllOf( TaskStatusUpdateTaskIdEq(shortLivedTaskInfo), TaskStatusUpdateStateEq(v1::TASK_FINISHED))))...
03:52:10    Expected arg #1: (task status update task id eq name: ""test-task""
03:52:10  task_id {
03:52:10    value: ""c6c81339-65c6-4f86-b0ab-c5be60ea5fbd""
03:52:10  }
03:52:10  agent_id {
03:52:10    value: ""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0""
03:52:10  }
03:52:10  resources {
03:52:10    name: ""cpus""
03:52:10    type: SCALAR
03:52:10    scalar {
03:52:10      value: 0.1
03:52:10    }
03:52:10  }
03:52:10  resources {
03:52:10    name: ""mem""
03:52:10    type: SCALAR
03:52:10    scalar {
03:52:10      value: 32
03:52:10    }
03:52:10  }
03:52:10  resources {
03:52:10    name: ""disk""
03:52:10    type: SCALAR
03:52:10    scalar {
03:52:10      value: 32
03:52:10    }
03:52:10  }
03:52:10  command {
03:52:10    value: ""exit 0""
03:52:10  }
03:52:10  ) and (task status update state eq TASK_FINISHED)
03:52:10             Actual: TASK_LOST (Status UUID: f5a0fab7-ad86-4d4d-94a7-8894f15521fd) Source: SOURCE_AGENT Reason: REASON_EXECUTOR_REREGISTRATION_TIMEOUT Message: 'Executor did not reregister within 2secs; Abnormal executor termination: Failed to destroy nested containers: Failed to kill all processes in the container: Timed out after 1mins' for task '2e8c13b6-fa45-4e3c-89cd-398a5abc192f' on agent: 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:10           Expected: to be called once
03:52:10             Actual: called once - saturated and active
03:52:10  I0907 03:52:10.327615  2373 gc.cpp:288] Deleted '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/c6c81339-65c6-4f86-b0ab-c5be60ea5fbd'
03:52:10  I0907 03:52:10.327862  2370 gc.cpp:188] Skipping deletion of '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c'  as it is already in progress
03:52:10  I0907 03:52:10.328534  2369 gc.cpp:188] Skipping deletion of '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c'  as it is already in progress
03:52:10  I0907 03:52:10.329061  2373 gc.cpp:272] Deleting /tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c
03:52:10  I0907 03:52:10.329295  2373 gc.cpp:288] Deleted '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/containers/05a1238c-5190-476a-945f-4d8f1225e45c'
03:52:10  I0907 03:52:10.860333  2377 process.cpp:2735] Returning '404 Not Found' for '/slave(90)/api/v1/executor'
03:52:10  W0907 03:52:10.860883 10114 executor.cpp:666] Received '404 Not Found' () for SUBSCRIBE
03:52:11  I0907 03:52:11.151208  2377 process.cpp:2735] Returning '404 Not Found' for '/slave(90)/api/v1/executor'
03:52:11  W0907 03:52:11.151674 10114 executor.cpp:666] Received '404 Not Found' () for SUBSCRIBE
03:52:11  I0907 03:52:11.213229  2371 hierarchical.cpp:1564] Performed allocation for 1 agents in 188978ns
03:52:11  I0907 03:52:11.213379  2371 containerizer.cpp:2957] Container dbe02af2-3122-4f2e-9747-0c4343627c2f.dbc72eae-9465-4bf7-a082-0bf8a055fecc has exited
03:52:11  I0907 03:52:11.213652  2371 master.cpp:9468] Sending offers [ 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O2 ] to framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:11  I0907 03:52:11.214164  2350 slave.cpp:909] Agent terminating
03:52:11  I0907 03:52:11.214273  2369 master.cpp:1366] Framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) disconnected
03:52:11  I0907 03:52:11.214293  2369 master.cpp:3230] Deactivating framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:11  W0907 03:52:11.214360  2369 master.hpp:2605] Unable to send event to framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default): connection closed
03:52:11  I0907 03:52:11.214375  2369 master.cpp:11462] Removing offer 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O2
03:52:11  W0907 03:52:11.214435  2369 master.hpp:2605] Unable to send event to framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default): connection closed
03:52:11  I0907 03:52:11.214445  2369 master.cpp:11462] Removing offer 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O1
03:52:11  I0907 03:52:11.214455  2369 master.cpp:3207] Disconnecting framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:11  I0907 03:52:11.214462  2369 master.cpp:1381] Giving framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) 0ns to failover
03:52:11  I0907 03:52:11.214606  2370 hierarchical.cpp:420] Deactivated framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:11  I0907 03:52:11.214704  2370 hierarchical.cpp:1236] Recovered cpus(allocated: *):0.2; mem(allocated: *):64; disk(allocated: *):64 (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: cpus(allocated: *):1.8; mem(allocated: *):960; disk(allocated: *):960; ports(allocated: *):[31000-32000]) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 from framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:11  I0907 03:52:11.214819  2370 hierarchical.cpp:1236] Recovered cpus(allocated: *):1.8; mem(allocated: *):960; disk(allocated: *):960; ports(allocated: *):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: {}) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 from framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:11  I0907 03:52:11.214895  2369 master.cpp:9261] Framework failover timeout, removing framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:11  I0907 03:52:11.214910  2369 master.cpp:10197] Removing framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)
03:52:11  I0907 03:52:11.214947  2369 master.cpp:10932] Updating the state of task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (latest state: TASK_LOST, status update state: TASK_KILLED)
03:52:11  I0907 03:52:11.214964  2369 master.cpp:11030] Removing task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(91)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)
03:52:11  I0907 03:52:11.215090  2369 hierarchical.cpp:359] Removed framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000
03:52:11  I0907 03:52:11.217375  2350 master.cpp:1093] Master terminating
03:52:11  I0907 03:52:11.217579  2375 hierarchical.cpp:637] Removed agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0
03:52:11  [  FAILED  ] GarbageCollectorIntegrationTest.LongLivedDefaultExecutorRestart (3519 ms)
{noformat}"	MESOS	Resolved	3	1	6057	flaky, flaky-test, test
12939312	Linux filesystem isolator tests are flaky.	"LinuxFilesystemIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem sometimes fails on CentOS 7 with this kind of output:
{noformat}
../../src/tests/containerizer/filesystem_isolator_tests.cpp:1054: Failure
Failed to wait 2mins for launch
{noformat}

LinuxFilesystemIsolatorTest.ROOT_MultipleContainers often has this output:
{noformat}
../../src/tests/containerizer/filesystem_isolator_tests.cpp:1138: Failure
Failed to wait 1mins for launch1
{noformat}

Whether SSL is configured makes no difference.

This test may also fail on other platforms, but more rarely.

"	MESOS	Resolved	3	1	6057	mesosphere
12982185	SSL downgrade support will leak sockets in CLOSE_WAIT status	"Repro steps:
1) Start a master:
{code}
bin/mesos-master.sh --work_dir=/tmp/master
{code}

2) Start an agent with SSL and downgrade enabled:
{code}
# Taken from http://mesos.apache.org/documentation/latest/ssl/
openssl genrsa -des3 -f4 -passout pass:some_password -out key.pem 4096
openssl req -new -x509 -passin pass:some_password -days 365 -key key.pem -out cert.pem

SSL_KEY_FILE=key.pem SSL_CERT_FILE=cert.pem SSL_ENABLED=true SSL_SUPPORT_DOWNGRADE=true sudo -E bin/mesos-agent.sh --master=localhost:5050 --work_dir=/tmp/agent
{code}

3) Start a framework that launches lots of executors, one after another:
{code}
sudo src/balloon-framework --master=localhost:5050 --task_memory=64mb --task_memory_usage_limit=256mb --long_running
{code}

4) Check FDs, repeatedly
{code}
sudo lsof -i | grep mesos | grep CLOSE_WAIT | wc -l
{code}

The number of sockets in {{CLOSE_WAIT}} will increase linearly with the number of launched executors."	MESOS	Resolved	1	1	6057	libprocess, mesosphere
13187473	Prevent subscribers to the master's event stream from leaking connections	"Some reverse proxies (e.g., ELB using an HTTP listener) won't close the upstream connection to Mesos when they detect that their client is disconnected.

This can make Mesos leak subscribers, which generates unnecessary authorization requests and affects performance.

We should evaluate methods (e.g., heartbeats) to enable Mesos to detect that a subscriber is gone, even if the TCP connection is still open.
"	MESOS	Resolved	2	4	6057	foundations, mesosphere
