{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c45b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52bf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000  # The maximum number of words to keep in the vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 250 # The fixed length for all sequences\n",
    "from parameters import trainable_data_folder, embeddings_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d2c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nStarting to process files for Neural Network input...\n",
      "\\nProcessing file: 3.2_AMBARI_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_AMBARI_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2000 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for AMBARI.\n",
      "\\nProcessing file: 3.2_ARROW_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_ARROW_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 1874 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for ARROW.\n",
      "\\nProcessing file: 3.2_CASSANDRA_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_CASSANDRA_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 2359 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for CASSANDRA.\n",
      "\\nProcessing file: 3.2_CB_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_CB_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2625 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for CB.\n",
      "\\nProcessing file: 3.2_DATALAB_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_DATALAB_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 992 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for DATALAB.\n",
      "\\nProcessing file: 3.2_FLINK_10_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_FLINK_10_assignees_features\n",
      "  Data split. Training set: 640, Test set: 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 3179 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for FLINK.\n",
      "\\nProcessing file: 3.2_FLINK_15_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_FLINK_15_assignees_features\n",
      "  Data split. Training set: 960, Test set: 240\n",
      "  Found 4283 unique tokens.\n",
      "  Sequences padded to length 250.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved processed data for FLINK.\n",
      "\\nProcessing file: 3.2_FLINK_20_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_FLINK_20_assignees_features\n",
      "  Data split. Training set: 1280, Test set: 320\n",
      "  Found 5081 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for FLINK.\n",
      "\\nProcessing file: 3.2_FLINK_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_FLINK_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 1856 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for FLINK.\n",
      "\\nProcessing file: 3.2_GEODE_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_GEODE_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2191 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for GEODE.\n",
      "\\nProcessing file: 3.2_HDDS_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_HDDS_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 2194 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for HDDS.\n",
      "\\nProcessing file: 3.2_HIVE_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_HIVE_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2406 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for HIVE.\n",
      "\\nProcessing file: 3.2_HUDI_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_HUDI_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2075 unique tokens.\n",
      "  Sequences padded to length 250.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved processed data for HUDI.\n",
      "\\nProcessing file: 3.2_IGNITE_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_IGNITE_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2376 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for IGNITE.\n",
      "\\nProcessing file: 3.2_IMPALA_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_IMPALA_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2414 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for IMPALA.\n",
      "\\nProcessing file: 3.2_IOTDB_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_IOTDB_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 1538 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for IOTDB.\n",
      "\\nProcessing file: 3.2_MESOS_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_MESOS_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2624 unique tokens.\n",
      "  Sequences padded to length 250.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully saved processed data for MESOS.\n",
      "\\nProcessing file: 3.2_OAK_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_OAK_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 2145 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for OAK.\n",
      "\\nProcessing file: 3.2_SPARK_5_assignees_processed.csv\n",
      "  Output directory: C:\\Users\\hp\\Desktop\\Module-3-Task-assigning\\data\\embeddings\\4_NN_SPARK_5_assignees_features\n",
      "  Data split. Training set: 320, Test set: 80\n",
      "  Found 1824 unique tokens.\n",
      "  Sequences padded to length 250.\n",
      "  Successfully saved processed data for SPARK.\n",
      "\\nScript finished processing all files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_5568\\1528373067.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\nStarting to process files for Neural Network input...\")\n",
    "\n",
    "for f in os.listdir(trainable_data_folder):\n",
    "    if f.startswith(\"3.2_\") and f.endswith(\"_processed.csv\"):\n",
    "        print(f\"\\\\nProcessing file: {f}\")\n",
    "\n",
    "        try:\n",
    "            # --- 1. Load Data ---\n",
    "            input_file_path = os.path.join(trainable_data_folder, f)\n",
    "            df = pd.read_csv(input_file_path, sep='\\\\t', encoding='utf-8')\n",
    "\n",
    "            # Extract project and assignee info from filename\n",
    "            parts = f.replace(\".csv\", \"\").split(\"_\")\n",
    "            project_name = parts[1]\n",
    "            num_assignees_config = int(parts[2])\n",
    "\n",
    "            # --- Create Output Directory ---\n",
    "            output_dir_name = f\"4_NN_{project_name}_{num_assignees_config}_assignees_features\"\n",
    "            full_output_path = os.path.join(embeddings_folder, output_dir_name)\n",
    "            os.makedirs(full_output_path, exist_ok=True)\n",
    "            print(f\"  Output directory: {full_output_path}\")\n",
    "\n",
    "            # --- 2. Combine Text and Prepare Data ---\n",
    "            df['text'] = df['processed_title'].fillna('') + ' ' + df['processed_description'].fillna('')\n",
    "            \n",
    "            X = df['text'].values\n",
    "            y = df['assignee_id'].values\n",
    "\n",
    "            # --- 3. Train-Test Split ---\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            print(f\"  Data split. Training set: {len(X_train)}, Test set: {len(X_test)}\")\n",
    "\n",
    "            # --- 4. Tokenization ---\n",
    "            tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "            tokenizer.fit_on_texts(X_train)\n",
    "            \n",
    "            X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "            X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "            \n",
    "            word_index = tokenizer.word_index\n",
    "            print(f\"  Found {len(word_index)} unique tokens.\")\n",
    "\n",
    "            # --- 5. Padding ---\n",
    "            X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "            X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "            print(f\"  Sequences padded to length {MAX_SEQUENCE_LENGTH}.\")\n",
    "\n",
    "            # --- 6. Save Processed Data as CSV ---\n",
    "            # Convert arrays to DataFrames and save as CSV. No headers or index for feature matrices.\n",
    "            pd.DataFrame(X_train_padded).to_csv(os.path.join(full_output_path, 'X_train.csv'), index=False, header=False)\n",
    "            pd.DataFrame(X_test_padded).to_csv(os.path.join(full_output_path, 'X_test.csv'), index=False, header=False)\n",
    "            \n",
    "            # Save labels with a header for clarity\n",
    "            pd.DataFrame(y_train, columns=['assignee_id']).to_csv(os.path.join(full_output_path, 'y_train.csv'), index=False)\n",
    "            pd.DataFrame(y_test, columns=['assignee_id']).to_csv(os.path.join(full_output_path, 'y_test.csv'), index=False)\n",
    "            \n",
    "            # The word_index is a dictionary and is best saved as JSON.\n",
    "            with open(os.path.join(full_output_path, 'word_index.json'), 'w') as f:\n",
    "                json.dump(word_index, f)\n",
    "\n",
    "            print(f\"  Successfully saved processed data for {project_name}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Failed to process {f}: {e}\")\n",
    "\n",
    "print(\"\\\\nScript finished processing all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf34a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
